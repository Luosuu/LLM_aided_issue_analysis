{
  "issue_number": 13554.0,
  "title": "[BUG] Redis cluster not recovering previously persisted data after host machine restart",
  "body": "_Redis Version: v7.0.12_\r\n\r\nHello.\r\n\r\nI have deployed a Redis Cluster in my Kubernetes Cluster using `ot-helm/redis-operator` with the following values:\r\n\r\n```yaml\r\nredisCluster:\r\n  redisSecret:\r\n    secretName: redis-password\r\n    secretKey: REDIS_PASSWORD\r\n  leader:\r\n    replicas: 3\r\n    affinity:\r\n        nodeAffinity:\r\n          requiredDuringSchedulingIgnoredDuringExecution:\r\n            nodeSelectorTerms:\r\n              - matchExpressions:\r\n                  - key: test\r\n                    operator: In\r\n                    values:\r\n                      - \"true\"\r\n  follower:\r\n    replicas: 3\r\n    affinity:\r\n        nodeAffinity:\r\n          requiredDuringSchedulingIgnoredDuringExecution:\r\n            nodeSelectorTerms:\r\n              - matchExpressions:\r\n                  - key: test\r\n                    operator: In\r\n                    values:\r\n                      - \"true\"\r\nexternalService:\r\n  enabled: true\r\n  serviceType: LoadBalancer\r\n  port: 6379\r\nredisExporter:\r\n  enabled: true\r\nstorageSpec:\r\n  volumeClaimTemplate:\r\n    spec:\r\n      resources:\r\n        requests:\r\n          storage: 10Gi\r\n  nodeConfVolumeClaimTemplate:\r\n    spec:\r\n      resources:\r\n        requests:\r\n          storage: 1Gi\r\n```\r\n\r\nAfter adding a couple of keys to the cluster, I stop the host machine (EC2 instance) where the Redis Cluster is deployed, and start it again. Upon the restart of the EC2 instance, and the Redis Cluster, the couple of keys that I have added before the restart disappear.\r\n\r\nI have [both](https://redis.io/docs/latest/operate/oss_and_stack/management/persistence/) persistence methods enabled (RDB & AOF), and this is my configuration (default) for Redis Cluster regarding persistency:\r\n\r\n```\r\nconfig get dir # /data\r\nconfig get dbfilename # dump.rdb\r\nconfig get appendonly # yes\r\nconfig get appendfilename # appendonly.aof\r\n```\r\n\r\nI have noticed that during/after the addition of the keys/data in Redis, `/data/dump.rdb`, and `/data/appendonlydir/appendonly.aof.1.incr.aof` (within my main Redis Cluster leader) increase in size, but when I restart the EC2 instance, `/data/dump.rdb` get back to 0 bytes, while `/data/appendonlydir/appendonly.aof.1.incr.aof` stays at the same size that was before the restart.\r\n\r\nI can confirm this with [this](https://imgur.com/a/FTt8Rc6) screenshot from my Grafana dashboard while monitoring the persistent volume that was attached to main leader of the Redis Cluster. From what I understood, the volume contains both AOF, and RDB data until few seconds after the restart of Redis Cluster, where RDB data is deleted.\r\n\r\nThis is the Prometheus metric I am using in case anyone is wondering:\r\n```\r\nsum(kubelet_volume_stats_used_bytes{namespace=\"test\", persistentvolumeclaim=\"redis-cluster-leader-redis-cluster-leader-0\"}/(1024*1024)) by (persistentvolumeclaim)\r\n```\r\n\r\nSo, Redis Cluster is actually backing up the data using RDB, and AOF, but as soon as it is restarted (after the EC2 restart), it loses RDB data, and AOF is not enough to retrieve the keys/data for some reason.\r\n\r\nHere are the logs of Redis Cluster when it is restarted:\r\n\r\n```\r\nACL_MODE is not true, skipping ACL file modification\r\nStarting redis service in cluster mode.....\r\n12:C 17 Sep 2024 00:49:39.351 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\r\n12:C 17 Sep 2024 00:49:39.351 # Redis version=7.0.12, bits=64, commit=00000000, modified=0, pid=12, just started\r\n12:C 17 Sep 2024 00:49:39.351 # Configuration loaded\r\n12:M 17 Sep 2024 00:49:39.352 * monotonic clock: POSIX clock_gettime\r\n12:M 17 Sep 2024 00:49:39.353 * Node configuration loaded, I'm ef200bc9befd1c4fb0f6e5acbb1432002a7c2822\r\n12:M 17 Sep 2024 00:49:39.353 * Running mode=cluster, port=6379.\r\n12:M 17 Sep 2024 00:49:39.353 # Server initialized\r\n12:M 17 Sep 2024 00:49:39.355 * Reading RDB base file on AOF loading...\r\n12:M 17 Sep 2024 00:49:39.355 * Loading RDB produced by version 7.0.12\r\n12:M 17 Sep 2024 00:49:39.355 * RDB age 2469 seconds\r\n12:M 17 Sep 2024 00:49:39.355 * RDB memory usage when created 1.51 Mb\r\n12:M 17 Sep 2024 00:49:39.355 * RDB is base AOF\r\n12:M 17 Sep 2024 00:49:39.355 * Done loading RDB, keys loaded: 0, keys expired: 0.\r\n12:M 17 Sep 2024 00:49:39.355 * DB loaded from base file appendonly.aof.1.base.rdb: 0.001 seconds\r\n12:M 17 Sep 2024 00:49:39.598 * DB loaded from incr file appendonly.aof.1.incr.aof: 0.243 seconds\r\n12:M 17 Sep 2024 00:49:39.598 * DB loaded from append only file: 0.244 seconds\r\n12:M 17 Sep 2024 00:49:39.598 * Opening AOF incr file appendonly.aof.1.incr.aof on server start\r\n12:M 17 Sep 2024 00:49:39.599 * Ready to accept connections\r\n12:M 17 Sep 2024 00:49:41.611 # Cluster state changed: ok\r\n12:M 17 Sep 2024 00:49:46.592 # Cluster state changed: fail\r\n12:M 17 Sep 2024 00:50:02.258 * DB saved on disk\r\n12:M 17 Sep 2024 00:50:21.376 # Cluster state changed: ok\r\n12:M 17 Sep 2024 00:51:26.284 * Replica 192.168.58.43:6379 asks for synchronization\r\n12:M 17 Sep 2024 00:51:26.284 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for '995d7ac6eedc09d95c4fc184519686e9dc8f9b41', my replication IDs are '654e768d51433cc24667323f8f884c66e8e55566' and '0000000000000000000000000000000000000000')\r\n12:M 17 Sep 2024 00:51:26.284 * Replication backlog created, my new replication IDs are 'de979d9aa433bf37f413a64aff751ed677794b00' and '0000000000000000000000000000000000000000'\r\n12:M 17 Sep 2024 00:51:26.284 * Delay next BGSAVE for diskless SYNC\r\n12:M 17 Sep 2024 00:51:31.195 * Starting BGSAVE for SYNC with target: replicas sockets\r\n12:M 17 Sep 2024 00:51:31.195 * Background RDB transfer started by pid 218\r\n218:C 17 Sep 2024 00:51:31.196 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB\r\n12:M 17 Sep 2024 00:51:31.196 # Diskless rdb transfer, done reading from pipe, 1 replicas still up.\r\n12:M 17 Sep 2024 00:51:31.202 * Background RDB transfer terminated with success\r\n12:M 17 Sep 2024 00:51:31.202 * Streamed RDB transfer with replica 192.168.58.43:6379 succeeded (socket). Waiting for REPLCONF ACK from slave to enable streaming\r\n12:M 17 Sep 2024 00:51:31.203 * Synchronization with replica 192.168.58.43:6379 succeeded\r\n```\r\nHere is the output of `INFO PERSISTENCE` redis-cli command, after the addition of some data:\r\n\r\n```\r\n# Persistence\r\nloading:0\r\nasync_loading:0\r\ncurrent_cow_peak:0\r\ncurrent_cow_size:0\r\ncurrent_cow_size_age:0\r\ncurrent_fork_perc:0.00\r\ncurrent_save_keys_processed:0\r\ncurrent_save_keys_total:0\r\nrdb_changes_since_last_save:0\r\nrdb_bgsave_in_progress:0\r\nrdb_last_save_time:1726552373\r\nrdb_last_bgsave_status:ok\r\nrdb_last_bgsave_time_sec:0\r\nrdb_current_bgsave_time_sec:-1\r\nrdb_saves:5\r\nrdb_last_cow_size:1093632\r\nrdb_last_load_keys_expired:0\r\nrdb_last_load_keys_loaded:0\r\naof_enabled:1\r\naof_rewrite_in_progress:0\r\naof_rewrite_scheduled:0\r\naof_last_rewrite_time_sec:-1\r\naof_current_rewrite_time_sec:-1\r\naof_last_bgrewrite_status:ok\r\naof_rewrites:0\r\naof_rewrites_consecutive_failures:0\r\naof_last_write_status:ok\r\naof_last_cow_size:0\r\nmodule_fork_in_progress:0\r\nmodule_fork_last_cow_size:0\r\naof_current_size:37092089\r\naof_base_size:89\r\naof_pending_rewrite:0\r\naof_buffer_length:0\r\naof_pending_bio_fsync:0\r\naof_delayed_fsync:0\r\n```\r\n\r\nIn case anyone is wondering, the persistent volume is attached correctly to the Redis Cluster in `/data` mount path. Here is a *snippet* of the YAML definition of the main Redis Cluster leader (this is automatically generated via Helm & Redis Operator):\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: redis-cluster-leader-0\r\n  namespace: test\r\n[...]\r\nspec:\r\n  containers:\r\n    [...]\r\n    volumeMounts:\r\n    - mountPath: /node-conf\r\n      name: node-conf\r\n    - mountPath: /data\r\n      name: redis-cluster-leader\r\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\r\n      name: kube-api-access-7ds8c\r\n      readOnly: true\r\n  [...]\r\n  volumes:\r\n  - name: node-conf\r\n    persistentVolumeClaim:\r\n      claimName: node-conf-redis-cluster-leader-0\r\n  - name: redis-cluster-leader\r\n    persistentVolumeClaim:\r\n      claimName: redis-cluster-leader-redis-cluster-leader-0\r\n[...]\r\n```\r\n\r\nI have already spent a couple of days on this issue, and I kind of looked everywhere, but in vain. I would appreciate any kind of help guys. I will also be available in case any additional information is needed. Thank you very much.",
  "state": "open",
  "created_at": "2024-09-17T06:25:11Z",
  "updated_at": "2024-09-19T02:28:04Z",
  "closed_at": null,
  "labels": [
    "state:to-be-closed"
  ],
  "comments_data": [
    {
      "id": 2355693672,
      "user": "sundb",
      "created_at": "2024-09-17T12:57:03Z",
      "body": "@MedAzizTousli did you check the content in the appendonly.aof.1.incr.aof? "
    },
    {
      "id": 2356337515,
      "user": "MedAzizTousli",
      "created_at": "2024-09-17T16:00:05Z",
      "body": "> @MedAzizTousli did you check the content in the appendonly.aof.1.incr.aof?\r\n\r\nYes, I did. It basically contains the data that I have added but in some sort of query format, just like how AOF is supposed to be working."
    },
    {
      "id": 2359837135,
      "user": "MedAzizTousli",
      "created_at": "2024-09-19T02:11:11Z",
      "body": "I noticed that `/data/appendonlydir/appendonly.aof.1.incr.aof` contains `FLUSHALL` command at the end, so, I solved the issue by adding `rename-command FLUSHALL \"\"` to my `redis.conf`."
    }
  ]
}