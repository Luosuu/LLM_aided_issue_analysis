{
  "issue_number": 9998.0,
  "title": "[BUG] Sentinel cluster is broken after a sentinel/redis restart",
  "body": "**Describe the bug**\r\nIf a sentinel restarts (e.g. server reboots or sentinel container restarts), the number of sentinels reported by `info sentinel` is wrong (increases by 1).\r\nAs a result, sentinel-based setup of 3 sentinels survives only a single failover. After a failover we have to manually reset the sentinels (or, sometimes even restart the entire redis cluster completely to fix this, which leads to additional downtime). \r\nIf we do not do this, the sentinels eventually fail to form a quorum and subsequent failovers become impossible.\r\nObviously we never know that failover have already occurred and that we need to take a manual action, until we get a production downtime on the second failover.\r\nDue to this issue, which lasts for so many releases and was discussed in so many issues there, sentinel-based failover is not reliable enough for production.\r\n\r\n**To reproduce**\r\nNote: we connect multiple redis instances to the same sentinels, so that there are many masters and there is a corresponding slave for each master. Let's call them master-slave pairs.\r\n1. Start sentinel-based setup with 3 sentinels on redis 6.2.6.\r\n2. Stop sentinel number 1 and redis masters (imitating a node failure). Wait for a failover to happen. It is OK on this step.\r\n3. Restart sentinel number 1 and redis masters. `info sentinel` on some of the other sentinels are now reporting 4 sentinels.\r\n3. Wait some time so that the next steps are not affected by sentinel failover timeout. \r\n4. Stop sentinel number 2  and redis slaves. Wait for failover to happen. The failover will fail on some of the redis pairs with the error below. The probability of the issue is approximately 0.25 for us.\r\n5. Repeat steps 2-4, if needed.\r\n\r\n**Expected behavior**\r\n1. Failover succeeded, redis cluster serving requests\r\n\r\n**Actual behaviour**\r\nRedis cluster is no longer serving requests and needs a restart. `info sentinel` reports something similar to this:\r\n```\r\nmaster0:name=pair1,status=odown,address=1.1.1.1:12345,slaves=1,sentinels=5\r\n```\r\n\r\n**Additional information**\r\nThe issue is reproducible on latest docker image 6.2.6.",
  "state": "closed",
  "created_at": "2021-12-25T14:54:24Z",
  "updated_at": "2022-12-08T17:49:09Z",
  "closed_at": "2022-12-08T17:49:09Z",
  "labels": [
    "state:to-be-closed"
  ],
  "comments_data": [
    {
      "id": 1001045020,
      "user": "enjoy-binbin",
      "created_at": "2021-12-25T16:59:35Z",
      "body": "thanks for the reporting. Let me try reproduce tomorrow"
    },
    {
      "id": 1001084714,
      "user": "RomanKisilenko",
      "created_at": "2021-12-26T00:00:16Z",
      "body": "I updated the reproduction steps to match exactly what we are doing to reproduce the issue. I'm not sure if the issue is related specifically to wrong sentinel count reported by sentinels because the issue reproduces even if we reset sentinels between steps 3 and 5 above. However restarting the redis cluster resolves the failure."
    },
    {
      "id": 1001084996,
      "user": "bashanyy",
      "created_at": "2021-12-26T00:02:50Z",
      "body": "谢谢"
    },
    {
      "id": 1001105498,
      "user": "enjoy-binbin",
      "created_at": "2021-12-26T04:35:46Z",
      "body": "i can't reproduce (sentinels++).  i am not using docker\r\ndid you happen to change the sentinel ID (or change the sentinel.conf) or (docker volume sentinel conf?)\r\n```\r\n# something like this in sentinel.conf\r\n# Generated by CONFIG REWRITE\r\nsentinel myid c283bea66ecbb8a0f6201c71cda17cb40df4e22b\r\n```\r\n\r\nif i remember correct, sentinels++ only happen when a new sentinel join, a new sentinel myid join.\r\nso i assume that you restart sentinel that the sentinel id changed (a new sentinel conf is used)"
    },
    {
      "id": 1001136909,
      "user": "RomanKisilenko",
      "created_at": "2021-12-26T09:23:51Z",
      "body": "OK, the wrong count issue is fixed by making sentinel configuration persistent. Thank you for heads up.\r\n\r\nHowever, I still get odown redis pair status in sentinel after following the above steps:\r\nmaster0:name=pair1,status=odown,address=1.1.1.1:12345,slaves=1,sentinels=5\r\n\r\nI noticed also that then this happens restarted sentinel often disagrees with other sentinels on what redis instance should be master.\r\n\r\nRestarting sentinel works around this issue?\r\n\r\nAny ideas? We learned how to reproduce it on staging, so that we can collect logs for you, whatever is required to get it fixed."
    },
    {
      "id": 1001139168,
      "user": "enjoy-binbin",
      "created_at": "2021-12-26T09:42:59Z",
      "body": "> OK, the wrong count issue is fixed by making sentinel configuration persistent\r\n\r\nafter this part, will you still get unexpected results?\r\n\r\nafter a failover, a replica became the new master, the sentinel will rewrite the sentinel conf\r\n```\r\nsentinel monitor mymaster 127.0.0.1 6380 2\r\n```\r\n\r\nlike this line, the maste ip address and port inside will change, so different sentinels may see mymaster differently?"
    },
    {
      "id": 1001230309,
      "user": "RomanKisilenko",
      "created_at": "2021-12-26T19:24:24Z",
      "body": ">after this part, will you still get unexpected results?\r\nYes.\r\n \r\nAll sentinels are configured with the identical master. I mean that sentinels often disagree if there should be a failover to slave or not. So that sentinel number 1 may elect the original master, but sentinel number 2 can elect original slave to become master. This issue usually precedes the subsequent odown issue which in turn crashes the application.\r\n\r\nIf you'll suggest what data should I collect I can reproduce the issue and provide you with the required information."
    },
    {
      "id": 1001297621,
      "user": "enjoy-binbin",
      "created_at": "2021-12-27T02:30:47Z",
      "body": "can you post some logs, like each sentinel serverlog (stdout / output), something like this, it may help\r\n```\r\n16085:X 26 Dec 2021 12:25:16.521 # +sdown master mymaster 127.0.0.1 6379\r\n16085:X 26 Dec 2021 12:25:16.579 # +odown master mymaster 127.0.0.1 6379 #quorum 2/2\r\n16085:X 26 Dec 2021 12:25:16.580 # +new-epoch 1\r\n16085:X 26 Dec 2021 12:25:16.580 # +try-failover master mymaster 127.0.0.1 6379\r\n16085:X 26 Dec 2021 12:25:16.608 # +vote-for-leader 554e08222894e7c56f3f7c9c61394f4f86dde209 1\r\n16085:X 26 Dec 2021 12:25:16.625 # e2af173ed82280bdc6688bab94a26d517379245a voted for 554e08222894e7c56f3f7c9c61394f4f86dde209 1\r\n16085:X 26 Dec 2021 12:25:16.688 # +elected-leader master mymaster 127.0.0.1 6379\r\n16085:X 26 Dec 2021 12:25:16.688 # +failover-state-select-slave master mymaster 127.0.0.1 6379\r\n16085:X 26 Dec 2021 12:25:16.743 # +selected-slave slave 127.0.0.1:6380 127.0.0.1 6380 @ mymaster 127.0.0.1 6379\r\n16085:X 26 Dec 2021 12:25:16.743 * +failover-state-send-slaveof-noone slave 127.0.0.1:6380 127.0.0.1 6380 @ mymaster 127.0.0.1 6379\r\n16085:X 26 Dec 2021 12:25:16.818 * +failover-state-wait-promotion slave 127.0.0.1:6380 127.0.0.1 6380 @ mymaster 127.0.0.1 6379\r\n16085:X 26 Dec 2021 12:25:17.622 # +promoted-slave slave 127.0.0.1:6380 127.0.0.1 6380 @ mymaster 127.0.0.1 6379\r\n16085:X 26 Dec 2021 12:25:17.623 # +failover-state-reconf-slaves master mymaster 127.0.0.1 6379\r\n16085:X 26 Dec 2021 12:25:17.688 # +failover-end master mymaster 127.0.0.1 6379\r\n16085:X 26 Dec 2021 12:25:17.689 # +switch-master mymaster 127.0.0.1 6379 127.0.0.1 6380\r\n```"
    },
    {
      "id": 1001761995,
      "user": "RomanKisilenko",
      "created_at": "2021-12-27T21:09:40Z",
      "body": "OK, so here is what happened on my last attempt to reproduce this issue.\r\n\r\nFirst, I checked that sentinels are OK in initial state and agree on failover state.\r\n```\r\nsentinel #1\r\nmaster0:name=shard3,status=ok,address=172.20.0.2:30662,slaves=1,sentinels=3\r\nmaster1:name=primary,status=ok,address=172.20.0.2:30693,slaves=1,sentinels=3\r\nmaster2:name=shard2,status=ok,address=172.20.0.2:30661,slaves=1,sentinels=3\r\nmaster3:name=shard1,status=ok,address=172.20.0.2:30658,slaves=1,sentinels=3\r\nsentinel #2\r\nmaster0:name=shard3,status=ok,address=172.20.0.2:30662,slaves=1,sentinels=3\r\nmaster1:name=shard2,status=ok,address=172.20.0.2:30661,slaves=1,sentinels=3\r\nmaster2:name=primary,status=ok,address=172.20.0.2:30693,slaves=1,sentinels=3\r\nmaster3:name=shard1,status=ok,address=172.20.0.2:30658,slaves=1,sentinels=3\r\nsentinel #3\r\nmaster0:name=shard3,status=ok,address=172.20.0.2:30662,slaves=1,sentinels=3\r\nmaster1:name=shard1,status=ok,address=172.20.0.2:30658,slaves=1,sentinels=3\r\nmaster2:name=shard2,status=ok,address=172.20.0.2:30661,slaves=1,sentinels=3\r\nmaster3:name=primary,status=ok,address=172.20.0.2:30693,slaves=1,sentinels=3\r\n```\r\n\r\nNext, I imitated a node reboot. Shut down all services (redis masters and sentinel number 1) supposed to be running on the same node.\r\n```\r\nsentinel #2\r\nmaster0:name=shard3,status=ok,address=172.20.0.2:30663,slaves=1,sentinels=3\r\nmaster1:name=shard2,status=ok,address=172.20.0.2:30661,slaves=1,sentinels=3\r\nmaster2:name=primary,status=ok,address=172.20.0.2:30693,slaves=1,sentinels=3\r\nmaster3:name=shard1,status=ok,address=172.20.0.2:30659,slaves=1,sentinels=3\r\nsentinel #3\r\nmaster0:name=shard3,status=ok,address=172.20.0.2:30663,slaves=1,sentinels=3\r\nmaster1:name=shard1,status=ok,address=172.20.0.2:30659,slaves=1,sentinels=3\r\nmaster2:name=shard2,status=ok,address=172.20.0.2:30661,slaves=1,sentinels=3\r\nmaster3:name=primary,status=ok,address=172.20.0.2:30693,slaves=1,sentinels=3\r\n```\r\n\r\nNote, that a failover did happen and remaining sentinels still agree on failover state, so that application is able to connect to redis instances.\r\n\r\nNow, I imitated that node is back up by concurrently starting redis masters and sentinel number 1, waited some time for discovery to happen. Note, that I beleive there is some kind of race condition involved so that it is important that the redises and sentinels are to be started concurrently. \r\n\r\n```\r\nsentinel #1\r\nmaster0:name=shard3,status=ok,address=172.20.0.2:30663,slaves=1,sentinels=3\r\nmaster1:name=shard1,status=ok,address=172.20.0.2:30659,slaves=1,sentinels=3\r\nmaster2:name=primary,status=odown,address=172.20.0.2:30692,slaves=1,sentinels=3\r\nmaster3:name=shard2,status=odown,address=172.20.0.2:30660,slaves=1,sentinels=3\r\nsentinel #2\r\nmaster0:name=shard3,status=ok,address=172.20.0.2:30663,slaves=1,sentinels=3\r\nmaster1:name=shard2,status=ok,address=172.20.0.2:30661,slaves=1,sentinels=3\r\nmaster2:name=primary,status=ok,address=172.20.0.2:30693,slaves=1,sentinels=3\r\nmaster3:name=shard1,status=ok,address=172.20.0.2:30659,slaves=1,sentinels=3\r\nsentinel #3\r\nmaster0:name=shard3,status=ok,address=172.20.0.2:30663,slaves=1,sentinels=3\r\nmaster1:name=shard1,status=ok,address=172.20.0.2:30659,slaves=1,sentinels=3\r\nmaster2:name=shard2,status=ok,address=172.20.0.2:30661,slaves=1,sentinels=3\r\nmaster3:name=primary,status=ok,address=172.20.0.2:30693,slaves=1,sentinels=3\r\n```\r\nNote, that sentinel 1 now disagrees on failover state with other sentinels. It also reports the status as odown. I guess if I would start the sentinel first, allowed for discovery to happen and then started the masters, then there would be no disagreement. \r\n\r\nAt this point the cluster is 1 step from failure. If sentinel #2 or sentinel #3 is stopped now, the application is no longer able to connect to the redises. The redises are healthy, but (it seems to me) since sentinels disagree on failover state, they fail to report current master to application properly (I guess application needs at least 2 sentinels to agree on which redis instance is a master).\r\n\r\nHere are the logs from sentinel 1:\r\n```\r\n26:X 27 Dec 2021 20:49:52.866 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\r\n26:X 27 Dec 2021 20:49:52.866 # Redis version=6.2.6, bits=64, commit=00000000, modified=0, pid=26, just started\r\n26:X 27 Dec 2021 20:49:52.866 # Configuration loaded\r\n26:X 27 Dec 2021 20:49:52.867 * monotonic clock: POSIX clock_gettime\r\n26:X 27 Dec 2021 20:49:52.870 * Running mode=sentinel, port=26379.\r\n26:X 27 Dec 2021 20:49:52.871 # Sentinel ID is 1d9837be48310d84428b1fd268ea740b54726c3b\r\n26:X 27 Dec 2021 20:49:52.871 # +monitor master shard3 172.20.0.2 30662 quorum 1\r\n26:X 27 Dec 2021 20:49:52.871 # +monitor master shard1 172.20.0.2 30658 quorum 1\r\n26:X 27 Dec 2021 20:49:52.871 # +monitor master primary 172.20.0.2 30692 quorum 1\r\n26:X 27 Dec 2021 20:49:52.871 # +monitor master shard2 172.20.0.2 30660 quorum 1\r\n26:X 27 Dec 2021 20:49:54.649 # +new-epoch 108\r\n26:X 27 Dec 2021 20:49:54.649 # +config-update-from sentinel 5febcad9eba3491f8ce04365bd544d7c4a770c2a 172.20.0.2 30665 @ shard1 172.20.0.2 30658\r\n26:X 27 Dec 2021 20:49:54.649 # +switch-master shard1 172.20.0.2 30658 172.20.0.2 30659\r\n26:X 27 Dec 2021 20:49:54.649 * +slave slave 172.20.0.2:30658 172.20.0.2 30658 @ shard1 172.20.0.2 30659\r\n26:X 27 Dec 2021 20:49:54.787 # +config-update-from sentinel 5febcad9eba3491f8ce04365bd544d7c4a770c2a 172.20.0.2 30665 @ shard3 172.20.0.2 30662\r\n26:X 27 Dec 2021 20:49:54.787 # +switch-master shard3 172.20.0.2 30662 172.20.0.2 30663\r\n26:X 27 Dec 2021 20:49:54.788 * +slave slave 172.20.0.2:30662 172.20.0.2 30662 @ shard3 172.20.0.2 30663\r\n26:X 27 Dec 2021 20:49:57.895 # +sdown master primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 20:49:57.895 # +odown master primary 172.20.0.2 30692 #quorum 1/1\r\n26:X 27 Dec 2021 20:49:57.895 # +new-epoch 109\r\n26:X 27 Dec 2021 20:49:57.895 # +try-failover master primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 20:49:57.899 # +vote-for-leader 1d9837be48310d84428b1fd268ea740b54726c3b 109\r\n26:X 27 Dec 2021 20:49:57.899 # +sdown slave 172.20.0.2:30692 172.20.0.2 30692 @ primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 20:49:58.169 # -sdown master primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 20:49:58.169 # -odown master primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 20:49:58.170 # -sdown slave 172.20.0.2:30692 172.20.0.2 30692 @ primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 20:50:05.956 * +convert-to-slave slave 172.20.0.2:30662 172.20.0.2 30662 @ shard3 172.20.0.2 30663\r\n26:X 27 Dec 2021 20:50:07.695 * +convert-to-slave slave 172.20.0.2:30658 172.20.0.2 30658 @ shard1 172.20.0.2 30659\r\n26:X 27 Dec 2021 20:50:08.901 # -failover-abort-not-elected master primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 20:50:21.051 # +sdown master shard2 172.20.0.2 30660\r\n26:X 27 Dec 2021 20:50:21.051 # +odown master shard2 172.20.0.2 30660 #quorum 1/1\r\n26:X 27 Dec 2021 20:50:21.051 # +new-epoch 110\r\n26:X 27 Dec 2021 20:50:21.051 # +try-failover master shard2 172.20.0.2 30660\r\n26:X 27 Dec 2021 20:50:21.055 # +vote-for-leader 1d9837be48310d84428b1fd268ea740b54726c3b 110\r\n26:X 27 Dec 2021 20:50:23.106 # +sdown master primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 20:50:23.106 # +odown master primary 172.20.0.2 30692 #quorum 1/1\r\n26:X 27 Dec 2021 20:50:23.106 # Next failover delay: I will not start a failover before Mon Dec 27 20:51:58 2021\r\n26:X 27 Dec 2021 20:50:31.140 # -failover-abort-not-elected master shard2 172.20.0.2 30660\r\n26:X 27 Dec 2021 20:50:31.216 # Next failover delay: I will not start a failover before Mon Dec 27 20:52:21 2021\r\n26:X 27 Dec 2021 20:51:58.885 # +new-epoch 111\r\n26:X 27 Dec 2021 20:51:58.885 # +try-failover master primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 20:51:58.903 # +vote-for-leader 1d9837be48310d84428b1fd268ea740b54726c3b 111\r\n26:X 27 Dec 2021 20:52:09.480 # -failover-abort-not-elected master primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 20:52:09.552 # Next failover delay: I will not start a failover before Mon Dec 27 20:53:59 2021\r\n26:X 27 Dec 2021 20:52:21.203 # +new-epoch 112\r\n26:X 27 Dec 2021 20:52:21.203 # +try-failover master shard2 172.20.0.2 30660\r\n26:X 27 Dec 2021 20:52:21.207 # +vote-for-leader 1d9837be48310d84428b1fd268ea740b54726c3b 112\r\n26:X 27 Dec 2021 20:52:31.830 # -failover-abort-not-elected master shard2 172.20.0.2 30660\r\n26:X 27 Dec 2021 20:52:31.882 # Next failover delay: I will not start a failover before Mon Dec 27 20:54:21 2021\r\n26:X 27 Dec 2021 20:53:59.498 # +new-epoch 113\r\n26:X 27 Dec 2021 20:53:59.498 # +try-failover master primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 20:53:59.501 # +vote-for-leader 1d9837be48310d84428b1fd268ea740b54726c3b 113\r\n26:X 27 Dec 2021 20:54:09.674 # -failover-abort-not-elected master primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 20:54:09.740 # Next failover delay: I will not start a failover before Mon Dec 27 20:55:59 2021\r\n26:X 27 Dec 2021 20:54:21.826 # +new-epoch 114\r\n26:X 27 Dec 2021 20:54:21.827 # +try-failover master shard2 172.20.0.2 30660\r\n26:X 27 Dec 2021 20:54:21.830 # +vote-for-leader 1d9837be48310d84428b1fd268ea740b54726c3b 114\r\n26:X 27 Dec 2021 20:54:32.460 # -failover-abort-not-elected master shard2 172.20.0.2 30660\r\n26:X 27 Dec 2021 20:54:32.519 # Next failover delay: I will not start a failover before Mon Dec 27 20:56:22 2021\r\n26:X 27 Dec 2021 20:55:59.703 # +new-epoch 115\r\n26:X 27 Dec 2021 20:55:59.703 # +try-failover master primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 20:55:59.707 # +vote-for-leader 1d9837be48310d84428b1fd268ea740b54726c3b 115\r\n26:X 27 Dec 2021 20:56:09.921 # -failover-abort-not-elected master primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 20:56:09.979 # Next failover delay: I will not start a failover before Mon Dec 27 20:57:59 2021\r\n26:X 27 Dec 2021 20:56:22.451 # +new-epoch 116\r\n26:X 27 Dec 2021 20:56:22.451 # +try-failover master shard2 172.20.0.2 30660\r\n26:X 27 Dec 2021 20:56:22.455 # +vote-for-leader 1d9837be48310d84428b1fd268ea740b54726c3b 116\r\n26:X 27 Dec 2021 20:56:32.634 # -failover-abort-not-elected master shard2 172.20.0.2 30660\r\n26:X 27 Dec 2021 20:56:32.734 # Next failover delay: I will not start a failover before Mon Dec 27 20:58:22 2021\r\n26:X 27 Dec 2021 20:58:00.001 # +new-epoch 117\r\n26:X 27 Dec 2021 20:58:00.001 # +try-failover master primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 20:58:00.005 # +vote-for-leader 1d9837be48310d84428b1fd268ea740b54726c3b 117\r\n26:X 27 Dec 2021 20:58:11.041 # -failover-abort-not-elected master primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 20:58:11.119 # Next failover delay: I will not start a failover before Mon Dec 27 21:00:00 2021\r\n26:X 27 Dec 2021 20:58:22.613 # +new-epoch 118\r\n26:X 27 Dec 2021 20:58:22.613 # +try-failover master shard2 172.20.0.2 30660\r\n26:X 27 Dec 2021 20:58:22.617 # +vote-for-leader 1d9837be48310d84428b1fd268ea740b54726c3b 118\r\n26:X 27 Dec 2021 20:58:33.057 # -failover-abort-not-elected master shard2 172.20.0.2 30660\r\n26:X 27 Dec 2021 20:58:33.141 # Next failover delay: I will not start a failover before Mon Dec 27 21:00:23 2021\r\n26:X 27 Dec 2021 21:00:00.992 # +new-epoch 119\r\n26:X 27 Dec 2021 21:00:00.992 # +try-failover master primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 21:00:00.995 # +vote-for-leader 1d9837be48310d84428b1fd268ea740b54726c3b 119\r\n26:X 27 Dec 2021 21:00:11.501 # -failover-abort-not-elected master primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 21:00:11.585 # Next failover delay: I will not start a failover before Mon Dec 27 21:02:01 2021\r\n26:X 27 Dec 2021 21:00:23.094 # +new-epoch 120\r\n26:X 27 Dec 2021 21:00:23.094 # +try-failover master shard2 172.20.0.2 30660\r\n26:X 27 Dec 2021 21:00:23.133 # +vote-for-leader 1d9837be48310d84428b1fd268ea740b54726c3b 120\r\n26:X 27 Dec 2021 21:00:33.984 # -failover-abort-not-elected master shard2 172.20.0.2 30660\r\n26:X 27 Dec 2021 21:00:34.056 # Next failover delay: I will not start a failover before Mon Dec 27 21:02:23 2021\r\n26:X 27 Dec 2021 21:02:01.521 # +new-epoch 121\r\n26:X 27 Dec 2021 21:02:01.521 # +try-failover master primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 21:02:01.524 # +vote-for-leader 1d9837be48310d84428b1fd268ea740b54726c3b 121\r\n26:X 27 Dec 2021 21:02:12.223 # -failover-abort-not-elected master primary 172.20.0.2 30692\r\n26:X 27 Dec 2021 21:02:12.323 # Next failover delay: I will not start a failover before Mon Dec 27 21:04:02 2021\r\n26:X 27 Dec 2021 21:02:24.018 # +new-epoch 122\r\n26:X 27 Dec 2021 21:02:24.018 # +try-failover master shard2 172.20.0.2 30660\r\n26:X 27 Dec 2021 21:02:24.022 # +vote-for-leader 1d9837be48310d84428b1fd268ea740b54726c3b 122\r\n26:X 27 Dec 2021 21:02:34.154 # -failover-abort-not-elected master shard2 172.20.0.2 30660\r\n26:X 27 Dec 2021 21:02:34.212 # Next failover delay: I will not start a failover before Mon Dec 27 21:04:24 2021\r\n```\r\n\r\nPlease let me know if any further information is required."
    },
    {
      "id": 1003359634,
      "user": "RomanKisilenko",
      "created_at": "2021-12-31T12:06:29Z",
      "body": "Hello, are there any insights available on this issue? Let me know if I can help any further."
    },
    {
      "id": 1003381993,
      "user": "enjoy-binbin",
      "created_at": "2021-12-31T13:53:16Z",
      "body": "sorry, busy with other things, i will take a look again this weekend.\r\nalso ping @hwware if you have some times, can also share some thoughts :)"
    },
    {
      "id": 1003736190,
      "user": "enjoy-binbin",
      "created_at": "2022-01-02T15:54:01Z",
      "body": "This part of the log looks a bit strange, there are many such block logs. \r\n- `new-epoch` means that sentinel1 rec the other sentinel HELLO message, a newer epoch\r\n- `+vote-for-leader` note that sentinel 1 voted for himself, but the following logs looks a bit strange, so i suppose we didn't reach the quorm ( sentinels_count / 2 + 1)\r\n- `-failover-abort-not-elected` after 10s, the failover was aborted. sentinel1 is not the leader, not able to continue failover.\r\n```\r\n26:X 27 Dec 2021 20:52:21.203 # +new-epoch 112\r\n26:X 27 Dec 2021 20:52:21.203 # +try-failover master shard2 172.20.0.2 30660\r\n26:X 27 Dec 2021 20:52:21.207 # +vote-for-leader 1d9837be48310d84428b1fd268ea740b54726c3b 112\r\n26:X 27 Dec 2021 20:52:31.830 # -failover-abort-not-elected master shard2 172.20.0.2 30660\r\n26:X 27 Dec 2021 20:52:31.882 # Next failover delay: I will not start a failover before Mon Dec 27 20:54:21 2021\r\n```\r\n\r\nI also noticed, the quorum is 1, I see that the number of sentries you configure is three, this should be 2?\r\n```\r\n26:X 27 Dec 2021 20:49:52.871 # +monitor master shard3 172.20.0.2 30662 quorum 1\r\n26:X 27 Dec 2021 20:49:52.871 # +monitor master shard1 172.20.0.2 30658 quorum 1\r\n26:X 27 Dec 2021 20:49:52.871 # +monitor master primary 172.20.0.2 30692 quorum 1\r\n26:X 27 Dec 2021 20:49:52.871 # +monitor master shard2 172.20.0.2 30660 quorum 1\r\n```\r\n\r\nI will send out what I know first, I suggest you set the quorum to 2 (if you have 3 sentinels) and then take a  another look.\r\nI also suppose we need a log, like in this case, sentinel1 voted for himself, but it looks like it didn’t get enough votes (>=2), so sentinel 1 cannot be called the leader, and the code set `leader_winner = NULL` without a log\r\n```\r\n# sentinel.conf\r\nsentinel monitor mymaster 127.0.0.1 6379 2\r\n\r\n# Note that whatever is the ODOWN quorum, a Sentinel will require to\r\n# be elected by the majority of the known Sentinels in order to\r\n# start a failover, so no failover can be performed in minority.\r\n```"
    },
    {
      "id": 1003737713,
      "user": "RomanKisilenko",
      "created_at": "2022-01-02T16:03:08Z",
      "body": "I tried already with quorum value of 2, the situation is pretty much the same - on the second failover sentinels report sdown/odown status and the app fails to connect to redis. I can collect the updated logs (with quorum=2) if this can help."
    },
    {
      "id": 1003738788,
      "user": "enjoy-binbin",
      "created_at": "2022-01-02T16:10:23Z",
      "body": "sure, please collect the logs. and this time, maybe we can also the other sentinels logs\r\nI am also curious about the output of other sentries...\r\n\r\nI am no expert to sentinels, i will try to response what i know, and in the meantime\r\nwe can wait for @hwware , he knows the sentinels better than I do"
    },
    {
      "id": 1004813583,
      "user": "RomanKisilenko",
      "created_at": "2022-01-04T13:30:14Z",
      "body": "OK, so I took another attempt to reproduce this and collected the logs along the way.\r\n\r\nstep 1: started a clean cluster\r\nstep 2: stopped server nr. 1 with first sentinel and masters and then restarted it\r\nstep 3: stopped server nr. 2\r\nstep 4: started server nr. 2. Strange thing: sentinel nr. 2 now reporting 2 slaves (should be 1):\r\n```\r\nsentinel #1\r\nmaster0:name=shard2,status=ok,address=172.20.0.2:30660,slaves=1,sentinels=3\r\nmaster1:name=shard1,status=ok,address=172.20.0.2:30658,slaves=1,sentinels=3\r\nmaster2:name=shard3,status=ok,address=172.20.0.2:30662,slaves=1,sentinels=3\r\nmaster3:name=primary,status=ok,address=172.20.0.2:30692,slaves=1,sentinels=3\r\nsentinel #2\r\nmaster0:name=shard1,status=ok,address=172.20.0.2:30658,slaves=2,sentinels=3\r\nmaster1:name=shard3,status=ok,address=172.20.0.2:30662,slaves=2,sentinels=3\r\nmaster2:name=primary,status=ok,address=172.20.0.2:30692,slaves=2,sentinels=3\r\nmaster3:name=shard2,status=ok,address=172.20.0.2:30660,slaves=2,sentinels=3\r\nsentinel #3\r\nmaster0:name=shard3,status=ok,address=172.20.0.2:30662,slaves=1,sentinels=3\r\nmaster1:name=shard1,status=ok,address=172.20.0.2:30658,slaves=1,sentinels=3\r\nmaster2:name=primary,status=ok,address=172.20.0.2:30692,slaves=1,sentinels=3\r\nmaster3:name=shard2,status=ok,address=172.20.0.2:30660,slaves=1,sentinels=3\r\n```\r\nstep 5: stopped server nr. 1\r\nstep 6: started server nr. 1. Strange behaviour: now sentinels disagree on failover state.\r\n```\r\nsentinel #1\r\nmaster0:name=shard2,status=ok,address=172.20.0.2:30660,slaves=1,sentinels=3\r\nmaster1:name=primary,status=ok,address=172.20.0.2:30692,slaves=1,sentinels=3\r\nmaster2:name=shard1,status=ok,address=172.20.0.2:30658,slaves=1,sentinels=3\r\nmaster3:name=shard3,status=ok,address=172.20.0.2:30662,slaves=1,sentinels=3\r\nsentinel #2\r\nmaster0:name=shard1,status=ok,address=172.20.0.2:30659,slaves=1,sentinels=3\r\nmaster1:name=shard3,status=ok,address=172.20.0.2:30663,slaves=1,sentinels=3\r\nmaster2:name=primary,status=ok,address=172.20.0.2:30693,slaves=1,sentinels=3\r\nmaster3:name=shard2,status=ok,address=172.20.0.2:30661,slaves=1,sentinels=3\r\nsentinel #3\r\nmaster0:name=shard3,status=ok,address=172.20.0.2:30663,slaves=1,sentinels=3\r\nmaster1:name=shard1,status=ok,address=172.20.0.2:30659,slaves=1,sentinels=3\r\nmaster2:name=primary,status=ok,address=172.20.0.2:30693,slaves=1,sentinels=3\r\nmaster3:name=shard2,status=ok,address=172.20.0.2:30661,slaves=1,sentinels=3\r\n``` \r\nstep 7: stopped server nr. 2. The cluster is broken now\r\n```\r\nsentinel #1\r\nmaster0:name=shard2,status=sdown,address=172.20.0.2:30660,slaves=1,sentinels=3\r\nmaster1:name=primary,status=sdown,address=172.20.0.2:30692,slaves=1,sentinels=3\r\nmaster2:name=shard1,status=sdown,address=172.20.0.2:30658,slaves=1,sentinels=3\r\nmaster3:name=shard3,status=sdown,address=172.20.0.2:30662,slaves=1,sentinels=3\r\nsentinel #2\r\nError from server (NotFound): pods \"ps-rs2-0\" not found\r\nsentinel #3\r\nmaster0:name=shard3,status=sdown,address=172.20.0.2:30663,slaves=1,sentinels=3\r\nmaster1:name=shard1,status=sdown,address=172.20.0.2:30659,slaves=1,sentinels=3\r\nmaster2:name=primary,status=sdown,address=172.20.0.2:30693,slaves=1,sentinels=3\r\nmaster3:name=shard2,status=sdown,address=172.20.0.2:30661,slaves=1,sentinels=3\r\n```\r\nstep 8: restarted server nr. 2. The cluster back to healthy but there is still sentinel problem. Note, that the cluster sometimes does not recover at all, this time I was \"lucky\" that it recovered\r\n```\r\nsentinel #1\r\nmaster0:name=shard2,status=sdown,address=172.20.0.2:30660,slaves=1,sentinels=3\r\nmaster1:name=primary,status=sdown,address=172.20.0.2:30692,slaves=1,sentinels=3\r\nmaster2:name=shard1,status=sdown,address=172.20.0.2:30658,slaves=1,sentinels=3\r\nmaster3:name=shard3,status=sdown,address=172.20.0.2:30662,slaves=1,sentinels=3\r\nsentinel #2\r\nmaster0:name=shard3,status=ok,address=172.20.0.2:30662,slaves=1,sentinels=3\r\nmaster1:name=shard2,status=ok,address=172.20.0.2:30660,slaves=1,sentinels=3\r\nmaster2:name=primary,status=ok,address=172.20.0.2:30692,slaves=1,sentinels=3\r\nmaster3:name=shard1,status=ok,address=172.20.0.2:30658,slaves=1,sentinels=3\r\nsentinel #3\r\nmaster0:name=shard3,status=ok,address=172.20.0.2:30663,slaves=1,sentinels=3\r\nmaster1:name=shard1,status=ok,address=172.20.0.2:30659,slaves=1,sentinels=3\r\nmaster2:name=primary,status=ok,address=172.20.0.2:30693,slaves=1,sentinels=3\r\nmaster3:name=shard2,status=ok,address=172.20.0.2:30661,slaves=1,sentinels=3\r\n```\r\nLogs are attached: https://we.tl/t-NVJ2kgoeLb\r\nI collected the logs for both redis instances and sentinels."
    },
    {
      "id": 1006943479,
      "user": "hwware",
      "created_at": "2022-01-06T21:21:09Z",
      "body": "@enjoy-binbin Thanks, I will take a look recently. @RomanKisilenko  "
    },
    {
      "id": 1009002756,
      "user": "hwware",
      "created_at": "2022-01-10T15:37:17Z",
      "body": "@RomanKisilenko  Sorry for late replying to you. I check most of your logs, I found in your cluster, only 2 redis instances right?  one master and one replic, because for the log on sentinel:  most showing as:  master0:name=shard2,status=sdown,address=172.20.0.2:30660,**_slaves=1_**,**sentinels=3**   \r\n\r\nPlease confirm this， Thanks"
    },
    {
      "id": 1009004649,
      "user": "RomanKisilenko",
      "created_at": "2022-01-10T15:39:20Z",
      "body": "@hwware Yes, for each of the redises added to sentinels there is just a single master and single slave."
    },
    {
      "id": 1009085764,
      "user": "hwware",
      "created_at": "2022-01-10T16:39:10Z",
      "body": "This is my reproduce result (only till to your step 4)\r\n\r\nFirst, create a clean cluster with the following instances:\r\n1 master, 1 replica, 3 sentinels.  Following is the config：\r\n\r\n**master.conf**\r\n\r\nport 6381\r\nbind 127.0.0.1\r\nprotected-mode no\r\n\r\n**replica1.conf**\r\n\r\nport 6382\r\nbind 127.0.0.1\r\nprotected-mode no\r\nreplicaof 127.0.0.1 6381\r\n\r\n\r\n**sentinel1.conf**\r\n\r\nport 26381\r\nbind 127.0.0.1\r\nsentinel monitor mymaster 127.0.0.1 6381 2\r\nsentinel down-after-milliseconds mymaster 5000\r\nsentinel failover-timeout mymaster 60000\r\n\r\n**sentinel2.conf**\r\n\r\nport 26382\r\nbind 127.0.0.1\r\nsentinel monitor mymaster 127.0.0.1 6381 2\r\nsentinel down-after-milliseconds mymaster 5000\r\nsentinel failover-timeout mymaster 60000\r\n\r\n**sentinel3.conf**\r\n\r\nport 26383\r\nbind 127.0.0.1\r\nsentinel monitor mymaster 127.0.0.1 6381 2\r\nsentinel down-after-milliseconds mymaster 5000\r\nsentinel failover-timeout mymaster 60000\r\n\r\n**Following is my reproduced steps:**\r\nstep 1: started a clean cluster: 1 master, 1 replica, 3 sentinels\r\nstep 2: stop master(127.0.0.1: 6381) and first sentinel.\r\n            After failover, restart old master instance and sentinel 1\r\nstep 3:  Wait sometime, stop new master instance (127.0.0.1:6382) and do the failover\r\nstep4:   restart 127.0.0.1:6382 instance.\r\n\r\nFollowing is the detail log in the sentinel3 because sentinel 3 never stops:\r\n\r\n21356:X 10 Jan 2022 11:11:44.977 * +sentinel sentinel db86adec1416aeb6ea2e8cc2dbcc997a44f99d06 127.0.0.1 26381 @ mymaster 127.0.0.1 6381\r\n21356:X 10 Jan 2022 11:11:51.059 * +sentinel sentinel 04e6daf544338c376fb40ba1c28604471c662c36 127.0.0.1 26383 @ mymaster 127.0.0.1 6381\r\n**// Starting to stop sentinel and master 127.0.0.1:6381**  This line is not log, I add it for comments\r\n21356:X 10 Jan 2022 11:16:37.390 # +sdown sentinel db86adec1416aeb6ea2e8cc2dbcc997a44f99d06 127.0.0.1 26381 @ mymaster 127.0.0.1 6381\r\n21356:X 10 Jan 2022 11:16:40.603 # +sdown master mymaster 127.0.0.1 6381\r\n21356:X 10 Jan 2022 11:16:40.684 # +odown master mymaster 127.0.0.1 6381 #quorum 2/2\r\n21356:X 10 Jan 2022 11:16:40.684 # +new-epoch 1\r\n21356:X 10 Jan 2022 11:16:40.684 # +try-failover master mymaster 127.0.0.1 6381\r\n21356:X 10 Jan 2022 11:16:40.687 # +vote-for-leader 0fb07518d8779a5b3ce02d500fa930b5aa7fd2f1 1\r\n21356:X 10 Jan 2022 11:16:40.690 # 04e6daf544338c376fb40ba1c28604471c662c36 voted for 0fb07518d8779a5b3ce02d500fa930b5aa7fd2f1 1\r\n21356:X 10 Jan 2022 11:16:40.745 # +elected-leader master mymaster 127.0.0.1 6381\r\n21356:X 10 Jan 2022 11:16:40.745 # +failover-state-select-slave master mymaster 127.0.0.1 6381\r\n21356:X 10 Jan 2022 11:16:40.815 # +selected-slave slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6381\r\n21356:X 10 Jan 2022 11:16:40.815 * +failover-state-send-slaveof-noone slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6381\r\n21356:X 10 Jan 2022 11:16:40.917 * +failover-state-wait-promotion slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6381\r\n21356:X 10 Jan 2022 11:16:41.777 # +promoted-slave slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6381\r\n21356:X 10 Jan 2022 11:16:41.777 # +failover-state-reconf-slaves master mymaster 127.0.0.1 6381\r\n21356:X 10 Jan 2022 11:16:41.843 # +failover-end master mymaster 127.0.0.1 6381\r\n21356:X 10 Jan 2022 11:16:41.843 # +switch-master mymaster 127.0.0.1 6381 127.0.0.1 6382\r\n21356:X 10 Jan 2022 11:16:41.843 * +slave slave 127.0.0.1:6381 127.0.0.1 6381 @ mymaster 127.0.0.1 6382\r\n21356:X 10 Jan 2022 11:16:46.855 # +sdown slave 127.0.0.1:6381 127.0.0.1 6381 @ mymaster 127.0.0.1 6382\r\n**// Starting to restart sentinel and master 127.0.0.1:6381**  This line is not log, I add it for comments\r\n21356:X 10 Jan 2022 11:18:18.577 # -sdown slave 127.0.0.1:6381 127.0.0.1 6381 @ mymaster 127.0.0.1 6382\r\n21356:X 10 Jan 2022 11:18:28.540 * +convert-to-slave slave 127.0.0.1:6381 127.0.0.1 6381 @ mymaster 127.0.0.1 6382\r\n21356:X 10 Jan 2022 11:18:39.322 # -sdown sentinel db86adec1416aeb6ea2e8cc2dbcc997a44f99d06 127.0.0.1 26381 @ mymaster 127.0.0.1 6382\r\n\r\n**// start restart 127.0.0.1:6382** \r\n21356:X 10 Jan 2022 11:20:36.620 # +sdown master mymaster 127.0.0.1 6382\r\n21356:X 10 Jan 2022 11:20:36.680 # +new-epoch 2\r\n21356:X 10 Jan 2022 11:20:36.681 # +vote-for-leader 04e6daf544338c376fb40ba1c28604471c662c36 2\r\n21356:X 10 Jan 2022 11:20:36.683 # +odown master mymaster 127.0.0.1 6382 #quorum 3/2\r\n21356:X 10 Jan 2022 11:20:36.683 # Next failover delay: I will not start a failover before Mon Jan 10 11:22:37 2022\r\n21356:X 10 Jan 2022 11:20:37.787 # +config-update-from sentinel 04e6daf544338c376fb40ba1c28604471c662c36 127.0.0.1 26383 @ mymaster 127.0.0.1 6382\r\n21356:X 10 Jan 2022 11:20:37.787 # +switch-master mymaster 127.0.0.1 6382 127.0.0.1 6381\r\n21356:X 10 Jan 2022 11:20:37.787 * +slave slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6381\r\n21356:X 10 Jan 2022 11:20:42.831 # +sdown slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6381\r\n21356:X 10 Jan 2022 11:21:39.482 # -sdown slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6381\r\n21356:X 10 Jan 2022 11:21:49.427 * +convert-to-slave slave 127.0.0.1:6382 127.0.0.1 6382 @ mymaster 127.0.0.1 6381\r\n\r\nBelow is the detail information from 3 sentinel clients after all these steps:\r\n\r\n127.0.0.1:26381> info sentinel\r\n# Sentinel\r\nsentinel_masters:1\r\nsentinel_tilt:0\r\nsentinel_running_scripts:0\r\nsentinel_scripts_queue_length:0\r\nsentinel_simulate_failure_flags:0\r\nmaster0:name=mymaster,status=ok,address=127.0.0.1:6381,slaves=1,sentinels=3\r\n127.0.0.1:26381> \r\n\r\n\r\n127.0.0.1:26382> info sentinel\r\n# Sentinel\r\nsentinel_masters:1\r\nsentinel_tilt:0\r\nsentinel_running_scripts:0\r\nsentinel_scripts_queue_length:0\r\nsentinel_simulate_failure_flags:0\r\nmaster0:name=mymaster,status=ok,address=127.0.0.1:6381,slaves=1,sentinels=3\r\n127.0.0.1:26382> \r\n\r\n\r\n127.0.0.1:26383> info sentinel\r\n# Sentinel\r\nsentinel_masters:1\r\nsentinel_tilt:0\r\nsentinel_running_scripts:0\r\nsentinel_scripts_queue_length:0\r\nsentinel_simulate_failure_flags:0\r\nmaster0:name=mymaster,status=ok,address=127.0.0.1:6381,slaves=1,sentinels=3\r\n127.0.0.1:26383> \r\n\r\nI run them in 6.2.6 version.\r\nI can not find the sentinel which display 2 slaves.   \r\nPlease check my reproduce instance config and reproduce steps, let me know any concern. Thanks a lot.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"
    },
    {
      "id": 1009124243,
      "user": "RomanKisilenko",
      "created_at": "2022-01-10T16:54:24Z",
      "body": "The difference might be related to the fact that in my environment redis and sentinel are started concurrently, also to the fact that I'm running them inside docker containers."
    },
    {
      "id": 1009130688,
      "user": "hwware",
      "created_at": "2022-01-10T17:00:13Z",
      "body": "I am not sure how the effect in docker container. but I suggest you startup the redis and sentinel one by one, give them sometime to exchange the information.  Thanks\r\nIf the issue still could be reproduced in your side, just let me know.  Do not worry."
    },
    {
      "id": 1009179599,
      "user": "moticless",
      "created_at": "2022-01-10T17:55:09Z",
      "body": "Hi @RomanKisilenko ,\r\nI didn't read entirely the thread (hope i will have more time tomorrow), but I suspect that your configuration files in the containers are not persisted across restarts. That is, an instance fails to read its identity that was written into configuration file prior restart. \r\n\r\nHere are few reasons, why it might happen with respect to containers:\r\n1. Configuration file is written to non-persistent volume. On container restart, the instance will allocate new ID.\r\n2. Configuration file is written into a persistent volume that shared among containers such that, on restart, multiple containers rely on the same configuration file. (For example scale=3 of sentinels that all unfortunately read and write the very same file).\r\n\r\nIn other words, to my best understanding (will verify it tomorrow...), once ID is allocated to an instance and written to a configuration file, it must persist over restarts. "
    },
    {
      "id": 1009188252,
      "user": "RomanKisilenko",
      "created_at": "2022-01-10T18:05:05Z",
      "body": "@moticless sentinels store configuration files on a persistent storage which is independent for each of the sentilels and the sentinels have permission to write to the config files. Same true for redis instances, but they can not write to config file since I did not expect that redis instances will need to modify config.\r\nShould I let the redis instances to write to be able to write to config as well (I do not see any error in redis logs)? I can try to let them write to config bit later on today, to see if it makes any difference."
    },
    {
      "id": 1009194526,
      "user": "moticless",
      "created_at": "2022-01-10T18:12:23Z",
      "body": "Please do. And verify that once the instances are running, the configuration file appended with details of the form:\r\n```\r\n # Generated by CONFIG REWRITE\r\n dir \"/root\"\r\n user default on nopass ~* &* +@all\r\n sentinel myid 0f67d4e5b64711e977ba64942bd3cfefc90e2ef3\r\n sentinel config-epoch mymaster 0\r\n sentinel leader-epoch mymaster 0\r\n sentinel current-epoch 0\r\n sentinel known-replica mymaster 172.20.0.12 6379\r\n sentinel known-replica mymaster 172.20.0.11 6379\r\n```\r\nPlease verify also that each instance has its own copy to rewrite. For example sentinel number 3 will always uniquely rewrite sentinel3.conf, etc."
    },
    {
      "id": 1009399815,
      "user": "RomanKisilenko",
      "created_at": "2022-01-10T22:20:23Z",
      "body": "OK, thank you for your input.\r\n\r\nSo I made sure that configs persists across sentinel and redis restarts, that they are stored on different disks, and made sure both redis and sentinel can write to config now. Then I executed 3 tests as described below.\r\n\r\nTest 1. Stopping and restarting sentinels one by one. The cluster seems to be working in a stable manner.\r\nTest 2. Stopping and restarting redis replicas one by one. First masters, then slaves. The cluster seems to be working in a stable manner, failover works as expected.\r\nTest 3. Imitating server failures (stopping and restarting redises with sentinels). Few iterations went just fine and I even though that the issue is resolved. But after 4 complete iterations suddenly:\r\n```\r\nsentinel #1\r\nmaster0:name=shard2,status=sdown,address=172.20.0.2:30660,slaves=1,sentinels=3\r\nmaster1:name=shard3,status=sdown,address=172.20.0.2:30662,slaves=1,sentinels=3\r\nmaster2:name=primary,status=sdown,address=172.20.0.2:30692,slaves=1,sentinels=3\r\nmaster3:name=shard1,status=sdown,address=172.20.0.2:30658,slaves=1,sentinels=3\r\nsentinel #2\r\nmaster0:name=shard2,status=ok,address=172.20.0.2:30660,slaves=1,sentinels=3\r\nmaster1:name=shard1,status=ok,address=172.20.0.2:30658,slaves=1,sentinels=3\r\nmaster2:name=shard3,status=ok,address=172.20.0.2:30662,slaves=1,sentinels=3\r\nmaster3:name=primary,status=ok,address=172.20.0.2:30692,slaves=1,sentinels=3\r\nsentinel #3\r\nmaster0:name=shard2,status=sdown,address=172.20.0.2:30661,slaves=1,sentinels=3\r\nmaster1:name=shard1,status=sdown,address=172.20.0.2:30659,slaves=1,sentinels=3\r\nmaster2:name=primary,status=sdown,address=172.20.0.2:30693,slaves=1,sentinels=3\r\nmaster3:name=shard3,status=sdown,address=172.20.0.2:30663,slaves=1,sentinels=3\r\n```\r\nAnd cluster did not restore to the working state, even after all services are up.\r\n\r\nI think making redis config writable has fixed the issue when number of slaves was reported as 2. But the cluster failure issue is still there.\r\n\r\nI'm not sure if Test 2 indeed passes or I just did not try sufficiently many times for the issue to happen."
    },
    {
      "id": 1009422430,
      "user": "RomanKisilenko",
      "created_at": "2022-01-10T22:48:02Z",
      "body": "Update: while I was writing above message, the cluster did recover into a stable state. It just took longer, probably a few minutes on this iteration.\r\n\r\nI then imitated a few more failures and hit this issue again:\r\n```\r\nsentinel #1\r\nmaster0:name=shard3,status=sdown,address=172.20.0.2:30662,slaves=1,sentinels=3\r\nmaster1:name=primary,status=sdown,address=172.20.0.2:30692,slaves=1,sentinels=3\r\nmaster2:name=shard2,status=sdown,address=172.20.0.2:30660,slaves=1,sentinels=3\r\nmaster3:name=shard1,status=sdown,address=172.20.0.2:30658,slaves=1,sentinels=3\r\nsentinel #2\r\nError from server (NotFound): pods \"ps-rs2-0\" not found\r\nsentinel #3\r\nmaster0:name=shard2,status=sdown,address=172.20.0.2:30661,slaves=1,sentinels=3\r\nmaster1:name=shard1,status=sdown,address=172.20.0.2:30659,slaves=1,sentinels=3\r\nmaster2:name=primary,status=sdown,address=172.20.0.2:30693,slaves=1,sentinels=3\r\nmaster3:name=shard3,status=sdown,address=172.20.0.2:30663,slaves=1,sentinels=3\r\n```\r\n\r\nThen I waited 5 minutes and the cluster did not recover. (checked later that on this failure after waiting 15 minutes the cluster still not recovered)\r\n\r\nThen I restarted all stopped redises and sentinels and waited 10 minutes. Cluster recovered partially:\r\n```\r\nsentinel #1\r\nmaster0:name=shard3,status=ok,address=172.20.0.2:30663,slaves=1,sentinels=3\r\nmaster1:name=primary,status=ok,address=172.20.0.2:30693,slaves=1,sentinels=3\r\nmaster2:name=shard2,status=ok,address=172.20.0.2:30661,slaves=1,sentinels=3\r\nmaster3:name=shard1,status=odown,address=172.20.0.2:30658,slaves=1,sentinels=3\r\nsentinel #2\r\nmaster0:name=shard3,status=ok,address=172.20.0.2:30663,slaves=1,sentinels=3\r\nmaster1:name=shard2,status=ok,address=172.20.0.2:30661,slaves=1,sentinels=3\r\nmaster2:name=shard1,status=odown,address=172.20.0.2:30658,slaves=1,sentinels=3\r\nmaster3:name=primary,status=ok,address=172.20.0.2:30693,slaves=1,sentinels=3\r\nsentinel #3\r\nmaster0:name=shard2,status=ok,address=172.20.0.2:30661,slaves=1,sentinels=3\r\nmaster1:name=shard1,status=sdown,address=172.20.0.2:30659,slaves=1,sentinels=3\r\nmaster2:name=primary,status=ok,address=172.20.0.2:30693,slaves=1,sentinels=3\r\nmaster3:name=shard3,status=ok,address=172.20.0.2:30663,slaves=1,sentinels=3\r\n```\r\n\r\nAfter waiting a few more minutes cluster recovered fully."
    },
    {
      "id": 1010029388,
      "user": "moticless",
      "created_at": "2022-01-11T14:41:00Z",
      "body": "@RomanKisilenko \r\nPlease supply the configuration of all your components. I would like to review it and maybe try reproduce it.\r\n"
    },
    {
      "id": 1010290929,
      "user": "RomanKisilenko",
      "created_at": "2022-01-11T19:28:22Z",
      "body": "These are starting redis/sentinels configurations:\r\nMaster redis:\r\n```\r\nrequirepass pass\r\nmasterauth pass\r\n\r\nslave-announce-ip 172.20.0.2\r\nslave-announce-port 30692\r\n\r\nmaxclients 100000\r\ntimeout 300\r\n```\r\nSlave redis:\r\n```\r\nrequirepass pass\r\nmasterauth pass\r\n\r\nslaveof 172.20.0.2 30692\r\n\r\nslave-announce-ip 172.20.0.2\r\nslave-announce-port 30693\r\n\r\nmaxclients 100000\r\ntimeout 300\r\n```\r\nSentinel 1:\r\n```\r\nrequirepass pass\r\nsentinel monitor primary 172.20.0.2 30692 2\r\n\r\nsentinel down-after-milliseconds primary 5000\r\nsentinel failover-timeout primary 60000\r\nsentinel parallel-syncs primary 1\r\n\r\nsentinel auth-pass primary pass\r\nsentinel announce-ip 172.20.0.2\r\nsentinel announce-port 30664\r\n\r\nmaxclients 100000\r\ntimeout 300\r\n```\r\nSentinel 2:\r\n```\r\nrequirepass pass\r\nsentinel monitor primary 172.20.0.2 30692 2\r\n\r\nsentinel down-after-milliseconds primary 5000\r\nsentinel failover-timeout primary 60000\r\nsentinel parallel-syncs primary 1\r\n\r\nsentinel auth-pass primary pass\r\nsentinel announce-ip 172.20.0.2\r\nsentinel announce-port 30665\r\n\r\nmaxclients 100000\r\ntimeout 300\r\n```\r\nSentinel 3:\r\n```\r\nrequirepass pass\r\nsentinel monitor primary 172.20.0.2 30692 2\r\n\r\nsentinel down-after-milliseconds primary 5000\r\nsentinel failover-timeout primary 60000\r\nsentinel parallel-syncs primary 1\r\n\r\nsentinel auth-pass primary pass\r\nsentinel announce-ip 172.20.0.2\r\nsentinel announce-port 30666\r\n\r\nmaxclients 100000\r\ntimeout 300\r\n```"
    },
    {
      "id": 1010404245,
      "user": "moticless",
      "created_at": "2022-01-11T22:11:58Z",
      "body": "As it looks like you are working in NAT environment, which explains why you are declaring `sentinel announce-ip/port`. \r\nBut we also need to configure similarly for replicas : \r\n```\r\nreplica-announce-ip 5.5.5.5\r\nreplica-announce-port 1234\r\n```\r\nPlease read [here](https://redis.io/topics/replication) and try it out.\r\n"
    },
    {
      "id": 1011014188,
      "user": "RomanKisilenko",
      "created_at": "2022-01-12T12:52:13Z",
      "body": "I believe `replica` in redis config is an alias of `slave`, however I went forward and updated my configs with `replica` instead of `slave` and re-run the tests.\r\n\r\nJust as before:\r\n1. Stopping then restarting after some time just sentinels or just redis replicas produces correct failover and does not lead to an issue. I initiated 10 failovers, all went just fine.\r\n2. Stopping and then restarting both redis replicas and a sentinel (scenario with 3 failure domains for a redis cluster) again leads to an issue just like described above. This time after 3 successful failovers it produced the following cluster state:\r\n```\r\nsentinel #1\r\nmaster0:name=shard2,status=sdown,address=172.20.0.2:30660,slaves=1,sentinels=3\r\nmaster1:name=primary,status=sdown,address=172.20.0.2:30692,slaves=1,sentinels=3\r\nmaster2:name=shard3,status=sdown,address=172.20.0.2:30662,slaves=1,sentinels=3\r\nmaster3:name=shard1,status=ok,address=172.20.0.2:30658,slaves=1,sentinels=3\r\nsentinel #2\r\nError from server (NotFound): pods \"ps-rs2-0\" not found\r\nsentinel #3\r\nmaster0:name=shard3,status=sdown,address=172.20.0.2:30663,slaves=1,sentinels=3\r\nmaster1:name=shard1,status=ok,address=172.20.0.2:30658,slaves=1,sentinels=3\r\nmaster2:name=primary,status=sdown,address=172.20.0.2:30693,slaves=1,sentinels=3\r\nmaster3:name=shard2,status=sdown,address=172.20.0.2:30661,slaves=1,sentinels=3\r\n```\r\nAnd it did not start to recover during 10 minutes.\r\n\r\nTaking test 1 into account above I think that using at least 5 failure domains for deploying redis cluster (3 for sentinels and 2 for redis replicas) might be an appropriate workaround."
    },
    {
      "id": 1011042261,
      "user": "moticless",
      "created_at": "2022-01-12T13:23:59Z",
      "body": "Can it be that your instances might change IP on restart? (Sentinel based hostnames doesn't support dynamic IP unfortunately. Pease see pending Issue #9103)"
    },
    {
      "id": 1011047489,
      "user": "RomanKisilenko",
      "created_at": "2022-01-12T13:29:25Z",
      "body": "Yes, since this setup is running inside a kubernetes cluster, each of the instance (internal, from 10.x.x.x subnet) IP changes frequently when the service is restarted. However the announced IP-port pair never changes, the kubernetes cluster routing/serivices are configured properly for this.\r\nNote, that I do not use announce-hostnames, I use announce-ip everywhere instead."
    },
    {
      "id": 1011105150,
      "user": "hwware",
      "created_at": "2022-01-12T14:32:19Z",
      "body": "@RomanKisilenko How about you try to reproduce your issue in the non-docker environment with same redis and sentinel configurations?  If it always happens in the non-docker environment , it will be highly possible belonging to the redis issue. Otherwise,  we can hardly check if the issue comes from redis or docker side.  \r\n\r\nThanks. "
    },
    {
      "id": 1012076817,
      "user": "moticless",
      "created_at": "2022-01-13T12:07:03Z",
      "body": "@RomanKisilenko , I failed to reproduce this issue with the configuration you supplied. \r\nYou can find more details in [here](https://github.com/moticless/redis-sentinel-tls/tree/main/discovery-by-announced-ip-issue-9998). \r\n\r\n\r\n"
    },
    {
      "id": 1013516677,
      "user": "RomanKisilenko",
      "created_at": "2022-01-14T22:29:35Z",
      "body": "We can not seem to reproduce this using docker-compose as well."
    },
    {
      "id": 1015461136,
      "user": "hwware",
      "created_at": "2022-01-18T14:23:47Z",
      "body": "@RomanKisilenko  Can we close this issue now?  Thanks"
    },
    {
      "id": 1016574729,
      "user": "RomanKisilenko",
      "created_at": "2022-01-19T15:22:22Z",
      "body": "Please do whatever you are supposed to do. The issue still actual for me, however for now I worked around it by splitting sentinels to run on separate hosts. "
    },
    {
      "id": 1016819110,
      "user": "yossigo",
      "created_at": "2022-01-19T19:58:12Z",
      "body": "@hwware I don't think we should close this until we understand the root cause, it could be a configuration issue but also a bug.\r\n\r\n@RomanKisilenko I noticed that when you experience this issue, there's also a discrepancy between sentinels around the port assigned to the master. I suppose the addr/port we see is the one loaded from configuration and represents the correct state for this sentinel before shutting down (and before that master was failed over), but it is not clear what prevents this from being refreshed - a bug or a network issue.\r\n\r\nIs this something that can be easily reproduced on a k3s/microk8s/other small scale Kubernetes environment? If it is and you can provide the necessary yaml files I think it can greatly help to make progress with this."
    },
    {
      "id": 1018829112,
      "user": "RomanKisilenko",
      "created_at": "2022-01-21T20:17:33Z",
      "body": "Thank you, I'll try to setup an minimalistic example kubernetes cluster which can be used to reproduce the issue. It can take some time before I'll arrange some time to do it, but I hope I will be able to do it soon."
    },
    {
      "id": 1019562770,
      "user": "moticless",
      "created_at": "2022-01-23T20:40:07Z",
      "body": "@RomanKisilenko ,\r\nI checked today what happen to Sentinel cluster once adding latency to the network (200msec with `tc`). \r\nI managed after a few tries of failovers to reach 3 consecutive failures of leader election (`-failover-abort-not-elected`) which resemble the sentinel1 log that you showed [above](https://github.com/redis/redis/issues/9998#issuecomment-1001761995) (Follow repetitive logs `Next failover delay: I will not start a failover before...`).\r\n\r\nAs each sentinel waits for a random period of time and attempts the same procedure again, in case of collision, the randomness is fairly limited to 1 second (after waiting `failover-timeout`) have yet high probability to collision to reoccur, especially if there is network latency. \r\n\r\nSince you are working with heavyweight k8s whereas redis ci/cd (or my docker-compose env) are running on a single machine, it might explains why we have failed to reproduce it. \r\n\r\nOne thing you can try is to have shorter retries:\r\nsentinel down-after-milliseconds mymaster 5000\r\nsentinel failover-timeout mymaster 60000\r\n\r\nCan you try please also measure network latency of your system?\r\n\r\n"
    },
    {
      "id": 1023457676,
      "user": "moticless",
      "created_at": "2022-01-27T17:16:14Z",
      "body": "@RomanKisilenko, I think i managed to reproduce your issue. \r\n\r\nJust like in your scenario, based on consensus algorithm,  all sentinel instances, randomize a time to initiate their next attempt to become the leader of the group. But time after time, all raffled the same value. \r\n\r\nThe problem is with this line:  `srandom(time(NULL)^getpid());` which at start each instance set its own seed for randomization - but all have the same seed! It is rather easy to have same value in seconds between containers that spins up at the same time. and the PID it is always 1 in containers. "
    },
    {
      "id": 1023463058,
      "user": "RomanKisilenko",
      "created_at": "2022-01-27T17:21:55Z",
      "body": "Thank you, I guess these might be excellent news. And sorry for delay in response, I missed your previous message somehow. \r\nIf it is still matters, I think latency on my system should be extremely low as our staging kubernetes cluster I used to reproduce the issue on runs on a single server and CPU load is around 10%. \r\nIf you still want me to set up a test kubernetes cluster to reproduce the issue on, please let me know."
    },
    {
      "id": 1023465331,
      "user": "moticless",
      "created_at": "2022-01-27T17:24:13Z",
      "body": "There is no need. I will create PR on sunday. "
    },
    {
      "id": 1023587259,
      "user": "moticless",
      "created_at": "2022-01-27T19:56:31Z",
      "body": "The PR: https://github.com/redis/redis/pull/10197"
    },
    {
      "id": 1025090372,
      "user": "moticless",
      "created_at": "2022-01-30T07:53:44Z",
      "body": "@RomanKisilenko, on second thought, it can be helpful if you can also try reproduce with pr  https://github.com/redis/redis/pull/10197 to verify you are experiencing the same issue. \r\nThanks."
    },
    {
      "id": 1027237417,
      "user": "RomanKisilenko",
      "created_at": "2022-02-01T20:05:01Z",
      "body": "@moticless I guess I'll have to build a docker image with updated sentinel, because there is no one available. Are there any instruction on how can I do this?\r\nI can see there is 7.0-rc1 was tagged yesterday, so probably I can use it somehow."
    },
    {
      "id": 1027262375,
      "user": "yossigo",
      "created_at": "2022-02-01T20:34:21Z",
      "body": "@RomanKisilenko I see that the docker-library repository was updated to support 7.0-rc, I suppose the official Docker image should be up soon."
    },
    {
      "id": 1031894418,
      "user": "RomanKisilenko",
      "created_at": "2022-02-07T20:34:04Z",
      "body": "@yossigo I tested on redis 7.0-rc and was able to reproduce the issue again. After several failovers sentinels disagree on failover state and fail to agree on failover state when there is a majority of sentinels available (2 of 3).\r\n\r\n```\r\n26:X 07 Feb 2022 20:22:20.077 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\r\n26:X 07 Feb 2022 20:22:20.078 # Redis version=6.9.240, bits=64, commit=00000000, modified=0, pid=26, just started\r\n26:X 07 Feb 2022 20:22:20.078 # Configuration loaded\r\n26:X 07 Feb 2022 20:22:20.078 * monotonic clock: POSIX clock_gettime\r\n26:X 07 Feb 2022 20:22:20.082 * Running mode=sentinel, port=26379.\r\n26:X 07 Feb 2022 20:22:20.083 # Sentinel ID is 88b5d5327f31e14e0d52eafc8527cb18fae7589f\r\n26:X 07 Feb 2022 20:22:20.083 # +monitor master shard1 172.20.0.2 30658 quorum 2\r\n26:X 07 Feb 2022 20:22:20.083 # +monitor master shard3 172.20.0.2 30662 quorum 2\r\n26:X 07 Feb 2022 20:22:20.083 # +monitor master shard2 172.20.0.2 30660 quorum 2\r\n26:X 07 Feb 2022 20:22:20.083 # +monitor master primary 172.20.0.2 30692 quorum 2\r\n26:X 07 Feb 2022 20:22:45.104 # +sdown master shard1 172.20.0.2 30658\r\n26:X 07 Feb 2022 20:22:45.104 # +sdown master shard3 172.20.0.2 30662\r\n26:X 07 Feb 2022 20:22:45.104 # +sdown master shard2 172.20.0.2 30660\r\n26:X 07 Feb 2022 20:22:45.104 # +sdown master primary 172.20.0.2 30692\r\n26:X 07 Feb 2022 20:23:44.021 # +sdown sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ shard1 172.20.0.2 30658\r\n26:X 07 Feb 2022 20:23:44.021 # +sdown sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ shard3 172.20.0.2 30662\r\n26:X 07 Feb 2022 20:23:44.021 # +sdown sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ shard2 172.20.0.2 30660\r\n26:X 07 Feb 2022 20:23:44.021 # +sdown sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ primary 172.20.0.2 30692\r\n```\r\n\r\n```\r\n22:X 07 Feb 2022 20:11:28.931 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\r\n22:X 07 Feb 2022 20:11:28.931 # Redis version=6.9.240, bits=64, commit=00000000, modified=0, pid=22, just started\r\n22:X 07 Feb 2022 20:11:28.931 # Configuration loaded\r\n22:X 07 Feb 2022 20:11:28.960 * monotonic clock: POSIX clock_gettime\r\n22:X 07 Feb 2022 20:11:28.965 * Running mode=sentinel, port=26379.\r\n22:X 07 Feb 2022 20:11:28.969 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:11:28.969 # Sentinel ID is 3427e74d08e85a3cb5861d0e160d7bc85a0ecd74\r\n22:X 07 Feb 2022 20:11:28.969 # +monitor master shard3 172.20.0.2 30662 quorum 2\r\n22:X 07 Feb 2022 20:11:28.969 # +monitor master shard2 172.20.0.2 30660 quorum 2\r\n22:X 07 Feb 2022 20:11:28.969 # +monitor master shard1 172.20.0.2 30658 quorum 2\r\n22:X 07 Feb 2022 20:11:28.969 # +monitor master primary 172.20.0.2 30692 quorum 2\r\n22:X 07 Feb 2022 20:11:28.973 * +slave slave 172.20.0.2:30693 172.20.0.2 30693 @ primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:11:28.975 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:11:29.025 * +slave slave 172.20.0.2:30663 172.20.0.2 30663 @ shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:11:29.027 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:11:29.406 * +sentinel sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:11:29.409 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:11:29.409 * +sentinel sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:11:29.410 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:11:29.410 * +sentinel sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:11:29.412 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:11:29.412 * +sentinel sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:11:29.414 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:11:29.505 * +sentinel sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:11:29.507 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:11:29.507 * +sentinel sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:11:29.509 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:11:29.640 * +sentinel sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:11:29.642 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:11:29.642 * +sentinel sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:11:29.644 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:11:39.013 * +slave slave 172.20.0.2:30659 172.20.0.2 30659 @ shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:11:39.016 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:11:39.016 * +slave slave 172.20.0.2:30661 172.20.0.2 30661 @ shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:11:39.019 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:12:11.269 # +sdown master primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:12:11.325 # +odown master primary 172.20.0.2 30692 #quorum 2/2\r\n22:X 07 Feb 2022 20:12:11.325 # +new-epoch 1\r\n22:X 07 Feb 2022 20:12:11.325 # +try-failover master primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:12:11.330 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:12:11.330 # +vote-for-leader 3427e74d08e85a3cb5861d0e160d7bc85a0ecd74 1\r\n22:X 07 Feb 2022 20:12:11.335 # 33b6962d39b2aba5c849f76cdc83dcda59d06d94 voted for 3427e74d08e85a3cb5861d0e160d7bc85a0ecd74 1\r\n22:X 07 Feb 2022 20:12:11.431 # +elected-leader master primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:12:11.431 # +failover-state-select-slave master primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:12:11.498 # +selected-slave slave 172.20.0.2:30693 172.20.0.2 30693 @ primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:12:11.498 * +failover-state-send-slaveof-noone slave 172.20.0.2:30693 172.20.0.2 30693 @ primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:12:11.561 * +failover-state-wait-promotion slave 172.20.0.2:30693 172.20.0.2 30693 @ primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:12:11.633 # +sdown master shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:12:11.696 # +odown master shard1 172.20.0.2 30658 #quorum 2/2\r\n22:X 07 Feb 2022 20:12:11.696 # +new-epoch 2\r\n22:X 07 Feb 2022 20:12:11.696 # +try-failover master shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:12:11.699 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:12:11.699 # +vote-for-leader 3427e74d08e85a3cb5861d0e160d7bc85a0ecd74 2\r\n22:X 07 Feb 2022 20:12:11.704 # 33b6962d39b2aba5c849f76cdc83dcda59d06d94 voted for 3427e74d08e85a3cb5861d0e160d7bc85a0ecd74 2\r\n22:X 07 Feb 2022 20:12:11.790 # +elected-leader master shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:12:11.790 # +failover-state-select-slave master shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:12:11.857 # +selected-slave slave 172.20.0.2:30659 172.20.0.2 30659 @ shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:12:11.857 * +failover-state-send-slaveof-noone slave 172.20.0.2:30659 172.20.0.2 30659 @ shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:12:11.929 * +failover-state-wait-promotion slave 172.20.0.2:30659 172.20.0.2 30659 @ shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:12:12.361 # +sdown master shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:12:12.365 # 33b6962d39b2aba5c849f76cdc83dcda59d06d94 voted for 3427e74d08e85a3cb5861d0e160d7bc85a0ecd74 2\r\n22:X 07 Feb 2022 20:12:12.427 # +odown master shard2 172.20.0.2 30660 #quorum 2/2\r\n22:X 07 Feb 2022 20:12:12.427 # +new-epoch 3\r\n22:X 07 Feb 2022 20:12:12.427 # +try-failover master shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:12:12.430 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:12:12.430 # +vote-for-leader 3427e74d08e85a3cb5861d0e160d7bc85a0ecd74 3\r\n22:X 07 Feb 2022 20:12:12.433 # 33b6962d39b2aba5c849f76cdc83dcda59d06d94 voted for 3427e74d08e85a3cb5861d0e160d7bc85a0ecd74 3\r\n22:X 07 Feb 2022 20:12:12.488 # +elected-leader master shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:12:12.488 # +failover-state-select-slave master shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:12:12.492 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:12:12.492 # +promoted-slave slave 172.20.0.2:30659 172.20.0.2 30659 @ shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:12:12.492 # +failover-state-reconf-slaves master shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:12:12.495 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:12:12.495 # +promoted-slave slave 172.20.0.2:30693 172.20.0.2 30693 @ primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:12:12.495 # +failover-state-reconf-slaves master primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:12:12.565 # +selected-slave slave 172.20.0.2:30661 172.20.0.2 30661 @ shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:12:12.565 * +failover-state-send-slaveof-noone slave 172.20.0.2:30661 172.20.0.2 30661 @ shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:12:12.565 # +failover-end master shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:12:12.565 # +failover-end master primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:12:12.565 # +switch-master primary 172.20.0.2 30692 172.20.0.2 30693\r\n22:X 07 Feb 2022 20:12:12.566 * +slave slave 172.20.0.2:30692 172.20.0.2 30692 @ primary 172.20.0.2 30693\r\n22:X 07 Feb 2022 20:12:12.568 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:12:12.645 * +failover-state-wait-promotion slave 172.20.0.2:30661 172.20.0.2 30661 @ shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:12:12.646 # +switch-master shard1 172.20.0.2 30658 172.20.0.2 30659\r\n22:X 07 Feb 2022 20:12:12.646 * +slave slave 172.20.0.2:30658 172.20.0.2 30658 @ shard1 172.20.0.2 30659\r\n22:X 07 Feb 2022 20:12:12.648 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:12:13.223 # +sdown master shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:12:13.278 # +odown master shard3 172.20.0.2 30662 #quorum 2/2\r\n22:X 07 Feb 2022 20:12:13.278 # +new-epoch 4\r\n22:X 07 Feb 2022 20:12:13.278 # +try-failover master shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:12:13.281 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:12:13.281 # +vote-for-leader 3427e74d08e85a3cb5861d0e160d7bc85a0ecd74 4\r\n22:X 07 Feb 2022 20:12:13.285 # 33b6962d39b2aba5c849f76cdc83dcda59d06d94 voted for 3427e74d08e85a3cb5861d0e160d7bc85a0ecd74 4\r\n22:X 07 Feb 2022 20:12:13.353 # +elected-leader master shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:12:13.353 # +failover-state-select-slave master shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:12:13.408 # +selected-slave slave 172.20.0.2:30663 172.20.0.2 30663 @ shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:12:13.408 * +failover-state-send-slaveof-noone slave 172.20.0.2:30663 172.20.0.2 30663 @ shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:12:13.461 * +failover-state-wait-promotion slave 172.20.0.2:30663 172.20.0.2 30663 @ shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:12:13.465 # 33b6962d39b2aba5c849f76cdc83dcda59d06d94 voted for 3427e74d08e85a3cb5861d0e160d7bc85a0ecd74 4\r\n22:X 07 Feb 2022 20:12:13.542 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:12:13.542 # +promoted-slave slave 172.20.0.2:30661 172.20.0.2 30661 @ shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:12:13.542 # +failover-state-reconf-slaves master shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:12:13.616 # +failover-end master shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:12:13.616 # +switch-master shard2 172.20.0.2 30660 172.20.0.2 30661\r\n22:X 07 Feb 2022 20:12:13.616 * +slave slave 172.20.0.2:30660 172.20.0.2 30660 @ shard2 172.20.0.2 30661\r\n22:X 07 Feb 2022 20:12:13.619 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:12:14.297 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:12:14.297 # +promoted-slave slave 172.20.0.2:30663 172.20.0.2 30663 @ shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:12:14.297 # +failover-state-reconf-slaves master shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:12:14.352 # +failover-end master shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:12:14.352 # +switch-master shard3 172.20.0.2 30662 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:12:14.353 * +slave slave 172.20.0.2:30662 172.20.0.2 30662 @ shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:12:14.354 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:12:14.445 # +sdown sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:12:14.446 # +sdown sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ shard2 172.20.0.2 30661\r\n22:X 07 Feb 2022 20:12:14.446 # +sdown sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ shard1 172.20.0.2 30659\r\n22:X 07 Feb 2022 20:12:14.446 # +sdown sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ primary 172.20.0.2 30693\r\n22:X 07 Feb 2022 20:12:17.573 # +sdown slave 172.20.0.2:30692 172.20.0.2 30692 @ primary 172.20.0.2 30693\r\n22:X 07 Feb 2022 20:12:17.693 # +sdown slave 172.20.0.2:30658 172.20.0.2 30658 @ shard1 172.20.0.2 30659\r\n22:X 07 Feb 2022 20:12:18.663 # +sdown slave 172.20.0.2:30660 172.20.0.2 30660 @ shard2 172.20.0.2 30661\r\n22:X 07 Feb 2022 20:12:19.357 # +sdown slave 172.20.0.2:30662 172.20.0.2 30662 @ shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:13:12.093 # -sdown slave 172.20.0.2:30658 172.20.0.2 30658 @ shard1 172.20.0.2 30659\r\n22:X 07 Feb 2022 20:13:13.154 # -sdown slave 172.20.0.2:30660 172.20.0.2 30660 @ shard2 172.20.0.2 30661\r\n22:X 07 Feb 2022 20:13:14.103 # -sdown slave 172.20.0.2:30692 172.20.0.2 30692 @ primary 172.20.0.2 30693\r\n22:X 07 Feb 2022 20:13:17.170 # -sdown sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:13:17.170 # -sdown sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ shard2 172.20.0.2 30661\r\n22:X 07 Feb 2022 20:13:17.170 # -sdown sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ shard1 172.20.0.2 30659\r\n22:X 07 Feb 2022 20:13:17.170 # -sdown sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ primary 172.20.0.2 30693\r\n22:X 07 Feb 2022 20:13:17.899 # -sdown slave 172.20.0.2:30662 172.20.0.2 30662 @ shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:14:04.575 # +sdown master primary 172.20.0.2 30693\r\n22:X 07 Feb 2022 20:14:05.181 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:14:05.181 # +new-epoch 5\r\n22:X 07 Feb 2022 20:14:05.183 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:14:05.183 # +vote-for-leader 88b5d5327f31e14e0d52eafc8527cb18fae7589f 5\r\n22:X 07 Feb 2022 20:14:05.449 # +sdown master shard1 172.20.0.2 30659\r\n22:X 07 Feb 2022 20:14:05.601 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:14:05.601 # +new-epoch 6\r\n22:X 07 Feb 2022 20:14:05.603 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:14:05.603 # +vote-for-leader 88b5d5327f31e14e0d52eafc8527cb18fae7589f 6\r\n22:X 07 Feb 2022 20:14:05.645 # +odown master primary 172.20.0.2 30693 #quorum 2/2\r\n22:X 07 Feb 2022 20:14:05.645 # Next failover delay: I will not start a failover before Mon Feb  7 20:16:05 2022\r\n22:X 07 Feb 2022 20:14:06.115 # +sdown master shard2 172.20.0.2 30661\r\n22:X 07 Feb 2022 20:14:06.251 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:14:06.251 # +new-epoch 7\r\n22:X 07 Feb 2022 20:14:06.253 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:14:06.253 # +vote-for-leader 88b5d5327f31e14e0d52eafc8527cb18fae7589f 7\r\n22:X 07 Feb 2022 20:14:06.256 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:14:06.256 # +vote-for-leader 88b5d5327f31e14e0d52eafc8527cb18fae7589f 7\r\n22:X 07 Feb 2022 20:14:06.304 # +config-update-from sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ primary 172.20.0.2 30693\r\n22:X 07 Feb 2022 20:14:06.304 # +switch-master primary 172.20.0.2 30693 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:14:06.305 * +slave slave 172.20.0.2:30693 172.20.0.2 30693 @ primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:14:06.307 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:14:06.561 # +odown master shard1 172.20.0.2 30659 #quorum 2/2\r\n22:X 07 Feb 2022 20:14:06.561 # Next failover delay: I will not start a failover before Mon Feb  7 20:16:06 2022\r\n22:X 07 Feb 2022 20:14:06.618 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:14:06.618 # +vote-for-leader 88b5d5327f31e14e0d52eafc8527cb18fae7589f 7\r\n22:X 07 Feb 2022 20:14:06.618 # Next failover delay: I will not start a failover before Mon Feb  7 20:16:07 2022\r\n22:X 07 Feb 2022 20:14:06.708 # +config-update-from sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ shard1 172.20.0.2 30659\r\n22:X 07 Feb 2022 20:14:06.708 # +switch-master shard1 172.20.0.2 30659 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:14:06.708 * +slave slave 172.20.0.2:30659 172.20.0.2 30659 @ shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:14:06.710 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:14:06.893 # +sdown master shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:14:06.955 # +odown master shard3 172.20.0.2 30663 #quorum 2/2\r\n22:X 07 Feb 2022 20:14:06.955 # +new-epoch 8\r\n22:X 07 Feb 2022 20:14:06.955 # +try-failover master shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:14:06.958 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:14:06.958 # +vote-for-leader 3427e74d08e85a3cb5861d0e160d7bc85a0ecd74 8\r\n22:X 07 Feb 2022 20:14:06.961 # 88b5d5327f31e14e0d52eafc8527cb18fae7589f voted for 3427e74d08e85a3cb5861d0e160d7bc85a0ecd74 8\r\n22:X 07 Feb 2022 20:14:07.021 # +elected-leader master shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:14:07.021 # +failover-state-select-slave master shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:14:07.092 # +selected-slave slave 172.20.0.2:30662 172.20.0.2 30662 @ shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:14:07.092 * +failover-state-send-slaveof-noone slave 172.20.0.2:30662 172.20.0.2 30662 @ shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:14:07.148 * +failover-state-wait-promotion slave 172.20.0.2:30662 172.20.0.2 30662 @ shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:14:07.211 # +odown master shard2 172.20.0.2 30661 #quorum 2/2\r\n22:X 07 Feb 2022 20:14:07.211 # Next failover delay: I will not start a failover before Mon Feb  7 20:16:06 2022\r\n22:X 07 Feb 2022 20:14:07.214 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:14:07.214 # +promoted-slave slave 172.20.0.2:30662 172.20.0.2 30662 @ shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:14:07.214 # +failover-state-reconf-slaves master shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:14:07.283 # +failover-end master shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:14:07.283 # +switch-master shard3 172.20.0.2 30663 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:14:07.283 * +slave slave 172.20.0.2:30663 172.20.0.2 30663 @ shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:14:07.286 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:14:07.324 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:14:07.324 # +vote-for-leader 88b5d5327f31e14e0d52eafc8527cb18fae7589f 8\r\n22:X 07 Feb 2022 20:14:07.363 # Next failover delay: I will not start a failover before Mon Feb  7 20:16:07 2022\r\n22:X 07 Feb 2022 20:14:07.455 # +config-update-from sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ shard2 172.20.0.2 30661\r\n22:X 07 Feb 2022 20:14:07.455 # +switch-master shard2 172.20.0.2 30661 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:14:07.455 * +slave slave 172.20.0.2:30661 172.20.0.2 30661 @ shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:14:07.457 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:14:07.629 # +sdown sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:14:07.629 # +sdown sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:14:07.629 # +sdown sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:14:07.629 # +sdown sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:14:11.344 # +sdown slave 172.20.0.2:30693 172.20.0.2 30693 @ primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:14:11.729 # +sdown slave 172.20.0.2:30659 172.20.0.2 30659 @ shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:14:12.307 # +sdown slave 172.20.0.2:30663 172.20.0.2 30663 @ shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:14:12.462 # +sdown slave 172.20.0.2:30661 172.20.0.2 30661 @ shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:15:03.364 # -sdown slave 172.20.0.2:30693 172.20.0.2 30693 @ primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:15:04.835 # -sdown slave 172.20.0.2:30659 172.20.0.2 30659 @ shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:15:07.077 # -sdown slave 172.20.0.2:30661 172.20.0.2 30661 @ shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:15:07.379 # -sdown sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:15:07.379 # -sdown sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:15:07.379 # -sdown sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:15:07.379 # -sdown sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:15:14.473 # -sdown slave 172.20.0.2:30663 172.20.0.2 30663 @ shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:15:42.139 # +sdown master shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:15:42.313 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:15:42.313 # +new-epoch 9\r\n22:X 07 Feb 2022 20:15:42.315 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:15:42.315 # +vote-for-leader 88b5d5327f31e14e0d52eafc8527cb18fae7589f 9\r\n22:X 07 Feb 2022 20:15:42.847 # +sdown master shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:15:42.938 # +sdown master shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:15:42.938 # +odown master shard2 172.20.0.2 30660 #quorum 2/2\r\n22:X 07 Feb 2022 20:15:42.938 # +new-epoch 10\r\n22:X 07 Feb 2022 20:15:42.938 # +try-failover master shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:15:42.941 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:15:42.941 # +vote-for-leader 3427e74d08e85a3cb5861d0e160d7bc85a0ecd74 10\r\n22:X 07 Feb 2022 20:15:42.944 # 88b5d5327f31e14e0d52eafc8527cb18fae7589f voted for 3427e74d08e85a3cb5861d0e160d7bc85a0ecd74 10\r\n22:X 07 Feb 2022 20:15:42.945 # 33b6962d39b2aba5c849f76cdc83dcda59d06d94 voted for 3427e74d08e85a3cb5861d0e160d7bc85a0ecd74 10\r\n22:X 07 Feb 2022 20:15:43.000 # +elected-leader master shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:15:43.000 # +failover-state-select-slave master shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:15:43.059 # +selected-slave slave 172.20.0.2:30661 172.20.0.2 30661 @ shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:15:43.059 * +failover-state-send-slaveof-noone slave 172.20.0.2:30661 172.20.0.2 30661 @ shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:15:43.149 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:15:43.149 # +new-epoch 11\r\n22:X 07 Feb 2022 20:15:43.150 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:15:43.150 # +vote-for-leader 88b5d5327f31e14e0d52eafc8527cb18fae7589f 11\r\n22:X 07 Feb 2022 20:15:43.150 * +failover-state-wait-promotion slave 172.20.0.2:30661 172.20.0.2 30661 @ shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:15:43.251 # +odown master shard1 172.20.0.2 30658 #quorum 2/2\r\n22:X 07 Feb 2022 20:15:43.251 # Next failover delay: I will not start a failover before Mon Feb  7 20:17:42 2022\r\n22:X 07 Feb 2022 20:15:43.355 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:15:43.355 # +promoted-slave slave 172.20.0.2:30661 172.20.0.2 30661 @ shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:15:43.355 # +failover-state-reconf-slaves master shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:15:43.368 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:15:43.368 # +vote-for-leader 88b5d5327f31e14e0d52eafc8527cb18fae7589f 11\r\n22:X 07 Feb 2022 20:15:43.445 # +config-update-from sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ shard1 172.20.0.2 30658\r\n22:X 07 Feb 2022 20:15:43.445 # +switch-master shard1 172.20.0.2 30658 172.20.0.2 30659\r\n22:X 07 Feb 2022 20:15:43.446 * +slave slave 172.20.0.2:30658 172.20.0.2 30658 @ shard1 172.20.0.2 30659\r\n22:X 07 Feb 2022 20:15:43.448 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:15:43.453 # +failover-end master shard2 172.20.0.2 30660\r\n22:X 07 Feb 2022 20:15:43.454 # +switch-master shard2 172.20.0.2 30660 172.20.0.2 30661\r\n22:X 07 Feb 2022 20:15:43.454 * +slave slave 172.20.0.2:30660 172.20.0.2 30660 @ shard2 172.20.0.2 30661\r\n22:X 07 Feb 2022 20:15:43.456 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:15:44.072 # +odown master shard3 172.20.0.2 30662 #quorum 2/2\r\n22:X 07 Feb 2022 20:15:44.072 # Next failover delay: I will not start a failover before Mon Feb  7 20:17:43 2022\r\n22:X 07 Feb 2022 20:15:44.250 # +config-update-from sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ shard3 172.20.0.2 30662\r\n22:X 07 Feb 2022 20:15:44.250 # +switch-master shard3 172.20.0.2 30662 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:15:44.251 * +slave slave 172.20.0.2:30662 172.20.0.2 30662 @ shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:15:44.254 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:15:51.703 # +sdown master primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:15:51.794 # +odown master primary 172.20.0.2 30692 #quorum 2/2\r\n22:X 07 Feb 2022 20:15:51.794 # +new-epoch 12\r\n22:X 07 Feb 2022 20:15:51.794 # +try-failover master primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:15:51.814 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:15:51.814 # +vote-for-leader 3427e74d08e85a3cb5861d0e160d7bc85a0ecd74 12\r\n22:X 07 Feb 2022 20:15:51.820 # 88b5d5327f31e14e0d52eafc8527cb18fae7589f voted for 3427e74d08e85a3cb5861d0e160d7bc85a0ecd74 12\r\n22:X 07 Feb 2022 20:15:51.820 # 33b6962d39b2aba5c849f76cdc83dcda59d06d94 voted for 3427e74d08e85a3cb5861d0e160d7bc85a0ecd74 12\r\n22:X 07 Feb 2022 20:15:51.866 # +elected-leader master primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:15:51.866 # +failover-state-select-slave master primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:15:51.938 # +selected-slave slave 172.20.0.2:30693 172.20.0.2 30693 @ primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:15:51.938 * +failover-state-send-slaveof-noone slave 172.20.0.2:30693 172.20.0.2 30693 @ primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:15:52.000 * +failover-state-wait-promotion slave 172.20.0.2:30693 172.20.0.2 30693 @ primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:15:52.831 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:15:52.831 # +promoted-slave slave 172.20.0.2:30693 172.20.0.2 30693 @ primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:15:52.831 # +failover-state-reconf-slaves master primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:15:52.905 # +failover-end master primary 172.20.0.2 30692\r\n22:X 07 Feb 2022 20:15:52.905 # +switch-master primary 172.20.0.2 30692 172.20.0.2 30693\r\n22:X 07 Feb 2022 20:15:52.906 * +slave slave 172.20.0.2:30692 172.20.0.2 30692 @ primary 172.20.0.2 30693\r\n22:X 07 Feb 2022 20:15:52.909 * Sentinel new configuration saved on disk\r\n22:X 07 Feb 2022 20:16:43.692 * +fix-slave-config slave 172.20.0.2:30660 172.20.0.2 30660 @ shard2 172.20.0.2 30661\r\n22:X 07 Feb 2022 20:21:15.670 # +sdown slave 172.20.0.2:30692 172.20.0.2 30692 @ primary 172.20.0.2 30693\r\n22:X 07 Feb 2022 20:21:16.549 # +sdown slave 172.20.0.2:30658 172.20.0.2 30658 @ shard1 172.20.0.2 30659\r\n22:X 07 Feb 2022 20:21:17.236 # +sdown slave 172.20.0.2:30660 172.20.0.2 30660 @ shard2 172.20.0.2 30661\r\n22:X 07 Feb 2022 20:21:17.988 # +sdown slave 172.20.0.2:30662 172.20.0.2 30662 @ shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:21:18.792 # +sdown sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:21:18.792 # +sdown sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ shard2 172.20.0.2 30661\r\n22:X 07 Feb 2022 20:21:18.792 # +sdown sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ shard1 172.20.0.2 30659\r\n22:X 07 Feb 2022 20:21:18.792 # +sdown sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ primary 172.20.0.2 30693\r\n22:X 07 Feb 2022 20:22:09.655 * +reboot slave 172.20.0.2:30692 172.20.0.2 30692 @ primary 172.20.0.2 30693\r\n22:X 07 Feb 2022 20:22:09.721 # -sdown slave 172.20.0.2:30692 172.20.0.2 30692 @ primary 172.20.0.2 30693\r\n22:X 07 Feb 2022 20:22:11.342 * +reboot slave 172.20.0.2:30658 172.20.0.2 30658 @ shard1 172.20.0.2 30659\r\n22:X 07 Feb 2022 20:22:11.395 # -sdown slave 172.20.0.2:30658 172.20.0.2 30658 @ shard1 172.20.0.2 30659\r\n22:X 07 Feb 2022 20:22:18.225 * +reboot slave 172.20.0.2:30660 172.20.0.2 30660 @ shard2 172.20.0.2 30661\r\n22:X 07 Feb 2022 20:22:18.302 # -sdown slave 172.20.0.2:30660 172.20.0.2 30660 @ shard2 172.20.0.2 30661\r\n22:X 07 Feb 2022 20:22:18.302 * +reboot slave 172.20.0.2:30662 172.20.0.2 30662 @ shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:22:18.355 # -sdown slave 172.20.0.2:30662 172.20.0.2 30662 @ shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:22:21.161 # -sdown sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:22:21.161 # -sdown sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ shard2 172.20.0.2 30661\r\n22:X 07 Feb 2022 20:22:21.161 # -sdown sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ shard1 172.20.0.2 30659\r\n22:X 07 Feb 2022 20:22:21.161 # -sdown sentinel 88b5d5327f31e14e0d52eafc8527cb18fae7589f 172.20.0.2 30664 @ primary 172.20.0.2 30693\r\n22:X 07 Feb 2022 20:23:41.167 # +sdown master primary 172.20.0.2 30693\r\n22:X 07 Feb 2022 20:23:42.027 # +sdown master shard1 172.20.0.2 30659\r\n22:X 07 Feb 2022 20:23:42.532 # +sdown master shard2 172.20.0.2 30661\r\n22:X 07 Feb 2022 20:23:43.301 # +sdown master shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:23:44.003 # +sdown sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ shard3 172.20.0.2 30663\r\n22:X 07 Feb 2022 20:23:44.003 # +sdown sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ shard2 172.20.0.2 30661\r\n22:X 07 Feb 2022 20:23:44.003 # +sdown sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ shard1 172.20.0.2 30659\r\n22:X 07 Feb 2022 20:23:44.003 # +sdown sentinel 33b6962d39b2aba5c849f76cdc83dcda59d06d94 172.20.0.2 30665 @ primary 172.20.0.2 30693\r\n```\r\n\r\nPlease let me know what are suggested next steps. Should I collect additional info or should I try to setup a kubernetes cluster which can be used to reproduce the issue and which I can share with you."
    },
    {
      "id": 1032488028,
      "user": "moticless",
      "created_at": "2022-02-08T11:08:53Z",
      "body": "@RomanKisilenko \r\nLooks to me like we encounter another issue. Before sentinels failed to elect the leader, the one that will select a promoted slave, and we got multiple `-failover-abort-not-elected` printouts . Based on new logs  we do get `+elected-leader` as expected and never encounter `-failover-abort-not-elected`. \r\n\r\n\r\nSome highlights from your logs:\r\n\r\n**Sentinel 1 ID 88b5(...)**\r\n20:22:20.077 Restarts running and load from configuration master <shard1, 30658>\r\n20:22:45.104 Subjectively identifies that <shard1, 30658>, is down\r\n20:23:44.021 Subjectively identifies that sentinel 3 is down <shard1, 30658>\r\n\r\n**Sentinel 2 ID 3427(...)**\r\n20:15:43.445 +switch-master <shard1, 30658> to <shard1,30659> (Sentinel 3 is the one to update)\r\n20:21:18.792 Subjectively identifies that sentinel 1 is down <shard1, 30659>\r\n20:22:21.161 Subjectively identifies that sentinel 1 is up and running <shard1, 30659>\r\n20:23:44.003 Subjectively identifies that sentinel 3 is down <shard1, 30659>\r\n\r\n**Sentinel 3 ID 33b6(...)**\r\n??\r\n\r\n\r\n* I suspect that sentinel 1 loads obsolete configuration file (shard1, 30658) whereas earlier in time (20:15:43.445) everybody agreed on port 30659... which should have lead to update of configuration file. Please verify that configuration file has write permission and persisted over restarts.\r\n* Can it be that sentinel 3 is down while sentinel 1 restarts?\r\n\r\nCan you verify please these two bullets and collect once again all the logs before trying reproduce it on k8s. Please collect all the logs, from the beginning.\r\nThank you\r\n\r\n\r\n\r\n"
    },
    {
      "id": 1033013527,
      "user": "RomanKisilenko",
      "created_at": "2022-02-08T20:06:27Z",
      "body": "@moticless Hello, the sentinel 3 was up while sentinel 1 restarted. The sentinel 1 was down while sentinel 2 and 3 agreed on failover to 30659. Then Sentinel 1 restarted and loaded its last known configuration which was saved earlier when there was an agreement between sentinels to use 30658. Then sentinel 3 was shut down.\r\nDoes it explain what is going on in the test? Do you think I should adjust the way I'm testing this issue or should I proceed reproducing it again and collecting the logs?"
    },
    {
      "id": 1033662136,
      "user": "moticless",
      "created_at": "2022-02-09T11:31:04Z",
      "body": "Hi @RomanKisilenko, \r\napparently your test sound reasonable (though I still not sure at what timing replicas goes down and up in the flow). Obviously timing is playing a role here as some of the tries do succeed. \r\n\r\nIt is better to supply entire logs in order to have better understanding. It is best if you manage to supply somehow the steps to reproduce it. Thank you"
    },
    {
      "id": 1298410625,
      "user": "FlorianVielhauer",
      "created_at": "2022-11-01T11:59:30Z",
      "body": "Hi @moticless,\r\n\r\nI am currently struggling with the same/similar issue as @RomanKisilenko:\r\n\r\n- REDIS Version: 6.2.7 (no difference with 7.0.5)\r\n- Docker Swarm Cluster\r\n- Node 1: Sentinel-1 + Master\r\n- Node 2: Sentinel-2 + Replica-1\r\n- Node 3: Sentinel-3 + Replica-2\r\n- Conf files of REDIS + Sentinel are persisted between container restarts (because Docker Swarm reuses the containers)\r\n- Tested with both IPs + Hostnames (no difference)\r\n\r\nSteps to reproduce:\r\n\r\n1. Deploy stack and wait that everything is up and running\r\n2. Drain Node 1 (this will stop Sentinel-1 + Master)\r\n3. Wait until failover to Replica-1 happened (this works fine and stable!)\r\n4. Activate Node 1 and wait until Sentinel-1 and Master are up\r\n5. Drain Node 2 (this will stop Sentinel-2 + Replica-1)\r\n\r\nNow this happens:\r\n1. Failover to Master is performed\r\n2. Everything is stable for approx. 8 seconds - then suddenly the Master is demoted to replica\r\n3. Master tries to sync with itself all the time and the whole system is unusable because there are only slaves and no master\r\n\r\nThis happens every time - always after approx. 8s. When looking at the code I found the following lines in sentinel.c in lines 2710ff:\r\n\r\n```\r\n            /* A slave turned into a master. We want to force our view and\r\n             * reconfigure as slave. Wait some time after the change before\r\n             * going forward, to receive new configs if any. */\r\n            mstime_t wait_time = sentinel_publish_period*4;\r\n\r\n            if (!(ri->flags & SRI_PROMOTED) &&\r\n                 sentinelMasterLooksSane(ri->master) &&\r\n                 sentinelRedisInstanceNoDownFor(ri,wait_time) &&\r\n                 mstime() - ri->role_reported_time > wait_time)\r\n            {\r\n                int retval = sentinelSendSlaveOf(ri,ri->master->addr);\r\n                if (retval == C_OK)\r\n                    sentinelEvent(LL_NOTICE,\"+convert-to-slave\",ri,\"%@\");\r\n            }\r\n```\r\n\r\nThe `wait_time` is 8s as far as I can see. I also see in the log that `+convert-to-slave` is logged after master promotion:\r\n\r\n```\r\nredis-test_redis-sentinel-1.1.9mvnk4gunawv@bocoa-dev-ubuntu-1    | 1:X 01 Nov 2022 10:05:15.894 # +new-epoch 3\r\nredis-test_redis-sentinel-1.1.9mvnk4gunawv@bocoa-dev-ubuntu-1    | 1:X 01 Nov 2022 10:05:15.894 # +try-failover master mymaster 10.0.1.26 6379\r\nredis-test_redis-sentinel-1.1.9mvnk4gunawv@bocoa-dev-ubuntu-1    | 1:X 01 Nov 2022 10:05:15.899 # +vote-for-leader 7cd73a1b60b73995f3316bb1b578da1d2d8cd9f8 3\r\nredis-test_redis-sentinel-1.1.9mvnk4gunawv@bocoa-dev-ubuntu-1    | 1:X 01 Nov 2022 10:05:15.907 # 513e0245bc169c1ac1b3b9a7f733b72478771302 voted for 7cd73a1b60b73995f3316bb1b578da1d2d8cd9f8 3\r\nredis-test_redis-sentinel-1.1.9mvnk4gunawv@bocoa-dev-ubuntu-1    | 1:X 01 Nov 2022 10:05:15.976 # +elected-leader master mymaster 10.0.1.26 6379\r\nredis-test_redis-sentinel-1.1.9mvnk4gunawv@bocoa-dev-ubuntu-1    | 1:X 01 Nov 2022 10:05:15.976 # +failover-state-select-slave master mymaster 10.0.1.26 6379\r\nredis-test_redis-sentinel-1.1.9mvnk4gunawv@bocoa-dev-ubuntu-1    | 1:X 01 Nov 2022 10:05:16.034 # +selected-slave slave 10.0.1.17:6379 10.0.1.28 6379 @ mymaster 10.0.1.26 6379\r\nredis-test_redis-sentinel-1.1.9mvnk4gunawv@bocoa-dev-ubuntu-1    | 1:X 01 Nov 2022 10:05:16.034 * +failover-state-send-slaveof-noone slave 10.0.1.17:6379 10.0.1.28 6379 @ mymaster 10.0.1.26 6379\r\nredis-test_redis-sentinel-1.1.9mvnk4gunawv@bocoa-dev-ubuntu-1    | 1:X 01 Nov 2022 10:05:16.098 * +failover-state-wait-promotion slave 10.0.1.17:6379 10.0.1.28 6379 @ mymaster 10.0.1.26 6379\r\nredis-test_redis-sentinel-1.1.9mvnk4gunawv@bocoa-dev-ubuntu-1    | 1:X 01 Nov 2022 10:05:16.292 # +failover-state-reconf-slaves master mymaster 10.0.1.26 6379\r\nredis-test_redis-sentinel-1.1.9mvnk4gunawv@bocoa-dev-ubuntu-1    | 1:X 01 Nov 2022 10:05:16.292 # +promoted-slave slave 10.0.1.17:6379 10.0.1.28 6379 @ mymaster 10.0.1.26 6379\r\nredis-test_redis-sentinel-1.1.9mvnk4gunawv@bocoa-dev-ubuntu-1    | 1:X 01 Nov 2022 10:05:16.345 # +failover-end master mymaster 10.0.1.26 6379\r\nredis-test_redis-sentinel-1.1.9mvnk4gunawv@bocoa-dev-ubuntu-1    | 1:X 01 Nov 2022 10:05:16.345 # +switch-master mymaster 10.0.1.26 6379 10.0.1.28 6379\r\nredis-test_redis-sentinel-1.1.9mvnk4gunawv@bocoa-dev-ubuntu-1    | 1:X 01 Nov 2022 10:05:16.347 * +slave slave 10.0.1.26:6379 10.0.1.26 6379 @ mymaster 10.0.1.17 6379\r\nredis-test_redis-sentinel-1.1.9mvnk4gunawv@bocoa-dev-ubuntu-1    | 1:X 01 Nov 2022 10:05:16.347 * +slave slave 10.0.1.28:6379 10.0.1.28 6379 @ mymaster 10.0.1.17 6379\r\nredis-test_redis-sentinel-1.1.9mvnk4gunawv@bocoa-dev-ubuntu-1    | 1:X 01 Nov 2022 10:05:18.352 # +sdown slave 10.0.1.26:6379 10.0.1.26 6379 @ mymaster 10.0.1.17 6379\r\nredis-test_redis-sentinel-1.1.9mvnk4gunawv@bocoa-dev-ubuntu-1    | 1:X 01 Nov 2022 10:05:26.452 * +convert-to-slave slave 10.0.1.28:6379 10.0.1.28 6379 @ mymaster 10.0.1.17 6379\r\n```\r\n\r\nMaybe this gives a hint what's going on? What is the Sentinel waiting for within these 8s what is not happening?\r\n\r\nPlease find attached full logs and a docker-compose.yml with my setup. \r\n\r\nHints:\r\n1. Configure NODE1, NODE2, NODE3 in .env according to your environment \r\n2. Optional: Build the image in poll-sentinels\\ - it will simulate the client-side with a simple python script which polls the sentinels every second and tries to write a key into the current master - you can view the container log to see what's going on. I found it useful to see when the failovers are performed and when the issue appears - but it is optional\r\n3. Start the stack: `docker stack deploy -c <(docker-compose config) redis-test`\r\n4. Drain node 1: `docker node <node1> --availability drain`\r\n5. Wait for failover and start node 1: `docker node <node1> --availability active`\r\n6. Drain node 2: `docker node <node2> --availability drain`\r\n7. Wait a few seconds for the issue to appear\r\n \r\n[redis-failover-test.zip](https://github.com/redis/redis/files/9909526/redis-failover-test.zip)\r\n\r\n\r\n"
    },
    {
      "id": 1298415373,
      "user": "RomanKisilenko",
      "created_at": "2022-11-01T12:04:32Z",
      "body": "I can confirm that we are still experiencing this issue as well, however I did not have enough time so far to continue debugging it."
    },
    {
      "id": 1304607101,
      "user": "FlorianVielhauer",
      "created_at": "2022-11-05T18:30:42Z",
      "body": "After more debugging I finally found a solution which works for me.\r\n\r\nI built a custom sentinel based on 6.2.7 with the following changes:\r\n\r\n1. PR from @moticless  https://github.com/redis/redis/pull/11419/commits/f53aa88dad8d866f4bbe6df651775732d2eeb259 \r\n\r\n2. Skip `+convert-to-slave` when ri addr and master addr are equal:\r\n \r\n```diff\r\n--- a/src/sentinel.c\r\n+++ b/src/sentinel.c\r\n@@ -2738,9 +2738,14 @@ void sentinelRefreshInstanceInfo(sentinelRedisInstance *ri, const char *info) {\r\n                  sentinelRedisInstanceNoDownFor(ri,wait_time) &&\r\n                  mstime() - ri->role_reported_time > wait_time)\r\n             {\r\n-                int retval = sentinelSendSlaveOf(ri,ri->master->addr);\r\n-                if (retval == C_OK)\r\n-                    sentinelEvent(LL_NOTICE,\"+convert-to-slave\",ri,\"%@\");\r\n+                if(ri->addr != NULL && ri->master->addr != NULL && !sentinelAddrOrHostnameEqual(ri->addr, ri->master->addr)) {\r\n+                    int retval = sentinelSendSlaveOf(ri,ri->master->addr);\r\n+                    if (retval == C_OK)\r\n+                        sentinelEvent(LL_NOTICE,\"+convert-to-slave\",ri,\"%@\");\r\n+                }\r\n+                else {\r\n+                    serverLog(LL_WARNING,\"skip convert-to-slave because master and slave addr are equal\");\r\n+                }\r\n             }\r\n         }\r\n     }\r\n````\r\n\r\nNow the failover works fine and stable in all combinations. I tested with both hostname + ip annoucements.\r\n\r\nTo be honest I have no idea why the situation appears where the sentinel tries to make a slave with itself as master. I think it is related to dynamic IPs in the cluster environments but I am not sure. I also found a similar address check for the event `+fix-slave-config` so maybe my patch makes sense?\r\n\r\nIt would be great to get some feedback and thoughts. We now have a solution which works for us - but of course it would be great if the patch could be merged into the codebase. Of course only if it makes sense and doesn't break anything else."
    },
    {
      "id": 1306077045,
      "user": "hwware",
      "created_at": "2022-11-07T19:19:00Z",
      "body": "@moticless Hi Moti, could you please take a look this thread? Thanks"
    },
    {
      "id": 1324344347,
      "user": "MarBanW",
      "created_at": "2022-11-22T22:59:08Z",
      "body": "@moticless : Hi, any possibilities to check it deeper as I face same issue... "
    },
    {
      "id": 1324584044,
      "user": "moticless",
      "created_at": "2022-11-23T05:55:14Z",
      "body": "Hi, sorry for the late response. I will try to investigate it tomorrow. "
    },
    {
      "id": 1326867225,
      "user": "moticless",
      "created_at": "2022-11-24T22:18:09Z",
      "body": "Hi @FlorianVielhauer, \r\nFor a start, I tried to use your code and it looks like the ip might change after node restart. Even if your sentinel  \"resolve-hostnames yes\" , it looks like \"announce-hostnames no\". sentinel is (most probably) not resliliant to dynamic ip. if I get it right, then, then it is also required you to set  \"announce-hostnames yes\" such to resolve each time the ip and not keep hardcoded IPs."
    },
    {
      "id": 1327403157,
      "user": "FlorianVielhauer",
      "created_at": "2022-11-25T12:15:42Z",
      "body": "Hi @moticless,\r\n\r\nThanks for investigation.\r\n\r\nAs I wrote in my first comment I \"Tested with both IPs + Hostnames\" and it made \"no difference\" (sorry for not beeing specific enough with regards to announce-hostnames setting). \r\n\r\nAs a proof I tested once more with \"announce-hostnames yes\" and the behaviour is exactly the same as with ip announcements:\r\n\r\nApprox 8-10s after failover the new master is always demoted to be a slave with itself as a master:\r\n\r\n```\r\n25 Nov 2022 13:08:03.075 # +promoted-slave slave redis-slave-2:6379 redis-slave-2 6379 @ mymaster redis-master-1 6379\r\n...\r\n25 Nov 2022 13:08:13.245 * +convert-to-slave slave redis-slave-2:6379 redis-slave-2 6379 @ mymaster redis-slave-2 6379\r\n```\r\n\r\nThis doesn't happen with my patch/custom build as I filtered out the +convert-to-slave by checking if slave+master are identical (please see my second post for details).\r\n\r\nMy main question is: Why does REDIS want to demote the promoted master to be a slave with itself as new master at all? \r\n\r\nI attached my setup with announce-hostnames=yes and logs.\r\n\r\n[redis-failover-test-with-hostnames.zip](https://github.com/redis/redis/files/10092720/redis-failover-test-with-hostnames.zip)\r\n"
    },
    {
      "id": 1328218436,
      "user": "moticless",
      "created_at": "2022-11-27T10:43:29Z",
      "body": "Hi @FlorianVielhauer,\r\nOnly based on you logs (redis-failover-test-with-hostnames.zip), it looks like sentinels still reports IP rather than hostnames:\r\n```\r\n+sentinel sentinel 0b0686fc1ad4545f42d40f1573075a9b05b218e4 10.0.5.3 26379 @ mymaster redis-master-1 6379\r\n ```\r\nWe must take care that sentinels and replicas will use only hostnames, as long as static-IP is not possible. \r\n"
    },
    {
      "id": 1328322630,
      "user": "FlorianVielhauer",
      "created_at": "2022-11-27T19:29:09Z",
      "body": "Hi @moticless,\r\n\r\nI get your point, so I also configured sentinel announce-ip. \r\n\r\nNow the sentinels announce their own address using hostnames, but unfortunately the issue remains. After few seconds the promoted master gets demoted to slave with itself as the new master. \r\n\r\nPlease also note that in my setup I only shut down one node to reproduce the issue. The services which were running on this node are NOT restarted on any of the other nodes so all the IPs are kind of static. \r\n\r\nThis is different to my original \"steps to reproduce\" because with `announce-hostnames yes` the issue already appears after shutdown of node 1. This is 100% reproducible... so announce-hostnames made it even worse? \r\n\r\nI still don't understand why the `+convert-to-slave` event appears... \r\n\r\n```\r\nredis-test_redis-sentinel-2.1.pw1q6hwcxlz3@bocoa-dev-ubuntu-2    | 1:X 27 Nov 2022 20:08:01.820 * +sentinel sentinel d80d28ac04ec22e956f08d5ca838d05e7bd0b340 redis-sentinel-1 26379 @ mymaster redis-master-1 6379\r\n...\r\nredis-test_redis-sentinel-3.1.3qpo69t9p2hg@bocoa-dev-ubuntu-3    | 1:X 27 Nov 2022 20:09:19.060 # +promoted-slave slave redis-slave-2:6379 redis-slave-2 6379 @ mymaster redis-master-1 6379\r\n...\r\nredis-test_redis-sentinel-2.1.pw1q6hwcxlz3@bocoa-dev-ubuntu-2    | 1:X 27 Nov 2022 20:09:29.194 * +convert-to-slave slave redis-slave-2:6379 redis-slave-2 6379 @ mymaster redis-slave-2 6379\r\n```\r\n\r\nFull logs and configuration attached.\r\n\r\n[redis-failover-test-with-hostnames-2.zip](https://github.com/redis/redis/files/10099045/redis-failover-test-with-hostnames-2.zip)\r\n\r\n\r\n"
    },
    {
      "id": 1338225687,
      "user": "moticless",
      "created_at": "2022-12-05T21:53:28Z",
      "body": "Hi @FlorianVielhauer , \r\nI tried to run the docker-swarm setup with few `docker-machines` on virtualbox. Without any luck so far to bringup the setup...\r\nI am looking again now at your logs `sentinel-2.logs` and I see the following lines:\r\n\r\n```\r\n+switch-master mymaster redis-master-1 6379 redis-slave-2 6379\r\n+slave slave redis-slave-1:6379 redis-slave-1 6379 @ mymaster redis-slave-2 6379\r\n+slave slave redis-slave-2:6379 redis-slave-2 6379 @ mymaster redis-slave-2 6379\r\n\r\n```\r\nThe printouts of `+slave` can happen only at two places. Either when processing INFO response or on `sentinelResetMasterAndChangeAddress`. Those printout are because of the second option. Looks like \r\nthe trouble starts there in which it aggregates slaves from the old master struct, except to the promoted \r\none (makes continue):\r\n```\r\n    while((de = dictNext(di)) != NULL) {\r\n        sentinelRedisInstance *slave = dictGetVal(de);\r\n\r\n        if (sentinelAddrIsEqual(slave->addr,newaddr)) continue;\r\n        slaves[numslaves++] = dupSentinelAddr(slave->addr);\r\n    }\r\n```\r\n\r\nIf I get it right, the function `sentinelAddrIsEqual()` does bad job if IPs are not static, which leads to see\r\n`redis-slave-2` recorded as `+slave` of `redis-slave-2`. \r\n\r\nCan you try please replace all occurences of `sentinelAddrIsEqual()` with `sentinelAddrOrHostnameEqual`\r\nand see if it fixes your problem?\r\n "
    },
    {
      "id": 1338355463,
      "user": "FlorianVielhauer",
      "created_at": "2022-12-05T23:32:16Z",
      "body": "Hi @moticless,\r\n\r\n> Can you try please replace all occurences of sentinelAddrIsEqual() with sentinelAddrOrHostnameEqual and see if it fixes your problem?\r\n\r\nYes, it fixes the issue for me. Thank you very much. 👍 \r\n\r\nFinal two questions from my side:\r\n\r\n1. When could this patch be integrated into an official release?\r\n\r\n2. Off-Topic: Although I configured `failover-timeout=2000` and `down-after-milliseconds=2000` the failover from master to slave takes approx 22s. This is much longer than I had expected. Any tips/settings to speed this up? \r\n\r\n\r\n\r\n"
    },
    {
      "id": 1338893345,
      "user": "moticless",
      "created_at": "2022-12-06T07:17:37Z",
      "body": "1. I will create PR that will eventually reach the next release and probably also backport.\r\n2. Sentinel is not designed or optimized to make quick failovers and even configured intervals being  factored as precaution. Note that behind the Sentinel algorithm which is lightweight algorithm there is also actual Redis server failover which can be a heavy task. I am not sure if it is a good idea to challenge with too low values, and if you do, then you have to test it and fine tune it carefully to your own environment. There are few potential places to save time  but looking forward, I am not sure if it is in our priority. \r\n"
    }
  ]
}