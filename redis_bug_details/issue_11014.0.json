{
  "issue_number": 11014.0,
  "title": "[CRASH] Failure to rebalance on 7.0.0-rc2",
  "body": "Running redis 7.0.0-rc2 on ubuntu in AWS.\r\n\r\nDuring a rebalance operation after adding nodes, this particular master containing these slots failed continuously on each rebalance attempt of the cluster. \r\n\r\n```\r\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\r\n9412:M 18 Jul 2022 15:59:55.278 # === ASSERTION FAILED ===\r\n9412:M 18 Jul 2022 15:59:55.278 # ==> blocked.c:624 'server.also_propagate.numops == 0' is not true\r\n\r\n------ STACK TRACE ------\r\n\r\nBacktrace:\r\n/usr/local/bin/redis-server *:6379 [cluster](handleClientsBlockedOnKeys+0x254)[0x4cecb4]\r\n/usr/local/bin/redis-server *:6379 [cluster](beforeSleep+0xe8)[0x446c58]\r\n/usr/local/bin/redis-server *:6379 [cluster](aeProcessEvents+0x3aa)[0x442b2a]\r\n/usr/local/bin/redis-server *:6379 [cluster](aeMain+0x1d)[0x442d4d]\r\n/usr/local/bin/redis-server *:6379 [cluster](main+0x409)[0x43ecc9]\r\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7fa725305840]\r\n/usr/local/bin/redis-server *:6379 [cluster](_start+0x29)[0x43f099]\r\n\r\n------ INFO OUTPUT ------\r\n# Server\r\nredis_version:6.9.242\r\nredis_git_sha1:00000000\r\nredis_git_dirty:0\r\nredis_build_id:50a29168b229999b\r\nredis_mode:cluster\r\nos:Linux 4.4.0-1060-aws x86_64\r\narch_bits:64\r\nmultiplexing_api:epoll\r\natomicvar_api:c11-builtin\r\ngcc_version:5.4.0\r\nprocess_id:9412\r\nprocess_supervised:no\r\nrun_id:5762aa4fe3d56afa0a26fa03a8dc80727c19652a\r\ntcp_port:6379\r\nserver_time_usec:1658159995278425\r\nuptime_in_seconds:7266858\r\nuptime_in_days:84\r\nhz:10\r\nconfigured_hz:10\r\nlru_clock:13992827\r\nexecutable:/usr/local/bin/redis-server\r\nconfig_file:/etc/redis/6379.conf\r\nio_threads_active:0\r\n\r\n# Clients\r\nconnected_clients:1341\r\ncluster_connections:70\r\nmaxclients:150000\r\nclient_recent_max_input_buffer:20480\r\nclient_recent_max_output_buffer:20504\r\nblocked_clients:0\r\ntracking_clients:0\r\nclients_in_timeout_table:0\r\n\r\n# Memory\r\nused_memory:45638469256\r\nused_memory_human:42.50G\r\nused_memory_rss:57257009152\r\nused_memory_rss_human:53.32G\r\nused_memory_peak:59139479568\r\nused_memory_peak_human:55.08G\r\nused_memory_peak_perc:77.17%\r\nused_memory_overhead:29487727300\r\nused_memory_startup:9423688\r\nused_memory_dataset:16150741956\r\nused_memory_dataset_perc:35.40%\r\nallocator_allocated:45639175592\r\nallocator_active:56496828416\r\nallocator_resident:57283624960\r\ntotal_system_memory:133661978624\r\ntotal_system_memory_human:124.48G\r\nused_memory_lua:37888\r\nused_memory_vm_eval:37888\r\nused_memory_lua_human:37.00K\r\nused_memory_scripts_eval:0\r\nnumber_of_cached_scripts:0\r\nnumber_of_functions:0\r\nnumber_of_libraries:0\r\nused_memory_vm_functions:37888\r\nused_memory_vm_total:75776\r\nused_memory_vm_total_human:74.00K\r\nused_memory_functions:184\r\nused_memory_scripts:184\r\nused_memory_scripts_human:184B\r\nmaxmemory:106929582899\r\nmaxmemory_human:99.59G\r\nmaxmemory_policy:noeviction\r\nallocator_frag_ratio:1.24\r\nallocator_frag_bytes:10857652824\r\nallocator_rss_ratio:1.01\r\nallocator_rss_bytes:786796544\r\nrss_overhead_ratio:1.00\r\nrss_overhead_bytes:-26615808\r\nmem_fragmentation_ratio:1.25\r\nmem_fragmentation_bytes:11618329144\r\nmem_not_counted_for_evict:3584\r\nmem_replication_backlog:104858916\r\nmem_total_replication_buffers:105206024\r\nmem_clients_slaves:348424\r\nmem_clients_normal:7086552\r\nmem_cluster_links:76160\r\nmem_aof_buffer:3584\r\nmem_allocator:jemalloc-5.2.1\r\nactive_defrag_running:0\r\nlazyfree_pending_objects:0\r\nlazyfreed_objects:10423\r\n\r\n# Persistence\r\nloading:0\r\nasync_loading:0\r\ncurrent_cow_peak:0\r\ncurrent_cow_size:0\r\ncurrent_cow_size_age:0\r\ncurrent_fork_perc:0.00\r\ncurrent_save_keys_processed:0\r\ncurrent_save_keys_total:0\r\nrdb_changes_since_last_save:21839092757\r\nrdb_bgsave_in_progress:0\r\nrdb_last_save_time:1650893137\r\nrdb_last_bgsave_status:ok\r\nrdb_last_bgsave_time_sec:410\r\nrdb_current_bgsave_time_sec:-1\r\nrdb_saves:0\r\nrdb_last_cow_size:6788845568\r\nrdb_last_load_keys_expired:0\r\nrdb_last_load_keys_loaded:120527186\r\naof_enabled:1\r\naof_rewrite_in_progress:0\r\naof_rewrite_scheduled:0\r\naof_last_rewrite_time_sec:433\r\naof_current_rewrite_time_sec:-1\r\naof_last_bgrewrite_status:ok\r\naof_rewrites:1626\r\naof_last_write_status:ok\r\naof_last_cow_size:31108997120\r\nmodule_fork_in_progress:0\r\nmodule_fork_last_cow_size:0\r\naof_current_size:16708212897\r\naof_base_size:15920075489\r\naof_pending_rewrite:0\r\naof_buffer_length:0\r\naof_pending_bio_fsync:0\r\naof_delayed_fsync:2\r\n\r\n# Stats\r\ntotal_connections_received:1493483\r\ntotal_commands_processed:21833443018\r\ninstantaneous_ops_per_sec:4734\r\ntotal_net_input_bytes:1661867530597\r\ntotal_net_output_bytes:21996788022\r\ninstantaneous_input_kbps:881.31\r\ninstantaneous_output_kbps:1865.17\r\nrejected_connections:0\r\nsync_full:1\r\nsync_partial_ok:1\r\nsync_partial_err:1\r\nexpired_keys:1009194\r\nexpired_stale_perc:11.69\r\nexpired_time_cap_reached_count:0\r\nexpire_cycle_cpu_milliseconds:4354\r\nevicted_keys:0\r\nevicted_clients:0\r\ntotal_eviction_exceeded_time:0\r\ncurrent_eviction_exceeded_time:0\r\nkeyspace_hits:18147550\r\nkeyspace_misses:4305787\r\npubsub_channels:0\r\npubsub_patterns:0\r\nlatest_fork_usec:792783\r\ntotal_forks:1627\r\nmigrate_cached_sockets:1\r\nslave_expires_tracked_keys:0\r\nactive_defrag_hits:0\r\nactive_defrag_misses:0\r\nactive_defrag_key_hits:0\r\nactive_defrag_key_misses:0\r\ntotal_active_defrag_time:0\r\ncurrent_active_defrag_time:0\r\ntracking_total_keys:0\r\ntracking_total_items:0\r\ntracking_total_prefixes:0\r\nunexpected_error_replies:0\r\ntotal_error_replies:1139\r\ndump_payload_sanitizations:0\r\ntotal_reads_processed:10690047223\r\ntotal_writes_processed:34801822\r\nio_threaded_reads_processed:0\r\nio_threaded_writes_processed:0\r\nreply_buffer_shrinks:27565\r\nreply_buffer_expands:3151\r\n\r\n# Replication\r\nrole:master\r\nconnected_slaves:2\r\nmin_slaves_good_slaves:2\r\nslave0:ip=20.0.83.95,port=6379,state=online,offset=44091096078,lag=0\r\nslave1:ip=20.0.83.12,port=6379,state=online,offset=44091048369,lag=0\r\nmaster_failover_state:no-failover\r\nmaster_replid:440b19326917cca6521c88c32af2eaa8336145c9\r\nmaster_replid2:f7e7603168b44d0a9db4fdd96cf1b907beb3ea41\r\nmaster_repl_offset:44091170424\r\nsecond_repl_offset:43585485265\r\nrepl_backlog_active:1\r\nrepl_backlog_size:104857600\r\nrepl_backlog_first_byte_offset:43986311517\r\nrepl_backlog_histlen:104858908\r\n\r\n# CPU\r\nused_cpu_sys:137268.968000\r\nused_cpu_user:180039.168000\r\nused_cpu_sys_children:39060.212000\r\nused_cpu_user_children:782172.496000\r\nused_cpu_sys_main_thread:133114.124000\r\nused_cpu_user_main_thread:179873.748000\r\n\r\n# Modules\r\n\r\n# Commandstats\r\ncmdstat_quit:calls=125459,usec=50132,usec_per_call=0.40,rejected_calls=0,failed_calls=0\r\ncmdstat_function|dump:calls=3,usec=10,usec_per_call=3.33,rejected_calls=0,failed_calls=0\r\ncmdstat_config|get:calls=484333,usec=1952954,usec_per_call=4.03,rejected_calls=0,failed_calls=0\r\ncmdstat_client|setname:calls=29306,usec=71317,usec_per_call=2.43,rejected_calls=0,failed_calls=0\r\ncmdstat_psync:calls=2,usec=792987,usec_per_call=396493.50,rejected_calls=0,failed_calls=0\r\ncmdstat_select:calls=7,usec=7,usec_per_call=1.00,rejected_calls=0,failed_calls=0\r\ncmdstat_dbsize:calls=21,usec=20,usec_per_call=0.95,rejected_calls=0,failed_calls=0\r\ncmdstat_readonly:calls=24241,usec=13403,usec_per_call=0.55,rejected_calls=0,failed_calls=0\r\ncmdstat_replconf:calls=2367,usec=1441,usec_per_call=0.61,rejected_calls=0,failed_calls=0\r\ncmdstat_get:calls=16108776,usec=52277307,usec_per_call=3.25,rejected_calls=137,failed_calls=0\r\ncmdstat_ping:calls=838630,usec=269615,usec_per_call=0.32,rejected_calls=32,failed_calls=0\r\ncmdstat_multi:calls=1095071,usec=216485,usec_per_call=0.20,rejected_calls=0,failed_calls=0\r\ncmdstat_migrate:calls=228217,usec=44765074,usec_per_call=196.15,rejected_calls=0,failed_calls=2\r\ncmdstat_del:calls=10735503943,usec=51089041306,usec_per_call=4.76,rejected_calls=0,failed_calls=0\r\ncmdstat_info:calls=1098655,usec=72714853,usec_per_call=66.19,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|nodes:calls=776,usec=85949,usec_per_call=110.76,rejected_calls=3,failed_calls=0\r\ncmdstat_cluster|forget:calls=31,usec=2708,usec_per_call=87.35,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|countkeysinslot:calls=4,usec=7,usec_per_call=1.75,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|info:calls=484335,usec=20396392,usec_per_call=42.11,rejected_calls=11,failed_calls=0\r\ncmdstat_cluster|getkeysinslot:calls=228229,usec=1237624,usec_per_call=5.42,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|replicate:calls=2,usec=226,usec_per_call=113.00,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|setslot:calls=30,usec=109,usec_per_call=3.63,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|meet:calls=1,usec=12,usec_per_call=12.00,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|reset:calls=1,usec=232112873,usec_per_call=232112880.00,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|slots:calls=4517,usec=338211,usec_per_call=74.88,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|delslots:calls=2,usec=55,usec_per_call=27.50,rejected_calls=0,failed_calls=0\r\ncmdstat_debug:calls=0,usec=0,usec_per_call=0.00,rejected_calls=2,failed_calls=0\r\ncmdstat_set:calls=11076073412,usec=55942994477,usec_per_call=5.05,rejected_calls=952,failed_calls=0\r\ncmdstat_restore-asking:calls=17576,usec=39680,usec_per_call=2.26,rejected_calls=0,failed_calls=0\r\ncmdstat_exec:calls=1095071,usec=49235816,usec_per_call=44.96,rejected_calls=0,failed_calls=0\r\n\r\n# Errorstats\r\nerrorstat_ASK:count=161\r\nerrorstat_ERR:count=4\r\nerrorstat_LOADING:count=46\r\nerrorstat_MOVED:count=928\r\n\r\n# Latencystats\r\nlatency_percentiles_usec_quit:p50=0.001,p99=6.015,p99.9=9.023\r\nlatency_percentiles_usec_function|dump:p50=3.007,p99=4.015,p99.9=4.015\r\nlatency_percentiles_usec_config|get:p50=3.007,p99=13.055,p99.9=74.239\r\nlatency_percentiles_usec_client|setname:p50=2.007,p99=10.047,p99.9=22.015\r\nlatency_percentiles_usec_psync:p50=39.167,p99=796917.759,p99.9=796917.759\r\nlatency_percentiles_usec_select:p50=1.003,p99=2.007,p99.9=2.007\r\nlatency_percentiles_usec_dbsize:p50=1.003,p99=2.007,p99.9=2.007\r\nlatency_percentiles_usec_readonly:p50=1.003,p99=2.007,p99.9=9.023\r\nlatency_percentiles_usec_replconf:p50=1.003,p99=1.003,p99.9=6.015\r\nlatency_percentiles_usec_get:p50=3.007,p99=10.047,p99.9=21.119\r\nlatency_percentiles_usec_ping:p50=0.001,p99=4.015,p99.9=11.007\r\nlatency_percentiles_usec_multi:p50=0.001,p99=1.003,p99.9=10.047\r\nlatency_percentiles_usec_migrate:p50=214.015,p99=280.575,p99.9=2883.583\r\nlatency_percentiles_usec_del:p50=4.015,p99=19.071,p99.9=44.031\r\nlatency_percentiles_usec_info:p50=64.255,p99=116.223,p99.9=187.391\r\nlatency_percentiles_usec_cluster|nodes:p50=105.471,p99=218.111,p99.9=242.687\r\nlatency_percentiles_usec_cluster|forget:p50=89.087,p99=122.367,p99.9=122.367\r\nlatency_percentiles_usec_cluster|countkeysinslot:p50=2.007,p99=2.007,p99.9=2.007\r\nlatency_percentiles_usec_cluster|info:p50=40.191,p99=65.023,p99.9=79.359\r\nlatency_percentiles_usec_cluster|getkeysinslot:p50=5.023,p99=10.047,p99.9=15.039\r\nlatency_percentiles_usec_cluster|replicate:p50=87.039,p99=139.263,p99.9=139.263\r\nlatency_percentiles_usec_cluster|setslot:p50=3.007,p99=6.015,p99.9=6.015\r\nlatency_percentiles_usec_cluster|meet:p50=12.031,p99=12.031,p99.9=12.031\r\nlatency_percentiles_usec_cluster|reset:p50=1002438.655,p99=1002438.655,p99.9=1002438.655\r\nlatency_percentiles_usec_cluster|slots:p50=59.135,p99=225.279,p99.9=323.583\r\nlatency_percentiles_usec_cluster|delslots:p50=20.095,p99=35.071,p99.9=35.071\r\nlatency_percentiles_usec_set:p50=3.007,p99=25.087,p99.9=145.407\r\nlatency_percentiles_usec_restore-asking:p50=2.007,p99=11.007,p99.9=22.015\r\nlatency_percentiles_usec_exec:p50=40.191,p99=101.375,p99.9=137.215\r\n\r\n# Cluster\r\ncluster_enabled:1\r\n\r\n# Keyspace\r\ndb0:keys=259699940,expires=259699940,avg_ttl=91233303\r\n\r\n------ CLIENT LIST OUTPUT ------\r\n...\r\n\r\n------ MODULES INFO OUTPUT ------\r\n\r\n------ CONFIG DEBUG OUTPUT ------\r\nsanitize-dump-payload no\r\nrepl-diskless-sync yes\r\nslave-read-only yes\r\nio-threads-do-reads no\r\nlazyfree-lazy-server-del no\r\nclient-query-buffer-limit 1gb\r\nproto-max-bulk-len 512mb\r\nlist-compress-depth 0\r\nreplica-read-only yes\r\nlazyfree-lazy-user-flush no\r\nio-threads 1\r\nrepl-diskless-load disabled\r\nlazyfree-lazy-expire no\r\nlazyfree-lazy-user-del no\r\nactivedefrag no\r\nlazyfree-lazy-eviction no\r\n\r\n------ FAST MEMORY TEST ------\r\n9412:M 18 Jul 2022 15:59:55.339 # Bio thread for job type #0 terminated\r\n9412:M 18 Jul 2022 15:59:55.339 # Bio thread for job type #1 terminated\r\n9412:M 18 Jul 2022 15:59:55.339 # Bio thread for job type #2 terminated\r\n*** Preparing to test memory region 893000 (2297856 bytes)\r\n*** Preparing to test memory region 2554000 (135168 bytes)\r\n*** Preparing to test memory region 7f95cc800000 (8388608 bytes)\r\n*** Preparing to test memory region 7f95cd000000 (28169080832 bytes)\r\n*** Preparing to test memory region 7f9c60200000 (2097152 bytes)\r\n*** Preparing to test memory region 7f9c60600000 (8388608 bytes)\r\n*** Preparing to test memory region 7f9c60e00000 (2097152 bytes)\r\n*** Preparing to test memory region 7f9c61200000 (8388608 bytes)\r\n*** Preparing to test memory region 7f9c61a00000 (46172995584 bytes)\r\n*** Preparing to test memory region 7fa721cfd000 (8388608 bytes)\r\n*** Preparing to test memory region 7fa7224fe000 (8388608 bytes)\r\n*** Preparing to test memory region 7fa722cff000 (8388608 bytes)\r\n*** Preparing to test memory region 7fa723500000 (8388608 bytes)\r\n*** Preparing to test memory region 7fa723d00000 (22020096 bytes)\r\n*** Preparing to test memory region 7fa7256ab000 (16384 bytes)\r\n*** Preparing to test memory region 7fa7258c8000 (16384 bytes)\r\n*** Preparing to test memory region 7fa7261f5000 (24576 bytes)\r\n*** Preparing to test memory region 7fa726208000 (4096 bytes)\r\n.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O\r\nFast memory test PASSED, however your memory can still be broken. Please run a memory test for several hours if possible.\r\n\r\n=== REDIS BUG REPORT END. Make sure to include from START to END. ===\r\n```\r\n",
  "state": "closed",
  "created_at": "2022-07-19T13:41:04Z",
  "updated_at": "2022-08-24T16:39:17Z",
  "closed_at": "2022-08-24T16:39:17Z",
  "labels": [],
  "comments_data": [
    {
      "id": 1189097703,
      "user": "guybe7",
      "created_at": "2022-07-19T14:05:41Z",
      "body": "@tbream are you using modules?"
    },
    {
      "id": 1189144729,
      "user": "tbream",
      "created_at": "2022-07-19T14:44:02Z",
      "body": "@guybe7, we are not using any modules."
    },
    {
      "id": 1189220926,
      "user": "guybe7",
      "created_at": "2022-07-19T15:48:09Z",
      "body": "@tbream can you please provide some 20-30 log lines before the crash report?"
    },
    {
      "id": 1189370339,
      "user": "tbream",
      "created_at": "2022-07-19T17:35:39Z",
      "body": "@guybe7 here is the log lines before the crash report.\r\n\r\n```\r\n9412:S 18 Jul 2022 15:07:24.073 * Background append only file rewriting started by pid 9385\r\n9385:C 18 Jul 2022 15:14:34.293 * SYNC append only file rewrite performed\r\n9385:C 18 Jul 2022 15:14:34.936 * Fork CoW for AOF rewrite: current 29667 MB, peak 29667 MB, average 19238 MB\r\n9412:S 18 Jul 2022 15:14:37.533 * Background AOF rewrite terminated with success\r\n9412:S 18 Jul 2022 15:14:37.536 * Removing the history file appendonly.aof.1626.incr.aof in the background\r\n9412:S 18 Jul 2022 15:14:37.536 * Removing the history file appendonly.aof.1626.base.rdb in the background\r\n9412:S 18 Jul 2022 15:14:37.540 * Background AOF rewrite finished successfully\r\n9412:S 18 Jul 2022 15:24:53.532 * FAIL message received from b377ebf66a3453ab8f050dcee11576d1e4ccb369 about 002132b04826f925bd1fda29366e5732767f975d\r\n9412:S 18 Jul 2022 15:24:53.592 # Start of election delayed for 553 milliseconds (rank #0, offset 43585485264).\r\n9412:S 18 Jul 2022 15:24:54.194 # Starting a failover election for epoch 103.\r\n9412:S 18 Jul 2022 15:24:54.202 # Failover election won: I'm the new master.\r\n9412:S 18 Jul 2022 15:24:54.202 # configEpoch set to 103 after successful failover\r\n9412:M 18 Jul 2022 15:24:54.202 # Connection with master lost.\r\n9412:M 18 Jul 2022 15:24:54.202 * Caching the disconnected master state.\r\n9412:M 18 Jul 2022 15:24:54.202 * Discarding previously cached master state.\r\n9412:M 18 Jul 2022 15:24:54.202 # Setting secondary replication ID to f7e7603168b44d0a9db4fdd96cf1b907beb3ea41, valid up to offset: 43585485265. New replication ID is 440b19326917cca6521c88c32af2eaa8336145c9\r\n9412:M 18 Jul 2022 15:24:54.210 * Replica 20.0.83.95:6379 asks for synchronization\r\n9412:M 18 Jul 2022 15:24:54.210 * Partial resynchronization request from 20.0.83.95:6379 accepted. Sending 0 bytes of backlog starting from offset 43585485265.\r\n9412:M 18 Jul 2022 15:35:02.692 * Replica 20.0.83.12:6379 asks for synchronization\r\n9412:M 18 Jul 2022 15:35:02.692 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for '46afee26b05b5b65a75e0a1df05bb93b36f9226b', my replication IDs are '440b19326917cca6521c88c32af2eaa8336145c9' and 'f7e7603168b44d0a9db4fdd96cf1b907beb3ea41')\r\n9412:M 18 Jul 2022 15:35:02.692 * Starting BGSAVE for SYNC with target: replicas sockets\r\n9412:M 18 Jul 2022 15:35:03.484 * Background RDB transfer started by pid 12787\r\n9412:M 18 Jul 2022 15:35:03.585 * Clear FAIL state for node 002132b04826f925bd1fda29366e5732767f975d: replica is reachable again.\r\n12787:C 18 Jul 2022 15:41:52.597 * Fork CoW for RDB: current 6474 MB, peak 6474 MB, average 3676 MB\r\n9412:M 18 Jul 2022 15:41:52.597 # Diskless rdb transfer, done reading from pipe, 1 replicas still up.\r\n9412:M 18 Jul 2022 15:41:53.910 * Background RDB transfer terminated with success\r\n9412:M 18 Jul 2022 15:41:53.910 * Streamed RDB transfer with replica 20.0.83.12:6379 succeeded (socket). Waiting for REPLCONF ACK from slave to enable streaming\r\n9412:M 18 Jul 2022 15:41:53.910 * Synchronization with replica 20.0.83.12:6379 succeeded\r\n9412:M 18 Jul 2022 15:42:13.579 * FAIL message received from ecf9f6a90b3374c94a3bd361d3a961231dd4792e about 002132b04826f925bd1fda29366e5732767f975d\r\n9412:M 18 Jul 2022 15:49:17.561 * Clear FAIL state for node 002132b04826f925bd1fda29366e5732767f975d: replica is reachable again.\r\n```"
    },
    {
      "id": 1198242784,
      "user": "tbream",
      "created_at": "2022-07-28T14:45:52Z",
      "body": "@guybe7, is there any guidance or advice after seeing the logs? Is the best course of action to upgrade the cluster to the latest release, or is this a new bug?\r\n\r\nOur cluster is almost balanced, but this prevents having an equal number of slots on each main instance."
    },
    {
      "id": 1198247325,
      "user": "guybe7",
      "created_at": "2022-07-28T14:48:20Z",
      "body": "@tbream does it reproduce consistently? "
    },
    {
      "id": 1198312880,
      "user": "tbream",
      "created_at": "2022-07-28T15:30:10Z",
      "body": "@guybe7, it was reproducing each time we ran. We tried three times before stopping since the max difference in slots was roughly around 6%."
    },
    {
      "id": 1198360785,
      "user": "guybe7",
      "created_at": "2022-07-28T16:15:57Z",
      "body": "@tbream I know it's a bit unusual, but is there any chance I can send you a patch with some extra logs so you can compile Redis and run it?\r\n\r\nso far we are unable to reproduce it...\r\n\r\nbut maybe before we do that you can try an official release instead of an RC? 7.0.4 for example\r\nmaybe we already solved it and we don't even know :)"
    },
    {
      "id": 1201293289,
      "user": "tbream",
      "created_at": "2022-08-01T14:38:40Z",
      "body": "@guybe7, we will update to RC 7.0.4 in our next sprint, and I will get back to you if the error goes away."
    },
    {
      "id": 1201307822,
      "user": "guybe7",
      "created_at": "2022-08-01T14:51:34Z",
      "body": "@tbream please get back to me even if the error stays :)\r\n\r\nlink to 7.0.4: https://github.com/redis/redis/releases/tag/7.0.4"
    },
    {
      "id": 1210497719,
      "user": "guybe7",
      "created_at": "2022-08-10T10:45:23Z",
      "body": "hi @tbream any luck?"
    },
    {
      "id": 1218215284,
      "user": "tbream",
      "created_at": "2022-08-17T16:01:10Z",
      "body": "Running cluster in mix state with 12 nodes and 2 replicas each. Each master node was 7.0.4 while the replicas where the rc2 release. The rebalance failed after 12 slots moved. We are attempting to try again, and if it fails again we are going to upgrade the cluster completely and try again.\r\n\r\nSome important notes, our workload only uses string data encoded in binary format. We also run a heavy workload and record our data with 48 hour expiration.\r\n\r\n```\r\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\r\n13285:M 17 Aug 2022 14:32:57.806 # === ASSERTION FAILED ===\r\n13285:M 17 Aug 2022 14:32:57.806 # ==> blocked.c:624 'server.also_propagate.numops == 0' is not true\r\n\r\n------ STACK TRACE ------\r\n\r\nBacktrace:\r\n/usr/local/bin/redis-server *:6379 [cluster](handleClientsBlockedOnKeys+0x255)[0x55735af3aea5]\r\n/usr/local/bin/redis-server *:6379 [cluster](beforeSleep+0x125)[0x55735aeac365]\r\n/usr/local/bin/redis-server *:6379 [cluster](aeProcessEvents+0x8f)[0x55735aea7eaf]\r\n/usr/local/bin/redis-server *:6379 [cluster](aeMain+0x1d)[0x55735aea841d]\r\n/usr/local/bin/redis-server *:6379 [cluster](main+0x342)[0x55735aea4012]\r\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xe7)[0x7f6a7a68ec87]\r\n/usr/local/bin/redis-server *:6379 [cluster](_start+0x2a)[0x55735aea46ca]\r\n\r\n------ INFO OUTPUT ------\r\n# Server\r\nredis_version:7.0.4\r\nredis_git_sha1:00000000\r\nredis_git_dirty:0\r\nredis_build_id:749f16780445eb0d\r\nredis_mode:cluster\r\nos:Linux 5.4.0-1083-aws x86_64\r\narch_bits:64\r\nmonotonic_clock:POSIX clock_gettime\r\nmultiplexing_api:epoll\r\natomicvar_api:c11-builtin\r\ngcc_version:7.5.0\r\nprocess_id:13285\r\nprocess_supervised:systemd\r\nrun_id:377005791680c672957cc5376a370a7f8ca5492c\r\ntcp_port:6379\r\nserver_time_usec:1660746777805837\r\nuptime_in_seconds:148541\r\nuptime_in_days:1\r\nhz:10\r\nconfigured_hz:10\r\nlru_clock:16579609\r\nexecutable:/usr/local/bin/redis-server\r\nconfig_file:/etc/redis/6379.conf\r\nio_threads_active:0\r\n\r\n# Clients\r\nconnected_clients:972\r\ncluster_connections:94\r\nmaxclients:150000\r\nclient_recent_max_input_buffer:20480\r\nclient_recent_max_output_buffer:20504\r\nblocked_clients:0\r\ntracking_clients:0\r\nclients_in_timeout_table:0\r\n\r\n# Memory\r\nused_memory:55864024944\r\nused_memory_human:52.03G\r\nused_memory_rss:57221574656\r\nused_memory_rss_human:53.29G\r\nused_memory_peak:56573307200\r\nused_memory_peak_human:52.69G\r\nused_memory_peak_perc:98.75%\r\nused_memory_overhead:33877639840\r\nused_memory_startup:9522656\r\nused_memory_dataset:21986385104\r\nused_memory_dataset_perc:39.36%\r\nallocator_allocated:55864167080\r\nallocator_active:56616329216\r\nallocator_resident:57248256000\r\ntotal_system_memory:133545504768\r\ntotal_system_memory_human:124.37G\r\nused_memory_lua:31744\r\nused_memory_vm_eval:31744\r\nused_memory_lua_human:31.00K\r\nused_memory_scripts_eval:0\r\nnumber_of_cached_scripts:0\r\nnumber_of_functions:0\r\nnumber_of_libraries:0\r\nused_memory_vm_functions:32768\r\nused_memory_vm_total:64512\r\nused_memory_vm_total_human:63.00K\r\nused_memory_functions:184\r\nused_memory_scripts:184\r\nused_memory_scripts_human:184B\r\nmaxmemory:80127302860\r\nmaxmemory_human:74.62G\r\nmaxmemory_policy:volatile-ttl\r\nallocator_frag_ratio:1.01\r\nallocator_frag_bytes:752162136\r\nallocator_rss_ratio:1.01\r\nallocator_rss_bytes:631926784\r\nrss_overhead_ratio:1.00\r\nrss_overhead_bytes:-26681344\r\nmem_fragmentation_ratio:1.02\r\nmem_fragmentation_bytes:1357597976\r\nmem_not_counted_for_evict:1792\r\nmem_replication_backlog:209717848\r\nmem_total_replication_buffers:210391544\r\nmem_clients_slaves:676344\r\nmem_clients_normal:7706952\r\nmem_cluster_links:102272\r\nmem_aof_buffer:1792\r\nmem_allocator:jemalloc-5.2.1\r\nactive_defrag_running:0\r\nlazyfree_pending_objects:0\r\nlazyfreed_objects:243322970\r\n\r\n# Persistence\r\nloading:0\r\nasync_loading:0\r\ncurrent_cow_peak:0\r\ncurrent_cow_size:0\r\ncurrent_cow_size_age:0\r\ncurrent_fork_perc:0.00\r\ncurrent_save_keys_processed:0\r\ncurrent_save_keys_total:0\r\nrdb_changes_since_last_save:327874249\r\nrdb_bgsave_in_progress:0\r\nrdb_last_save_time:1660598236\r\nrdb_last_bgsave_status:ok\r\nrdb_last_bgsave_time_sec:-1\r\nrdb_current_bgsave_time_sec:-1\r\nrdb_saves:0\r\nrdb_last_cow_size:0\r\nrdb_last_load_keys_expired:0\r\nrdb_last_load_keys_loaded:258914843\r\naof_enabled:1\r\naof_rewrite_in_progress:0\r\naof_rewrite_scheduled:0\r\naof_last_rewrite_time_sec:532\r\naof_current_rewrite_time_sec:-1\r\naof_last_bgrewrite_status:ok\r\naof_rewrites:30\r\naof_rewrites_consecutive_failures:0\r\naof_last_write_status:ok\r\naof_last_cow_size:10088341504\r\nmodule_fork_in_progress:0\r\nmodule_fork_last_cow_size:0\r\naof_current_size:21965410581\r\naof_base_size:21226510422\r\naof_pending_rewrite:0\r\naof_buffer_length:0\r\naof_pending_bio_fsync:0\r\naof_delayed_fsync:0\r\n\r\n# Stats\r\ntotal_connections_received:41832\r\ntotal_commands_processed:331668272\r\ninstantaneous_ops_per_sec:4008\r\ntotal_net_input_bytes:60974351624\r\ntotal_net_output_bytes:44776370873\r\ntotal_net_repl_input_bytes:48562434759\r\ntotal_net_repl_output_bytes:43910213916\r\ninstantaneous_input_kbps:542.12\r\ninstantaneous_output_kbps:1591.75\r\ninstantaneous_input_repl_kbps:0.00\r\ninstantaneous_output_repl_kbps:1444.08\r\nrejected_connections:0\r\nsync_full:0\r\nsync_partial_ok:3\r\nsync_partial_err:0\r\nexpired_keys:74351009\r\nexpired_stale_perc:15.31\r\nexpired_time_cap_reached_count:67\r\nexpire_cycle_cpu_milliseconds:371542\r\nevicted_keys:0\r\nevicted_clients:0\r\ntotal_eviction_exceeded_time:0\r\ncurrent_eviction_exceeded_time:0\r\nkeyspace_hits:6390349\r\nkeyspace_misses:102817387\r\npubsub_channels:0\r\npubsub_patterns:0\r\npubsubshard_channels:0\r\nlatest_fork_usec:880244\r\ntotal_forks:30\r\nmigrate_cached_sockets:1\r\nslave_expires_tracked_keys:0\r\nactive_defrag_hits:0\r\nactive_defrag_misses:0\r\nactive_defrag_key_hits:0\r\nactive_defrag_key_misses:0\r\ntotal_active_defrag_time:0\r\ncurrent_active_defrag_time:0\r\ntracking_total_keys:0\r\ntracking_total_items:0\r\ntracking_total_prefixes:0\r\nunexpected_error_replies:0\r\ntotal_error_replies:2950\r\ndump_payload_sanitizations:0\r\ntotal_reads_processed:232467081\r\ntotal_writes_processed:398041292\r\nio_threaded_reads_processed:0\r\nio_threaded_writes_processed:0\r\nreply_buffer_shrinks:8041\r\nreply_buffer_expands:177\r\n\r\n# Replication\r\nrole:master\r\nconnected_slaves:3\r\nmin_slaves_good_slaves:3\r\nslave0:ip=20.0.83.95,port=6379,state=online,offset=657187170092,lag=0\r\nslave1:ip=20.0.83.12,port=6379,state=online,offset=657187221714,lag=0\r\nslave2:ip=20.0.81.74,port=6379,state=online,offset=657187104672,lag=0\r\nmaster_failover_state:no-failover\r\nmaster_replid:95e5e13c9f6a6ed4f24969b612f9db6e3fa2fba4\r\nmaster_replid2:4639133f0c2ff856a2b2014f9cb481d17a7bc865\r\nmaster_repl_offset:657187307138\r\nsecond_repl_offset:642550566777\r\nrepl_backlog_active:1\r\nrepl_backlog_size:209715200\r\nrepl_backlog_first_byte_offset:656977573759\r\nrepl_backlog_histlen:209733380\r\n\r\n# CPU\r\nused_cpu_sys:4130.038555\r\nused_cpu_user:5044.700985\r\nused_cpu_sys_children:832.636074\r\nused_cpu_user_children:13167.762578\r\nused_cpu_sys_main_thread:4051.275663\r\nused_cpu_user_main_thread:4616.482400\r\n\r\n# Modules\r\n\r\n# Commandstats\r\ncmdstat_cluster|getkeysinslot:calls=268155,usec=1555421,usec_per_call=5.80,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|setslot:calls=25,usec=75,usec_per_call=3.00,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|slots:calls=53,usec=4513,usec_per_call=85.15,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|failover:calls=1,usec=45,usec_per_call=45.00,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|replicate:calls=1,usec=162,usec_per_call=162.00,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|nodes:calls=675,usec=80480,usec_per_call=119.23,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|info:calls=9803,usec=525400,usec_per_call=53.60,rejected_calls=6,failed_calls=0\r\ncmdstat_psync:calls=3,usec=190,usec_per_call=63.33,rejected_calls=0,failed_calls=0\r\ncmdstat_config|get:calls=9802,usec=37921,usec_per_call=3.87,rejected_calls=0,failed_calls=0\r\ncmdstat_config|set:calls=10,usec=53,usec_per_call=5.30,rejected_calls=0,failed_calls=1\r\ncmdstat_client|setname:calls=8050,usec=16978,usec_per_call=2.11,rejected_calls=0,failed_calls=0\r\ncmdstat_select:calls=1,usec=1,usec_per_call=1.00,rejected_calls=0,failed_calls=0\r\ncmdstat_command|docs:calls=2,usec=1950,usec_per_call=975.00,rejected_calls=0,failed_calls=0\r\ncmdstat_dbsize:calls=15,usec=9,usec_per_call=0.60,rejected_calls=0,failed_calls=0\r\ncmdstat_ping:calls=12306,usec=5460,usec_per_call=0.44,rejected_calls=26,failed_calls=0\r\ncmdstat_quit:calls=4499,usec=1243,usec_per_call=0.28,rejected_calls=0,failed_calls=0\r\ncmdstat_unlink:calls=33476847,usec=115751069,usec_per_call=3.46,rejected_calls=0,failed_calls=0\r\ncmdstat_set:calls=228660769,usec=983676173,usec_per_call=4.30,rejected_calls=2755,failed_calls=0\r\ncmdstat_del:calls=62121094,usec=214596668,usec_per_call=3.45,rejected_calls=0,failed_calls=0\r\ncmdstat_migrate:calls=268142,usec=119755939,usec_per_call=446.61,rejected_calls=0,failed_calls=0\r\ncmdstat_replconf:calls=204406,usec=113893,usec_per_call=0.56,rejected_calls=0,failed_calls=0\r\ncmdstat_get:calls=6599438,usec=10273174,usec_per_call=1.56,rejected_calls=161,failed_calls=0\r\ncmdstat_info:calls=24175,usec=1571990,usec_per_call=65.03,rejected_calls=0,failed_calls=0\r\n\r\n# Errorstats\r\nerrorstat_ASK:count=1075\r\nerrorstat_ERR:count=2\r\nerrorstat_LOADING:count=32\r\nerrorstat_MOVED:count=1841\r\n\r\n# Latencystats\r\nlatency_percentiles_usec_cluster|getkeysinslot:p50=6.015,p99=11.007,p99.9=15.039\r\nlatency_percentiles_usec_cluster|setslot:p50=3.007,p99=6.015,p99.9=6.015\r\nlatency_percentiles_usec_cluster|slots:p50=82.431,p99=150.527,p99.9=181.247\r\nlatency_percentiles_usec_cluster|failover:p50=45.055,p99=45.055,p99.9=45.055\r\nlatency_percentiles_usec_cluster|replicate:p50=162.815,p99=162.815,p99.9=162.815\r\nlatency_percentiles_usec_cluster|nodes:p50=119.295,p99=184.319,p99.9=230.399\r\nlatency_percentiles_usec_cluster|info:p50=49.151,p99=110.079,p99.9=130.047\r\nlatency_percentiles_usec_psync:p50=70.143,p99=77.311,p99.9=77.311\r\nlatency_percentiles_usec_config|get:p50=4.015,p99=10.047,p99.9=28.031\r\nlatency_percentiles_usec_config|set:p50=5.023,p99=10.047,p99.9=10.047\r\nlatency_percentiles_usec_client|setname:p50=2.007,p99=10.047,p99.9=17.023\r\nlatency_percentiles_usec_select:p50=1.003,p99=1.003,p99.9=1.003\r\nlatency_percentiles_usec_command|docs:p50=974.847,p99=978.943,p99.9=978.943\r\nlatency_percentiles_usec_dbsize:p50=1.003,p99=1.003,p99.9=1.003\r\nlatency_percentiles_usec_ping:p50=0.001,p99=5.023,p99.9=11.007\r\nlatency_percentiles_usec_quit:p50=0.001,p99=1.003,p99.9=7.007\r\nlatency_percentiles_usec_unlink:p50=2.007,p99=19.071,p99.9=36.095\r\nlatency_percentiles_usec_set:p50=3.007,p99=21.119,p99.9=43.007\r\nlatency_percentiles_usec_del:p50=3.007,p99=16.063,p99.9=31.103\r\nlatency_percentiles_usec_migrate:p50=444.415,p99=602.111,p99.9=1056.767\r\nlatency_percentiles_usec_replconf:p50=1.003,p99=1.003,p99.9=4.015\r\nlatency_percentiles_usec_get:p50=1.003,p99=6.015,p99.9=17.023\r\nlatency_percentiles_usec_info:p50=64.255,p99=108.031,p99.9=134.143\r\n\r\n# Cluster\r\ncluster_enabled:1\r\n\r\n# Keyspace\r\ndb0:keys=313249715,expires=313249715,avg_ttl=83194746\r\n\r\n------ CLIENT LIST OUTPUT ------\r\n...\r\n------ MODULES INFO OUTPUT ------\r\n\r\n------ CONFIG DEBUG OUTPUT ------\r\nlazyfree-lazy-expire yes\r\nreplica-read-only yes\r\nlazyfree-lazy-server-del yes\r\nproto-max-bulk-len 512mb\r\nlazyfree-lazy-eviction yes\r\nlazyfree-lazy-user-flush no\r\nlist-compress-depth 0\r\nsanitize-dump-payload no\r\nrepl-diskless-load on-empty-db\r\nactivedefrag no\r\nio-threads-do-reads no\r\nio-threads 1\r\nlazyfree-lazy-user-del no\r\nclient-query-buffer-limit 1gb\r\nrepl-diskless-sync yes\r\nslave-read-only yes\r\n\r\n------ FAST MEMORY TEST ------\r\n13285:M 17 Aug 2022 14:32:57.814 # Bio thread for job type #0 terminated\r\n13285:M 17 Aug 2022 14:32:57.814 # Bio thread for job type #1 terminated\r\n13285:M 17 Aug 2022 14:32:57.814 # Bio thread for job type #2 terminated\r\n*** Preparing to test memory region 55735b2fd000 (2301952 bytes)\r\n*** Preparing to test memory region 55735d077000 (135168 bytes)\r\n*** Preparing to test memory region 7f5747000000 (36188590080 bytes)\r\n*** Preparing to test memory region 7f5fba200000 (8388608 bytes)\r\n*** Preparing to test memory region 7f5fbaa00000 (2097152 bytes)\r\n*** Preparing to test memory region 7f5fbae00000 (8388608 bytes)\r\n*** Preparing to test memory region 7f5fbb600000 (2097152 bytes)\r\n*** Preparing to test memory region 7f5fbba00000 (8388608 bytes)\r\n*** Preparing to test memory region 7f5fbc200000 (46072332288 bytes)\r\n*** Preparing to test memory region 7f6a7658a000 (8388608 bytes)\r\n*** Preparing to test memory region 7f6a76d8b000 (8388608 bytes)\r\n*** Preparing to test memory region 7f6a7758c000 (8388608 bytes)\r\n*** Preparing to test memory region 7f6a77d8d000 (8388608 bytes)\r\n*** Preparing to test memory region 7f6a7858d000 (13631488 bytes)\r\n*** Preparing to test memory region 7f6a79400000 (8388608 bytes)\r\n*** Preparing to test memory region 7f6a7a22a000 (4096 bytes)\r\n*** Preparing to test memory region 7f6a7aa5a000 (16384 bytes)\r\n*** Preparing to test memory region 7f6a7ac79000 (16384 bytes)\r\n*** Preparing to test memory region 7f6a7af00000 (4096 bytes)\r\n*** Preparing to test memory region 7f6a7b8c0000 (40960 bytes)\r\n*** Preparing to test memory region 7f6a7b8d6000 (4096 bytes)\r\n.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O\r\nFast memory test PASSED, however your memory can still be broken. Please run a memory test for several hours if possible.\r\n\r\n=== REDIS BUG REPORT END. Make sure to include from START to END. ===\r\n\r\n       Please report the crash by opening an issue on github:\r\n```"
    },
    {
      "id": 1218239727,
      "user": "guybe7",
      "created_at": "2022-08-17T16:23:50Z",
      "body": "@tbream ok so the problem still exists...\r\ni have a setup in which the issue reproduces, but quite rarely.\r\ni'll try to add some more logs and run the tests in a loop\r\ni'll keep you updated"
    },
    {
      "id": 1218366026,
      "user": "tbream",
      "created_at": "2022-08-17T18:38:49Z",
      "body": "Our second attempt today was able to successfully rebalance the remaining 71 nodes without failures."
    },
    {
      "id": 1220618515,
      "user": "guybe7",
      "created_at": "2022-08-19T12:30:01Z",
      "body": "update: i managed to find and solve the issue in my setup but unfortunately it was module-related (different bug, same outcome)\r\n\r\nre this case: i'm pretty sure the problem is that a key is lazy-expired somewhere outside of `call` and therefore leaves the `also_propagate` array full, causing the assertion.\r\ni went over all uses of `lookupKey*` (which can lazy-expire) and the immediate suspect is `getNodeByQuery` which is called in `processCommand`, which in turn can early-exit before reaching `call`\r\n\r\ni'm very unfamiliar with the cluster code, so forgive me if i'm saying nonsense: my guess is that during rebalance, some `GET` reaches a node that contains the key now, but it shouldn't belong there (slot moves to another node). perhaps in this case, the node that handles the `GET` calls `getNodeByQuery`, lazy-expires the key, and rejects the command, thinking the key shouldn't belong to it? i do see some rejected `GET`s in the bug report.\r\n\r\ni would say that we should add `propagatePendingCommands` at the end of `getNodeByQuery`, with a comment and a test. i'm not entirely sure because i didn't dig deep enough yet.\r\nanother option is to make `getNodeByQuery` avoid lazy-expire altogether.  i feel that this is the way to go, because the purpose of this function is to get the node to which the key belongs, it feels that a side effect like deleting the key(s) is unwanted in this case.\r\n\r\n@oranagra i will be on vacation until next Monday, please have a look, maybe it's better if we handle it ASAP."
    },
    {
      "id": 1220689320,
      "user": "tbream",
      "created_at": "2022-08-19T13:33:25Z",
      "body": "Our workload entirely consists of using the `SET` with the `EX`, `NX`, `GET`options."
    },
    {
      "id": 1220699935,
      "user": "guybe7",
      "created_at": "2022-08-19T13:43:09Z",
      "body": "@tbream ok, also `SET` can cause the same situation\r\n\r\nbut i do see some GETs:\r\n```\r\ncmdstat_get:calls=6599438,usec=10273174,usec_per_call=1.56,rejected_calls=161,failed_calls=0\r\n```"
    },
    {
      "id": 1220837515,
      "user": "tbream",
      "created_at": "2022-08-19T16:00:37Z",
      "body": "Yes, i did forget that we have a workload around the gets in addition. Both of these are happening during the rebalance operation."
    },
    {
      "id": 1221569830,
      "user": "oranagra",
      "created_at": "2022-08-21T15:41:39Z",
      "body": "@madolson, please see Guy's https://github.com/redis/redis/issues/11014#issuecomment-1220618515\r\ni feel that the right solution is to keep calling lookupKey, but make sure that it'll avoid del + propagate (same as we do when client pause write is enabled).\r\n\r\nanyway, first thing's first, we need to reproduce it in a test, so far i tried and failed."
    },
    {
      "id": 1221805400,
      "user": "madolson",
      "created_at": "2022-08-22T04:35:23Z",
      "body": "Here is a reproduction: https://github.com/redis/redis/blob/e4e35a96f36e88d19270c123a6c831fe20363286/tests/unit/cluster/test.tcl\r\n\r\nThe theory is correct. If a key is expired while being accessed in `getNodesByQuery`, the expiration will be generated but propagatePendingCommands will not be triggered."
    }
  ]
}