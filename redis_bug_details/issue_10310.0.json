{
  "issue_number": 10310.0,
  "title": "[BUG] ZREVRANGE 50% slower after upgrading from 5.0.7 to 6.2.6",
  "body": "**Describe the bug**\r\n\r\nHi all,\r\n\r\nI was wondering if anyone might be able to shed some light on this rather unusual behaviour I've been experiencing.\r\n\r\nI have a sorted set with 1m entries, and using ZREVRANGE to get the player's ranking, plus the two players above and below. I'm using node-redis on Ubuntu 20.04.3. However, now that I've upgraded from Redis 5.0.7 to 6.2.6, the very same code is taking 50% longer to complete. I'm using the node-redis multi() command, as (for reasons I'm not entirely clear about) it has better performance than batch(), Whether I bunch my requests up into 100k batches, or 10k batches, the result is the same.\r\n\r\nOn Redis 5.0.7 (the version on apt-get), I can pull the results for every player in 966ms. On 6.2.6 that same set of queries takes 1.415s to complete.\r\n\r\nOther queries are unaffected - I also get ZREVRANK for every player, and that took 1.8s before, and 1.8s now.\r\n\r\n**To reproduce**\r\n\r\nIt should be possible to reproduce with a 1 million entry sorted set. The sorted set is literally just a list of incremental IDs, and a randomly allocated score between 0 and 500.\r\n\r\nHere's the code I'm using for the retrieval\r\n\r\n```\r\nfunction getdata(n){\r\n            var loopinc = 100000;\r\n            var final = 1000000;\r\n            if(n == 1){\r\n                var target = loopinc;\r\n                var initial = 1;\r\n            }else{\r\n                var initial = (n - 1) * loopinc;\r\n                var target = n * loopinc;\r\n            }\r\n            console.log('start from: '+initial+',  stop at:  '+target);\r\n\r\n           for(i = initial; i < target; i++){\r\n                if(i < 2){\r\n                    multi.zrevrange('lb', 0, 5);\r\n                }else{\r\n                    multi.zrevrange('lb',(i -2), (i + 2));\r\n                }\r\n            }\r\n\r\n           var start = performance.now();\r\n           multi.exec(function(err, reply){\r\n                    var end = performance.now();\r\n                    var redisstart = (parseInt(reply[0][0]) * 1000) + (parseInt(reply[0][1]) / 1000);\r\n                    var redisend   = (parseInt(reply[(reply.length - 1)][0]) * 1000) + (parseInt(reply[(reply.length - 1)][1]) / 1000);            \r\n                    var execution_time = redisend - redisstart; // in milliseconds\r\n                    console.log('Execution time: '+execution_time);\r\n                    totalarr.push(execution_time);\r\n                    console.log('Retrieved in '+(end - start)+' ms'); \r\n    \r\n                    if(target < final){\r\n                        n++\r\n                        getdata(n);\r\n                    }else{\r\n                        var total = totalarr.reduce(function(previousvalue, currentvalue) { return previousvalue + currentvalue; }, 0);\r\n                        console.log('total execution time: '+total);\r\n                    }\r\n                });\r\n }\r\n\r\ngetdata(1)\r\n```\r\n\r\nSteps to reproduce the behavior and/or a minimal code sample.\r\n\r\n**Expected behavior**\r\n\r\nThe same performance as on 5.0.7 rather than being 50% slower.\r\n\r\n**Additional information**\r\n\r\nPlatform: Ubuntu 20.04.3 LTS\r\nRedis: 5.0.7 or 6.2.6\r\nInterfacing with node-redis 3.1.2\r\n",
  "state": "closed",
  "created_at": "2022-02-17T10:53:40Z",
  "updated_at": "2022-02-24T12:20:01Z",
  "closed_at": "2022-02-24T12:20:01Z",
  "labels": [
    "action:run-benchmark"
  ],
  "comments_data": [
    {
      "id": 1044231190,
      "user": "filipecosta90",
      "created_at": "2022-02-18T09:45:13Z",
      "body": "@v-flashpoint you're right about the regression 172K ops/sec on v5 vs 140K ops/sec on v6.2. the overall regression sits around 25%.\r\n\r\nI've added a reproduction in https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-zset-1M-elements-zrevrange-5-elements.yml, and this can be easily tested solely with memtier. **This is now being tracked as part of our CI. So this type of regression on this command will be sure to not happen again.** thank you @v-flashpoint!\r\n\r\nTLDR @redis/core-team the introduction of deferred reply seems to be the main reason for the performance difference. \r\n\r\n@redis/core-team I'll go over the numbers using a standalone deployment:\r\n### v5.0.7 - 172K ops/sec :\r\n```\r\n# populate\r\nmemtier_benchmark --key-maximum 1000000 --key-prefix \"\" --command=\"ZADD lb __key__ __key__\"  --command-key-pattern P --hide-histogram -t 4 -c 100 -s <SERVER>\r\n\r\n\r\n# benchmark\r\nmemtier_benchmark --command=\"ZREVRANGE lb 5 10\"  --hide-histogram --test-time 60  -s <SERVER>\r\n\r\nWriting results to stdout\r\n[RUN #1] Preparing benchmark client...\r\n[RUN #1] Launching threads now...\r\n[RUN #1 100%,  60 secs]  0 threads:    10315288 ops,  172839 (avg:  171915) ops/sec, 19.45MB/sec (avg: 19.35MB/sec),  1.16 (avg:  1.16) msec latency\r\n\r\n4         Threads\r\n50        Connections per thread\r\n60        Seconds\r\n\r\n\r\nALL STATS\r\n=====================================================================================================\r\nType            Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n-----------------------------------------------------------------------------------------------------\r\nZrevranges    171916.03         1.16247         1.07100         2.09500         2.38300     19810.64 \r\nTotals        171916.03         1.16247         1.07100         2.09500         2.38300     19810.64 \r\n```\r\n\r\n\r\n### details on v5.0.7:\r\nHere's the flamegraph: https://s3.us-east-2.amazonaws.com/ci.benchmarks.redislabs/redis/redis/profiles//profile___primary-1-of-1___perf_2022-02-18-08-44-58.out.flamegraph.svg\r\n\r\nOne thing to keep in mind is that before we used  addReplyMultiBulkLen https://github.com/redis/redis/blob/5.0/src/t_zset.c#L2461\r\n\r\n\r\n----------------------\r\n\r\n### v6.2.6 - 140K ops/sec :\r\n```\r\nmemtier_benchmark --command=\"ZREVRANGE lb 5 10\"  --hide-histogram --test-time 60 -s <SERVER>\r\nWriting results to stdout\r\n[RUN #1] Preparing benchmark client...\r\n[RUN #1] Launching threads now...\r\n[RUN #1 100%,  60 secs]  0 threads:     8346655 ops,  137191 (avg:  139105) ops/sec, 15.44MB/sec (avg: 15.65MB/sec),  1.46 (avg:  1.44) msec latency\r\n\r\n4         Threads\r\n50        Connections per thread\r\n60        Seconds\r\n\r\n\r\nALL STATS\r\n=====================================================================================================\r\nType            Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n-----------------------------------------------------------------------------------------------------\r\nZrevranges    139105.74         1.43686         1.33500         2.65500         2.83100     16029.76 \r\nTotals        139105.74         1.43686         1.33500         2.65500         2.83100     16029.76 \r\n```\r\n\r\n### details on v6.2:\r\nHere's the flamegraph: https://s3.us-east-2.amazonaws.com/ci.benchmarks.redislabs/redis/redis/profiles//profile___primary-1-of-1___perf_2022-02-18-08-51-53.out.flamegraph.svg\r\n\r\nOne thing to keep in mind is that NOW we used  addReplyDeferredLen https://github.com/redis/redis/blob/6.2/src/t_zset.c#L3044\r\n\r\nAnd as expected doing a difference between stacks of versions, the one's that pop up (increase in CPU cycles) are related to write+command performance.\r\n\r\nlibc_write itself consumes 6% more cycles. \r\n![image](https://user-images.githubusercontent.com/5832149/154657414-3fe05875-46fa-4425-940d-5f7e3ed9ee98.png)\r\n\r\n\r\n## potential follow up\r\n\r\n@redis/core-team given within the range code we don't change the result cardinality IMHO we can completely avoid this deferred len and issue the proper length size at start. result_cardinality does not change! : https://github.com/redis/redis/blob/6.2/src/t_zset.c#L3054\r\nAgree?"
    },
    {
      "id": 1046177173,
      "user": "oranagra",
      "created_at": "2022-02-20T07:10:07Z",
      "body": "ZRANGE and ZREVRANGE don't need to use deferred replies (the count is known), but ZRANGEBYSCORE and ZRANGEBYLEX do!\r\n\r\nThe work that's been done in #7844 made ZRANGE and ZRANGESTORE handle these cases too, so the only way to avoid that regression is to add some specific hacks to the code to avoid deferred reply only when using indexes.\r\n\r\nNote that these commands use only one deferred reply per command (unlike what we used to have in CLUSTER SLOTS and COMMAND command, see https://github.com/redis/redis/pull/10056, https://github.com/redis/redis/pull/7123), which isn't expected to cause a big impact, and the reason it does cause a big impact here is because it is used in a pipeline.\r\n\r\nI think the solution for this issue is gonna be #9934.\r\n@filipecosta90 since you did reproduce this issue, maybe you can test the effect of that PR on this use case."
    },
    {
      "id": 1046698483,
      "user": "filipecosta90",
      "created_at": "2022-02-21T10:13:28Z",
      "body": "> I think the solution for this issue is gonna be https://github.com/redis/redis/pull/9934.\r\n@filipecosta90 since you did reproduce this issue, maybe you can test the effect of that PR on this use case.\r\n\r\n@oranagra WRT to `pan/use-writev` branch impact on this use-case we can see it's small / nearly imperceptible:\r\n\r\n- v5.0.7 - 172K ops/sec\r\n- v6.2.6 - 140K ops/sec\r\n- pan/use-writev -  141K ops/sec\r\n\r\n```\r\n$ memtier_benchmark --command=\"ZREVRANGE lb 5 10\"  --hide-histogram --test-time 60  -s 10.3.0.175 -p 6380\r\nWriting results to stdout\r\n[RUN #1] Preparing benchmark client...\r\n[RUN #1] Launching threads now...\r\n[RUN #1 100%,  60 secs]  0 threads:     8435799 ops,  139497 (avg:  140591) ops/sec, 15.70MB/sec (avg: 15.82MB/sec),  1.43 (avg:  1.42) msec latency\r\n\r\n4         Threads\r\n50        Connections per thread\r\n60        Seconds\r\n\r\n\r\nALL STATS\r\n=====================================================================================================\r\nType            Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n-----------------------------------------------------------------------------------------------------\r\nZrevranges    140591.87         1.42170         1.32700         2.63900         2.81500     16201.02 \r\nTotals        140591.87         1.42170         1.32700         2.63900         2.81500     16201.02 \r\n```\r\n\r\nWRT to:\r\n\r\n> and the reason it does cause a big impact here is because it is used in a pipeline.\r\n\r\nnotice that on my simple reproduction I'm not using pipelining. \r\n\r\n>  so the only way to avoid that regression is to add some specific hacks to the code to avoid deferred reply only when using indexes.\r\n\r\nCan we move forward with this solution? "
    },
    {
      "id": 1046707348,
      "user": "oranagra",
      "created_at": "2022-02-21T10:19:34Z",
      "body": "> Can we move forward with this solution?\r\n\r\nif we have no other choice we can do that, but i'd rather avoid adding explicit hacks for ZRANGE that will not work for ZRANGE BYSCORE.\r\n\r\ni wanna try figuring out why writev doesn't solve the problem.\r\nhow did you conclude that it's a result of the deferred replies? did you do a POC that changes that and saw it fixed?"
    },
    {
      "id": 1046710315,
      "user": "filipecosta90",
      "created_at": "2022-02-21T10:22:22Z",
      "body": "> how did you conclude that it's a result of the deferred replies? did you do a POC that changes that and saw it fixed?\r\n\r\nI've noticed an increase in write/read/and deferred code within the command logic:\r\n\r\n> doing a difference between stacks of versions, the one's that pop up (increase in CPU cycles) are related to write+command performance.\r\n> libc_write itself consumes 6% more cycles.\r\n\r\n![image](https://user-images.githubusercontent.com/5832149/154935863-879b5eb5-7d62-4f10-b870-299977ce0a4f.png)\r\n\r\n\r\nWRT to:\r\n>  did you do a POC that changes that and saw it fixed?\r\n\r\nI'll do a quick POC and confirm indeed it's fixed. "
    },
    {
      "id": 1046741032,
      "user": "oranagra",
      "created_at": "2022-02-21T10:49:46Z",
      "body": "p.s. there might be a small impact from an additional write (per round trip), and that impact might be very visible in case all the surrounding is small (fast command with not a lot of data to write).\r\nwith a long pipeline or a transaction (like described at the top), it should have a bigger impact, but i expect this impact to be dramatically reduced by writev\r\n\r\nMy point is that we have quite a few other commands with a single deferred reply per command, and i don't like to consider that pattern an problematic one.\r\nSo i hope writev can drastically reduce the overhead when these are used with a pipeline,\r\nand i hope we can overlook their overhead when no pipeline is used."
    },
    {
      "id": 1046768789,
      "user": "filipecosta90",
      "created_at": "2022-02-21T11:18:47Z",
      "body": "@oranagra you're absolutely right about #9934 positive impact when using pipeline. \r\n\r\n  -  v5.0.7 pipeline 16 - 810K ops/sec, p50=4.04ms\r\n  -  v6.2.6 pipeline 16 - 331K ops/sec, p50=9.59ms\r\n  -  pan/use-writev pipeline 16 - 540K ops/sec, p50=5.11ms\r\n\r\nstill, the gap between 540 and 810K ops/sec is relevant right? Meaning we need further changes apart from #9934 correct? "
    },
    {
      "id": 1046792719,
      "user": "oranagra",
      "created_at": "2022-02-21T11:48:14Z",
      "body": "yes. let's try to figure that out.. please try your POC, taking unstable and avoiding the use of deferred reply (even without pipeline).\r\nwe'll see if 100% of the regression comes from this, or form another reason."
    },
    {
      "id": 1048351485,
      "user": "filipecosta90",
      "created_at": "2022-02-23T00:47:53Z",
      "body": "@oranagra simply going back to 5.0 code on zrevrange ( with the changes to listpack https://github.com/filipecosta90/redis/tree/zset.regression ) gets us back to 170K ops/sec. This is using unstable base commit de6be8850f1cec69263ad4f5713ef06f0274d50e ( meaning the writev is already merged ). \r\nNote: there is still a difference between 5.0.7 and filipecosta90/zset.regression so I need to check exactly where the 9K ops/sec difference is still coming from. \r\n\r\n```\r\n\r\n######################\r\n5.0.7\r\n######################\r\n\r\nTop Hotspots\r\nFunction                    Module           CPU Time\r\n--------------------------  ---------------  --------\r\n__libc_write                libpthread.so.0   39.478s\r\n__libc_read                 libpthread.so.0    6.332s\r\nread                        redis-server       1.739s\r\nepoll_wait                  libc.so.6          1.673s\r\n[Outside any known module]  [Unknown]          1.247s\r\n[Others]                    N/A                9.370s\r\n\r\n\r\n\r\nALL STATS\r\n=====================================================================================================\r\nType            Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n-----------------------------------------------------------------------------------------------------\r\nZrevranges    178644.09         1.11903         1.11900         1.41500         1.95900     20585.94 \r\nTotals        178644.09         1.11903         1.11900         1.41500         1.95900     20585.94 \r\n\r\n\r\n######################\r\nunstable\r\n######################\r\n\r\nTop Hotspots\r\nFunction                    Module           CPU Time\r\n--------------------------  ---------------  --------\r\n__GI___writev               libc.so.6         32.613s\r\n__libc_read                 libpthread.so.0    5.022s\r\nread                        redis-server       1.346s\r\nzslGetElementByRank         redis-server       1.298s\r\n[Outside any known module]  [Unknown]          1.046s\r\n[Others]                    N/A               18.526s\r\n\r\n\r\n\r\nALL STATS\r\n=====================================================================================================\r\nType            Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n-----------------------------------------------------------------------------------------------------\r\nZrevranges    142641.60         1.40161         1.39900         1.86300         2.28700     16437.22 \r\nTotals        142641.60         1.40161         1.39900         1.86300         2.28700     16437.22\r\n\r\n\r\n\r\n######################\r\nfilipecosta90/zset.regression\r\n######################\r\n\r\nALL STATS\r\n=====================================================================================================\r\nType            Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n-----------------------------------------------------------------------------------------------------\r\nZrevranges    167960.71         1.19021         1.19100         1.53500         2.07900     19354.85 \r\nTotals        167960.71         1.19021         1.19100         1.53500         2.07900     19354.85\r\n```"
    },
    {
      "id": 1048479543,
      "user": "oranagra",
      "created_at": "2022-02-23T06:20:19Z",
      "body": "@filipecosta90 i don't understand.. did you use the latest unstable (with writev), and completely revert https://github.com/redis/redis/pull/7844?\r\ni assume this it the test without pipeline?\r\nanyway, there are two many factors here, i was looking at either taking 6.2 and add some code to avoid using deferred replies, or, maybe slightly easier, taking 5.0 and changing it to use deferred replies."
    },
    {
      "id": 1048485950,
      "user": "panjf2000",
      "created_at": "2022-02-23T06:36:08Z",
      "body": "> @filipecosta90 i don't understand.. did you use the latest unstable (with writev), and completely revert #7844? i assume this it the test without pipeline? anyway, there are two many factors here, i was looking at either taking 6.2 and add some code to avoid using deferred replies, or, maybe slightly easier, taking 5.0 and changing it to use deferred replies.\r\n\r\nIt seems that @filipecosta90 added `zrangeGenericCommand()` back and replace the latest code in unstable with it rather than reverting #7844, see the commits history: https://github.com/filipecosta90/redis/commits/zset.regression\r\n@oranagra "
    },
    {
      "id": 1048591851,
      "user": "oranagra",
      "created_at": "2022-02-23T09:30:24Z",
      "body": "recipe:\r\n```\r\nrm dump.rdb ; src/redis-server --save \"\" &\r\nredis-cli zadd zz 0 a 1 b 2 c 3 d 4 e 5 f\r\nmemtier_benchmark --pipeline 16 --command \"zrange zz 0 -1\" --hide-histogram\r\n```\r\n\r\non 5.0:\r\n```\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nZranges   1003869.41         3.15023         3.00700         4.89500         6.23900     83329.00 \r\nwithout pipeline:\r\nZranges    195546.03         1.02185         0.91100         1.50300         2.04700     16231.85 \r\n```\r\non 6.2:\r\n```\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nZranges    240024.84        13.32925        12.92700        15.16700        20.73500     19923.94 \r\nwithout pipeline:\r\nZranges    160567.46         1.24477         1.12700         1.84700         2.31900     13328.35 \r\n```\r\n\r\napplying diff on 5.0:\r\n```diff\r\ndiff --git a/src/t_zset.c b/src/t_zset.c\r\nindex 989d5855e..25e8aec9b 100644\r\n--- a/src/t_zset.c\r\n+++ b/src/t_zset.c\r\n@@ -2458,7 +2458,8 @@ void zrangeGenericCommand(client *c, int reverse) {\r\n     rangelen = (end-start)+1;\r\n \r\n     /* Return the result in form of a multi-bulk reply */\r\n-    addReplyMultiBulkLen(c, withscores ? (rangelen*2) : rangelen);\r\n+    void *replylen = addDeferredMultiBulkLength(c);\r\n+    long orig_range = rangelen;\r\n \r\n     if (zobj->encoding == OBJ_ENCODING_ZIPLIST) {\r\n         unsigned char *zl = zobj->ptr;\r\n@@ -2520,6 +2521,7 @@ void zrangeGenericCommand(client *c, int reverse) {\r\n     } else {\r\n         serverPanic(\"Unknown sorted set encoding\");\r\n     }\r\n+    setDeferredMultiBulkLength(c, replylen, withscores ? (orig_range*2) : orig_range);\r\n }\r\n \r\n void zrangeCommand(client *c) {\r\n````\r\n\r\n```\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nZranges    202284.34        15.81500        15.74300        18.81500        21.88700     16791.18 \r\nwithout pipeline:\r\nZranges    166672.24         1.19906         1.07900         1.75100         2.94300     13835.10 \r\n```\r\n\r\nraw patch on 6.2:\r\n```diff\r\ndiff --git a/src/t_zset.c b/src/t_zset.c\r\nindex 2abc1b49b..f59f96e56 100644\r\n--- a/src/t_zset.c\r\n+++ b/src/t_zset.c\r\n@@ -2870,7 +2870,7 @@ typedef enum {\r\n \r\n typedef struct zrange_result_handler zrange_result_handler;\r\n \r\n-typedef void (*zrangeResultBeginFunction)(zrange_result_handler *c);\r\n+typedef void (*zrangeResultBeginFunction)(zrange_result_handler *c, long hint);\r\n typedef void (*zrangeResultFinalizeFunction)(\r\n     zrange_result_handler *c, size_t result_count);\r\n typedef void (*zrangeResultEmitCBufferFunction)(\r\n@@ -2899,7 +2899,12 @@ struct zrange_result_handler {\r\n };\r\n \r\n /* Result handler methods for responding the ZRANGE to clients. */\r\n-static void zrangeResultBeginClient(zrange_result_handler *handler) {\r\n+static void zrangeResultBeginClient(zrange_result_handler *handler, long hint) {\r\n+    if (hint > 0) {\r\n+        addReplyArrayLen(handler->client, hint);\r\n+        handler->userdata = NULL;\r\n+        return;\r\n+    }\r\n     handler->userdata = addReplyDeferredLen(handler->client);\r\n }\r\n \r\n@@ -2941,12 +2946,14 @@ static void zrangeResultFinalizeClient(zrange_result_handler *handler,\r\n         result_count *= 2;\r\n     }\r\n \r\n-    setDeferredArrayLen(handler->client, handler->userdata, result_count);\r\n+    if (handler->userdata)\r\n+        setDeferredArrayLen(handler->client, handler->userdata, result_count);\r\n }\r\n \r\n /* Result handler methods for storing the ZRANGESTORE to a zset. */\r\n-static void zrangeResultBeginStore(zrange_result_handler *handler)\r\n+static void zrangeResultBeginStore(zrange_result_handler *handler, long hint)\r\n {\r\n+    UNUSED(hint);\r\n     handler->dstobj = createZsetZiplistObject();\r\n }\r\n \r\n@@ -3041,7 +3048,7 @@ void genericZrangebyrankCommand(zrange_result_handler *handler,\r\n     if (end < 0) end = llen+end;\r\n     if (start < 0) start = 0;\r\n \r\n-    handler->beginResultEmission(handler);\r\n+    handler->beginResultEmission(handler, end-start+1);\r\n \r\n     /* Invariant: start >= 0, so this test will be true when end < 0.\r\n      * The range is empty when start > end or start >= length. */\r\n@@ -3148,7 +3155,7 @@ void genericZrangebyscoreCommand(zrange_result_handler *handler,\r\n     client *c = handler->client;\r\n     unsigned long rangelen = 0;\r\n \r\n-    handler->beginResultEmission(handler);\r\n+    handler->beginResultEmission(handler,-1);\r\n \r\n     /* For invalid offset, return directly. */\r\n     if (offset > 0 && offset >= (long)zsetLength(zobj)) {\r\n@@ -3437,7 +3444,7 @@ void genericZrangebylexCommand(zrange_result_handler *handler,\r\n     client *c = handler->client;\r\n     unsigned long rangelen = 0;\r\n \r\n-    handler->beginResultEmission(handler);\r\n+    handler->beginResultEmission(handler,-1);\r\n \r\n     if (zobj->encoding == OBJ_ENCODING_ZIPLIST) {\r\n         unsigned char *zl = zobj->ptr;\r\n@@ -3680,7 +3687,7 @@ void zrangeGenericCommand(zrange_result_handler *handler, int argc_start, int st\r\n         lookupKeyRead(c->db,key);\r\n     if (zobj == NULL) {\r\n         if (store) {\r\n-            handler->beginResultEmission(handler);\r\n+            handler->beginResultEmission(handler,-1);\r\n             handler->finalizeResultEmission(handler, 0);\r\n         } else {\r\n             addReply(c, shared.emptyarray);\r\n```\r\nbenchmark:\r\n```\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nZranges   1455472.72         2.19597         2.27100         3.75900         5.37500     61118.48 \r\nwithout pipeline\r\nZranges    203077.08         0.98460         0.87900         1.40700         2.11100      8527.65 \r\n```\r\n\r\nbottom line:\r\n1. other than the regression about the deferred replies (which does have some impact on non-pipelined too), 6.2 also includes some improvement.\r\n2. 7.0 (unstable), might contain other improvements (or regressions)"
    },
    {
      "id": 1048601391,
      "user": "oranagra",
      "created_at": "2022-02-23T09:42:25Z",
      "body": "for reference\r\n\r\n7.0-RC1 (before writev):\r\n```\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nZranges    228047.73        14.02753        13.95100        17.15100        20.99100     18929.74 \r\nwithout pipeline:\r\nZranges    141687.86         1.41228         1.44700         2.17500         3.90300     11761.20 \r\n```\r\n\r\nunstable (with writev):\r\n```\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nZranges    656797.63         4.86072         4.51100         8.89500        11.39100     54519.33 \r\nwithout pipeline\r\nZranges    139986.54         1.42811         1.47900         2.14300         2.94300     11619.98 \r\n```\r\n\r\n7.0 with the above patch to avoid using deferred replies (without writev):\r\n```\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nZranges    868107.17         3.67039         3.59900         5.59900         7.83900     72059.68 \r\nwithout pipeline:\r\nZranges    175740.25         1.13698         1.05500         1.69500         2.41500     14587.81 \r\n```\r\n\r\nunstable with the above patch to avoid using deferred replies (with writev):\r\n```\r\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec \r\n--------------------------------------------------------------------------------------------------\r\nZranges    801576.54         3.96586         3.93500         7.77500         8.95900     66537.12 \r\nwithout pipeline:\r\nZranges    163546.07         1.22226         1.26300         2.06300         5.56700     13575.60 \r\n```\r\n\r\n\r\n\r\nbottom line:\r\n1. writev doesn't completely eliminate the overheads of deferred replies.\r\n2. there is another regression in unstable compared to 6.2"
    },
    {
      "id": 1048608645,
      "user": "filipecosta90",
      "created_at": "2022-02-23T09:51:41Z",
      "body": "@oranagra  WRT to: \r\n\r\n> 1. writev doesn't completely eliminate the overheads of deferred replies.\r\n\r\non unstable ( de6be8850f1cec69263ad4f5713ef06f0274d50e ) even with writev usage there is 2.4% CPU cycles which are within zrange code that are not related to write/writev ( the sprintf of setDefferedAggregatelen is costly ) \r\n![image](https://user-images.githubusercontent.com/5832149/155295244-43cee512-c0ac-450b-9d4a-c495e1e696bf.png)\r\n\r\n> 2. there is another regression in unstable compared to 6.2\r\n\r\napply patch and profile? agree?"
    },
    {
      "id": 1048627685,
      "user": "oranagra",
      "created_at": "2022-02-23T10:14:24Z",
      "body": "yes, obviously other than write system calls, deferred reply involve more heap allocations.\r\nand now i realize they also use sprintf instead of the optimization of using the pre-created `shared.mbulkhdr`.\r\nwe can probably easily fix the second part (use `shared.mbulkhdr` (and maybe create one for maps too).\r\n\r\nfeel free to apply my patch (still needs correct handling of `withscores` and `c->resp`), code this sprintf optimization and profile / benchmark to see what's left...\r\nwe'll surely be left with the heap allocation dereference, and cache misses overheads."
    },
    {
      "id": 1049730172,
      "user": "filipecosta90",
      "created_at": "2022-02-24T10:50:15Z",
      "body": "@oranagra using the changes on https://github.com/redis/redis/pull/10337 we see that for pipeline 1 we're at the same level of v5 and that for pipeline 16 we're still with a gap of around 17% of CPU cycles.\r\n\r\nWe can pinpoint ~8% of extra CPU cycles to the following added logic ( profiling pipeline 16). Please advise if in your opinion we can squeeze further/reduce the overhead of the following features:\r\n\r\n| Function\t| CPU Time: Total\t| Introduced |\r\n-- | -- |  -- | \r\n| updateClientMemUsage\t| 3.6%\t| https://github.com/redis/redis/pull/8687 |\r\n| updateCachedTime\t| 1.8%\t| https://github.com/redis/redis/pull/9194 |\r\n| ACLCheckAllUserCommandPerm\t| 1.2%\t| https://github.com/redis/redis/pull/9974 |\r\n| updateCommandLatencyHistogram\t| 0.8%\t| https://github.com/redis/redis/pull/9462/ |\r\n\r\nWRT to cache misses overhead difference between v5 and https://github.com/redis/redis/pull/10337 I was surprised to see that the % of cache misses and the total stall cycles remained the same between v5 and the PR. \r\n- v5.0.7 stall cycles % ( uops_executed.stall_cycles / uops_executed.core  ): 6.09%\r\n- PR stall cycles % ( uops_executed.stall_cycles / uops_executed.core  ): 5,90%\r\n\r\nThis points us that indeed the added logic / added CPU cycles are the cause of regression and that according to the data there seems to be NO change on memory overhead/stall cycles. agree?\r\n\r\n#### perf stat v5.0.7:\r\n\r\n```\r\n Performance counter stats for process id '5063':\r\n\r\n      59931.618609      cpu-clock (msec)          #    0.999 CPUs utilized          \r\n      213421613490      cpu-cycles                #    3.561 GHz                      (72.72%)\r\n      526851248464      instructions              #    2.47  insn per cycle           (81.81%)\r\n      592920536223      uops_executed.core        # 9893.284 M/sec                    (81.81%)\r\n       35011134086      uops_executed.stall_cycles #  584.185 M/sec                    (81.81%)\r\n         828763445      cache-references          #   13.828 M/sec                    (81.82%)\r\n            709741      cache-misses              #    0.086 % of all cache refs      (81.83%)\r\n       35030337184      cycle_activity.stalls_total #  584.505 M/sec                    (81.83%)\r\n       23099126236      cycle_activity.stalls_mem_any #  385.425 M/sec                    (81.83%)\r\n          28164639      cycle_activity.stalls_l3_miss #    0.470 M/sec                    (81.82%)\r\n        9240003766      cycle_activity.stalls_l2_miss #  154.176 M/sec                    (81.82%)\r\n       10527552768      cycle_activity.stalls_l1d_miss #  175.659 M/sec                    (72.72%)\r\n\r\n      60.001101633 seconds time elapsed\r\n```\r\n\r\n#### perf stat PR:\r\n\r\n```\r\n\r\n Performance counter stats for process id '361':\r\n\r\n      59918.131991      cpu-clock (msec)          #    0.999 CPUs utilized          \r\n      212043287497      cpu-cycles                #    3.539 GHz                      (72.72%)\r\n      510785333068      instructions              #    2.41  insn per cycle           (81.81%)\r\n      554512273393      uops_executed.core        # 9254.499 M/sec                    (81.82%)\r\n       33775863959      uops_executed.stall_cycles #  563.700 M/sec                    (81.82%)\r\n         740966569      cache-references          #   12.366 M/sec                    (81.82%)\r\n            429245      cache-misses              #    0.058 % of all cache refs      (81.82%)\r\n       33779169255      cycle_activity.stalls_total #  563.755 M/sec                    (81.82%)\r\n       21438524734      cycle_activity.stalls_mem_any #  357.797 M/sec                    (81.82%)\r\n          13705439      cycle_activity.stalls_l3_miss #    0.229 M/sec                    (81.82%)\r\n        7519901229      cycle_activity.stalls_l2_miss #  125.503 M/sec                    (81.82%)\r\n        8720668007      cycle_activity.stalls_l1d_miss #  145.543 M/sec                    (72.72%)\r\n\r\n      60.001379977 seconds time elapsed\r\n```\r\n"
    },
    {
      "id": 1049745591,
      "user": "oranagra",
      "created_at": "2022-02-24T11:07:05Z",
      "body": "@filipecosta90 please correct me if i'm wrong.\r\n1. it doesn't matter if #10337 contains #9934 and #10334 or not, since it doesn't use deferred replies.\r\n2. the list of PRs you're showing that introduced performance loss are all not specific for `ZRANGE`, i.e. they'll affect a pipeline of `SET`s too"
    }
  ]
}