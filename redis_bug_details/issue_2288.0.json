{
  "issue_number": 2288.0,
  "title": "[2.9.102] Assertion failed: cluster.c:4641 'n != NULL' is not true",
  "body": "This happened upon issuing a manual CLUSTER FAILOVER to the slave. It was the former master where the error occured, the slave is fine and the cluster check reports okay, so hooray for that :)\n\nThis was during a test setup with a client molesting the cluster with a number of SET/MGET. Before the manual failover the cluster looked like this:\n\n```\nConnecting to node 192.168.67.140:7001: OK\nConnecting to node 192.168.67.140:7003: OK\nConnecting to node 192.168.67.140:7002: OK\nConnecting to node 192.168.67.140:7004: OK\n>>> Performing Cluster Check (using node 192.168.67.140:7001)\nM: d6f491085bba2bbcb651f84aee5aaef44b5714c4 192.168.67.140:7001\n   slots:2000-6461,10923-11921 (5461 slots) master\n   1 additional replica(s)\nM: 1419cb638df1943b92ced1afc087d538fb92cbba 192.168.67.140:7003\n   slots:0-1999,6462-9923 (5462 slots) master\n   0 additional replica(s)\nM: d5a68246ace4982f403f95f6a850d2192454c4db 192.168.67.140:7002\n   slots:9924-10922,11922-16383 (5461 slots) master\n   0 additional replica(s)\nS: 3c97c3f44fe2a231335348e2d7cbdb81312115e9 192.168.67.140:7004\n   slots: (0 slots) slave\n   replicates d6f491085bba2bbcb651f84aee5aaef44b5714c4\n[OK] All nodes agree about slots configuration.\n>>> Check for open slots...\n>>> Check slots coverage...\n[OK] All 16384 slots covered.\n```\n\nAnd after the manual failover (and crash of node 1419cb638df1943b92ced1afc087d538fb92cbba) it looked like this:\n\n```\nConnecting to node 192.168.67.140:7002: OK\nConnecting to node 192.168.67.140:7001: OK\nConnecting to node 192.168.67.140:7004: OK\n>>> Performing Cluster Check (using node 192.168.67.140:7002)\nM: d5a68246ace4982f403f95f6a850d2192454c4db 192.168.67.140:7002\n   slots:9924-10922,11922-16383 (5461 slots) master\n   0 additional replica(s)\nM: d6f491085bba2bbcb651f84aee5aaef44b5714c4 192.168.67.140:7001\n   slots:2000-6461,10923-11921 (5461 slots) master\n   0 additional replica(s)\nM: 3c97c3f44fe2a231335348e2d7cbdb81312115e9 192.168.67.140:7004\n   slots:0-1999,6462-9923 (5462 slots) master\n   0 additional replica(s)\n[OK] All nodes agree about slots configuration.\n>>> Check for open slots...\n>>> Check slots coverage...\n[OK] All 16384 slots covered.\n```\n\nCrash report:\n\n```\n21402:M 15 Jan 10:23:36.048 # Manual failover requested by slave 3c97c3f44fe2a231335348e2d7cbdb81312115e9.\n21402:M 15 Jan 10:23:36.203 # Failover auth granted to 3c97c3f44fe2a231335348e2d7cbdb81312115e9 for epoch 9\n21402:M 15 Jan 10:23:36.204 # Configuration change detected. Reconfiguring myself as a replica of 3c97c3f44fe2a231335348e2d7cbdb81312115e9\n21402:S 15 Jan 10:23:36.204 # Connection with slave 192.168.67.140:7004 lost.\n21402:S 15 Jan 10:23:36.204 # \n\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\n21402:S 15 Jan 10:23:36.205 # === ASSERTION FAILED CLIENT CONTEXT ===\n21402:S 15 Jan 10:23:36.205 # client->flags = 0\n21402:S 15 Jan 10:23:36.205 # client->fd = 18\n21402:S 15 Jan 10:23:36.205 # client->argc = 2\n21402:S 15 Jan 10:23:36.205 # client->argv[0] = \"GET\" (refcount: 1)\n21402:S 15 Jan 10:23:36.205 # client->argv[1] = \"696\" (refcount: 1)\n21402:S 15 Jan 10:23:36.205 # === ASSERTION FAILED OBJECT CONTEXT ===\n21402:S 15 Jan 10:23:36.205 # Object type: 0\n21402:S 15 Jan 10:23:36.205 # Object encoding: 8\n21402:S 15 Jan 10:23:36.205 # Object refcount: 1\n21402:S 15 Jan 10:23:36.205 # Object raw string len: 3\n21402:S 15 Jan 10:23:36.205 # Object raw string content: \"696\"\n21402:S 15 Jan 10:23:36.205 # === ASSERTION FAILED ===\n21402:S 15 Jan 10:23:36.205 # ==> cluster.c:4641 'n != NULL' is not true\n21402:S 15 Jan 10:23:36.205 # (forcing SIGSEGV to print the bug report.)\n21402:S 15 Jan 10:23:36.205 #     Redis 2.9.102 crashed by signal: 11\n21402:S 15 Jan 10:23:36.205 #     Failed assertion: n != NULL (cluster.c:4641)\n21402:S 15 Jan 10:23:36.205 # --- STACK TRACE\nredis-server *:7003 [cluster](logStackTrace+0x3e)[0x4507be]\nredis-server *:7003 [cluster](_redisAssert+0x6f)[0x44f78f]\n/lib/x86_64-linux-gnu/libpthread.so.0(+0xfcb0)[0x7f0bbc518cb0]\nredis-server *:7003 [cluster](_redisAssert+0x6f)[0x44f78f]\nredis-server *:7003 [cluster](getNodeByQuery+0x3e5)[0x45a7c5]\nredis-server *:7003 [cluster](processCommand+0x21d)[0x422acd]\nredis-server *:7003 [cluster](processInputBuffer+0x51)[0x42cb01]\nredis-server *:7003 [cluster](processUnblockedClients+0x5a)[0x46719a]\nredis-server *:7003 [cluster](beforeSleep+0x112)[0x41e442]\nredis-server *:7003 [cluster](aeMain+0x1e)[0x41acde]\nredis-server *:7003 [cluster](main+0x334)[0x4199e4]\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xed)[0x7f0bbc16a76d]\nredis-server *:7003 [cluster][0x419b8d]\n21402:S 15 Jan 10:23:36.208 # --- INFO OUTPUT\n21402:S 15 Jan 10:23:36.208 # # Server\nredis_version:2.9.102\nredis_git_sha1:00000000\nredis_git_dirty:0\nredis_build_id:1aa46b248d9d0fea\nredis_mode:cluster\nos:Linux 3.5.0-23-generic x86_64\narch_bits:64\nmultiplexing_api:epoll\ngcc_version:4.6.3\nprocess_id:21402\nrun_id:27bacc2ba37e7b91306c430bd84ae8aae01318d3\ntcp_port:7003\nuptime_in_seconds:259\nuptime_in_days:0\nhz:10\nlru_clock:12060072\nconfig_file:/home/ingmar/redis3/redis.conf\n\n# Clients\nconnected_clients:1\nclient_longest_output_list:0\nclient_biggest_input_buf:0\nblocked_clients:0\n\n# Memory\nused_memory:1029640\nused_memory_human:1005.51K\nused_memory_rss:3903488\nused_memory_peak:2171840\nused_memory_peak_human:2.07M\nused_memory_lua:35840\nmem_fragmentation_ratio:3.79\nmem_allocator:jemalloc-3.6.0\n\n# Persistence\nloading:0\nrdb_changes_since_last_save:48651\nrdb_bgsave_in_progress:0\nrdb_last_save_time:1421346149\nrdb_last_bgsave_status:ok\nrdb_last_bgsave_time_sec:0\nrdb_current_bgsave_time_sec:-1\naof_enabled:1\naof_rewrite_in_progress:0\naof_rewrite_scheduled:0\naof_last_rewrite_time_sec:-1\naof_current_rewrite_time_sec:-1\naof_last_bgrewrite_status:ok\naof_last_write_status:ok\naof_current_size:3787424\naof_base_size:0\naof_pending_rewrite:0\naof_buffer_length:0\naof_rewrite_buffer_length:0\naof_pending_bio_fsync:0\naof_delayed_fsync:0\n\n# Stats\ntotal_connections_received:89\ntotal_commands_processed:246918\ninstantaneous_ops_per_sec:1312\ntotal_net_input_bytes:7525025\ntotal_net_output_bytes:3539185\ninstantaneous_input_kbps:35.76\ninstantaneous_output_kbps:32.09\nrejected_connections:0\nsync_full:1\nsync_partial_ok:0\nsync_partial_err:0\nexpired_keys:0\nevicted_keys:0\nkeyspace_hits:117153\nkeyspace_misses:686\npubsub_channels:0\npubsub_patterns:0\nlatest_fork_usec:140\nmigrate_cached_sockets:0\n\n# Replication\nrole:slave\nmaster_host:192.168.67.140\nmaster_port:7004\nmaster_link_status:down\nmaster_last_io_seconds_ago:-1\nmaster_sync_in_progress:0\nslave_repl_offset:1\nmaster_link_down_since_seconds:1421346216\nslave_priority:100\nslave_read_only:1\nconnected_slaves:0\nmaster_repl_offset:0\nrepl_backlog_active:0\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:504936\nrepl_backlog_histlen:1048576\n\n# CPU\nused_cpu_sys:15.12\nused_cpu_user:0.28\nused_cpu_sys_children:0.00\nused_cpu_user_children:0.00\n\n# Commandstats\ncmdstat_get:calls=117153,usec=170044,usec_per_call=1.45\ncmdstat_set:calls=117290,usec=236851,usec_per_call=2.02\ncmdstat_select:calls=686,usec=721,usec_per_call=1.05\ncmdstat_ping:calls=8,usec=8,usec_per_call=1.00\ncmdstat_psync:calls=1,usec=252,usec_per_call=252.00\ncmdstat_replconf:calls=68,usec=110,usec_per_call=1.62\ncmdstat_info:calls=2,usec=84,usec_per_call=42.00\ncmdstat_cluster:calls=11024,usec=42181,usec_per_call=3.83\ncmdstat_restore-asking:calls=686,usec=2780,usec_per_call=4.05\n\n# Cluster\ncluster_enabled:1\n\n# Keyspace\ndb0:keys=686,expires=0,avg_ttl=0\nhash_init_value: 1421558963\n\n21402:S 15 Jan 10:23:36.211 # --- CLIENT LIST OUTPUT\n21402:S 15 Jan 10:23:36.211 # id=7 addr=192.168.67.1:65129 fd=18 name= age=201 idle=1 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=22 obl=0 oll=0 omem=0 events=r cmd=get\n\n21402:S 15 Jan 10:23:36.211 # --- CURRENT CLIENT INFO\n21402:S 15 Jan 10:23:36.211 # client: id=7 addr=192.168.67.1:65129 fd=18 name= age=201 idle=1 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=22 obl=0 oll=0 omem=0 events=r cmd=get\n21402:S 15 Jan 10:23:36.211 # argv[0]: 'GET'\n21402:S 15 Jan 10:23:36.211 # argv[1]: '696'\n21402:S 15 Jan 10:23:36.211 # key '696' found in DB containing the following object:\n21402:S 15 Jan 10:23:36.211 # Object type: 0\n21402:S 15 Jan 10:23:36.211 # Object encoding: 1\n21402:S 15 Jan 10:23:36.211 # Object refcount: 2\n21402:S 15 Jan 10:23:36.211 # --- REGISTERS\n21402:S 15 Jan 10:23:36.211 # \nRAX:0000000000000000 RBX:0000000000001221\nRCX:0000000000000001 RDX:0000000000000000\nRDI:00007f0bbc503260 RSI:00007f0bbc504ab0\nRBP:00000000004c2811 RSP:00007fff19391230\nR8 :00007f0bbcc38780 R9 :0000000000000001\nR10:7562206568742074 R11:0000000000000000\nR12:00000000004bdddb R13:00007f0bbb581760\nR14:0000000000000000 R15:0000000000000000\nRIP:000000000044f78f EFL:0000000000010202\nCSGSFS:0000000000000033\n21402:S 15 Jan 10:23:36.211 # (00007fff1939123f) -> 00007fff19391348\n21402:S 15 Jan 10:23:36.211 # (00007fff1939123e) -> 00007f0bbb5a0000\n21402:S 15 Jan 10:23:36.211 # (00007fff1939123d) -> 00000000006ed9a0\n21402:S 15 Jan 10:23:36.211 # (00007fff1939123c) -> 00007fff1939134c\n21402:S 15 Jan 10:23:36.211 # (00007fff1939123b) -> 00007fff193912b0\n21402:S 15 Jan 10:23:36.211 # (00007fff1939123a) -> 0000000000000000\n21402:S 15 Jan 10:23:36.211 # (00007fff19391239) -> 0000000000000000\n21402:S 15 Jan 10:23:36.211 # (00007fff19391238) -> 0000000000000000\n21402:S 15 Jan 10:23:36.211 # (00007fff19391237) -> 0000000000000000\n21402:S 15 Jan 10:23:36.211 # (00007fff19391236) -> 00007f0bbb571e30\n21402:S 15 Jan 10:23:36.211 # (00007fff19391235) -> 00007f0bbb40e070\n21402:S 15 Jan 10:23:36.211 # (00007fff19391234) -> 0000000000000010\n21402:S 15 Jan 10:23:36.211 # (00007fff19391233) -> 000000000045a7c5\n21402:S 15 Jan 10:23:36.211 # (00007fff19391232) -> 0000000000001e9d\n21402:S 15 Jan 10:23:36.211 # (00007fff19391231) -> 0000000000000000\n21402:S 15 Jan 10:23:36.211 # (00007fff19391230) -> 0000000000000000\n21402:S 15 Jan 10:23:36.211 # --- FAST MEMORY TEST\n21402:S 15 Jan 10:23:36.211 # Bio thread for job type #0 terminated\n21402:S 15 Jan 10:23:36.212 # Bio thread for job type #1 terminated\nTesting 6f2000 90112\nTesting 1a9a000 135168\nTesting 7f0bba3ff000 8388608\nTesting 7f0bbac00000 12582912\nTesting 7f0bbbc00000 4194304\nTesting 7f0bbc504000 20480\nTesting 7f0bbc722000 16384\nTesting 7f0bbcc38000 16384\nTesting 7f0bbcc40000 16384\n21402:S 15 Jan 10:23:36.404 # Fast memory test PASSED, however your memory can still be broken. Please run a memory test for several hours if possible.\n21402:S 15 Jan 10:23:36.404 # \n=== REDIS BUG REPORT END. Make sure to include from START to END. ===\n```\n\nThe test script was very simple:\n\n```\nfrom rediscluster.client import RedisCluster\nfrom time import time\n\nstartup_nodes = [\n    {'host': '192.168.67.140', 'port': '7002'},\n    {'host': '192.168.67.140', 'port': '7004'},\n]\n\nrc = RedisCluster(startup_nodes=startup_nodes, decode_responses=True)\n\nwhile True:\n    t = time()\n    for i in range(2048):\n        rc.set(i, i)\n    l = rc.mget(list(range(2048)))\n    duration = time() - t\n    print len(l), \"%f ms\" % (duration * 1000.0)\n```\n",
  "state": "closed",
  "created_at": "2015-01-15T18:42:19Z",
  "updated_at": "2015-03-20T09:36:59Z",
  "closed_at": "2015-03-20T09:36:59Z",
  "labels": [
    "critical bug",
    "crash report",
    "state-op-waiting-for-reply",
    "cluster"
  ],
  "comments_data": [
    {
      "id": 83769257,
      "user": "antirez",
      "created_at": "2015-03-19T21:38:25Z",
      "body": "Hello, thank you for reporting. Checking the code, it makes sense it crashes like that in certain edge conditions. The code pretends that we never dispatch commands if the cluster is logically down (one or more slots uncovered), but this is not the case. The fix will be the same anyway (since handling this in getNodeByQuery() will prevent the next bug similar to that), however I also want to check the exact conditions that trigger it in order to also trap it before, but I've an idea about what the state is. Fixing tomorrow morning CEST giving priority over the doc writing effort, and releasing a new RC. However this is not very critical (it is in the effects, but not conceptually), so the stable release will not be delayed.\n"
    },
    {
      "id": 83956911,
      "user": "antirez",
      "created_at": "2015-03-20T08:36:55Z",
      "body": "@ingmar checking the issue I see this happened with Redis 3.0.0-rc2, please note that rc3 fixed several critical bugs so you are likely to run into strange consistency issues in the CLUSTER NODES output and other crashes. This issue itself may be due to the same set of bugs, however whatever this is the case or not, the issue posted contains an hit about an implementation fragility that should be fixed :-)\n"
    },
    {
      "id": 83962701,
      "user": "antirez",
      "created_at": "2015-03-20T09:20:11Z",
      "body": "Problem fixed, a few notes:\n1. It's not 100% clear to me, in this exact setup, how the hash slots were unbound. As a result of the configuration update received from the (old) slave I expected the master to have all the hash slots still covered. Maybe it's due to the many RC2 serious memory bugs, fixed by RC3.\n2. Anyway the getNodeByQuery() implementation was fragile. It's not a good idea to assert for `node != NULL`, and in general, to the fact that the cluster state is atomically updated.\n3. And... indeed the cluster state is _not_ atomically updated, so there was a bug here regardless of what exactly caused this instance of the bug. The cluster state is updated in `clusterBeforeSleep()` in a lazy way.\n4. Moreover the `clusterBeforeSleep()` call was misplaced at the end of the chain of the `beforeSleep()` call in `redis.c`. It should be at the top, before processing un blocking clients. This is exactly the reason in the specific instance of the bug as reported, why the state was not updated in time before clients served.\n\nReleasing RC5 in a second. Thank you for reporting!\n"
    }
  ]
}