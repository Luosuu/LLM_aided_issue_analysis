{
  "issue_number": 9956.0,
  "title": "[BUG] redis sentinel 100% cpu usage ",
  "body": "**Describe the bug**\r\nredis sentinel is having 100% cpu usage. we are using redis-ha helm chart version 4.12.15  under default configuration but we have increased resource requests and limits by a significant amount. The redis version used by us is 6.0.7.\r\n\r\n**To reproduce**\r\n\r\nI'm not sure if it can be reproduced by anyone else.\r\nEven after redis restart, one or two sentinel reaches 100% CPU utilization. \r\nOther redis metrics are fine and seems okay.\r\n\r\n**Expected behavior**\r\nSentinel should be taking so much CPU.\r\n\r\n\r\n**Additional information**\r\n\r\nI ran strace on one of the sentinel and found this: \r\n```\r\n1 09:20:08 recvfrom(11, 0x7ffd55fab4a0, 16384, 0, NULL, NULL) = -1 EAGAIN (Resource temporarily unavailable)\r\n1 09:20:08 recvfrom(10, 0x7ffd55fab4a0, 16384, 0, NULL, NULL) = -1 EAGAIN (Resource temporarily unavailable)\r\n1 09:20:08 epoll_pwait(5, [{events=EPOLLERR, data={u32=11, u64=11}}, {events=EPOLLERR, data={u32=10, u64=10}}], 10128, 8, NULL, ðŸ˜Ž = 2\r\n```\r\nThis seems like a repeating pattern; where redis is trying to read from two sockets and failing continuously and this followed by a epoll_wait. It happens more than 100x times in a second.\r\n\r\nsocket 11 and 10 seems to point to a connection with the redis process for this sentinel. To be exact it points to the ClusterIP of the redis-cluster-announce pod on which sentinel was running and the port is 6379.\r\n\r\nredis\r\n```\r\n# Server\r\nredis_version:6.0.7\r\nredis_git_sha1:00000000\r\nredis_git_dirty:0\r\nredis_build_id:4b6e03cc4dd13d7e\r\nredis_mode:standalone\r\nos:Linux 5.10.68-62.173.amzn2.x86_64 x86_64\r\narch_bits:64\r\nmultiplexing_api:epoll\r\natomicvar_api:atomic-builtin\r\ngcc_version:9.3.0\r\nprocess_id:1\r\nrun_id:9e5a59931ada7e4b1198b6a5beb98defb5834cc0\r\ntcp_port:6379\r\nuptime_in_seconds:66843\r\nuptime_in_days:0\r\nhz:10\r\nconfigured_hz:10\r\nlru_clock:12347300\r\nexecutable:/data/redis-server\r\nconfig_file:/data/conf/redis.conf\r\nio_threads_active:0\r\n\r\n# Clients\r\nconnected_clients:8\r\nclient_recent_max_input_buffer:2\r\nclient_recent_max_output_buffer:0\r\nblocked_clients:0\r\ntracking_clients:0\r\nclients_in_timeout_table:0\r\n\r\n# Memory\r\nused_memory:2727256\r\nused_memory_human:2.60M\r\nused_memory_rss:8065024\r\nused_memory_rss_human:7.69M\r\nused_memory_peak:2981024\r\nused_memory_peak_human:2.84M\r\nused_memory_peak_perc:91.49%\r\nused_memory_overhead:2137792\r\nused_memory_startup:803016\r\nused_memory_dataset:589464\r\nused_memory_dataset_perc:30.63%\r\nallocator_allocated:2827152\r\nallocator_active:3174400\r\nallocator_resident:6668288\r\ntotal_system_memory:66245685248\r\ntotal_system_memory_human:61.70G\r\nused_memory_lua:48128\r\nused_memory_lua_human:47.00K\r\nused_memory_scripts:3072\r\nused_memory_scripts_human:3.00K\r\nnumber_of_cached_scripts:4\r\nmaxmemory:0\r\nmaxmemory_human:0B\r\nmaxmemory_policy:volatile-lru\r\nallocator_frag_ratio:1.12\r\nallocator_frag_bytes:347248\r\nallocator_rss_ratio:2.10\r\nallocator_rss_bytes:3493888\r\nrss_overhead_ratio:1.21\r\nrss_overhead_bytes:1396736\r\nmem_fragmentation_ratio:2.96\r\nmem_fragmentation_bytes:5339328\r\nmem_not_counted_for_evict:0\r\nmem_replication_backlog:1048576\r\nmem_clients_slaves:0\r\nmem_clients_normal:135888\r\nmem_aof_buffer:0\r\nmem_allocator:jemalloc-5.1.0\r\nactive_defrag_running:0\r\nlazyfree_pending_objects:0\r\n\r\n# Persistence\r\nloading:0\r\nrdb_changes_since_last_save:23\r\nrdb_bgsave_in_progress:0\r\nrdb_last_save_time:1639737083\r\nrdb_last_bgsave_status:ok\r\nrdb_last_bgsave_time_sec:0\r\nrdb_current_bgsave_time_sec:-1\r\nrdb_last_cow_size:524288\r\naof_enabled:0\r\naof_rewrite_in_progress:0\r\naof_rewrite_scheduled:0\r\naof_last_rewrite_time_sec:-1\r\naof_current_rewrite_time_sec:-1\r\naof_last_bgrewrite_status:ok\r\naof_last_write_status:ok\r\naof_last_cow_size:0\r\nmodule_fork_in_progress:0\r\nmodule_fork_last_cow_size:0\r\n\r\n# Stats\r\ntotal_connections_received:20058\r\ntotal_commands_processed:491483\r\ninstantaneous_ops_per_sec:5\r\ntotal_net_input_bytes:33351575\r\ntotal_net_output_bytes:275568645\r\ninstantaneous_input_kbps:0.54\r\ninstantaneous_output_kbps:1.59\r\nrejected_connections:0\r\nsync_full:0\r\nsync_partial_ok:0\r\nsync_partial_err:0\r\nexpired_keys:0\r\nexpired_stale_perc:0.00\r\nexpired_time_cap_reached_count:0\r\nexpire_cycle_cpu_milliseconds:0\r\nevicted_keys:0\r\nkeyspace_hits:0\r\nkeyspace_misses:0\r\npubsub_channels:1\r\npubsub_patterns:0\r\nlatest_fork_usec:331\r\nmigrate_cached_sockets:0\r\nslave_expires_tracked_keys:0\r\nactive_defrag_hits:0\r\nactive_defrag_misses:0\r\nactive_defrag_key_hits:0\r\nactive_defrag_key_misses:0\r\ntracking_total_keys:0\r\ntracking_total_items:0\r\ntracking_total_prefixes:0\r\nunexpected_error_replies:0\r\ntotal_reads_processed:501438\r\ntotal_writes_processed:1017489\r\nio_threaded_reads_processed:0\r\nio_threaded_writes_processed:0\r\n\r\n# Replication\r\nrole:slave\r\nmaster_host:172.20.61.168\r\nmaster_port:6379\r\nmaster_link_status:up\r\nmaster_last_io_seconds_ago:0\r\nmaster_sync_in_progress:0\r\nslave_repl_offset:226855520\r\nslave_priority:100\r\nslave_read_only:1\r\nconnected_slaves:0\r\nmin_slaves_good_slaves:0\r\nmaster_replid:6d50a55ecc1d386809fc186708bcc56db6f92765\r\nmaster_replid2:0000000000000000000000000000000000000000\r\nmaster_repl_offset:226855520\r\nsecond_repl_offset:-1\r\nrepl_backlog_active:1\r\nrepl_backlog_size:1048576\r\nrepl_backlog_first_byte_offset:225806945\r\nrepl_backlog_histlen:1048576\r\n\r\n# CPU\r\nused_cpu_sys:81.727130\r\nused_cpu_user:54.375754\r\nused_cpu_sys_children:0.204695\r\nused_cpu_user_children:0.501349\r\n\r\n# Modules\r\n\r\n# Cluster\r\ncluster_enabled:0\r\n\r\n# Keyspace\r\ndb0:keys=2861,expires=0,avg_ttl=0\r\n```\r\n\r\nsentinel\r\n```\r\n127.0.0.1:26379> info\r\n# Server\r\nredis_version:6.0.7\r\nredis_git_sha1:00000000\r\nredis_git_dirty:0\r\nredis_build_id:4b6e03cc4dd13d7e\r\nredis_mode:sentinel\r\nos:Linux 5.10.68-62.173.amzn2.x86_64 x86_64\r\narch_bits:64\r\nmultiplexing_api:epoll\r\natomicvar_api:atomic-builtin\r\ngcc_version:9.3.0\r\nprocess_id:1\r\nrun_id:0919ce29eb1b7f9dce36cea0a2eb73bf1740939c\r\ntcp_port:26379\r\nuptime_in_seconds:66926\r\nuptime_in_days:0\r\nhz:11\r\nconfigured_hz:10\r\nlru_clock:12347383\r\nexecutable:/data/redis-sentinel\r\nconfig_file:/data/conf/sentinel.conf\r\nio_threads_active:0\r\n\r\n# Clients\r\nconnected_clients:4\r\nclient_recent_max_input_buffer:2\r\nclient_recent_max_output_buffer:0\r\nblocked_clients:0\r\ntracking_clients:0\r\nclients_in_timeout_table:0\r\n\r\n# CPU\r\nused_cpu_sys:30052.725830\r\nused_cpu_user:9940.032902\r\nused_cpu_sys_children:0.000000\r\nused_cpu_user_children:0.005161\r\n\r\n# Stats\r\ntotal_connections_received:8925\r\ntotal_commands_processed:200173\r\ninstantaneous_ops_per_sec:5\r\ntotal_net_input_bytes:11111722\r\ntotal_net_output_bytes:1205818\r\ninstantaneous_input_kbps:0.35\r\ninstantaneous_output_kbps:0.03\r\nrejected_connections:0\r\nsync_full:0\r\nsync_partial_ok:0\r\nsync_partial_err:0\r\nexpired_keys:0\r\nexpired_stale_perc:0.00\r\nexpired_time_cap_reached_count:0\r\nexpire_cycle_cpu_milliseconds:772\r\nevicted_keys:0\r\nkeyspace_hits:0\r\nkeyspace_misses:0\r\npubsub_channels:1\r\npubsub_patterns:0\r\nlatest_fork_usec:0\r\nmigrate_cached_sockets:0\r\nslave_expires_tracked_keys:0\r\nactive_defrag_hits:0\r\nactive_defrag_misses:0\r\nactive_defrag_key_hits:0\r\nactive_defrag_key_misses:0\r\ntracking_total_keys:0\r\ntracking_total_items:0\r\ntracking_total_prefixes:0\r\nunexpected_error_replies:0\r\ntotal_reads_processed:204104\r\ntotal_writes_processed:195182\r\nio_threaded_reads_processed:0\r\nio_threaded_writes_processed:0\r\n\r\n# Sentinel\r\nsentinel_masters:1\r\nsentinel_tilt:0\r\nsentinel_running_scripts:0\r\nsentinel_scripts_queue_length:0\r\nsentinel_simulate_failure_flags:0\r\nmaster0:name=mymaster,status=ok,address=172.20.61.168:6379,slaves=2,sentinels=3\r\n```\r\nAny tops for debugging this?",
  "state": "open",
  "created_at": "2021-12-17T10:36:19Z",
  "updated_at": "2024-11-19T17:10:30Z",
  "closed_at": null,
  "labels": [
    "state:help-wanted"
  ],
  "comments_data": [
    {
      "id": 1303308826,
      "user": "betermieux",
      "created_at": "2022-11-04T11:45:55Z",
      "body": "Same problem here.\r\nAfter a few weeks of usage the slave sentinels spike 100% cpu, the master sentinel is idle.\r\nI am using the redis-ha chart from https://dandydeveloper.github.io/charts\r\nLet me know if I can provide more information to this issue."
    },
    {
      "id": 1304445937,
      "user": "oranagra",
      "created_at": "2022-11-05T08:51:59Z",
      "body": "can you run `pstack` or `gdb -batch -ex \"bt\" -p <pid>` a few times so we know which code path runs in a loop?\r\nanything you can tell us about the configuration so we can reproduce it? which version are you using?\r\n@moticless PTAL."
    },
    {
      "id": 1304798102,
      "user": "moticless",
      "created_at": "2022-11-06T13:08:31Z",
      "body": "@sedflix , \r\nAccording to INFO you are using rather old version (6.0.7). \r\nCan you try newer version? \r\n\r\n@oranagra made some fix that might be related to your issue: \r\nFix busy loop in ae.c when timer event is about to fire (https://github.com/redis/redis/pull/8764)"
    },
    {
      "id": 1306060277,
      "user": "betermieux",
      "created_at": "2022-11-07T19:06:03Z",
      "body": "Due to a reboot the sentinel was restarted and is currently not using 100% cpu any more. I will have to wait until it happens again.\r\nI am using docker image `redis:7.0.4-alpine3.16`. Since the containers are running on a locked down Fedora CoreOS machine, I will have to debug them with a parallel container running gdb, but this should pose no problem. \r\n"
    },
    {
      "id": 1317540271,
      "user": "yevhen-harmonizehr",
      "created_at": "2022-11-16T19:13:49Z",
      "body": "Faced same issue today on 3-node cluster. Restart of single pod didn't help. Only scale to 0 and scale back to 3 nodes solved the issue."
    },
    {
      "id": 1317995716,
      "user": "oranagra",
      "created_at": "2022-11-17T03:05:04Z",
      "body": "@yevhen-harmonizehr please specify which version you're using. "
    },
    {
      "id": 1318277242,
      "user": "yevhen-harmonizehr",
      "created_at": "2022-11-17T08:38:40Z",
      "body": "> @yevhen-harmonizehr please specify which version you're using.\r\n\r\nredis:7.0.5-alpine"
    },
    {
      "id": 1321080548,
      "user": "oranagra",
      "created_at": "2022-11-20T09:31:51Z",
      "body": "so it can't be the above mentioned fix, which was part of 6.2.\r\nif anyone reproduces this again (regardless of the version), we'd love to get some info.\r\nsee the instructions i posted here https://github.com/redis/redis/issues/9956#issuecomment-1304445937\r\n"
    },
    {
      "id": 1322502106,
      "user": "kfirfer",
      "created_at": "2022-11-21T18:49:07Z",
      "body": "Its reproducing with this helm chart:\r\n\r\nhttps://artifacthub.io/packages/helm/dandydev-charts/redis-ha\r\n\r\nTried multiple versions of redis ( 6.X + 7.X )\r\n\r\nIts doesn't reproducing all the times\r\naround 1 of every 3 times its reproduces\r\n\r\n### Some info:\r\n```\r\n kubectl top po --containers=true | grep redis                                                                                                                                                             \r\n\r\n\r\nredis-ha-haproxy-679fcbf55b-6bgxj        haproxy                   4m           69Mi\r\nredis-ha-haproxy-679fcbf55b-hjrf4        haproxy                   3m           69Mi\r\nredis-ha-haproxy-679fcbf55b-jcgk6        haproxy                   3m           69Mi\r\nredis-ha-server-0                        redis                     5m           2Mi\r\nredis-ha-server-0                        sentinel                  7m           2Mi\r\nredis-ha-server-0                        split-brain-fix           0m           0Mi\r\nredis-ha-server-1                        redis                     5m           2Mi\r\nredis-ha-server-1                        sentinel                  8m           2Mi\r\nredis-ha-server-1                        split-brain-fix           0m           0Mi\r\nredis-ha-server-2                        redis                     2m           3Mi\r\nredis-ha-server-2                        sentinel                  955m         3Mi\r\nredis-ha-server-2                        split-brain-fix           0m           0Mi\r\n\r\n```\r\n\r\nInside the sentinel container:\r\n\r\n```\r\nMem: 21717868K used, 1839616K free, 441888K shrd, 635924K buff, 9427004K cached\r\nCPU:  25% usr  23% sys   0% nic  50% idle   0% io   0% irq   0% sirq\r\nLoad average: 2.40 6.70 9.04 5/4816 741\r\n  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND\r\n    1     0 1000     R    33452   0%   1  24% redis-sentinel *:26379 [sentinel]\r\n  456     0 1000     S     1784   0%   1   0% sh\r\n  464   456 1000     R     1692   0%   2   0% top\r\n\r\n```\r\n\r\nIts 24% cause the vm machine is 4 cores (so its 100% of 1 core)\r\n\r\n\r\n\r\nlsof sentinel container:\r\n\r\n```\r\n/data $ lsof\r\n1       /usr/local/bin/redis-server     0       /dev/null\r\n1       /usr/local/bin/redis-server     1       pipe:[29328910]\r\n1       /usr/local/bin/redis-server     2       pipe:[29328911]\r\n1       /usr/local/bin/redis-server     3       pipe:[29328991]\r\n1       /usr/local/bin/redis-server     4       pipe:[29328991]\r\n1       /usr/local/bin/redis-server     5       anon_inode:[eventpoll]\r\n1       /usr/local/bin/redis-server     6       socket:[29328992]\r\n1       /usr/local/bin/redis-server     7       socket:[29328993]\r\n1       /usr/local/bin/redis-server     8       socket:[29328998]\r\n1       /usr/local/bin/redis-server     9       socket:[29328999]\r\n1       /usr/local/bin/redis-server     10      socket:[29327138]\r\n1       /usr/local/bin/redis-server     11      socket:[29327139]\r\n1       /usr/local/bin/redis-server     12      socket:[29327140]\r\n1       /usr/local/bin/redis-server     13      socket:[29327141]\r\n1       /usr/local/bin/redis-server     14      socket:[29327142]\r\n1       /usr/local/bin/redis-server     17      socket:[29330538]\r\n1       /usr/local/bin/redis-server     18      socket:[29596831]\r\n1       /usr/local/bin/redis-server     19      socket:[29596847]\r\n1       /usr/local/bin/redis-server     20      socket:[29330576]\r\n1       /usr/local/bin/redis-server     21      socket:[29330578]\r\n456     /bin/busybox    0       /dev/pts/0\r\n456     /bin/busybox    1       /dev/pts/0\r\n456     /bin/busybox    2       /dev/pts/0\r\n456     /bin/busybox    10      /dev/tty\r\n\r\n```\r\n\r\n\r\n"
    },
    {
      "id": 1322598332,
      "user": "oranagra",
      "created_at": "2022-11-21T20:22:33Z",
      "body": "i'd like a stack trace to see what it's doing.\r\nyou can maybe do that with the instructions here https://github.com/redis/redis/issues/9956#issuecomment-1304445937\r\nor if you're willing to kill the process, then `kill -11 <pid>` will cause it to print the trace to the log file.\r\nthe first method is preferred since you can take several samples."
    },
    {
      "id": 1322673789,
      "user": "kfirfer",
      "created_at": "2022-11-21T21:31:21Z",
      "body": "@oranagra \r\n\r\n```\r\n/tmp # gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n0x00007fc15ceb44a3 in ?? () from /lib/ld-musl-x86_64.so.1\r\n#0  0x00007fc15ceb44a3 in ?? () from /lib/ld-musl-x86_64.so.1\r\n#1  0x00007fc15ceb16fa in ?? () from /lib/ld-musl-x86_64.so.1\r\n#2  0x00007fc15cb5b914 in ?? ()\r\n#3  0x0000000000000008 in ?? ()\r\n#4  0x0000000012b82ba2 in ?? ()\r\n#5  0x0000000000000000 in ?? ()\r\n[Inferior 1 (process 1) detached]\r\n```\r\n\r\nRunning as 'root' and he doesnt shows the file and line number\r\nIdk why"
    },
    {
      "id": 1322714392,
      "user": "kfirfer",
      "created_at": "2022-11-21T22:08:49Z",
      "body": "@oranagra \r\n\r\nTried with bullseye docker image and he gives me more info:\r\n\r\n\r\n1)\r\n\r\n```\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n__libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=10) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n28      ../sysdeps/unix/sysv/linux/recv.c: No such file or directory.\r\n#0  __libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=10) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n#1  __libc_recv (fd=10, buf=0x7ffe2acb3ce0, len=16384, flags=flags@entry=0) at ../sysdeps/unix/sysv/linux/recv.c:23\r\n#2  0x00005569536c44a7 in redisNetRead (c=0x556953d19500, buf=<optimized out>, bufcap=<optimized out>) at net.c:61\r\n#3  0x00005569536c7be9 in redisBufferRead (c=c@entry=0x556953d19500) at hiredis.c:950\r\n#4  0x00005569536c3959 in redisAsyncRead (ac=0x556953d19500) at async.c:640\r\n#5  0x00005569535ea90a in aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:436\r\n#6  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#7  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n\r\n```\r\n\r\n\r\n2)\r\n```\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=100) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n30      ../sysdeps/unix/sysv/linux/epoll_wait.c: No such file or directory.\r\n#0  0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=100) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n#1  0x00005569535ea866 in aeApiPoll (tvp=<optimized out>, eventLoop=0x7f71fd82b1e0) at ae_epoll.c:113\r\n#2  aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:404\r\n#3  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#4  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n\r\n```\r\n\r\n\r\n3)\r\n\r\n```\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=68) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n30      ../sysdeps/unix/sysv/linux/epoll_wait.c: No such file or directory.\r\n#0  0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=68) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n#1  0x00005569535ea866 in aeApiPoll (tvp=<optimized out>, eventLoop=0x7f71fd82b1e0) at ae_epoll.c:113\r\n#2  aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:404\r\n#3  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#4  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n\r\n```\r\n\r\n4)\r\n```\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n__libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n28      ../sysdeps/unix/sysv/linux/recv.c: No such file or directory.\r\n#0  __libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n#1  __libc_recv (fd=14, buf=0x7ffe2acb3ce0, len=16384, flags=flags@entry=0) at ../sysdeps/unix/sysv/linux/recv.c:23\r\n#2  0x00005569536c44a7 in redisNetRead (c=0x556953d1f220, buf=<optimized out>, bufcap=<optimized out>) at net.c:61\r\n#3  0x00005569536c7be9 in redisBufferRead (c=c@entry=0x556953d1f220) at hiredis.c:950\r\n#4  0x00005569536c3959 in redisAsyncRead (ac=0x556953d1f220) at async.c:640\r\n#5  0x00005569535ea90a in aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:436\r\n#6  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#7  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n```\r\n\r\n\r\n5)\r\n\r\n```\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n__libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n28      ../sysdeps/unix/sysv/linux/recv.c: No such file or directory.\r\n#0  __libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n#1  __libc_recv (fd=14, buf=0x7ffe2acb3ce0, len=16384, flags=flags@entry=0) at ../sysdeps/unix/sysv/linux/recv.c:23\r\n#2  0x00005569536c44a7 in redisNetRead (c=0x556953d1f220, buf=<optimized out>, bufcap=<optimized out>) at net.c:61\r\n#3  0x00005569536c7be9 in redisBufferRead (c=c@entry=0x556953d1f220) at hiredis.c:950\r\n#4  0x00005569536c3959 in redisAsyncRead (ac=0x556953d1f220) at async.c:640\r\n#5  0x00005569535ea90a in aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:436\r\n#6  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#7  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n\r\n```\r\n\r\n\r\nThe Stacktraces running on sentinel that uses high cpu for no reason\r\n\r\n\r\n\r\n"
    },
    {
      "id": 1322719420,
      "user": "kfirfer",
      "created_at": "2022-11-21T22:13:48Z",
      "body": "Running multiple times:\r\n\r\n```\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\ngdb -batch -ex \"bt\" -p 1__libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n28      ../sysdeps/unix/sysv/linux/recv.c: No such file or directory.\r\n#0  __libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n#1  __libc_recv (fd=14, buf=0x7ffe2acb3ce0, len=16384, flags=flags@entry=0) at ../sysdeps/unix/sysv/linux/recv.c:23\r\n#2  0x00005569536c44a7 in redisNetRead (c=0x556953d1f220, buf=<optimized out>, bufcap=<optimized out>) at net.c:61\r\n#3  0x00005569536c7be9 in redisBufferRead (c=c@entry=0x556953d1f220) at hiredis.c:950\r\n#4  0x00005569536c3959 in redisAsyncRead (ac=0x556953d1f220) at async.c:640\r\n#5  0x00005569535ea90a in aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:436\r\n#6  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#7  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# \r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\naeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:362\r\n362     ae.c: No such file or directory.\r\n#0  aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:362\r\n#1  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#2  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=59) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n30      ../sysdeps/unix/sysv/linux/epoll_wait.c: No such file or directory.\r\n#0  0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=59) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n#1  0x00005569535ea866 in aeApiPoll (tvp=<optimized out>, eventLoop=0x7f71fd82b1e0) at ae_epoll.c:113\r\n#2  aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:404\r\n#3  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#4  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n0x00005569536b9cd1 in getMonotonicUs_posix () at monotonic.c:141\r\n141     monotonic.c: No such file or directory.\r\n#0  0x00005569536b9cd1 in getMonotonicUs_posix () at monotonic.c:141\r\n#1  0x00005569535eab33 in usUntilEarliestTimer (eventLoop=0x7f71fd82b1e0, eventLoop=0x7f71fd82b1e0) at ae.c:271\r\n#2  aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:375\r\n#3  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#4  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n__libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n28      ../sysdeps/unix/sysv/linux/recv.c: No such file or directory.\r\n#0  __libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n#1  __libc_recv (fd=14, buf=0x7ffe2acb3ce0, len=16384, flags=flags@entry=0) at ../sysdeps/unix/sysv/linux/recv.c:23\r\n#2  0x00005569536c44a7 in redisNetRead (c=0x556953d1f220, buf=<optimized out>, bufcap=<optimized out>) at net.c:61\r\n#3  0x00005569536c7be9 in redisBufferRead (c=c@entry=0x556953d1f220) at hiredis.c:950\r\n#4  0x00005569536c3959 in redisAsyncRead (ac=0x556953d1f220) at async.c:640\r\n#5  0x00005569535ea90a in aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:436\r\n#6  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#7  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=69) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n30      ../sysdeps/unix/sysv/linux/epoll_wait.c: No such file or directory.\r\n#0  0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=69) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n#1  0x00005569535ea866 in aeApiPoll (tvp=<optimized out>, eventLoop=0x7f71fd82b1e0) at ae_epoll.c:113\r\n#2  aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:404\r\n#3  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#4  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n__libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=10) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n28      ../sysdeps/unix/sysv/linux/recv.c: No such file or directory.\r\n#0  __libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=10) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n#1  __libc_recv (fd=10, buf=0x7ffe2acb3ce0, len=16384, flags=flags@entry=0) at ../sysdeps/unix/sysv/linux/recv.c:23\r\n#2  0x00005569536c44a7 in redisNetRead (c=0x556953d19500, buf=<optimized out>, bufcap=<optimized out>) at net.c:61\r\n#3  0x00005569536c7be9 in redisBufferRead (c=c@entry=0x556953d19500) at hiredis.c:950\r\n#4  0x00005569536c3959 in redisAsyncRead (ac=0x556953d19500) at async.c:640\r\n#5  0x00005569535ea90a in aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:436\r\n#6  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#7  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=99) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n30      ../sysdeps/unix/sysv/linux/epoll_wait.c: No such file or directory.\r\n#0  0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=99) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n#1  0x00005569535ea866 in aeApiPoll (tvp=<optimized out>, eventLoop=0x7f71fd82b1e0) at ae_epoll.c:113\r\n#2  aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:404\r\n#3  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#4  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n__libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n28      ../sysdeps/unix/sysv/linux/recv.c: No such file or directory.\r\n#0  __libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n#1  __libc_recv (fd=14, buf=0x7ffe2acb3ce0, len=16384, flags=flags@entry=0) at ../sysdeps/unix/sysv/linux/recv.c:23\r\n#2  0x00005569536c44a7 in redisNetRead (c=0x556953d1f220, buf=<optimized out>, bufcap=<optimized out>) at net.c:61\r\n#3  0x00005569536c7be9 in redisBufferRead (c=c@entry=0x556953d1f220) at hiredis.c:950\r\n#4  0x00005569536c3959 in redisAsyncRead (ac=0x556953d1f220) at async.c:640\r\n#5  0x00005569535ea90a in aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:436\r\n#6  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#7  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=40) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n30      ../sysdeps/unix/sysv/linux/epoll_wait.c: No such file or directory.\r\n#0  0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=40) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n#1  0x00005569535ea866 in aeApiPoll (tvp=<optimized out>, eventLoop=0x7f71fd82b1e0) at ae_epoll.c:113\r\n#2  aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:404\r\n#3  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#4  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n__libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n28      ../sysdeps/unix/sysv/linux/recv.c: No such file or directory.\r\n#0  __libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n#1  __libc_recv (fd=14, buf=0x7ffe2acb3ce0, len=16384, flags=flags@entry=0) at ../sysdeps/unix/sysv/linux/recv.c:23\r\n#2  0x00005569536c44a7 in redisNetRead (c=0x556953d1f220, buf=<optimized out>, bufcap=<optimized out>) at net.c:61\r\n#3  0x00005569536c7be9 in redisBufferRead (c=c@entry=0x556953d1f220) at hiredis.c:950\r\n#4  0x00005569536c3959 in redisAsyncRead (ac=0x556953d1f220) at async.c:640\r\n#5  0x00005569535ea90a in aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:436\r\n#6  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#7  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n__libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=10) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n28      ../sysdeps/unix/sysv/linux/recv.c: No such file or directory.\r\n#0  __libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=10) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n#1  __libc_recv (fd=10, buf=0x7ffe2acb3ce0, len=16384, flags=flags@entry=0) at ../sysdeps/unix/sysv/linux/recv.c:23\r\n#2  0x00005569536c44a7 in redisNetRead (c=0x556953d19500, buf=<optimized out>, bufcap=<optimized out>) at net.c:61\r\n#3  0x00005569536c7be9 in redisBufferRead (c=c@entry=0x556953d19500) at hiredis.c:950\r\n#4  0x00005569536c3959 in redisAsyncRead (ac=0x556953d19500) at async.c:640\r\n#5  0x00005569535ea90a in aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:436\r\n#6  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#7  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=69) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n30      ../sysdeps/unix/sysv/linux/epoll_wait.c: No such file or directory.\r\n#0  0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=69) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n#1  0x00005569535ea866 in aeApiPoll (tvp=<optimized out>, eventLoop=0x7f71fd82b1e0) at ae_epoll.c:113\r\n#2  aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:404\r\n#3  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#4  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n__libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n28      ../sysdeps/unix/sysv/linux/recv.c: No such file or directory.\r\n#0  __libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n#1  __libc_recv (fd=14, buf=0x7ffe2acb3ce0, len=16384, flags=flags@entry=0) at ../sysdeps/unix/sysv/linux/recv.c:23\r\n#2  0x00005569536c44a7 in redisNetRead (c=0x556953d1f220, buf=<optimized out>, bufcap=<optimized out>) at net.c:61\r\n#3  0x00005569536c7be9 in redisBufferRead (c=c@entry=0x556953d1f220) at hiredis.c:950\r\n#4  0x00005569536c3959 in redisAsyncRead (ac=0x556953d1f220) at async.c:640\r\n#5  0x00005569535ea90a in aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:436\r\n#6  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#7  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n__libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n28      ../sysdeps/unix/sysv/linux/recv.c: No such file or directory.\r\n#0  __libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n#1  __libc_recv (fd=14, buf=0x7ffe2acb3ce0, len=16384, flags=flags@entry=0) at ../sysdeps/unix/sysv/linux/recv.c:23\r\n#2  0x00005569536c44a7 in redisNetRead (c=0x556953d1f220, buf=<optimized out>, bufcap=<optimized out>) at net.c:61\r\n#3  0x00005569536c7be9 in redisBufferRead (c=c@entry=0x556953d1f220) at hiredis.c:950\r\n#4  0x00005569536c3959 in redisAsyncRead (ac=0x556953d1f220) at async.c:640\r\n#5  0x00005569535ea90a in aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:436\r\n#6  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#7  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n__libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=10) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n28      ../sysdeps/unix/sysv/linux/recv.c: No such file or directory.\r\n#0  __libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=10) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n#1  __libc_recv (fd=10, buf=0x7ffe2acb3ce0, len=16384, flags=flags@entry=0) at ../sysdeps/unix/sysv/linux/recv.c:23\r\n#2  0x00005569536c44a7 in redisNetRead (c=0x556953d19500, buf=<optimized out>, bufcap=<optimized out>) at net.c:61\r\n#3  0x00005569536c7be9 in redisBufferRead (c=c@entry=0x556953d19500) at hiredis.c:950\r\n#4  0x00005569536c3959 in redisAsyncRead (ac=0x556953d19500) at async.c:640\r\n#5  0x00005569535ea90a in aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:436\r\n#6  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#7  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n__libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n28      ../sysdeps/unix/sysv/linux/recv.c: No such file or directory.\r\n#0  __libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n#1  __libc_recv (fd=14, buf=0x7ffe2acb3ce0, len=16384, flags=flags@entry=0) at ../sysdeps/unix/sysv/linux/recv.c:23\r\n#2  0x00005569536c44a7 in redisNetRead (c=0x556953d1f220, buf=<optimized out>, bufcap=<optimized out>) at net.c:61\r\n#3  0x00005569536c7be9 in redisBufferRead (c=c@entry=0x556953d1f220) at hiredis.c:950\r\n#4  0x00005569536c3959 in redisAsyncRead (ac=0x556953d1f220) at async.c:640\r\n#5  0x00005569535ea90a in aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:436\r\n#6  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#7  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=49) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n30      ../sysdeps/unix/sysv/linux/epoll_wait.c: No such file or directory.\r\n#0  0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=49) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n#1  0x00005569535ea866 in aeApiPoll (tvp=<optimized out>, eventLoop=0x7f71fd82b1e0) at ae_epoll.c:113\r\n#2  aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:404\r\n#3  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#4  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n__libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n28      ../sysdeps/unix/sysv/linux/recv.c: No such file or directory.\r\n#0  __libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n#1  __libc_recv (fd=14, buf=0x7ffe2acb3ce0, len=16384, flags=flags@entry=0) at ../sysdeps/unix/sysv/linux/recv.c:23\r\n#2  0x00005569536c44a7 in redisNetRead (c=0x556953d1f220, buf=<optimized out>, bufcap=<optimized out>) at net.c:61\r\n#3  0x00005569536c7be9 in redisBufferRead (c=c@entry=0x556953d1f220) at hiredis.c:950\r\n#4  0x00005569536c3959 in redisAsyncRead (ac=0x556953d1f220) at async.c:640\r\n#5  0x00005569535ea90a in aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:436\r\n#6  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#7  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=89) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n30      ../sysdeps/unix/sysv/linux/epoll_wait.c: No such file or directory.\r\n#0  0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=89) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n#1  0x00005569535ea866 in aeApiPoll (tvp=<optimized out>, eventLoop=0x7f71fd82b1e0) at ae_epoll.c:113\r\n#2  aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:404\r\n#3  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#4  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=69) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n30      ../sysdeps/unix/sysv/linux/epoll_wait.c: No such file or directory.\r\n#0  0x00007f71fddaed56 in epoll_wait (epfd=5, events=0x7f71fd8e9a00, maxevents=10128, timeout=69) at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\r\n#1  0x00005569535ea866 in aeApiPoll (tvp=<optimized out>, eventLoop=0x7f71fd82b1e0) at ae_epoll.c:113\r\n#2  aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:404\r\n#3  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#4  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n__libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=10) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n28      ../sysdeps/unix/sysv/linux/recv.c: No such file or directory.\r\n#0  __libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=10) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n#1  __libc_recv (fd=10, buf=0x7ffe2acb3ce0, len=16384, flags=flags@entry=0) at ../sysdeps/unix/sysv/linux/recv.c:23\r\n#2  0x00005569536c44a7 in redisNetRead (c=0x556953d19500, buf=<optimized out>, bufcap=<optimized out>) at net.c:61\r\n#3  0x00005569536c7be9 in redisBufferRead (c=c@entry=0x556953d19500) at hiredis.c:950\r\n#4  0x00005569536c3959 in redisAsyncRead (ac=0x556953d19500) at async.c:640\r\n#5  0x00005569535ea90a in aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:436\r\n#6  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#7  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n# gdb -batch -ex \"bt\" -p 1\r\n[New LWP 7]\r\n[New LWP 8]\r\n[New LWP 9]\r\n[New LWP 10]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n__libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n28      ../sysdeps/unix/sysv/linux/recv.c: No such file or directory.\r\n#0  __libc_recv (flags=<optimized out>, len=16384, buf=0x7ffe2acb3ce0, fd=14) at ../sysdeps/unix/sysv/linux/recv.c:28\r\n#1  __libc_recv (fd=14, buf=0x7ffe2acb3ce0, len=16384, flags=flags@entry=0) at ../sysdeps/unix/sysv/linux/recv.c:23\r\n#2  0x00005569536c44a7 in redisNetRead (c=0x556953d1f220, buf=<optimized out>, bufcap=<optimized out>) at net.c:61\r\n#3  0x00005569536c7be9 in redisBufferRead (c=c@entry=0x556953d1f220) at hiredis.c:950\r\n#4  0x00005569536c3959 in redisAsyncRead (ac=0x556953d1f220) at async.c:640\r\n#5  0x00005569535ea90a in aeProcessEvents (eventLoop=eventLoop@entry=0x7f71fd82b1e0, flags=flags@entry=27) at ae.c:436\r\n#6  0x00005569535eacad in aeMain (eventLoop=0x7f71fd82b1e0) at ae.c:496\r\n#7  0x00005569535e65a1 in main (argc=<optimized out>, argv=0x7ffe2acb7ee8) at server.c:7075\r\n[Inferior 1 (process 1) detached]\r\n\r\n```"
    },
    {
      "id": 1322740516,
      "user": "oranagra",
      "created_at": "2022-11-21T22:32:59Z",
      "body": "thanks. this is useful.\r\n\r\n@michael-grunder can you take a look? any reason why redisNetRead will eat CPU, or do a blocking read (following an epoll indication that it's readable)?"
    },
    {
      "id": 1322918019,
      "user": "tillkruss",
      "created_at": "2022-11-22T02:09:04Z",
      "body": "\r\n> @michael-grunder can you take a look? any reason why redisNetRead will eat CPU, or do a blocking read (following an epoll indication that it's readable)?\r\n\r\nNo, heâ€™s busy fixing macOS bugs."
    },
    {
      "id": 1322924246,
      "user": "michael-grunder",
      "created_at": "2022-11-22T02:16:07Z",
      "body": "> any reason why redisNetRead will eat CPU, or do a blocking read (following an epoll indication that it's readable)\r\n\r\nFrom the call stack, it seems like maybe it's getting into a loop where `recv` immediately fails with `EAGAIN`, so it just spins eating CPU.\r\n\r\nI'll take a closer look tomorrow.\r\n\r\nEdit:  It would also be interesting to see if there was anything of note in the sentinel logs."
    },
    {
      "id": 1334539261,
      "user": "kfirfer",
      "created_at": "2022-12-01T22:42:11Z",
      "body": "@michael-grunder \r\n\r\nLogs of sentinel that using 100% CPU:\r\n\r\n```\r\n$ kubectl logs redis-ha-server-2 sentinel                              \r\n1:X 29 Nov 2022 19:38:35.509 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\r\n1:X 29 Nov 2022 19:38:35.509 # Redis version=7.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\r\n1:X 29 Nov 2022 19:38:35.509 # Configuration loaded\r\n1:X 29 Nov 2022 19:38:35.510 * monotonic clock: POSIX clock_gettime\r\n1:X 29 Nov 2022 19:38:35.511 * Running mode=sentinel, port=26379.\r\n1:X 29 Nov 2022 19:38:35.511 # Sentinel ID is ce2a8c84e9e96e3cf079fafb2ad20bcde4a7b533\r\n1:X 29 Nov 2022 19:38:35.511 # +monitor master mymaster 10.48.8.79 6379 quorum 2\r\n1:X 29 Nov 2022 19:38:35.513 * +slave slave 10.48.6.161:6379 10.48.6.161 6379 @ mymaster 10.48.8.79 6379\r\n1:X 29 Nov 2022 19:38:35.521 * Sentinel new configuration saved on disk\r\n1:X 29 Nov 2022 19:38:35.522 * +slave slave 10.48.12.117:6379 10.48.12.117 6379 @ mymaster 10.48.8.79 6379\r\n1:X 29 Nov 2022 19:38:35.527 * Sentinel new configuration saved on disk\r\n1:X 29 Nov 2022 19:38:35.854 * +sentinel sentinel a0ddf4367a72f8e768e2546287bf0b303985c19f 10.48.6.161 26379 @ mymaster 10.48.8.79 6379\r\n1:X 29 Nov 2022 19:38:35.860 * Sentinel new configuration saved on disk\r\n1:X 29 Nov 2022 19:38:35.866 * Sentinel new configuration saved on disk\r\n1:X 29 Nov 2022 19:38:35.866 # +new-epoch 10\r\n1:X 29 Nov 2022 19:38:36.637 * +sentinel sentinel bc16ddc9a19ce8f3ff5ec8f31a2d6b94436af8a3 10.48.8.79 26379 @ mymaster 10.48.8.79 6379\r\n1:X 29 Nov 2022 19:38:36.645 * Sentinel new configuration saved on disk\r\n1:X 29 Nov 2022 19:39:52.343 * Sentinel new configuration saved on disk\r\n1:X 29 Nov 2022 19:39:52.343 # +new-epoch 11\r\n1:X 29 Nov 2022 19:39:53.281 # +config-update-from sentinel bc16ddc9a19ce8f3ff5ec8f31a2d6b94436af8a3 10.48.8.79 26379 @ mymaster 10.48.8.79 6379\r\n1:X 29 Nov 2022 19:39:53.281 # +switch-master mymaster 10.48.8.79 6379 10.48.12.117 6379\r\n1:X 29 Nov 2022 19:39:53.337 * +slave slave 10.48.6.161:6379 10.48.6.161 6379 @ mymaster 10.48.12.117 6379\r\n1:X 29 Nov 2022 19:39:53.337 * +slave slave 10.48.8.79:6379 10.48.8.79 6379 @ mymaster 10.48.12.117 6379\r\n1:X 29 Nov 2022 19:39:53.343 * Sentinel new configuration saved on disk\r\n1:X 29 Nov 2022 19:40:07.245 # +sdown sentinel bc16ddc9a19ce8f3ff5ec8f31a2d6b94436af8a3 10.48.8.79 26379 @ mymaster 10.48.12.117 6379\r\n1:X 29 Nov 2022 19:40:14.554 # +sdown slave 10.48.8.79:6379 10.48.8.79 6379 @ mymaster 10.48.12.117 6379\r\n1:X 29 Nov 2022 19:41:08.813 * +reboot slave 10.48.8.79:6379 10.48.8.79 6379 @ mymaster 10.48.12.117 6379\r\n1:X 29 Nov 2022 19:41:08.911 # -sdown slave 10.48.8.79:6379 10.48.8.79 6379 @ mymaster 10.48.12.117 6379\r\n1:X 29 Nov 2022 19:41:11.977 # -sdown sentinel bc16ddc9a19ce8f3ff5ec8f31a2d6b94436af8a3 10.48.8.79 26379 @ mymaster 10.48.12.117 6379\r\n1:X 29 Nov 2022 19:42:19.449 # +sdown slave 10.48.6.161:6379 10.48.6.161 6379 @ mymaster 10.48.12.117 6379\r\n1:X 29 Nov 2022 19:42:24.029 # +sdown sentinel a0ddf4367a72f8e768e2546287bf0b303985c19f 10.48.6.161 26379 @ mymaster 10.48.12.117 6379\r\n1:X 29 Nov 2022 19:43:24.637 # -sdown sentinel a0ddf4367a72f8e768e2546287bf0b303985c19f 10.48.6.161 26379 @ mymaster 10.48.12.117 6379\r\n1:X 29 Nov 2022 19:43:25.051 * +reboot slave 10.48.6.161:6379 10.48.6.161 6379 @ mymaster 10.48.12.117 6379\r\n```\r\n"
    },
    {
      "id": 1340800143,
      "user": "yevhen-harmonizehr",
      "created_at": "2022-12-07T11:03:27Z",
      "body": "can see same lines for gdb\r\n```\r\n#9  0x000056375716c6fc in beforeSleep (eventLoop=<optimized out>) at server.c:1648\r\n#10 0x00005637571680c1 in aeApiPoll (tvp=<optimized out>, eventLoop=0x7f689822b1e0) at ae.c:113\r\n#11 aeProcessEvents (eventLoop=eventLoop@entry=0x7f689822b1e0, flags=flags@entry=27) at ae.c:404\r\n#12 0x0000563757168c5d in aeProcessEvents (flags=27, eventLoop=0x7f689822b1e0) at ae.c:496\r\n#13 aeMain (eventLoop=0x7f689822b1e0) at ae.c:496\r\n#14 0x000056375716450a in main (argc=<optimized out>, argv=0x7ffdab146bf8) at server.c:7075\r\n```\r\n\r\nand for sentinel logs\r\n```\r\nk logs -f argo-cd-redis-ha-server-1 -c sentinel\r\n1:X 06 Dec 2022 20:14:13.642 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\r\n1:X 06 Dec 2022 20:14:13.642 # Redis version=7.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\r\n1:X 06 Dec 2022 20:14:13.642 # Configuration loaded\r\n1:X 06 Dec 2022 20:14:13.643 * monotonic clock: POSIX clock_gettime\r\n1:X 06 Dec 2022 20:14:13.643 * Running mode=sentinel, port=26379.\r\n1:X 06 Dec 2022 20:14:13.644 # Sentinel ID is c524e76c625cc68c94f25c99e20efce31df6f4c2\r\n1:X 06 Dec 2022 20:14:13.644 # +monitor master argocd 172.20.173.199 6379 quorum 2\r\n1:X 06 Dec 2022 20:14:13.646 * +slave slave 172.20.112.236:6379 172.20.112.236 6379 @ argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:14:13.651 * Sentinel new configuration saved on disk\r\n1:X 06 Dec 2022 20:14:13.651 * +slave slave 172.20.150.209:6379 172.20.150.209 6379 @ argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:14:13.656 * Sentinel new configuration saved on disk\r\n1:X 06 Dec 2022 20:14:13.939 * +sentinel sentinel 2f679f20284b54ed430b97819302f9dc0d48cb09 172.20.112.236 26379 @ argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:14:13.945 * Sentinel new configuration saved on disk\r\n1:X 06 Dec 2022 20:14:14.099 * +sentinel sentinel 4794d0beec6abdd90a3821f7d9bf66804f290941 172.20.173.199 26379 @ argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:14:14.104 * Sentinel new configuration saved on disk\r\n1:X 06 Dec 2022 20:18:07.337 # +sdown sentinel 2f679f20284b54ed430b97819302f9dc0d48cb09 172.20.112.236 26379 @ argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:18:07.798 # +sdown slave 172.20.112.236:6379 172.20.112.236 6379 @ argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:19:04.549 * +reboot slave 172.20.112.236:6379 172.20.112.236 6379 @ argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:19:04.593 # -sdown slave 172.20.112.236:6379 172.20.112.236 6379 @ argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:19:05.762 # -sdown sentinel 2f679f20284b54ed430b97819302f9dc0d48cb09 172.20.112.236 26379 @ argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:24:26.004 # +sdown sentinel 4794d0beec6abdd90a3821f7d9bf66804f290941 172.20.173.199 26379 @ argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:24:28.601 # +sdown master argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:24:28.659 # +odown master argocd 172.20.173.199 6379 #quorum 2/2\r\n1:X 06 Dec 2022 20:24:28.659 # +new-epoch 1\r\n1:X 06 Dec 2022 20:24:28.659 # +try-failover master argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:24:28.763 * Sentinel new configuration saved on disk\r\n1:X 06 Dec 2022 20:24:28.764 # +vote-for-leader c524e76c625cc68c94f25c99e20efce31df6f4c2 1\r\n1:X 06 Dec 2022 20:24:28.776 # 2f679f20284b54ed430b97819302f9dc0d48cb09 voted for c524e76c625cc68c94f25c99e20efce31df6f4c2 1\r\n1:X 06 Dec 2022 20:24:28.822 # +elected-leader master argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:24:28.822 # +failover-state-select-slave master argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:24:28.884 # +selected-slave slave 172.20.150.209:6379 172.20.150.209 6379 @ argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:24:28.884 * +failover-state-send-slaveof-noone slave 172.20.150.209:6379 172.20.150.209 6379 @ argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:24:28.955 * +failover-state-wait-promotion slave 172.20.150.209:6379 172.20.150.209 6379 @ argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:24:29.940 * Sentinel new configuration saved on disk\r\n1:X 06 Dec 2022 20:24:29.940 # +promoted-slave slave 172.20.150.209:6379 172.20.150.209 6379 @ argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:24:29.940 # +failover-state-reconf-slaves master argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:24:29.993 * +slave-reconf-sent slave 172.20.112.236:6379 172.20.112.236 6379 @ argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:24:30.420 * +slave-reconf-inprog slave 172.20.112.236:6379 172.20.112.236 6379 @ argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:24:30.420 * +slave-reconf-done slave 172.20.112.236:6379 172.20.112.236 6379 @ argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:24:30.505 # +failover-end master argocd 172.20.173.199 6379\r\n1:X 06 Dec 2022 20:24:30.505 # +switch-master argocd 172.20.173.199 6379 172.20.150.209 6379\r\n1:X 06 Dec 2022 20:24:30.505 * +slave slave 172.20.112.236:6379 172.20.112.236 6379 @ argocd 172.20.150.209 6379\r\n1:X 06 Dec 2022 20:24:30.505 * +slave slave 172.20.173.199:6379 172.20.173.199 6379 @ argocd 172.20.150.209 6379\r\n1:X 06 Dec 2022 20:24:30.511 * Sentinel new configuration saved on disk\r\n1:X 06 Dec 2022 20:24:40.539 # +sdown slave 172.20.173.199:6379 172.20.173.199 6379 @ argocd 172.20.150.209 6379\r\n1:X 06 Dec 2022 20:25:45.791 # -sdown slave 172.20.173.199:6379 172.20.173.199 6379 @ argocd 172.20.150.209 6379\r\n1:X 06 Dec 2022 20:25:49.641 # -sdown sentinel 4794d0beec6abdd90a3821f7d9bf66804f290941 172.20.173.199 26379 @ argocd 172.20.150.209 6379\r\n```\r\n"
    },
    {
      "id": 1356866164,
      "user": "kfirfer",
      "created_at": "2022-12-18T20:08:01Z",
      "body": "There some workaround ?\r\n\r\nEdit:\r\nIts happens only on docker ? maybe incompatibility with the OS?"
    },
    {
      "id": 1466281932,
      "user": "silvpol",
      "created_at": "2023-03-13T14:46:34Z",
      "body": "This also happens on GKE using ContainerD so not Docker specific. We try to manage this by setting up livenessCheck on Sentinel CPU usage so that it gets restarted automatically if it uses too much CPU."
    },
    {
      "id": 1485044287,
      "user": "Zebradil",
      "created_at": "2023-03-27T12:22:20Z",
      "body": "We observe this issue on GKE as well."
    },
    {
      "id": 1576396251,
      "user": "davidln777",
      "created_at": "2023-06-05T08:54:42Z",
      "body": "@oranagra - any update on this issue? \r\nWe keep getting it, and it's being resolved after the pod restart. "
    },
    {
      "id": 1578490092,
      "user": "oranagra",
      "created_at": "2023-06-06T11:06:59Z",
      "body": "@michael-grunder can you please take a closer look? "
    },
    {
      "id": 1814066929,
      "user": "Sven1410",
      "created_at": "2023-11-16T09:21:01Z",
      "body": "is there any workaround or hint how to prevent/solve this ?\r\nWe have the same problem after upgrading redis-ha from 7.0.0 to 7.0.13 (inside of argocd)\r\n(I think the problem was there before - but now it occurs in nearly every restart)\r\n\r\nwhat additionally confuses me:\r\n\r\n`kubectl top pods` shows that the (sentinel) container runs on its limit (CPU 1005m)\r\nbut when I execute `top` inside the container the result always is CPU usage: 12% (?)\r\n\r\nthanks in advance!\r\n\r\nedit: \r\nlooks like the 12% CPU (busybox top command) inside the container reflects the capacity of the node\r\n--> the limit is 1000m which is 1/8 cpu's of the node = 12%\r\nfor a lifeness check the \"load\" would be a better indicator (also available via command \"uptime\")"
    },
    {
      "id": 1829420957,
      "user": "Sven1410",
      "created_at": "2023-11-28T09:27:17Z",
      "body": "maybe a redis-expert can see something here - this is the \"info\" command output of the \"bad\" sentinel container with 100% cpu consumption (\"sentinel pending-scripts\" returns nothing, failover works fine):\r\n\r\n```\r\nredis-cli -p 26379\r\ninfo\r\n\r\n# Server\r\nredis_version:7.0.11\r\nredis_git_sha1:00000000\r\nredis_git_dirty:0\r\nredis_build_id:183dd15edc8d4d6a\r\nredis_mode:sentinel\r\nos:Linux 5.15.135-gardenlinux-cloud-amd64 x86_64\r\narch_bits:64\r\nmonotonic_clock:POSIX clock_gettime\r\nmultiplexing_api:epoll\r\natomicvar_api:c11-builtin\r\ngcc_version:12.2.1\r\nprocess_id:1\r\nprocess_supervised:no\r\nrun_id:8986c630a7fd774f791223f327bea3154f794675\r\ntcp_port:26379\r\nserver_time_usec:1701163193604078\r\nuptime_in_seconds:66249\r\nuptime_in_days:0\r\nhz:13\r\nconfigured_hz:10\r\nlru_clock:6664377\r\nexecutable:/data/redis-sentinel\r\nconfig_file:/data/conf/sentinel.conf\r\nio_threads_active:0\r\n\r\n# Clients\r\nconnected_clients:6\r\ncluster_connections:0\r\nmaxclients:10000\r\nclient_recent_max_input_buffer:20480\r\nclient_recent_max_output_buffer:0\r\nblocked_clients:0\r\ntracking_clients:0\r\nclients_in_timeout_table:0\r\n\r\n# Stats\r\ntotal_connections_received:340692\r\ntotal_commands_processed:863856\r\ninstantaneous_ops_per_sec:8\r\ntotal_net_input_bytes:27922297\r\ntotal_net_output_bytes:23261548\r\ntotal_net_repl_input_bytes:0\r\ntotal_net_repl_output_bytes:0\r\ninstantaneous_input_kbps:0.29\r\ninstantaneous_output_kbps:0.22\r\ninstantaneous_input_repl_kbps:0.00\r\ninstantaneous_output_repl_kbps:0.00\r\nrejected_connections:0\r\nsync_full:0\r\nsync_partial_ok:0\r\nsync_partial_err:0\r\nexpired_keys:0\r\nexpired_stale_perc:0.00\r\nexpired_time_cap_reached_count:0\r\nexpire_cycle_cpu_milliseconds:574\r\nevicted_keys:0\r\nevicted_clients:0\r\ntotal_eviction_exceeded_time:0\r\ncurrent_eviction_exceeded_time:0\r\nkeyspace_hits:0\r\nkeyspace_misses:0\r\npubsub_channels:0\r\npubsub_patterns:0\r\npubsubshard_channels:0\r\nlatest_fork_usec:0\r\ntotal_forks:0\r\nmigrate_cached_sockets:0\r\nslave_expires_tracked_keys:0\r\nactive_defrag_hits:0\r\nactive_defrag_misses:0\r\nactive_defrag_key_hits:0\r\nactive_defrag_key_misses:0\r\ntotal_active_defrag_time:0\r\ncurrent_active_defrag_time:0\r\ntracking_total_keys:0\r\ntracking_total_items:0\r\ntracking_total_prefixes:0\r\nunexpected_error_replies:0\r\ntotal_error_replies:198318\r\ndump_payload_sanitizations:0\r\ntotal_reads_processed:1345332\r\ntotal_writes_processed:1057540\r\nio_threaded_reads_processed:0\r\nio_threaded_writes_processed:0\r\nreply_buffer_shrinks:132427\r\nreply_buffer_expands:6\r\n\r\n# CPU\r\nused_cpu_sys:40470.884147\r\nused_cpu_user:25442.201681\r\nused_cpu_sys_children:0.000000\r\nused_cpu_user_children:0.000000\r\nused_cpu_sys_main_thread:40470.870627\r\nused_cpu_user_main_thread:25442.177889\r\n\r\n# Sentinel\r\nsentinel_masters:1\r\nsentinel_tilt:0\r\nsentinel_tilt_since_seconds:-1\r\nsentinel_running_scripts:0\r\nsentinel_scripts_queue_length:0\r\nsentinel_simulate_failure_flags:0\r\nmaster0:name=argocd,status=ok,address=100.xx.xxx.38:6379,slaves=2,sentinels=3\r\n\r\nsentinel pending-scripts\r\n<--empty line returned-->\r\nsentinel CKQUORUM argocd\r\nOK 3 usable Sentinels. Quorum and failover authorization can be reached\r\n```\r\n\r\n"
    },
    {
      "id": 1836105658,
      "user": "Sven1410",
      "created_at": "2023-12-01T13:17:44Z",
      "body": "I found a workaround (for docker/k8s):\r\n\r\nI played a bit with the sentinel CLI and found out that a \"sentinel reset *\" always solves the problem and the CPU consumption is dropped down to normal.\r\nhttps://lzone.de/cheat-sheet/Redis%20Sentinel\r\nhttps://redis.io/docs/management/sentinel/#sentinel-commands\r\n\r\nand so far the problem occurs only after a rolling update or container restarts.\r\nSo I added a delayed reset to all the sentinel containers via helm chart values:\r\n\r\n```\r\nargo-cd:\r\n  redis-ha:\r\n    sentinel:\r\n      lifecycle: \r\n        postStart:\r\n          exec:\r\n            command: [\"/bin/sh\", \"-c\", \"sleep 30; redis-cli -p 26379 sentinel reset argocd \"]\r\n```\r\n"
    },
    {
      "id": 1840942815,
      "user": "jortkoopmans",
      "created_at": "2023-12-05T14:49:21Z",
      "body": "Thank you for the proposed workaround @Sven1410. Did the issue return for you in the meantime over the last few days (as it is intermittent)?\r\nWhat I've done based on your post:\r\n- Run the manual sentinel reset command. This indeed worked for me to bring back the utilization to normal.\r\n- Implement the lifecycle hook, but while I used 60s sleep in the command, I had at least one occasion where the hook did not prevent the utilization spike. It is possible that timing matters (30s vs 60s, in relation to the entrypoint or other containers in the pod), alternatively it could be reducing the likelihood of running into this condition (which probably is a race condition?) but not preventing it. More/longer testing should give clearer results, specifically on this workaround. I'll report back if I obtain robust conclusions.\r\n\r\nA root cause analysis is (still) preferred though :blush: ."
    },
    {
      "id": 1840971453,
      "user": "Sven1410",
      "created_at": "2023-12-05T15:02:33Z",
      "body": "so far the problem did not came back.\r\nI would also think a longer delay would be better, but if the delay is too long our alert already fires.\r\nAnother option is to do this reset periodically (once a day maybe?)\r\nmy versions: (redis-ha: 7.0.13-alpine, haproxy: 2.8.3-alpine)\r\n\r\nThanks for testing + sharing the results!"
    },
    {
      "id": 1849858623,
      "user": "jortkoopmans",
      "created_at": "2023-12-11T11:12:23Z",
      "body": "Unfortunately, the workaround does not (consistently) work on my deployments. I have 2 sets of versions and configurations, I still observe the cpu drain, at least on the 6.2.5 version.\r\n* redis 7.0.7-alpine, ha-proxy 2.6.4\r\n* redis 6.2.5-alpine, no ha-proxy\r\n\r\nNote that I do not observe the `sentinel pending-scripts` line, described [here](https://github.com/redis/redis/issues/9956#issuecomment-1829420957)"
    },
    {
      "id": 1850020721,
      "user": "Sven1410",
      "created_at": "2023-12-11T12:52:49Z",
      "body": "\"sentinel pending-scripts\" was already the next command I typed in - it returns nothing, only an empty line..."
    },
    {
      "id": 1850112385,
      "user": "jortkoopmans",
      "created_at": "2023-12-11T13:47:08Z",
      "body": "> \"sentinel pending-scripts\" was already the next command I typed in - it returns nothing, only an empty line...\r\n\r\nAh OK my bad, should have checked the sentinel cli directives. I'll verify this the next time I observe the issue.\r\nIn normal condition I get this response:\r\n```\r\n(empty array)\r\n```"
    },
    {
      "id": 1856310960,
      "user": "silvpol",
      "created_at": "2023-12-14T17:43:18Z",
      "body": "We have experienced this in prod today and `sentinel pending-scripts` did return an empty array."
    },
    {
      "id": 1872610894,
      "user": "jortkoopmans",
      "created_at": "2023-12-30T21:32:17Z",
      "body": "Following up after observing this for a while (on a number of independent redis-ha deployments), findings:\r\n- The workaround to reset the sentinel after pod start (on k8s) does not work in my case. I don't have long term statistics on the bug likelihood on specific configuration combinations, but in all cases it does not alleviate the issue substantially enough to be able to run this independently in production (i.e. other mitigation required).\r\n- `sentinel pending-scripts` still returns an empty array (same to normal conditions, identical to finding by @silvpol ).\r\n\r\nNow I'll stop posting behavior patterns for this issue, in the hope of a root cause analysis. :pray: "
    },
    {
      "id": 1891685793,
      "user": "Sven1410",
      "created_at": "2024-01-15T09:33:34Z",
      "body": "After debugging with gdb I've got exactly the same stack traces like others posted here.\r\nI think it must be this epoll_wait() call. But the timeout (-1) looks correct.\r\nIf you google \"epoll_wait\" \"100% cpu\" you will find dozens of nearly identical problems.\r\nMaybe we have a similar problem like described here: https://github.com/joyent/libuv/issues/1099\r\nThe fact that we restart the redis process(es) on the same (linux) node and use the same sockets - especially the \"announce\" sockets could also be relevant.\r\n\r\nIs there an option to fall back (for linux docker images) to \"ae_kqueue.c\"? (#define HAVE_EPOLL 0)?"
    },
    {
      "id": 1904622550,
      "user": "posh-raj-kuni",
      "created_at": "2024-01-22T19:01:22Z",
      "body": "We are seeing the same issue in our environment. Our environment is a GKE cluster running on k8s version 1.26.\r\n\r\nThe perplexing thing is, this CPU issue is only happening in one namespace in the cluster. If we try deploying redis-sentinel in another namespace, we are not seeing the issue!\r\n\r\nWhen I read it may be a networking issue, I thought maybe the network policies in the namespace where the CPU ramp up was observed were the issue. So I added a couple policies that allowed all egress/ingress traffic. But that didn't resolve the issue either.\r\n\r\nSo still stumped as to what's causing this issue and why it's only happening in one namespace!"
    },
    {
      "id": 1918030797,
      "user": "posh-raj-kuni",
      "created_at": "2024-01-30T22:37:59Z",
      "body": "We figured out what the issue was on our end.\r\n\r\nWhen I monitored the processes on the redis and sentinel containers, I noticed that the processes that get created for the health checks (liveness, readiness probes) were taking up a lot of CPU (around 40% - 50%). This didn't seem right. In the namespaces where the overall CPU usage was low, these health check processes took up a negligible amount of CPU.\r\n\r\nOne key difference between the two namespaces was that the pod in the namespace where we saw high CPU usage had a very large number of environment variables set by k8s service links.\r\n\r\nWhen we disabled the service links, we no longer observed the high cpu usage! It looks like the large number of environment variables causes the liveness/readiness probes to take up a lot of cpu.\r\n\r\nBut then again, I'm not sure if the high number of environment variables is the root cause of the CPU usage. Because we have other redis bitnami deployments running in standalone mode where the same health checks are run. And we don't see the health checks using up the CPU like in the redis-sentinel pods. \r\n\r\nPerhaps a combination of having a large number of environment variables and multiple containers in a pod is causing the health check processes to take up a lot of CPU?\r\n\r\nI also found an [article](https://medium.com/netcracker/exec-probes-a-story-about-experiment-and-relevant-problems-12de616c0a76) discussing how exec probes can result in large cpu usage but not sure if it is related to the health check problems in redis-sentinel.\r\n\r\nAnd another [github thread](https://github.com/kubernetes/kubernetes/issues/82440) discussing the same high CPU issue for exec probes.\r\n\r\n\r\n"
    },
    {
      "id": 2176285720,
      "user": "denniskorbginski",
      "created_at": "2024-06-18T14:44:56Z",
      "body": "I encountered the same issue today with redis 7.2.4-alpine. Anything we can do help find the root cause?"
    },
    {
      "id": 2176630845,
      "user": "michael-grunder",
      "created_at": "2024-06-18T17:35:52Z",
      "body": "A container based reproducer (e.g. docker-compose, docker, etc) would work :smile:  I don't personally use sentinel in anywhere so have never encountered the state.\r\n\r\nThe fix is likely simple.  The hard part is reproducing the issue."
    },
    {
      "id": 2178565471,
      "user": "denniskorbginski",
      "created_at": "2024-06-19T12:20:51Z",
      "body": "I'll try ðŸ˜…. While this happens frequently in my production workloads, I was not yet able to find a clue to the underlying event that triggered this behavior. Reproducing it in a sandbox environment failed so far as well, but I'll keep trying."
    },
    {
      "id": 2218064889,
      "user": "enisn",
      "created_at": "2024-07-09T15:49:52Z",
      "body": "I use it in Coolify and got the same issue"
    },
    {
      "id": 2222695054,
      "user": "Sven1410",
      "created_at": "2024-07-11T11:34:02Z",
      "body": "Here is a way to reproduce the problem - worked locally on my windows notebook (should also work on Linux & macOS).\r\n(I spent approx. ~1h to reproduce the issue)\r\n\t\t\r\n-install \"minikube\" https://minikube.sigs.k8s.io/docs/\r\n-start k8s: \r\n\r\n`minikube start`\r\n\r\n-install metrics (this enables the \"kubectl top pods\" command): \r\n\r\n`minikube addons enable metrics-server` \r\n\r\n-install \"kubectl\" and connect it to minikube or do it directly with \"minikube\" command https://kubernetes.io/docs/tasks/tools/#kubectl  , https://kubernetes.io/docs/reference/kubectl/  or  https://minikube.sigs.k8s.io/docs/handbook/kubectl/ \r\n\r\n-check that \"kubectl\" works and is connected to the minikube:\r\n\r\n```\r\n$ kubectl get pods -A\r\n\r\nNAMESPACE     NAME                               READY   STATUS    RESTARTS         AGE\r\nkube-system   coredns-565d847f94-grcq9           1/1     Running   0                38m\r\nkube-system   etcd-minikube                      1/1     Running   0                38m\r\nkube-system   kube-apiserver-minikube            1/1     Running   0                38m    ...\r\n```\r\n\r\n-install \"helm\" command https://helm.sh/docs/intro/install/\r\n\r\n-install/deploy redis-ha helm chart:\r\n\r\n```\r\nhelm repo add dandydev https://dandydeveloper.github.io/charts\r\nhelm install redis-ha dandydev/redis-ha --set hardAntiAffinity=false\r\n```\r\n\r\n-after this you will found 3 pods running redis and sentinel containers\r\n```\r\n\r\n$ kubectl get pods -A\r\n\r\nNAMESPACE     NAME                               READY   STATUS    RESTARTS         AGE\r\ndefault       redis-ha-server-0                  3/3     Running   0                9m41s\r\ndefault       redis-ha-server-1                  3/3     Running   0                8m24s\r\ndefault       redis-ha-server-2                  3/3     Running   0                7m8s\r\nkube-system   coredns-565d847f94-grcq9           1/1     Running   0                38m\r\nkube-system   etcd-minikube                      1/1     Running   0                38m\r\nkube-system   kube-apiserver-minikube            1/1     Running   0                38m\r\n...\r\n\r\n```\r\n```\r\n\r\n$ kubectl top pods --containers\r\nPOD                 NAME              CPU(cores)   MEMORY(bytes)\r\nredis-ha-server-0   redis             21m          2Mi\r\nredis-ha-server-0   sentinel          21m          2Mi\r\nredis-ha-server-0   split-brain-fix   1m           0Mi\r\nredis-ha-server-1   redis             21m          2Mi\r\nredis-ha-server-1   sentinel          22m          2Mi\r\nredis-ha-server-1   split-brain-fix   1m           0Mi\r\nredis-ha-server-2   redis             21m          2Mi\r\nredis-ha-server-2   sentinel          21m          2Mi\r\nredis-ha-server-2   split-brain-fix   1m           0Mi\r\n```\r\n\r\n-now do 1 or more restarts of the 3 pods (in my case 1 restart was sufficient to reproduce the problem)\r\n\r\n```\r\n$ kubectl rollout restart sts/redis-ha-server\r\nstatefulset.apps/redis-ha-server restarted\r\n```\r\n\r\n-wait until restart is done (check with \"kubectl get pods -A\")\r\n-check with \"kubectl top pods --containers\" if CPU consuption is too high (near 1)\r\n\r\n```\r\n$ kubectl top pods --containers\r\nPOD                 NAME              CPU(cores)   MEMORY(bytes)\r\nredis-ha-server-0   redis             10m          3Mi\r\nredis-ha-server-0   sentinel          13m          2Mi\r\nredis-ha-server-0   split-brain-fix   3m           0Mi\r\nredis-ha-server-1   redis             13m          2Mi\r\nredis-ha-server-1   sentinel          24m          2Mi\r\nredis-ha-server-1   split-brain-fix   1m           0Mi\r\nredis-ha-server-2   redis             21m          3Mi\r\nredis-ha-server-2   sentinel          985m         2Mi  <----- here is the problem container, see \"CPU\"\r\nredis-ha-server-2   split-brain-fix   1m           0Mi\r\n```\r\n\r\n-you can now connect to the container and run linux commands \"top\",\"ps\" etc. with e.g.\r\n\r\n```\r\n$ kubectl exec -i redis-ha-server-2  --container sentinel sh\r\n\r\n>top\r\nMem: 2229800K used, 3695920K free, 720612K shrd, 30620K buff, 1389952K cached\r\nCPU:  25% usr  25% sys   0% nic  35% idle  15% io   0% irq   0% sirq\r\nLoad average: 2.51 1.59 0.78 4/602 581\r\n  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND\r\n    1     0 1000     R    32576   1%   1  50% redis-sentinel *:26379 [sentinel]  <----- here is the problem process\r\n  574     0 1000     S     1628   0%   0   0% sh\r\n  581   574 1000     R     1624   0%   0   0% top\r\n```\r\n\r\n-at this point also the fan of my laptop indicated the 100% CPU usage..."
    },
    {
      "id": 2263108486,
      "user": "outime",
      "created_at": "2024-08-01T13:48:06Z",
      "body": "Any update on this?"
    },
    {
      "id": 2263165184,
      "user": "denniskorbginski",
      "created_at": "2024-08-01T14:04:17Z",
      "body": "I never had any success reproducing this, but I am also not seeing this problem in my setup anymore. I can't explain why my changes helped, or if they are related at all, but here's what I did. Hope this helps you guys as well.\r\n\r\nI'm also running the `dandydev/redis-ha` helm chart and I noticed a problem with failover. When a pod was terminated, the preStop-Hook in the redis container triggered the failover. The sentinel running in the same pod was elected as leader for this failover, but it was killed by k8s as well and never had the change to actually finish the failover procedure. I then added a preStop-hook to the sentinel container with essentially a `sleep 20`. The failover issues were resolved, and somehow, I have never seen the excessive CPU usage for any redis deployment in my clusters again. Â¯\\\\\\_(ãƒ„)\\_\\/Â¯"
    },
    {
      "id": 2276224894,
      "user": "bootc",
      "created_at": "2024-08-08T16:30:16Z",
      "body": "I just tried your workaround @denniskorbginski and I'm afraid it doesn't work for me.\r\n\r\nI added the following in my `redis-ha` config:\r\n```yaml\r\nsentinel:\r\n  lifecycle:\r\n    preStop:\r\n      exec:\r\n        command: [/bin/sh, -c, sleep 20]\r\n```\r\nI verified that the `sentinel` containers indeed have the `preStop` hook, and I'm still seeing my sentinels regularly spin at 100% CPU.\r\n\r\nHas anyone else tried this? What were your results?"
    },
    {
      "id": 2276462104,
      "user": "Sven1410",
      "created_at": "2024-08-08T18:55:09Z",
      "body": "I've tested it with `sleep 25` and `sleep 120` in the minikube -->still 100% CPU usage after the first rolling restart. \r\n"
    },
    {
      "id": 2276488870,
      "user": "kfirfer",
      "created_at": "2024-08-08T19:12:33Z",
      "body": "As a temporary ugly workaround, I have set a CPU resource limit of `200m` for the sentinel container. This prevents the entire node(s) from being on fire."
    },
    {
      "id": 2446525394,
      "user": "hronix",
      "created_at": "2024-10-30T10:42:03Z",
      "body": "experience the same problem with redis:7.2.4-alpine"
    },
    {
      "id": 2449074511,
      "user": "tstollin",
      "created_at": "2024-10-31T06:14:02Z",
      "body": "We are experiecing the same issue with our redis-ha helm charts. Neither the postStart hook executing the sentinel reset (tested with different sleeps from 30 to 300 s) nor the preStop hook have fixed the issue for us.\r\n\r\nThe only solution that worked so far is manually executing the sentinel reset in each affected container. But obviously this is not viable in a production environment. \r\n\r\nMy guess is that the rolling deployments of the stateful set are the reason for the issue for the other already running containers. Hence, It would require executing the reset after all containers are ready.  This is why the postStart hook won't work: a container is marked ready AFTER the postStart hook executed and the rolling deployment only continues with the next pod if the previous pod is ready. Hence, the sleep will only delay the rolling deployment and the following container starts may create the issue for the resetted container.\r\n\r\nSince there are no \"after deployment\" hooks in k8s, maybe a CronJob executing the reset using kubectl could be a temporary fix.\r\n\r\nFor now, we are following the idea proposed by @kfirfer and set a cpu limit-"
    },
    {
      "id": 2452937894,
      "user": "harry91",
      "created_at": "2024-11-02T09:48:03Z",
      "body": "We encountered the same problem and get the same gdb output as @kfirfer pasted above."
    },
    {
      "id": 2486286874,
      "user": "pfaelzerchen",
      "created_at": "2024-11-19T17:10:28Z",
      "body": "I also encountered the same problem, but only with the redis-ha instance that was deployed by the argo-cd helm chart (which uses `dandydev/redis-ha`). My deployments made with the bitnami redis chart do not encounter this problem.\r\n\r\nMaybe this can be of any help. I am a newbie and do not really know where I would have to look into when comparing both deployments."
    }
  ]
}