{
  "issue_number": 12441.0,
  "title": "[CRASH] Redis cluster.c:1946 crashed by signal 11, si_code: 1",
  "body": "We found a crash (segfault) in 7.0.11 in a 3-node cluster. Addr2line indicates that it happens at [cluster.c:1946](https://github.com/redis/redis/blob/7.0.11/src/cluster.c#L1946), where `curmaster` is NULL when being dereferenced. But this bug seems flaky so I cannot provide concrete instructions to reproduce.\r\n\r\nHere's the bug report.\r\n\r\n**Crash report**\r\n\r\n\r\n```\r\n\r\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\r\n12:M 25 Jul 2023 20:16:44.449 # Redis 7.0.11 crashed by signal: 11, si_code: 1\r\n12:M 25 Jul 2023 20:16:44.449 # Accessing address: 0x84c\r\n12:M 25 Jul 2023 20:16:44.449 # Crashed running the instruction at: 0x6b8b8d\r\n\r\n------ STACK TRACE ------\r\nEIP:\r\n/bin/redis-server *:6379 [cluster](clusterUpdateSlotsConfigWith+0x56d)[0x6b8b8d]\r\n\r\nBacktrace:\r\n/bin/redis-server *:6379 [cluster](sigsegvHandler+0x2c9)[0x6a1f49]\r\n/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420)[0x7ffff7e58420]\r\n/bin/redis-server *:6379 [cluster](clusterUpdateSlotsConfigWith+0x56d)[0x6b8b8d]\r\n/bin/redis-server *:6379 [cluster](clusterProcessPacket+0x3157)[0x6be317]\r\n/bin/redis-server *:6379 [cluster](clusterReadHandler+0x3c4)[0x6c2ec4]\r\n/bin/redis-server *:6379 [cluster][0x808cf1]\r\n/bin/redis-server *:6379 [cluster](aeProcessEvents+0x82f)[0x50ca7f]\r\n/bin/redis-server *:6379 [cluster](aeMain+0x3d)[0x50d75d]\r\n/bin/redis-server *:6379 [cluster](main+0x1274)[0x53fce4]\r\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3)[0x7ffff7c5b083]\r\n/bin/redis-server *:6379 [cluster](__libc_start_main+0x493)[0x8ec063]\r\n/bin/redis-server *:6379 [cluster](_start+0x2e)[0x450a1e]\r\n\r\n------ REGISTERS ------\r\n12:M 25 Jul 2023 20:16:44.479 #\r\nRAX:0000000000000100 RBX:00007fffffffd2e0\r\nRCX:0000000000004ccc RDX:00000000000001a4\r\nRDI:000000000000084c RSI:0000000000001554\r\nRBP:00007fffffffd3b0 RSP:00007fffffff51c0\r\nR8 :0000000000000030 R9 :0000000000000000\r\nR10:0000000000000002 R11:0000000000000001\r\nR12:0000000000000000 R13:000061d000006e80\r\nR14:00007fffffff51c0 R15:0000000000001555\r\nRIP:00000000006b8b8d EFL:0000000000010246\r\nCSGSFS:002b000000000033\r\n12:M 25 Jul 2023 20:16:44.479 # (00007fffffff51cf) -> 0000000000000000\r\n12:M 25 Jul 2023 20:16:44.479 # (00007fffffff51ce) -> 0000000000000000\r\n12:M 25 Jul 2023 20:16:44.479 # (00007fffffff51cd) -> 0000000000000000\r\n12:M 25 Jul 2023 20:16:44.479 # (00007fffffff51cc) -> 0000000000000000\r\n12:M 25 Jul 2023 20:16:44.479 # (00007fffffff51cb) -> 0000000000000000\r\n12:M 25 Jul 2023 20:16:44.479 # (00007fffffff51ca) -> 0000000000000000\r\n12:M 25 Jul 2023 20:16:44.479 # (00007fffffff51c9) -> 0000000000000000\r\n12:M 25 Jul 2023 20:16:44.479 # (00007fffffff51c8) -> 0000000000000000\r\n12:M 25 Jul 2023 20:16:44.479 # (00007fffffff51c7) -> 0000000000000000\r\n12:M 25 Jul 2023 20:16:44.479 # (00007fffffff51c6) -> 0000000000000000\r\n12:M 25 Jul 2023 20:16:44.479 # (00007fffffff51c5) -> 0000000000000000\r\n12:M 25 Jul 2023 20:16:44.479 # (00007fffffff51c4) -> 0000000000000000\r\n12:M 25 Jul 2023 20:16:44.479 # (00007fffffff51c3) -> 0000000000000000\r\n12:M 25 Jul 2023 20:16:44.479 # (00007fffffff51c2) -> 00000000006b8620\r\n12:M 25 Jul 2023 20:16:44.479 # (00007fffffff51c1) -> 0000000000956f2b\r\n12:M 25 Jul 2023 20:16:44.479 # (00007fffffff51c0) -> 0000000041b58ab3\r\n\r\n------ INFO OUTPUT ------\r\n# Server\r\nredis_version:7.0.11\r\nredis_git_sha1:391aa407\r\nredis_git_dirty:0\r\nredis_build_id:e1cfd8d84705f27e\r\nredis_mode:cluster\r\nos:Linux 5.15.68 x86_64\r\narch_bits:64\r\nmonotonic_clock:POSIX clock_gettime\r\nmultiplexing_api:epoll\r\natomicvar_api:c11-builtin\r\ngcc_version:4.2.1\r\nprocess_id:12\r\nprocess_supervised:no\r\nrun_id:87417ce7414ac80853927717729141f42aee787a\r\ntcp_port:6379\r\nserver_time_usec:1690316204438089\r\nuptime_in_seconds:9\r\nuptime_in_days:0\r\nhz:10\r\nconfigured_hz:10\r\nlru_clock:12594604\r\nexecutable:/bin/redis-server\r\nconfig_file:\r\nio_threads_active:0\r\n\r\n# Clients\r\nconnected_clients:0\r\ncluster_connections:2\r\nmaxclients:4064\r\nclient_recent_max_input_buffer:16390\r\nclient_recent_max_output_buffer:0\r\nblocked_clients:0\r\ntracking_clients:0\r\nclients_in_timeout_table:0\r\n\r\n# Memory\r\nused_memory:1555608\r\nused_memory_human:1.48M\r\nused_memory_rss:21348352\r\nused_memory_rss_human:20.36M\r\nused_memory_peak:1590828\r\nused_memory_peak_human:1.52M\r\nused_memory_peak_perc:97.79%\r\nused_memory_overhead:1189834\r\nused_memory_startup:1189649\r\nused_memory_dataset:365774\r\nused_memory_dataset_perc:99.95%\r\nallocator_allocated:1544425\r\nallocator_active:21316608\r\nallocator_resident:21316608\r\ntotal_system_memory:2078552064\r\ntotal_system_memory_human:1.94G\r\nused_memory_lua:31744\r\nused_memory_vm_eval:31744\r\nused_memory_lua_human:31.00K\r\nused_memory_scripts_eval:0\r\nnumber_of_cached_scripts:0\r\nnumber_of_functions:0\r\nnumber_of_libraries:0\r\nused_memory_vm_functions:32768\r\nused_memory_vm_total:64512\r\nused_memory_vm_total_human:63.00K\r\nused_memory_functions:181\r\nused_memory_scripts:181\r\nused_memory_scripts_human:181B\r\nmaxmemory:0\r\nmaxmemory_human:0B\r\nmaxmemory_policy:noeviction\r\nallocator_frag_ratio:13.80\r\nallocator_frag_bytes:19772183\r\nallocator_rss_ratio:1.00\r\nallocator_rss_bytes:0\r\nrss_overhead_ratio:1.00\r\nrss_overhead_bytes:31744\r\nmem_fragmentation_ratio:13.82\r\nmem_fragmentation_bytes:19803927\r\nmem_not_counted_for_evict:0\r\nmem_replication_backlog:4\r\nmem_total_replication_buffers:0\r\nmem_clients_slaves:0\r\nmem_clients_normal:0\r\nmem_cluster_links:0\r\nmem_aof_buffer:0\r\nmem_allocator:libc\r\nactive_defrag_running:0\r\nlazyfree_pending_objects:0\r\nlazyfreed_objects:0\r\n\r\n# Persistence\r\nloading:0\r\nasync_loading:0\r\ncurrent_cow_peak:0\r\ncurrent_cow_size:0\r\ncurrent_cow_size_age:0\r\ncurrent_fork_perc:0.00\r\ncurrent_save_keys_processed:0\r\ncurrent_save_keys_total:0\r\nrdb_changes_since_last_save:0\r\nrdb_bgsave_in_progress:0\r\nrdb_last_save_time:1690316195\r\nrdb_last_bgsave_status:ok\r\nrdb_last_bgsave_time_sec:-1\r\nrdb_current_bgsave_time_sec:-1\r\nrdb_saves:0\r\nrdb_last_cow_size:0\r\nrdb_last_load_keys_expired:0\r\nrdb_last_load_keys_loaded:0\r\naof_enabled:0\r\naof_rewrite_in_progress:0\r\naof_rewrite_scheduled:0\r\naof_last_rewrite_time_sec:-1\r\naof_current_rewrite_time_sec:-1\r\naof_last_bgrewrite_status:ok\r\naof_rewrites:0\r\naof_rewrites_consecutive_failures:0\r\naof_last_write_status:ok\r\naof_last_cow_size:0\r\nmodule_fork_in_progress:0\r\nmodule_fork_last_cow_size:0\r\n\r\n# Stats\r\ntotal_connections_received:19\r\ntotal_commands_processed:29\r\ninstantaneous_ops_per_sec:10\r\ntotal_net_input_bytes:54920\r\ntotal_net_output_bytes:16941\r\ntotal_net_repl_input_bytes:0\r\ntotal_net_repl_output_bytes:0\r\ninstantaneous_input_kbps:0.67\r\ninstantaneous_output_kbps:0.33\r\ninstantaneous_input_repl_kbps:0.00\r\ninstantaneous_output_repl_kbps:0.00\r\nrejected_connections:0\r\nsync_full:2\r\nsync_partial_ok:0\r\nsync_partial_err:2\r\nexpired_keys:0\r\nexpired_stale_perc:0.00\r\nexpired_time_cap_reached_count:0\r\nexpire_cycle_cpu_milliseconds:0\r\nevicted_keys:0\r\nevicted_clients:0\r\ntotal_eviction_exceeded_time:0\r\ncurrent_eviction_exceeded_time:0\r\nkeyspace_hits:0\r\nkeyspace_misses:0\r\npubsub_channels:0\r\npubsub_patterns:0\r\npubsubshard_channels:0\r\nlatest_fork_usec:0\r\ntotal_forks:0\r\nmigrate_cached_sockets:0\r\nslave_expires_tracked_keys:0\r\nactive_defrag_hits:0\r\nactive_defrag_misses:0\r\nactive_defrag_key_hits:0\r\nactive_defrag_key_misses:0\r\ntotal_active_defrag_time:0\r\ncurrent_active_defrag_time:0\r\ntracking_total_keys:0\r\ntracking_total_items:0\r\ntracking_total_prefixes:0\r\nunexpected_error_replies:0\r\ntotal_error_replies:7\r\ndump_payload_sanitizations:0\r\ntotal_reads_processed:56\r\ntotal_writes_processed:33\r\nio_threaded_reads_processed:0\r\nio_threaded_writes_processed:0\r\nreply_buffer_shrinks:3\r\nreply_buffer_expands:0\r\n\r\n# Replication\r\nrole:master\r\nconnected_slaves:0\r\nmaster_failover_state:no-failover\r\nmaster_replid:558fce6645c495d52247a26acde2ed2f21712fa6\r\nmaster_replid2:f0462026e8365b312f488046ab9e66cd078891d3\r\nmaster_repl_offset:0\r\nsecond_repl_offset:1\r\nrepl_backlog_active:1\r\nrepl_backlog_size:1048576\r\nrepl_backlog_first_byte_offset:1\r\nrepl_backlog_histlen:0\r\n\r\n# CPU\r\nused_cpu_sys:0.044078\r\nused_cpu_user:0.087177\r\nused_cpu_sys_children:0.000000\r\nused_cpu_user_children:0.000000\r\nused_cpu_sys_main_thread:0.044010\r\nused_cpu_user_main_thread:0.087043\r\n\r\n# Modules\r\n\r\n# Commandstats\r\ncmdstat_ping:calls=2,usec=33,usec_per_call=16.50,rejected_calls=0,failed_calls=0\r\ncmdstat_info:calls=3,usec=445,usec_per_call=148.33,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|set-config-epoch:calls=1,usec=15,usec_per_call=15.00,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|nodes:calls=5,usec=1536,usec_per_call=307.20,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|meet:calls=2,usec=384,usec_per_call=192.00,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|info:calls=1,usec=90,usec_per_call=90.00,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|keyslot:calls=1,usec=40,usec_per_call=40.00,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|reset:calls=1,usec=40073,usec_per_call=40073.00,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|setslot:calls=1,usec=34,usec_per_call=34.00,rejected_calls=0,failed_calls=1\r\ncmdstat_cluster|countkeysinslot:calls=0,usec=0,usec_per_call=0.00,rejected_calls=1,failed_calls=0\r\ncmdstat_cluster|myid:calls=4,usec=116,usec_per_call=29.00,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|addslots:calls=1,usec=536,usec_per_call=536.00,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|delslotsrange:calls=1,usec=225,usec_per_call=225.00,rejected_calls=0,failed_calls=0\r\ncmdstat_psync:calls=2,usec=323,usec_per_call=161.50,rejected_calls=0,failed_calls=0\r\ncmdstat_rename:calls=0,usec=0,usec_per_call=0.00,rejected_calls=1,failed_calls=0\r\ncmdstat_dump:calls=0,usec=0,usec_per_call=0.00,rejected_calls=1,failed_calls=0\r\ncmdstat_set:calls=0,usec=0,usec_per_call=0.00,rejected_calls=3,failed_calls=0\r\ncmdstat_replconf:calls=4,usec=70,usec_per_call=17.50,rejected_calls=0,failed_calls=0\r\n\r\n# Errorstats\r\nerrorstat_CLUSTERDOWN:count=2\r\nerrorstat_ERR:count=3\r\nerrorstat_MOVED:count=2\r\n\r\n# Latencystats\r\nlatency_percentiles_usec_ping:p50=12.031,p99=21.119,p99.9=21.119\r\nlatency_percentiles_usec_info:p50=174.079,p99=201.727,p99.9=201.727\r\nlatency_percentiles_usec_cluster|set-config-epoch:p50=15.039,p99=15.039,p99.9=15.039\r\nlatency_percentiles_usec_cluster|nodes:p50=354.303,p99=360.447,p99.9=360.447\r\nlatency_percentiles_usec_cluster|meet:p50=172.031,p99=212.991,p99.9=212.991\r\nlatency_percentiles_usec_cluster|info:p50=90.111,p99=90.111,p99.9=90.111\r\nlatency_percentiles_usec_cluster|keyslot:p50=40.191,p99=40.191,p99.9=40.191\r\nlatency_percentiles_usec_cluster|reset:p50=40108.031,p99=40108.031,p99.9=40108.031\r\nlatency_percentiles_usec_cluster|setslot:p50=34.047,p99=34.047,p99.9=34.047\r\nlatency_percentiles_usec_cluster|myid:p50=20.095,p99=50.175,p99.9=50.175\r\nlatency_percentiles_usec_cluster|addslots:p50=536.575,p99=536.575,p99.9=536.575\r\nlatency_percentiles_usec_cluster|delslotsrange:p50=225.279,p99=225.279,p99.9=225.279\r\nlatency_percentiles_usec_psync:p50=115.199,p99=208.895,p99.9=208.895\r\nlatency_percentiles_usec_replconf:p50=7.007,p99=39.167,p99.9=39.167\r\n\r\n# Cluster\r\ncluster_enabled:1\r\n\r\n# Keyspace\r\n\r\n------ CLIENT LIST OUTPUT ------\r\n\r\n------ MODULES INFO OUTPUT ------\r\n\r\n------ CONFIG DEBUG OUTPUT ------\r\nactivedefrag no\r\nrepl-diskless-sync yes\r\nlazyfree-lazy-eviction no\r\nclient-query-buffer-limit 1gb\r\nlazyfree-lazy-user-flush no\r\nlazyfree-lazy-expire no\r\nio-threads 1\r\nio-threads-do-reads no\r\nrepl-diskless-load disabled\r\nlazyfree-lazy-user-del no\r\nreplica-read-only yes\r\nslave-read-only yes\r\nsanitize-dump-payload no\r\nproto-max-bulk-len 512mb\r\nlazyfree-lazy-server-del no\r\nlist-compress-depth 0\r\n\r\n------ FAST MEMORY TEST ------\r\n12:M 25 Jul 2023 20:16:44.497 # Bio thread for job type #0 terminated\r\n12:M 25 Jul 2023 20:16:44.497 # Bio thread for job type #1 terminated\r\n12:M 25 Jul 2023 20:16:44.499 # Bio thread for job type #2 terminated\r\n*** Preparing to test memory region b4d000 (9236480 bytes)\r\n*** Preparing to test memory region 7fff7000 (268435456 bytes)\r\n*** Preparing to test memory region 2008fff7000 (15392867561472 bytes)\r\n*** Preparing to test memory region 10007e66a000 (1044480 bytes)\r\n*** Preparing to test memory region 10007e769000 (12288 bytes)\r\n*** Preparing to test memory region 10007e76c000 (1044480 bytes)\r\n*** Preparing to test memory region 10007e86b000 (12288 bytes)\r\n*** Preparing to test memory region 10007e86e000 (1044480 bytes)\r\n*** Preparing to test memory region 10007e96d000 (23638016 bytes)\r\n*** Preparing to test memory region 602000000000 (458752 bytes)\r\n*** Preparing to test memory region 602e00000000 (65536 bytes)\r\n*** Preparing to test memory region 603000000000 (2031616 bytes)\r\n*** Preparing to test memory region 603e00000000 (65536 bytes)\r\n*** Preparing to test memory region 604000000000 (1507328 bytes)\r\n*** Preparing to test memory region 604e00000000 (65536 bytes)\r\n*** Preparing to test memory region 606000000000 (65536 bytes)\r\n*** Preparing to test memory region 606e00000000 (65536 bytes)\r\n*** Preparing to test memory region 607000000000 (65536 bytes)\r\n*** Preparing to test memory region 607e00000000 (65536 bytes)\r\n*** Preparing to test memory region 608000000000 (65536 bytes)\r\n*** Preparing to test memory region 608e00000000 (65536 bytes)\r\n*** Preparing to test memory region 60b000000000 (65536 bytes)\r\n*** Preparing to test memory region 60be00000000 (65536 bytes)\r\n*** Preparing to test memory region 60c000000000 (65536 bytes)\r\n*** Preparing to test memory region 60ce00000000 (65536 bytes)\r\n*** Preparing to test memory region 60d000000000 (65536 bytes)\r\n*** Preparing to test memory region 60de00000000 (65536 bytes)\r\n*** Preparing to test memory region 60e000000000 (65536 bytes)\r\n*** Preparing to test memory region 60ee00000000 (65536 bytes)\r\n*** Preparing to test memory region 60f000000000 (65536 bytes)\r\n*** Preparing to test memory region 60fe00000000 (65536 bytes)\r\n*** Preparing to test memory region 610000000000 (65536 bytes)\r\n*** Preparing to test memory region 610e00000000 (65536 bytes)\r\n*** Preparing to test memory region 611000000000 (65536 bytes)\r\n*** Preparing to test memory region 611e00000000 (65536 bytes)\r\n*** Preparing to test memory region 612000000000 (65536 bytes)\r\n*** Preparing to test memory region 612e00000000 (65536 bytes)\r\n*** Preparing to test memory region 613000000000 (65536 bytes)\r\n*** Preparing to test memory region 613e00000000 (65536 bytes)\r\n*** Preparing to test memory region 614000000000 (65536 bytes)\r\n*** Preparing to test memory region 614e00000000 (65536 bytes)\r\n*** Preparing to test memory region 615000000000 (65536 bytes)\r\n*** Preparing to test memory region 615e00000000 (65536 bytes)\r\n*** Preparing to test memory region 616000000000 (65536 bytes)\r\n*** Preparing to test memory region 616e00000000 (65536 bytes)\r\n*** Preparing to test memory region 617000000000 (65536 bytes)\r\n*** Preparing to test memory region 617e00000000 (65536 bytes)\r\n*** Preparing to test memory region 618000000000 (65536 bytes)\r\n*** Preparing to test memory region 618e00000000 (65536 bytes)\r\n*** Preparing to test memory region 619000000000 (131072 bytes)\r\n*** Preparing to test memory region 619e00000000 (65536 bytes)\r\n*** Preparing to test memory region 61a000000000 (65536 bytes)\r\n*** Preparing to test memory region 61ae00000000 (65536 bytes)\r\n*** Preparing to test memory region 61b000000000 (65536 bytes)\r\n*** Preparing to test memory region 61be00000000 (65536 bytes)\r\n*** Preparing to test memory region 61c000000000 (65536 bytes)\r\n*** Preparing to test memory region 61ce00000000 (65536 bytes)\r\n*** Preparing to test memory region 61d000000000 (65536 bytes)\r\n*** Preparing to test memory region 61de00000000 (65536 bytes)\r\n*** Preparing to test memory region 61e000000000 (65536 bytes)\r\n*** Preparing to test memory region 61ee00000000 (65536 bytes)\r\n*** Preparing to test memory region 621000000000 (524288 bytes)\r\n*** Preparing to test memory region 621e00000000 (65536 bytes)\r\n*** Preparing to test memory region 624000000000 (65536 bytes)\r\n*** Preparing to test memory region 624000010000 (655360 bytes)\r\n*** Preparing to test memory region 624e00000000 (65536 bytes)\r\n*** Preparing to test memory region 625000000000 (65536 bytes)\r\n*** Preparing to test memory region 625e00000000 (65536 bytes)\r\n*** Preparing to test memory region 629000000000 (983040 bytes)\r\n*** Preparing to test memory region 629e00000000 (65536 bytes)\r\n*** Preparing to test memory region 62a000000000 (720896 bytes)\r\n*** Preparing to test memory region 62ae00000000 (65536 bytes)\r\n*** Preparing to test memory region 62b000000000 (458752 bytes)\r\n*** Preparing to test memory region 62be00000000 (65536 bytes)\r\n*** Preparing to test memory region 62d000000000 (131072 bytes)\r\n*** Preparing to test memory region 62de00000000 (65536 bytes)\r\n*** Preparing to test memory region 62e000000000 (196608 bytes)\r\n*** Preparing to test memory region 62ee00000000 (65536 bytes)\r\n*** Preparing to test memory region 62f000000000 (65536 bytes)\r\n*** Preparing to test memory region 62fe00000000 (65536 bytes)\r\n*** Preparing to test memory region 631000000000 (131072 bytes)\r\n*** Preparing to test memory region 631e00000000 (65536 bytes)\r\n*** Preparing to test memory region 640000000000 (12288 bytes)\r\n*** Preparing to test memory region 7ffff3390000 (8388608 bytes)\r\n*** Preparing to test memory region 7ffff3b9f000 (8388608 bytes)\r\n*** Preparing to test memory region 7ffff43ae000 (8388608 bytes)\r\n*** Preparing to test memory region 7ffff4bbc000 (819200 bytes)\r\n*** Preparing to test memory region 7ffff5084000 (135168 bytes)\r\n*** Preparing to test memory region 7ffff50a5000 (33726464 bytes)\r\n*** Preparing to test memory region 7ffff70cf000 (131072 bytes)\r\n*** Preparing to test memory region 7ffff78cf000 (3526656 bytes)\r\n*** Preparing to test memory region 7ffff7c2c000 (4096 bytes)\r\n*** Preparing to test memory region 7ffff7c2d000 (40960 bytes)\r\n*** Preparing to test memory region 7ffff7e25000 (16384 bytes)\r\n*** Preparing to test memory region 7ffff7e63000 (16384 bytes)\r\n*** Preparing to test memory region 7ffff7fc6000 (16384 bytes)\r\n*** Preparing to test memory region 7ffff7ffb000 (4096 bytes)\r\n*** Preparing to test memory region 7ffff7ffe000 (4096 bytes)\r\n\r\n```\r\n\r\n**Additional information**\r\n\r\nSteps to reproduce (if any)\r\n\r\nThe input involves some reconfigurations: at first other 2 nodes become the replica of this node; later one of them failover and becomes the new master; after this node becomes the replica, it gets soft reset (before this it is instructed to send itself a meet message), and eventually crashes when the other replica connects to it.\r\n\r\nI am looking into this and trying to figure out the root cause.",
  "state": "open",
  "created_at": "2023-07-26T19:07:50Z",
  "updated_at": "2024-08-19T18:45:51Z",
  "closed_at": null,
  "labels": [],
  "comments_data": [
    {
      "id": 1655421002,
      "user": "CATTechnology",
      "created_at": "2023-07-28T10:01:59Z",
      "body": "Believe yourself"
    },
    {
      "id": 1657000973,
      "user": "Congyu-Liu",
      "created_at": "2023-07-30T02:40:22Z",
      "body": "I think somehow I manged to figure out how this bug happened:\r\n1. let a node becomes a replica\r\n2. ask the node to meet itself, which then sends a meet message\r\n3. soft reset the node, which then turns into a master\r\n4. the stale meet message arrives; in `clusterProcessPacket`, `sender` is determined to be `myself`, [whose flag is updated to a replica](https://github.com/redis/redis/blob/5.0.11/src/cluster.c#L1939C18-L1939C18); now, myself->flags is set to be a replica, but myself->slaveof is NULL\r\n6. when another node's gossip message arrives, in `clusterUpdateSlotsConfigWith`, `curmaster` is updated to NULL, eventually causing the segfault at [cluster.c:1946](https://github.com/redis/redis/blob/7.0.11/src/cluster.c#L1946)"
    },
    {
      "id": 1925669696,
      "user": "enjoy-binbin",
      "created_at": "2024-02-04T09:55:16Z",
      "body": "@Congyu-Liu thanks for the report, and sorry for the late response.\r\n\r\nby any luck, do you still remember the cluster topology? Are all 3 nodes master or ? and i may need more info, for example, in step 1 above, what command was used to turn the node into a replica? Is the node originally the master node in the cluster, or is it a newly added node?"
    },
    {
      "id": 1933079092,
      "user": "Congyu-Liu",
      "created_at": "2024-02-07T23:04:02Z",
      "body": "@enjoy-binbin Hello! Thanks for your response. IIRC, 3 nodes are initially master nodes. In step 1, I used `CLUSTER REPLICATE` to turn a node into a replica. The node is originally in the cluster. \nFeel free to reach out to me for more information."
    },
    {
      "id": 2249442649,
      "user": "sundb",
      "created_at": "2024-07-25T05:07:17Z",
      "body": "@Congyu-Liu did you change the cluster nodes config manually?\r\ndo you still keep the cluster nodes config file of the crash server?"
    },
    {
      "id": 2249810350,
      "user": "sundb",
      "created_at": "2024-07-25T08:51:28Z",
      "body": "@Congyu-Liu how can you get a stale meet message after the node turns to a replica? thanks."
    },
    {
      "id": 2297213658,
      "user": "Congyu-Liu",
      "created_at": "2024-08-19T18:45:49Z",
      "body": "Hi @sundb! Sorry for the late reply.\r\n\r\n> did you change the cluster nodes config manually?\r\n\r\nI used `CLUSTER REPLICATE` and `CLUSTER RESET` commands (assuming you are asking how I changed the node roles).\r\n\r\n> do you still keep the cluster nodes config file of the crash server?\r\n\r\nHere is the command line arguments I used for each node:\r\n```\r\n--protected-mode no --port 6379 --cluster-enabled yes --loglevel verbose\r\n```\r\n\r\n> how can you get a stale meet message after the node turns to a replica?\r\n\r\nThe meet message was delayed. It arrived after the node turned into a replica.\r\n\r\nLet me know if you have more questions and I am happy to help."
    }
  ]
}