{
  "issue_number": 13306.0,
  "title": "[BUG] Redis hangs during upgrade from v7.0.10 to v7.2.4",
  "body": "**Describe the bug**\r\n\r\nRedis instance randomly hangs and becomes unresponsive after the upgrade from v7.0.10 to v7.2.4. \r\n\r\n**To reproduce**\r\n\r\nThe following upgrade process was used when we noticed the issue:\r\n1. Setup Redis cluster running v7.0.10 - in our case, we've provisioned a cluster with 9 masters, 2 replicas per master\r\n2. Install Redis v7.2.4 RPM package\r\n3. Restart all the replicas in a rolling fashion. All replicas are now running v7.2.4, all the masters are still on v7.0.10\r\n4. Failover masters and restart instances in a rolling fashion. **At this point masters are transitioning to the instances running v7.2.4. Some of these new masters become unresponsive**. \r\n\r\nThis isn't happening all the time and not all Redis processes hang after the failover. We haven't established any pattern here. When the process hangs it does not respond to any `redis-cli` commands, for example, `PING`, `INFO`, etc. You can't set or get any keys. Running `strace` on this Redis instance produces no output. It seems like the process is completely stuck. You need to `SIGKILL` the process to stop it.\r\nThe logs of the Redis process that hangs:\r\n```\r\n--> Failover happens\r\n2258807:S 30 May 2024 11:00:15.191 * Manual failover user request accepted.\r\n2258807:S 30 May 2024 11:00:15.192 * Received replication offset for paused master manual failover: 9037\r\n2258807:S 30 May 2024 11:00:15.192 * All master replication stream processed, manual failover can start.\r\n2258807:S 30 May 2024 11:00:15.192 * Start of election delayed for 0 milliseconds (rank #0, offset 9037).\r\n2258807:S 30 May 2024 11:00:15.192 * Starting a failover election for epoch 12.\r\n2258807:S 30 May 2024 11:00:15.193 * Failover election won: I'm the new master.\r\n2258807:S 30 May 2024 11:00:15.193 * configEpoch set to 12 after successful failover\r\n2258807:M 30 May 2024 11:00:15.193 * Connection with master lost.\r\n2258807:M 30 May 2024 11:00:15.193 * Caching the disconnected master state.\r\n2258807:M 30 May 2024 11:00:15.193 * Discarding previously cached master state.\r\n2258807:M 30 May 2024 11:00:15.193 * Setting secondary replication ID to c35c915f9851c84d47057806cfba70f81ee97138, valid up to offset: 9038. New replication ID is b7c7225ef698b5f8da7d8960fde1025435dc3d7c\r\n2258807:M 30 May 2024 11:00:15.196 * Replica 10.37.28.117:6552 asks for synchronization\r\n2258807:M 30 May 2024 11:00:15.196 * Partial resynchronization request from 10.37.28.117:6552 accepted. Sending 0 bytes of backlog starting from offset 9038.\r\n2258807:M 30 May 2024 11:00:15.199 * Replica 10.37.19.113:6552 asks for synchronization\r\n2258807:M 30 May 2024 11:00:15.199 * Partial resynchronization request from 10.37.19.113:6552 accepted. Sending 0 bytes of backlog starting from offset 9038.\r\n2258807:M 30 May 2024 11:00:15.844 * Connection with replica 10.37.28.117:6552 lost.\r\n2258807:M 30 May 2024 11:00:16.447 * Replica 10.37.28.117:6552 asks for synchronization\r\n2258807:M 30 May 2024 11:00:16.447 * Partial resynchronization request from 10.37.28.117:6552 accepted. Sending 0 bytes of backlog starting from offset 9038.\r\n2258807:M 30 May 2024 11:00:22.548 * Failover auth granted to bd552e6f185d8040f02e739286b83d9f39cb795b () for epoch 13\r\n2258807:M 30 May 2024 11:00:23.736 * Manual failover requested by replica c49eed33ca2420fa1634f701df494872d9d278fe ().\r\n2258807:M 30 May 2024 11:00:23.737 * Failover auth granted to c49eed33ca2420fa1634f701df494872d9d278fe () for epoch 14\r\n2258807:M 30 May 2024 11:00:23.738 * Connection with replica 10.37.28.117:6552 lost.\r\n2258807:M 30 May 2024 11:00:23.739 * Configuration change detected. Reconfiguring myself as a replica of c49eed33ca2420fa1634f701df494872d9d278fe ()\r\n2258807:S 30 May 2024 11:00:23.739 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.\r\n2258807:S 30 May 2024 11:00:23.739 * Connecting to MASTER 10.37.28.117:6552\r\n2258807:S 30 May 2024 11:00:23.739 * MASTER <-> REPLICA sync started\r\n2258807:S 30 May 2024 11:00:23.741 * Connection with replica 10.37.19.113:6552 lost.\r\n2258807:S 30 May 2024 11:00:23.741 * Non blocking connect for SYNC fired the event.\r\n2258807:S 30 May 2024 11:00:23.741 * Master replied to PING, replication can continue...\r\n2258807:S 30 May 2024 11:00:23.741 * Trying a partial resynchronization (request b7c7225ef698b5f8da7d8960fde1025435dc3d7c:10221).\r\n2258807:S 30 May 2024 11:00:23.741 * Successful partial resynchronization with master.\r\n2258807:S 30 May 2024 11:00:23.741 * Master replication ID changed to 4f857c5e8a419baa6d70bfcd5c571c3c29113857\r\n2258807:S 30 May 2024 11:00:23.741 * MASTER <-> REPLICA sync: Master accepted a Partial Resynchronization.\r\n--> Redis process hangs\r\n```\r\n\r\n\r\n**Expected behavior**\r\n\r\nUpgrade succeeds without any issues.\r\n\r\n",
  "state": "open",
  "created_at": "2024-05-30T13:51:48Z",
  "updated_at": "2024-07-31T11:20:27Z",
  "closed_at": null,
  "labels": [
    "critical bug"
  ],
  "comments_data": [
    {
      "id": 2238636324,
      "user": "sundb",
      "created_at": "2024-07-19T08:22:17Z",
      "body": "@zygisa do you still have the same problem? is there reproducible steps?\r\ni tried serveral time locally but can't reproduce it."
    },
    {
      "id": 2260279491,
      "user": "zygisa",
      "created_at": "2024-07-31T11:16:51Z",
      "body": "Hi @sundb, sorry, I don't have any additional details. The steps are listed in the description. Here's the config we're using, maybe that's gonna be helpful:\r\n```\r\npidfile /var/run/redis/<cluster_name>/redis_<cluster_name>.pid\r\nport 6749\r\ntcp-backlog 4096\r\nbind <IP>\r\nprotected-mode no\r\ntimeout 0\r\ntcp-keepalive 300\r\nloglevel notice\r\nsyslog-enabled yes\r\nsyslog-ident redis-<cluster_name>\r\nsyslog-facility local0\r\ndatabases 16\r\nsave 3600 1\r\nstop-writes-on-bgsave-error yes\r\nrdbcompression yes\r\nrdbchecksum yes\r\ndbfilename dump-<cluster_name>.rdb\r\ndir /var/lib/redis\r\nslave-serve-stale-data yes\r\nslave-read-only yes\r\nrepl-diskless-sync yes\r\nrepl-diskless-sync-delay 5\r\nslave-read-only yes\r\nrepl-ping-slave-period 10\r\nrepl-timeout 60\r\nrepl-disable-tcp-nodelay no\r\nrepl-backlog-size 1mb\r\nrepl-backlog-ttl 3600\r\nslave-priority 100\r\nmaxclients 131072\r\nappendonly no\r\nappendfilename appendonly-<cluster_name>.aof\r\nappendfsync everysec\r\nno-appendfsync-on-rewrite no\r\nauto-aof-rewrite-percentage 100\r\nauto-aof-rewrite-min-size 64mb\r\naof-load-truncated yes\r\nlua-time-limit 5000\r\nslowlog-log-slower-than 2500\r\nslowlog-max-len 10000\r\nnotify-keyspace-events \"\"\r\nhash-max-ziplist-entries 512\r\nhash-max-ziplist-value 64\r\nlist-max-ziplist-entries 512\r\nlist-max-ziplist-value 64\r\nset-max-intset-entries 512\r\nzset-max-ziplist-entries 128\r\nzset-max-ziplist-value 64\r\nhll-sparse-max-bytes 3000\r\nactiverehashing yes\r\nclient-output-buffer-limit normal 0 0 0\r\nclient-output-buffer-limit slave 1024mb 1024mb 60\r\nclient-output-buffer-limit pubsub 32mb 8mb 60\r\nhz 10\r\naof-rewrite-incremental-fsync yes\r\ncluster-enabled yes\r\ncluster-config-file nodes-<cluster_name>.conf\r\ncluster-node-timeout 5000\r\naclfile /etc/redis/cluster_<cluster_name>.acl\r\nlazyfree-lazy-user-flush yes\r\nlazyfree-lazy-user-del yes\r\nlazyfree-lazy-expire yes\r\nlazyfree-lazy-server-del yes\r\nreplica-lazy-flush yes\r\nactive-defrag-cycle-max 5\r\nactivedefrag yes\r\noom-score-adj yes\r\nenable-debug-command yes\r\nio-threads 4\r\nio-threads-do-reads yes\r\ncluster-allow-replica-migration no\r\ncluster-migration-barrier 99\r\nrepl-backlog-size 10mb\r\nreplica-priority 10\r\n```\r\n\r\nWe also ran into some issues (specifically https://github.com/redis/redis/issues/13205) in v7.2.4 because `activedefrag` is enabled in our setup but not sure if this can be related in any way."
    }
  ]
}