{
  "issue_number": 12717.0,
  "title": " [BUG] Redis Cluster node after becoming Replica allows adding of primary slot range",
  "body": "Bug Description:\r\nVersion: Redis version=7.0.11\r\n\r\nHi, I observed an issue with Redis cluster nodes where the following happened with our automated Redis management control plane:\r\n1. A node N1 was issued a **_CLUSTER REPLICATE_** command\r\n2. Control Plane checked if the node was unassigned (replication didn't happen yet so it returned unassigned)\r\n3. The node N1 was reassigned for Primary role - and issued **_CLUSTER ADDSLOTSRANGE_** for some other range\r\n4. Final state had the node both as replica and primary: \r\n\r\n\r\nCLUSTER NODES O/P for Node on Port 7001\r\n\r\n 3dd70494e109dfdfa44fd2ff69d1a12be1f3642b 172.18.133.131:7001@17001,padgupta-lappy. myself,**_slave_**,nofailover c23aa8a150e0291eaad4b88cc505b88b8b2479b0 0 1698913926000 3 connected **_8192-16383_**\r\n\r\nc23aa8a150e0291eaad4b88cc505b88b8b2479b0 172.18.133.131:7000@17000,padgupta-lappy. master,nofailover - 0 1698914350449 3 connected 0-8191\r\n\r\nCLUSTER NODES O/P for Node on Port 7000\r\n\r\nc23aa8a150e0291eaad4b88cc505b88b8b2479b0 172.18.133.131:7000@17000,padgupta-lappy. myself,master,nofailover - 0 0 3 connected 0-8191\r\n\r\n3dd70494e109dfdfa44fd2ff69d1a12be1f3642b 172.18.133.131:7001@17001,padgupta-lappy. slave,nofailover c23aa8a150e0291eaad4b88cc505b88b8b2479b0 0 1698914345168 3 connected\r\n\r\n\r\n Post this, if you resend CLUSTER REPLICATE command to the NodeN1, Redis crashes.\r\n \r\nI'm not sure if this dual role is expected in the first place, but it does cause a crash later.\r\n\r\nBUG REPORT:\r\n\r\n\r\n------ STACK TRACE ------\r\n\r\nBacktrace:\r\n/usr/bin/redis-server *:7001 [cluster] (clusterSetMaster+0xdf)[0x563c35e2a22f]\r\n/usr/bin/redis-server *:7001 [cluster] (clusterCommand+0x1c13)[0x563c35e31823]\r\n/usr/bin/redis-server *:7001 [cluster] (call+0xee)[0x563c35da510e]\r\n/usr/bin/redis-server *:7001 [cluster] (processCommand+0x6fd)[0x563c35da617d]\r\n/usr/bin/redis-server *:7001 [cluster] (processInputBuffer+0x107)[0x563c35dc2517]\r\n/usr/bin/redis-server *:7001 [cluster] (readQueryFromClient+0x318)[0x563c35dc2a58]\r\n/usr/bin/redis-server *:7001 [cluster] (+0x17323c)[0x563c35e9823c]\r\n/usr/bin/redis-server *:7001 [cluster] (aeProcessEvents+0x1e2)[0x563c35d9c182]\r\n/usr/bin/redis-server *:7001 [cluster] (aeMain+0x1d)[0x563c35d9c4bd]\r\n/usr/bin/redis-server *:7001 [cluster] (main+0x354)[0x563c35d93df4]\r\n/lib/x86_64-linux-gnu/libc.so.6 (+0x29d90)[0x7f5f788ccd90]\r\n/lib/x86_64-linux-gnu/libc.so.6 (__libc_start_main+0x80)[0x7f5f788cce40]\r\n/usr/bin/redis-server *:7001 [cluster] (_start+0x25)[0x563c35d944c5]\r\n\r\nRepro Steps:\r\n\r\nMinimal Pseduo-Code for Repro:\r\n\r\n```\r\nfirstShard.ClusterAddSlots(context.TODO(), 0, 8191)\r\nfirstShard.ClusterBumpEpoch(context.TODO()))\r\nsecondShard.ClusterReplicate(context.TODO(), NodeIdOfFirstShard)\r\nsecondShard.ClusterAddSlots(context.TODO(), 8192, 16383)\r\nsecondShard.ClusterBumpEpoch(context.TODO()))\r\n\r\n[For Crash- issue another replicate]\r\nsecondShard.ClusterReplicate(context.TODO(), NodeIdOfFirstShard)\r\n```\r\n\r\nExpectation:\r\n\r\nIf we send CLUSTER REPLICATE to a master node, it denies replication command. I would expect similar if we send CLUSTER ADDSLOTSRANGE to a replica node (until it's state is reset). It should atleast be consistent with view from other nodes eventually from gossip propogation. Currently, The CLUSTER NODES of the 2 shards don't converge (before the crash).\r\n",
  "state": "open",
  "created_at": "2023-11-02T09:14:13Z",
  "updated_at": "2023-11-02T10:15:19Z",
  "closed_at": null,
  "labels": [],
  "comments_data": [
    {
      "id": 1790362307,
      "user": "codeplayer14",
      "created_at": "2023-11-02T09:24:50Z",
      "body": "Updating with full bug report\r\n\r\n\r\n== REDIS BUG REPORT START: Cut & paste starting from here ===\r\n11939:S 02 Nov 2023 14:24:59.396 # === ASSERTION FAILED ===\r\n11939:S 02 Nov 2023 14:24:59.396 # ==> cluster.c:4580 'myself->numslots == 0' is not true\r\n\r\n------ STACK TRACE ------\r\n\r\nBacktrace:\r\n/usr/bin/redis-server *:7001 [cluster] (clusterSetMaster+0xdf)[0x563c35e2a22f]\r\n/usr/bin/redis-server *:7001 [cluster] (clusterCommand+0x1c13)[0x563c35e31823]\r\n/usr/bin/redis-server *:7001 [cluster] (call+0xee)[0x563c35da510e]\r\n/usr/bin/redis-server *:7001 [cluster] (processCommand+0x6fd)[0x563c35da617d]\r\n/usr/bin/redis-server *:7001 [cluster] (processInputBuffer+0x107)[0x563c35dc2517]\r\n/usr/bin/redis-server *:7001 [cluster] (readQueryFromClient+0x318)[0x563c35dc2a58]\r\n/usr/bin/redis-server *:7001 [cluster] (+0x17323c)[0x563c35e9823c]\r\n/usr/bin/redis-server *:7001 [cluster] (aeProcessEvents+0x1e2)[0x563c35d9c182]\r\n/usr/bin/redis-server *:7001 [cluster] (aeMain+0x1d)[0x563c35d9c4bd]\r\n/usr/bin/redis-server *:7001 [cluster] (main+0x354)[0x563c35d93df4]\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x29d90) [0x7f5f788ccd90]\r\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7f5f788cce40]\r\n/usr/bin/redis-server *:7001 [cluster] (_start+0x25)[0x563c35d944c5]\r\n\r\n------ INFO OUTPUT ------\r\n# Server\r\nredis_version:7.0.11\r\nredis_git_sha1:00000000\r\nredis_git_dirty:0\r\nredis_build_id:3af367a78d5e21e9\r\nredis_mode:cluster\r\nos:Linux 5.15.90.1-microsoft-standard-WSL2 x86_64\r\narch_bits:64\r\nmonotonic_clock:POSIX clock_gettime\r\nmultiplexing_api:epoll\r\natomicvar_api:c11-builtin\r\ngcc_version:11.3.0\r\nprocess_id:11939\r\nprocess_supervised:no\r\nrun_id:a28a5f2e211968330ac236a0d8dda9bf32709b7c\r\ntcp_port:7001\r\nserver_time_usec:1698915299396196\r\nuptime_in_seconds:198\r\nuptime_in_days:0\r\nhz:10\r\nconfigured_hz:10\r\nlru_clock:4416483\r\nexecutable:/usr/bin/redis-server\r\nconfig_file:/home/paddy14/GoProjects/WebXTCache/services/garnet-operator/redis-server/redis-config/redis7001.conf\r\nio_threads_active:0\r\n\r\n# Clients\r\nconnected_clients:4\r\ncluster_connections:4\r\nmaxclients:10000\r\nclient_recent_max_input_buffer:24\r\nclient_recent_max_output_buffer:0\r\nblocked_clients:0\r\ntracking_clients:0\r\nclients_in_timeout_table:0\r\n\r\n# Memory\r\nused_memory:1821368\r\nused_memory_human:1.74M\r\nused_memory_rss:16105472\r\nused_memory_rss_human:15.36M\r\nused_memory_peak:2082824\r\nused_memory_peak_human:1.99M\r\nused_memory_peak_perc:87.45%\r\nused_memory_overhead:1618556\r\nused_memory_startup:1586296\r\nused_memory_dataset:202812\r\nused_memory_dataset_perc:86.28%\r\nallocator_allocated:2149576\r\nallocator_active:2609152\r\nallocator_resident:6033408\r\ntotal_system_memory:16686968832\r\ntotal_system_memory_human:15.54G\r\nused_memory_lua:31744\r\nused_memory_vm_eval:31744\r\nused_memory_lua_human:31.00K\r\nused_memory_scripts_eval:0\r\nnumber_of_cached_scripts:0\r\nnumber_of_functions:0\r\nnumber_of_libraries:0\r\nused_memory_vm_functions:32768\r\nused_memory_vm_total:64512\r\nused_memory_vm_total_human:63.00K\r\nused_memory_functions:184\r\nused_memory_scripts:184\r\nused_memory_scripts_human:184B\r\nmaxmemory:10737418240\r\nmaxmemory_human:10.00G\r\nmaxmemory_policy:allkeys-lru\r\nallocator_frag_ratio:1.21\r\nallocator_frag_bytes:459576\r\nallocator_rss_ratio:2.31\r\nallocator_rss_bytes:3424256\r\nrss_overhead_ratio:2.67\r\nrss_overhead_bytes:10072064\r\nmem_fragmentation_ratio:8.95\r\nmem_fragmentation_bytes:14306632\r\nmem_not_counted_for_evict:0\r\nmem_replication_backlog:20508\r\nmem_total_replication_buffers:20504\r\nmem_clients_slaves:0\r\nmem_clients_normal:7216\r\nmem_cluster_links:4352\r\nmem_aof_buffer:0\r\nmem_allocator:jemalloc-5.2.1\r\nactive_defrag_running:0\r\nlazyfree_pending_objects:0\r\nlazyfreed_objects:0\r\n\r\n# Persistence\r\nloading:0\r\nasync_loading:0\r\ncurrent_cow_peak:0\r\ncurrent_cow_size:0\r\ncurrent_cow_size_age:0\r\ncurrent_fork_perc:0.00\r\ncurrent_save_keys_processed:0\r\ncurrent_save_keys_total:0\r\nrdb_changes_since_last_save:0\r\nrdb_bgsave_in_progress:0\r\nrdb_last_save_time:1698915101\r\nrdb_last_bgsave_status:ok\r\nrdb_last_bgsave_time_sec:-1\r\nrdb_current_bgsave_time_sec:-1\r\nrdb_saves:0\r\nrdb_last_cow_size:0\r\nrdb_last_load_keys_expired:0\r\nrdb_last_load_keys_loaded:0\r\naof_enabled:0\r\naof_rewrite_in_progress:0\r\naof_rewrite_scheduled:0\r\naof_last_rewrite_time_sec:-1\r\naof_current_rewrite_time_sec:-1\r\naof_last_bgrewrite_status:ok\r\naof_rewrites:0\r\naof_rewrites_consecutive_failures:0\r\naof_last_write_status:ok\r\naof_last_cow_size:0\r\nmodule_fork_in_progress:0\r\nmodule_fork_last_cow_size:0\r\n\r\n# Stats\r\ntotal_connections_received:3\r\ntotal_commands_processed:25\r\ninstantaneous_ops_per_sec:0\r\ntotal_net_input_bytes:89180\r\ntotal_net_output_bytes:178624\r\ntotal_net_repl_input_bytes:511\r\ntotal_net_repl_output_bytes:0\r\ninstantaneous_input_kbps:0.01\r\ninstantaneous_output_kbps:0.02\r\ninstantaneous_input_repl_kbps:0.01\r\ninstantaneous_output_repl_kbps:0.00\r\nrejected_connections:0\r\nsync_full:0\r\nsync_partial_ok:0\r\nsync_partial_err:0\r\nexpired_keys:0\r\nexpired_stale_perc:0.00\r\nexpired_time_cap_reached_count:0\r\nexpire_cycle_cpu_milliseconds:0\r\nevicted_keys:0\r\nevicted_clients:0\r\ntotal_eviction_exceeded_time:0\r\ncurrent_eviction_exceeded_time:0\r\nkeyspace_hits:0\r\nkeyspace_misses:0\r\npubsub_channels:0\r\npubsub_patterns:0\r\npubsubshard_channels:0\r\nlatest_fork_usec:0\r\ntotal_forks:0\r\nmigrate_cached_sockets:0\r\nslave_expires_tracked_keys:0\r\nactive_defrag_hits:0\r\nactive_defrag_misses:0\r\nactive_defrag_key_hits:0\r\nactive_defrag_key_misses:0\r\ntotal_active_defrag_time:0\r\ncurrent_active_defrag_time:0\r\ntracking_total_keys:0\r\ntracking_total_items:0\r\ntracking_total_prefixes:0\r\nunexpected_error_replies:0\r\ntotal_error_replies:0\r\ndump_payload_sanitizations:0\r\ntotal_reads_processed:47\r\ntotal_writes_processed:188\r\nio_threaded_reads_processed:0\r\nio_threaded_writes_processed:0\r\nreply_buffer_shrinks:4\r\nreply_buffer_expands:0\r\n\r\n# Replication\r\nrole:slave\r\nmaster_host:172.18.133.131\r\nmaster_port:7000\r\nmaster_link_status:up\r\nmaster_last_io_seconds_ago:2\r\nmaster_sync_in_progress:0\r\nslave_read_repl_offset:266\r\nslave_repl_offset:266\r\nslave_priority:100\r\nslave_read_only:1\r\nreplica_announced:1\r\nconnected_slaves:0\r\nmaster_failover_state:no-failover\r\nmaster_replid:7c5fafd84676dd352225f1c5edac2b0c9f41e72d\r\nmaster_replid2:0000000000000000000000000000000000000000\r\nmaster_repl_offset:266\r\nsecond_repl_offset:-1\r\nrepl_backlog_active:1\r\nrepl_backlog_size:1048576\r\nrepl_backlog_first_byte_offset:15\r\nrepl_backlog_histlen:252\r\n\r\n# CPU\r\nused_cpu_sys:0.253435\r\nused_cpu_user:0.158397\r\nused_cpu_sys_children:0.000000\r\nused_cpu_user_children:0.000000\r\nused_cpu_sys_main_thread:0.252074\r\nused_cpu_user_main_thread:0.157546\r\n\r\n# Modules\r\n\r\n# Commandstats\r\ncmdstat_ping:calls=18,usec=23,usec_per_call=1.28,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|meet:calls=1,usec=28,usec_per_call=28.00,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|nodes:calls=2,usec=635,usec_per_call=317.50,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|addslots:calls=1,usec=620,usec_per_call=620.00,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|bumpepoch:calls=1,usec=64,usec_per_call=64.00,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|replicate:calls=1,usec=240,usec_per_call=240.00,rejected_calls=0,failed_calls=0\r\ncmdstat_command|docs:calls=1,usec=1041,usec_per_call=1041.00,rejected_calls=0,failed_calls=0\r\n\r\n# Errorstats\r\n\r\n# Latencystats\r\nlatency_percentiles_usec_ping:p50=1.003,p99=2.007,p99.9=2.007\r\nlatency_percentiles_usec_cluster|meet:p50=28.031,p99=28.031,p99.9=28.031\r\nlatency_percentiles_usec_cluster|nodes:p50=109.055,p99=528.383,p99.9=528.383\r\nlatency_percentiles_usec_cluster|addslots:p50=622.591,p99=622.591,p99.9=622.591\r\nlatency_percentiles_usec_cluster|bumpepoch:p50=64.255,p99=64.255,p99.9=64.255\r\nlatency_percentiles_usec_cluster|replicate:p50=240.639,p99=240.639,p99.9=240.639\r\nlatency_percentiles_usec_command|docs:p50=1044.479,p99=1044.479,p99.9=1044.479\r\n\r\n# Cluster\r\ncluster_enabled:1\r\n\r\n# Keyspace\r\n\r\nargv[1]: '\"replicate\"'\r\nargv[2]: '\"59a38a2564efca9b132732424c643870fff819e9\"'\r\n\r\n------ MODULES INFO OUTPUT ------\r\n\r\n------ CONFIG DEBUG OUTPUT ------\r\nproto-max-bulk-len 512mb\r\nlazyfree-lazy-server-del no\r\nio-threads-do-reads no\r\nlazyfree-lazy-expire no\r\nrepl-diskless-sync yes\r\nlist-compress-depth 0\r\nio-threads 1\r\nsanitize-dump-payload no\r\nlazyfree-lazy-user-del no\r\nclient-query-buffer-limit 1gb\r\nlazyfree-lazy-user-flush no\r\nactivedefrag no\r\nrepl-diskless-load disabled\r\nreplica-read-only yes\r\nslave-read-only yes\r\nlazyfree-lazy-eviction no\r\n\r\n------ FAST MEMORY TEST ------\r\n11939:S 02 Nov 2023 14:24:59.397 # Bio thread for job type #0 terminated\r\n11939:S 02 Nov 2023 14:24:59.398 # Bio thread for job type #1 terminated\r\n11939:S 02 Nov 2023 14:24:59.398 # Bio thread for job type #2 terminated\r\n*** Preparing to test memory region 563c36052000 (2306048 bytes)\r\n*** Preparing to test memory region 563c37a6c000 (270336 bytes)\r\n*** Preparing to test memory region 7f5f75b7c000 (2621440 bytes)\r\n*** Preparing to test memory region 7f5f75dfd000 (8388608 bytes)\r\n*** Preparing to test memory region 7f5f765fe000 (8388608 bytes)\r\n*** Preparing to test memory region 7f5f76dff000 (8388608 bytes)\r\n*** Preparing to test memory region 7f5f77600000 (8388608 bytes)\r\n*** Preparing to test memory region 7f5f77e00000 (8388608 bytes)\r\n*** Preparing to test memory region 7f5f78610000 (24576 bytes)\r\n*** Preparing to test memory region 7f5f7863c000 (8192 bytes)\r\n*** Preparing to test memory region 7f5f7877b000 (4096 bytes)\r\n*** Preparing to test memory region 7f5f788a1000 (8192 bytes)\r\n*** Preparing to test memory region 7f5f78abe000 (53248 bytes)\r\n*** Preparing to test memory region 7f5f78f0b000 (12288 bytes)\r\n*** Preparing to test memory region 7f5f79078000 (4096 bytes)\r\n*** Preparing to test memory region 7f5f79166000 (8192 bytes)\r\n.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O\r\nFast memory test PASSED, however your memory can still be broken. Please run a memory test for several hours if possible.\r\n\r\n=== REDIS BUG REPORT END. Make sure to include from START to END. ===\r\n"
    },
    {
      "id": 1790370020,
      "user": "enjoy-binbin",
      "created_at": "2023-11-02T09:29:53Z",
      "body": "we discussed this in #12282"
    },
    {
      "id": 1790442383,
      "user": "codeplayer14",
      "created_at": "2023-11-02T10:15:19Z",
      "body": "Great. Thanks for the prompt response @enjoy-binbin !"
    }
  ]
}