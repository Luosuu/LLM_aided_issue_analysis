{
  "issue_number": 12677.0,
  "title": "[CRASH] <Redis7.2.1, replica crashes when set activedefrag to yes.>",
  "body": "**Crash report**\r\n\r\n```\r\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\r\n15455:S 19 Oct 2023 15:30:38.116 # Redis 7.2.1 crashed by signal: 11, si_code: 1\r\n15455:S 19 Oct 2023 15:30:38.116 # Accessing address: 0x50\r\n15455:S 19 Oct 2023 15:30:38.116 # Crashed running the instruction at: 0x5675c7\r\n\r\n------ STACK TRACE ------\r\nEIP:\r\n/usr/local/redis-7.2.1/redis-server *:7390 [cluster](dbDictAfterReplaceEntry+0x77)[0x5675c7]\r\n\r\nBacktrace:\r\n/lib64/libpthread.so.0(+0xf5e0)[0x7f4dcbeee5e0]\r\n/usr/local/redis-7.2.1/redis-server *:7390 [cluster](dbDictAfterReplaceEntry+0x77)[0x5675c7]\r\n/usr/local/redis-7.2.1/redis-server *:7390 [cluster][0x5681d0]\r\n/usr/local/redis-7.2.1/redis-server *:7390 [cluster](activeDefragCycle+0x723)[0x4c4a73]\r\n/usr/local/redis-7.2.1/redis-server *:7390 [cluster](databasesCron+0x52)[0x577bd2]\r\n/usr/local/redis-7.2.1/redis-server *:7390 [cluster](serverCron+0x25f)[0x57911f]\r\n/usr/local/redis-7.2.1/redis-server *:7390 [cluster][0x45f8be]\r\n/usr/local/redis-7.2.1/redis-server *:7390 [cluster](aeMain+0x145)[0x4cb3f5]\r\n/usr/local/redis-7.2.1/redis-server *:7390 [cluster](main+0x4d0)[0x456070]\r\n/lib64/libc.so.6(__libc_start_main+0xf5)[0x7f4dcbb3dc05]\r\n/usr/local/redis-7.2.1/redis-server *:7390 [cluster][0x4566ba]\r\n\r\n------ REGISTERS ------\r\n15455:S 19 Oct 2023 15:30:38.117 # \r\nRAX:0000000000033b00 RBX:00007f4dc4c0b2a0\r\nRCX:00007f4a87c4bae7 RDX:0000000000000000\r\nRDI:00007f4a87c4bab3 RSI:00007f4a87c4bae7\r\nRBP:00007f4dcb629900 RSP:00007ffef9b39820\r\nR8 :0000000000000000 R9 :0000000006666667\r\nR10:00007f4dcca1a740 R11:00007f4dcca1aa80\r\nR12:00007f4a87c4bab3 R13:0000000000000000\r\nR14:00007f4dcb629900 R15:0000000000000000\r\nRIP:00000000005675c7 EFL:0000000000010206\r\nCSGSFS:0000000000000033\r\n15455:S 19 Oct 2023 15:30:38.117 # (00007ffef9b3982f) -> 00007f4dcb629900\r\n15455:S 19 Oct 2023 15:30:38.117 # (00007ffef9b3982e) -> 00000000008fa740\r\n15455:S 19 Oct 2023 15:30:38.117 # (00007ffef9b3982d) -> 00000000004c4a73\r\n15455:S 19 Oct 2023 15:30:38.117 # (00007ffef9b3982c) -> 000000000ba90000\r\n15455:S 19 Oct 2023 15:30:38.117 # (00007ffef9b3982b) -> 00007f4dcb629900\r\n15455:S 19 Oct 2023 15:30:38.117 # (00007ffef9b3982a) -> 0000000000000000\r\n15455:S 19 Oct 2023 15:30:38.117 # (00007ffef9b39829) -> 0000000001752000\r\n15455:S 19 Oct 2023 15:30:38.117 # (00007ffef9b39828) -> 00007f4dcb62f2a0\r\n15455:S 19 Oct 2023 15:30:38.117 # (00007ffef9b39827) -> ffffffffff752000\r\n15455:S 19 Oct 2023 15:30:38.117 # (00007ffef9b39826) -> 00007f4dcb629900\r\n15455:S 19 Oct 2023 15:30:38.117 # (00007ffef9b39825) -> 00000000004588b0\r\n15455:S 19 Oct 2023 15:30:38.117 # (00007ffef9b39824) -> 0000000000000000\r\n15455:S 19 Oct 2023 15:30:38.117 # (00007ffef9b39823) -> 00000000005681d0\r\n15455:S 19 Oct 2023 15:30:38.117 # (00007ffef9b39822) -> 0000000000000000\r\n15455:S 19 Oct 2023 15:30:38.117 # (00007ffef9b39821) -> 00007f4b55490000\r\n15455:S 19 Oct 2023 15:30:38.117 # (00007ffef9b39820) -> 00007f4dc4c0b2a0\r\n\r\n------ INFO OUTPUT ------\r\n# Server\r\nredis_version:7.2.1\r\nredis_git_sha1:00000000\r\nredis_git_dirty:0\r\nredis_build_id:c93627bd17bcfee\r\nredis_mode:cluster\r\nos:Linux 3.10.0-693.el7.x86_64 x86_64\r\narch_bits:64\r\nmonotonic_clock:POSIX clock_gettime\r\nmultiplexing_api:epoll\r\natomicvar_api:atomic-builtin\r\ngcc_version:4.8.5\r\nprocess_id:15455\r\nprocess_supervised:no\r\nrun_id:9cb501b162c397640a697c67d94164acd329b886\r\ntcp_port:7390\r\nserver_time_usec:1697700638109759\r\nuptime_in_seconds:1890\r\nuptime_in_days:0\r\nhz:10\r\nconfigured_hz:10\r\nlru_clock:3201822\r\nexecutable:/usr/local/redis-7.2.1/redis-server\r\nconfig_file:/opt/data/redis-7.2.1/7390/redis.conf\r\nio_threads_active:0\r\nlistener0:name=tcp,bind=*,bind=-::*,port=7390\r\n\r\n# Clients\r\nconnected_clients:3\r\ncluster_connections:10\r\nmaxclients:20000\r\nclient_recent_max_input_buffer:20480\r\nclient_recent_max_output_buffer:0\r\nblocked_clients:0\r\ntracking_clients:0\r\nclients_in_timeout_table:0\r\ntotal_blocking_keys:0\r\ntotal_blocking_keys_on_nokey:0\r\n\r\n# Memory\r\nused_memory:7107782656\r\nused_memory_human:6.62G\r\nused_memory_rss:12982214656\r\nused_memory_rss_human:12.09G\r\nused_memory_peak:12867371624\r\nused_memory_peak_human:11.98G\r\nused_memory_peak_perc:55.24%\r\nused_memory_overhead:2094369872\r\nused_memory_startup:2137288\r\nused_memory_dataset:5013412784\r\nused_memory_dataset_perc:70.56%\r\nallocator_allocated:7107895992\r\nallocator_active:12867813376\r\nallocator_resident:12991401984\r\ntotal_system_memory:33448808448\r\ntotal_system_memory_human:31.15G\r\nused_memory_lua:31744\r\nused_memory_vm_eval:31744\r\nused_memory_lua_human:31.00K\r\nused_memory_scripts_eval:0\r\nnumber_of_cached_scripts:0\r\nnumber_of_functions:0\r\nnumber_of_libraries:0\r\nused_memory_vm_functions:32768\r\nused_memory_vm_total:64512\r\nused_memory_vm_total_human:63.00K\r\nused_memory_functions:184\r\nused_memory_scripts:184\r\nused_memory_scripts_human:184B\r\nmaxmemory:21474836480\r\nmaxmemory_human:20.00G\r\nmaxmemory_policy:allkeys-lfu\r\nallocator_frag_ratio:1.81\r\nallocator_frag_bytes:5759917384\r\nallocator_rss_ratio:1.01\r\nallocator_rss_bytes:123588608\r\nrss_overhead_ratio:1.00\r\nrss_overhead_bytes:-9187328\r\nmem_fragmentation_ratio:1.83\r\nmem_fragmentation_bytes:5874433904\r\nmem_not_counted_for_evict:0\r\nmem_replication_backlog:1077129528\r\nmem_total_replication_buffers:1077116128\r\nmem_clients_slaves:0\r\nmem_clients_normal:26272\r\nmem_cluster_links:10720\r\nmem_aof_buffer:0\r\nmem_allocator:jemalloc-5.3.0\r\nactive_defrag_running:19\r\nlazyfree_pending_objects:0\r\nlazyfreed_objects:0\r\n\r\n# Persistence\r\nloading:0\r\nasync_loading:0\r\ncurrent_cow_peak:0\r\ncurrent_cow_size:0\r\ncurrent_cow_size_age:0\r\ncurrent_fork_perc:0.00\r\ncurrent_save_keys_processed:0\r\ncurrent_save_keys_total:0\r\nrdb_changes_since_last_save:39997271\r\nrdb_bgsave_in_progress:0\r\nrdb_last_save_time:1697698748\r\nrdb_last_bgsave_status:ok\r\nrdb_last_bgsave_time_sec:-1\r\nrdb_current_bgsave_time_sec:-1\r\nrdb_saves:0\r\nrdb_last_cow_size:0\r\nrdb_last_load_keys_expired:0\r\nrdb_last_load_keys_loaded:0\r\naof_enabled:0\r\naof_rewrite_in_progress:0\r\naof_rewrite_scheduled:0\r\naof_last_rewrite_time_sec:-1\r\naof_current_rewrite_time_sec:-1\r\naof_last_bgrewrite_status:ok\r\naof_rewrites:0\r\naof_rewrites_consecutive_failures:0\r\naof_last_write_status:ok\r\naof_last_cow_size:0\r\nmodule_fork_in_progress:0\r\nmodule_fork_last_cow_size:0\r\n\r\n# Stats\r\ntotal_connections_received:352\r\ntotal_commands_processed:39997784\r\ninstantaneous_ops_per_sec:0\r\ntotal_net_input_bytes:9913814462\r\ntotal_net_output_bytes:559718\r\ntotal_net_repl_input_bytes:9913805445\r\ntotal_net_repl_output_bytes:0\r\ninstantaneous_input_kbps:0.03\r\ninstantaneous_output_kbps:0.03\r\ninstantaneous_input_repl_kbps:0.00\r\ninstantaneous_output_repl_kbps:0.00\r\nrejected_connections:0\r\nsync_full:0\r\nsync_partial_ok:0\r\nsync_partial_err:0\r\nexpired_keys:0\r\nexpired_stale_perc:0.00\r\nexpired_time_cap_reached_count:0\r\nexpire_cycle_cpu_milliseconds:0\r\nevicted_keys:0\r\nevicted_clients:0\r\ntotal_eviction_exceeded_time:0\r\ncurrent_eviction_exceeded_time:0\r\nkeyspace_hits:0\r\nkeyspace_misses:0\r\npubsub_channels:0\r\npubsub_patterns:0\r\npubsubshard_channels:0\r\nlatest_fork_usec:0\r\ntotal_forks:0\r\nmigrate_cached_sockets:0\r\nslave_expires_tracked_keys:0\r\nactive_defrag_hits:3645\r\nactive_defrag_misses:20\r\nactive_defrag_key_hits:916\r\nactive_defrag_key_misses:0\r\ntotal_active_defrag_time:0\r\ncurrent_active_defrag_time:0\r\ntracking_total_keys:0\r\ntracking_total_items:0\r\ntracking_total_prefixes:0\r\nunexpected_error_replies:0\r\ntotal_error_replies:0\r\ndump_payload_sanitizations:0\r\ntotal_reads_processed:8197323\r\ntotal_writes_processed:2205\r\nio_threaded_reads_processed:0\r\nio_threaded_writes_processed:0\r\nreply_buffer_shrinks:3\r\nreply_buffer_expands:0\r\neventloop_cycles:8210302\r\neventloop_duration_sum:166479367\r\neventloop_duration_cmd_sum:75742306\r\ninstantaneous_eventloop_cycles_per_sec:15\r\ninstantaneous_eventloop_duration_usec:69\r\nacl_access_denied_auth:0\r\nacl_access_denied_cmd:0\r\nacl_access_denied_key:0\r\nacl_access_denied_channel:0\r\n\r\n# Replication\r\nrole:slave\r\nmaster_host:\r\nmaster_port:7380\r\nmaster_link_status:up\r\nmaster_last_io_seconds_ago:4\r\nmaster_sync_in_progress:0\r\nslave_read_repl_offset:9913805187\r\nslave_repl_offset:9913805187\r\nslave_priority:100\r\nslave_read_only:1\r\nreplica_announced:1\r\nconnected_slaves:0\r\nmaster_failover_state:no-failover\r\nmaster_replid:25b3854d029679334288fa8da4a51f98abe44585\r\nmaster_replid2:0000000000000000000000000000000000000000\r\nmaster_repl_offset:9913805187\r\nsecond_repl_offset:-1\r\nrepl_backlog_active:1\r\nrepl_backlog_size:1073741824\r\nrepl_backlog_first_byte_offset:8840054721\r\nrepl_backlog_histlen:1073750467\r\n\r\n# CPU\r\nused_cpu_sys:54.278523\r\nused_cpu_user:166.850066\r\nused_cpu_sys_children:0.000000\r\nused_cpu_user_children:0.000000\r\nused_cpu_sys_main_thread:54.272030\r\nused_cpu_user_main_thread:166.848998\r\n\r\n# Modules\r\n\r\n# Commandstats\r\ncmdstat_set:calls=26664978,usec=45725409,usec_per_call=1.71,rejected_calls=0,failed_calls=0\r\ncmdstat_info:calls=189,usec=6915,usec_per_call=36.59,rejected_calls=0,failed_calls=0\r\ncmdstat_slowlog|get:calls=32,usec=178,usec_per_call=5.56,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|nodes:calls=3,usec=438,usec_per_call=146.00,rejected_calls=0,failed_calls=0\r\ncmdstat_cluster|info:calls=64,usec=3071,usec_per_call=47.98,rejected_calls=0,failed_calls=0\r\ncmdstat_dbsize:calls=31,usec=69,usec_per_call=2.23,rejected_calls=0,failed_calls=0\r\ncmdstat_ping:calls=188,usec=58,usec_per_call=0.31,rejected_calls=0,failed_calls=0\r\ncmdstat_command|docs:calls=1,usec=1122,usec_per_call=1122.00,rejected_calls=0,failed_calls=0\r\ncmdstat_del:calls=13332292,usec=30004998,usec_per_call=2.25,rejected_calls=0,failed_calls=0\r\ncmdstat_config|set:calls=3,usec=22,usec_per_call=7.33,rejected_calls=0,failed_calls=0\r\ncmdstat_config|get:calls=2,usec=26,usec_per_call=13.00,rejected_calls=0,failed_calls=0\r\ncmdstat_select:calls=1,usec=0,usec_per_call=0.00,rejected_calls=0,failed_calls=0\r\n\r\n# Errorstats\r\n\r\n# Latencystats\r\nlatency_percentiles_usec_set:p50=1.003,p99=13.055,p99.9=19.071\r\nlatency_percentiles_usec_info:p50=31.103,p99=110.079,p99.9=118.271\r\nlatency_percentiles_usec_slowlog|get:p50=4.015,p99=17.023,p99.9=17.023\r\nlatency_percentiles_usec_cluster|nodes:p50=147.455,p99=203.775,p99.9=203.775\r\nlatency_percentiles_usec_cluster|info:p50=43.007,p99=102.399,p99.9=161.791\r\nlatency_percentiles_usec_dbsize:p50=1.003,p99=14.015,p99.9=14.015\r\nlatency_percentiles_usec_ping:p50=0.001,p99=1.003,p99.9=2.007\r\nlatency_percentiles_usec_command|docs:p50=1122.303,p99=1122.303,p99.9=1122.303\r\nlatency_percentiles_usec_del:p50=2.007,p99=15.039,p99.9=20.095\r\nlatency_percentiles_usec_config|set:p50=5.023,p99=14.015,p99.9=14.015\r\nlatency_percentiles_usec_config|get:p50=5.023,p99=21.119,p99.9=21.119\r\nlatency_percentiles_usec_select:p50=0.001,p99=0.001,p99.9=0.001\r\n\r\n# Cluster\r\ncluster_enabled:1\r\n\r\n# Keyspace\r\ndb0:keys=13332686,expires=0,avg_ttl=0\r\n\r\n# Cluster info\r\ncluster_state:ok\r\ncluster_slots_assigned:16384\r\ncluster_slots_ok:16384\r\ncluster_slots_pfail:0\r\ncluster_slots_fail:0\r\ncluster_known_nodes:6\r\ncluster_size:3\r\ncluster_current_epoch:6\r\ncluster_my_epoch:3\r\ncluster_stats_messages_ping_sent:1887\r\ncluster_stats_messages_pong_sent:1788\r\ncluster_stats_messages_sent:3675\r\ncluster_stats_messages_ping_received:1788\r\ncluster_stats_messages_pong_received:1887\r\ncluster_stats_messages_received:3675\r\ntotal_cluster_links_buffer_limit_exceeded:0\r\n\r\n------ MODULES INFO OUTPUT ------\r\n\r\n------ CONFIG DEBUG OUTPUT ------\r\nsanitize-dump-payload no\r\nreplica-read-only yes\r\nlazyfree-lazy-eviction yes\r\nlist-compress-depth 0\r\nlazyfree-lazy-server-del yes\r\nslave-read-only yes\r\nio-threads-do-reads no\r\nrepl-diskless-load on-empty-db\r\nlazyfree-lazy-user-flush yes\r\nrepl-diskless-sync yes\r\nactivedefrag yes\r\nlazyfree-lazy-user-del yes\r\nproto-max-bulk-len 512mb\r\nlazyfree-lazy-expire yes\r\nclient-query-buffer-limit 1gb\r\nio-threads 1\r\n\r\n------ FAST MEMORY TEST ------\r\n15455:S 19 Oct 2023 15:30:38.118 # Bio worker thread #0 terminated\r\n15455:S 19 Oct 2023 15:30:38.118 # Bio worker thread #1 terminated\r\n15455:S 19 Oct 2023 15:30:38.118 # Bio worker thread #2 terminated\r\n*** Preparing to test memory region 8dd000 (2273280 bytes)\r\n*** Preparing to test memory region 10fe000 (135168 bytes)\r\n*** Preparing to test memory region 7f4a56600000 (14191427584 bytes)\r\n*** Preparing to test memory region 7f4da4500000 (397545472 bytes)\r\n*** Preparing to test memory region 7f4dc017f000 (19398656 bytes)\r\n*** Preparing to test memory region 7f4dc1c00000 (2097152 bytes)\r\n*** Preparing to test memory region 7f4dc2800000 (6291456 bytes)\r\n*** Preparing to test memory region 7f4dc2f55000 (8388608 bytes)\r\n*** Preparing to test memory region 7f4dc3756000 (8388608 bytes)\r\n*** Preparing to test memory region 7f4dc3f57000 (8388608 bytes)\r\n*** Preparing to test memory region 7f4dc4757000 (5767168 bytes)\r\n*** Preparing to test memory region 7f4dcb200000 (8388608 bytes)\r\n*** Preparing to test memory region 7f4dcbeda000 (20480 bytes)\r\n*** Preparing to test memory region 7f4dcc0f7000 (16384 bytes)\r\n*** Preparing to test memory region 7f4dcca1a000 (16384 bytes)\r\n*** Preparing to test memory region 7f4dcca28000 (4096 bytes)\r\n*** Preparing to test memory region 7f4dcca29000 (4096 bytes)\r\n*** Preparing to test memory region 7f4dcca2c000 (4096 bytes)\r\n.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O.O\r\nFast memory test PASSED, however your memory can still be broken. Please run a memory test for several hours if possible.\r\n\r\n------ DUMPING CODE AROUND EIP ------\r\nSymbol: dbDictAfterReplaceEntry (base: 0x567550)\r\nModule: /usr/local/redis-7.2.1/redis-server *:7390 [cluster] (base 0x400000)\r\n$ xxd -r -p /tmp/dump.hex /tmp/dump.bin\r\n$ objdump --adjust-vma=0x567550 -D -b binary -m i386:x86-64 /tmp/dump.bin\r\n------\r\n15455:S 19 Oct 2023 15:31:16.230 # dump of function (hexdump of 247 bytes):\r\n8b05423d390085c07506c30f1f440000415440f6c60755534889f37565488b56204889fd488b46184885d27409f6c2077550488972184885c07415a8077543488958205b5d415cc30f1f8400000000004889dfe848f6ffff4989c44889c7e8fd5eefff4c89e789c6e8e359f0ff488b553889c048c1e004488b525048895c0208ebc1e898c5edff660f1f840000000000415641554154554889d55348837f28ff4889fb0f95c00fb6d00fb6c04c8d24c7498b4424084839c50f82cd0000000fb64c173231d24989f580f9ff7409b20848d3e24883ea084801d04839c50f87a9000000488b13488b42404885c00f84c10000004889dfffd0\r\n\r\n=== REDIS BUG REPORT END. Make sure to include from START to END. ===\r\n```\r\n\r\n**Additional information**\r\n\r\n1. on cluster mode, I don't know if normal replica node will crash.\r\n\r\nSteps to reproduce:\r\n1. set activedefrag to no\r\n2. set keys then del keys to produce memory defragmentation\r\n3. set activedefrag to yes on replica\r\n4. lead to a crash\r\n",
  "state": "open",
  "created_at": "2023-10-19T09:34:32Z",
  "updated_at": "2023-10-25T02:46:22Z",
  "closed_at": null,
  "labels": [],
  "comments_data": [
    {
      "id": 1771802331,
      "user": "hpatro",
      "created_at": "2023-10-19T22:42:26Z",
      "body": "@Nanciico Do you have information on `allocator_fragmentation_ratio` before you enabled `activedefrag`? Did you generate random data via `DEBUG POPULATE` or loaded some specific data to reach this condition?"
    },
    {
      "id": 1771865262,
      "user": "hpatro",
      "created_at": "2023-10-20T00:04:40Z",
      "body": "This issue seems to be applicable only to cluster-enabled setup as it crashes while updating the linked list for maintaining slot to key information.\r\n\r\nI had written some tests for #11695 to test defrag on cluster mode. It successfully passes on 7.2.1 https://github.com/redis/redis/commit/6ff263b10d0399307e508d3b0564a1dd91bc30f0. Note: It only runs on the primary though."
    },
    {
      "id": 1771977459,
      "user": "Nanciico",
      "created_at": "2023-10-20T02:27:14Z",
      "body": "I do not generate keys via `DEBUG POPULATE`.\r\nI just set string type to the cluster, and randomly delete them to make memory defragmentation.\r\n\r\n`memory_fragmentation_ratio = 1.83`\r\n\r\nI thought it will just happen in cluster mode, but I do not know why it just happens on replica."
    },
    {
      "id": 1773437959,
      "user": "madolson",
      "created_at": "2023-10-20T21:51:34Z",
      "body": "@zuiderkwast FYI, might be related to some of the changes we made in 7.0 for the linked list for key->slot, given that it's only on cluster mode."
    },
    {
      "id": 1773903584,
      "user": "zuiderkwast",
      "created_at": "2023-10-21T19:35:30Z",
      "body": "@madolson I'm not sure what you're refering to. The slot->keys api using dict entry metadata which is now gone?"
    },
    {
      "id": 1773904426,
      "user": "hpatro",
      "created_at": "2023-10-21T19:38:58Z",
      "body": "@zuiderkwast I think this slot to keys DLL was introduced in 7.0. The crash seems to be happening while updating the DLL. "
    },
    {
      "id": 1776024148,
      "user": "hpatro",
      "created_at": "2023-10-23T21:08:15Z",
      "body": "@Nanciico I tried to follow the steps and reproduce the scenario. However, couldn't make the server crash. Would you be able to share the set of command/data to load to reproduce this?"
    },
    {
      "id": 1777114621,
      "user": "Nanciico",
      "created_at": "2023-10-24T12:31:25Z",
      "body": "@hpatro \r\n1. Set activedefrag to no for all the nodes，include master and replica.\r\n3. Just set string type up to used memory is 7G，key: 26bytes, value: 256 bytes.\r\n4. Delete key every other key. \r\n\r\n```\r\nkey1 key2 key3 key4 key5\r\nkey1      key3      key5\r\n```\r\n\r\n5. set activedefrag to yes on replica\r\n6. crash.\r\n\r\n\r\nIt just use set and del command."
    },
    {
      "id": 1777117228,
      "user": "Nanciico",
      "created_at": "2023-10-24T12:33:15Z",
      "body": "while set activedefrag to yes on replica, the replica do not response to `cluster nodes` command, and the cpu rise to 100%."
    },
    {
      "id": 1778415899,
      "user": "hpatro",
      "created_at": "2023-10-25T02:46:22Z",
      "body": "@Nanciico I tried the above suggestion still haven't been able to reproduce locally. Are you able to reproduce this consistently? If yes, please provide the exact set of commands to help debug this further. "
    }
  ]
}