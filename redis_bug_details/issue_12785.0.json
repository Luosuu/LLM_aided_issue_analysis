{
  "issue_number": 12785.0,
  "title": "[CRASH] Redis crashing on 7.2.3 version",
  "body": "Notice!\r\n- If a Redis module was involved, please open an issue in the module's repo instead!\r\n- If you're using docker on Apple M1, please make sure the image you're using was compiled for ARM!\r\n\r\n```\r\n**Crash report**\r\n\r\nPaste the complete crash log between the quotes below. Please include a few lines from the log preceding the crash report to provide some context.\r\n\r\n537283:M 20 Nov 2023 12:31:18.005 * Background saving started by pid 741186\r\n741186:C 20 Nov 2023 12:31:32.883 * DB saved on disk\r\n741186:C 20 Nov 2023 12:31:33.041 * Fork CoW for RDB: current 89 MB, peak 89 MB, average 59 MB\r\n537283:M 20 Nov 2023 12:31:33.273 * Background saving terminated with success\r\n537283:M 20 Nov 2023 12:33:34.073 * 10000 changes in 120 seconds. Saving...\r\n537283:M 20 Nov 2023 12:33:34.340 * Background saving started by pid 743130\r\n743130:C 20 Nov 2023 12:33:49.702 * DB saved on disk\r\n743130:C 20 Nov 2023 12:33:49.845 * Fork CoW for RDB: current 77 MB, peak 77 MB, average 47 MB\r\n537283:M 20 Nov 2023 12:33:50.101 * Background saving terminated with success\r\n537283:M 20 Nov 2023 12:35:51.097 * 10000 changes in 120 seconds. Saving...\r\n537283:M 20 Nov 2023 12:35:51.338 * Background saving started by pid 744548\r\n744548:C 20 Nov 2023 12:36:05.883 * DB saved on disk\r\n744548:C 20 Nov 2023 12:36:06.047 * Fork CoW for RDB: current 107 MB, peak 107 MB, average 59 MB\r\n537283:M 20 Nov 2023 12:36:06.381 * Background saving terminated with success\r\n\r\n=== REDIS BUG REPORT START: Cut & paste starting from here ===\r\n537283:M 20 Nov 2023 12:36:42.435 # === ASSERTION FAILED ===\r\n537283:M 20 Nov 2023 12:36:42.435 # ==> networking.c:1480 'io_threads_op == IO_THREADS_OP_IDLE' is not true\r\n\r\n------ STACK TRACE ------\r\n\r\nBacktrace:\r\n/home/redis/redis-work/redis_sentinel_prod/redis-6.0.10/src/redis-server 0.0.0.0:6379(unlinkClient+0x221)[0x542dd1]\r\n/home/redis/redis-work/redis_sentinel_prod/redis-6.0.10/src/redis-server 0.0.0.0:6379(freeClient+0x263)[0x543053]\r\n/home/redis/redis-work/redis_sentinel_prod/redis-6.0.10/src/redis-server 0.0.0.0:6379(RM_FreeThreadSafeContext+0x9)[0x497cf9]\r\n/home/redis/redis-work/redis_sentinel_prod/data/modules/redisearch.so(+0x157142)[0x7fffe8eec142]\r\n/lib64/libpthread.so.0(+0x81ca)[0x7ffff74281ca]\r\n/lib64/libc.so.6(clone+0x43)[0x7ffff7094e73]\r\n\r\n------ INFO OUTPUT ------\r\n# Server\r\nredis_version:7.2.3\r\nredis_git_sha1:00000000\r\nredis_git_dirty:0\r\nredis_build_id:b77510c9559a2463\r\nredis_mode:standalone\r\nos:Linux 5.15.128-1 x86_64\r\narch_bits:64\r\nmonotonic_clock:POSIX clock_gettime\r\nmultiplexing_api:epoll\r\natomicvar_api:c11-builtin\r\ngcc_version:8.5.0\r\nprocess_id:537283\r\nprocess_supervised:no\r\nrun_id:1dd4d12c8e661a13c31ee400fb9548b8b071bb15\r\ntcp_port:6379\r\nserver_time_usec:1700483802438266\r\nuptime_in_seconds:18688\r\nuptime_in_days:0\r\nhz:10\r\nconfigured_hz:10\r\nlru_clock:5984986\r\nexecutable:/home/redis/redis-work/redis_sentinel_prod/redis-6.0.10/src/redis-server\r\nconfig_file:/home/redis/redis-work/redis_sentinel_prod/data/redis.conf\r\nio_threads_active:0\r\nlistener0:name=tcp,bind=0.0.0.0,port=6379\r\n\r\n# Clients\r\nconnected_clients:1271\r\ncluster_connections:0\r\nmaxclients:10000\r\nclient_recent_max_input_buffer:20480\r\nclient_recent_max_output_buffer:20504\r\nblocked_clients:0\r\ntracking_clients:0\r\nclients_in_timeout_table:0\r\ntotal_blocking_keys:0\r\ntotal_blocking_keys_on_nokey:0\r\n\r\n# Memory\r\nused_memory:8048754928\r\nused_memory_human:7.50G\r\nused_memory_rss:8188547072\r\nused_memory_rss_human:7.63G\r\nused_memory_peak:8069676160\r\nused_memory_peak_human:7.52G\r\nused_memory_peak_perc:99.74%\r\nused_memory_overhead:799139696\r\nused_memory_startup:1013888\r\nused_memory_dataset:7249615232\r\nused_memory_dataset_perc:90.08%\r\nallocator_allocated:8050302064\r\nallocator_active:8062902272\r\nallocator_resident:8210100224\r\ntotal_system_memory:675916099584\r\ntotal_system_memory_human:629.50G\r\nused_memory_lua:31744\r\nused_memory_vm_eval:31744\r\nused_memory_lua_human:31.00K\r\nused_memory_scripts_eval:0\r\nnumber_of_cached_scripts:0\r\nnumber_of_functions:0\r\nnumber_of_libraries:0\r\nused_memory_vm_functions:32768\r\nused_memory_vm_total:64512\r\nused_memory_vm_total_human:63.00K\r\nused_memory_functions:184\r\nused_memory_scripts:184\r\nused_memory_scripts_human:184B\r\nmaxmemory:0\r\nmaxmemory_human:0B\r\nmaxmemory_policy:noeviction\r\nallocator_frag_ratio:1.00\r\nallocator_frag_bytes:12600208\r\nallocator_rss_ratio:1.02\r\nallocator_rss_bytes:147197952\r\nrss_overhead_ratio:1.00\r\nrss_overhead_bytes:-21553152\r\nmem_fragmentation_ratio:1.02\r\nmem_fragmentation_bytes:139794344\r\nmem_not_counted_for_evict:0\r\nmem_replication_backlog:536877608\r\nmem_total_replication_buffers:538578568\r\nmem_clients_slaves:1707656\r\nmem_clients_normal:5342864\r\nmem_cluster_links:0\r\nmem_aof_buffer:0\r\nmem_allocator:jemalloc-5.3.0\r\nactive_defrag_running:0\r\nlazyfree_pending_objects:0\r\nlazyfreed_objects:38832\r\n\r\n# Persistence\r\nloading:0\r\nasync_loading:0\r\ncurrent_cow_peak:0\r\ncurrent_cow_size:0\r\ncurrent_cow_size_age:0\r\ncurrent_fork_perc:0.00\r\ncurrent_save_keys_processed:0\r\ncurrent_save_keys_total:0\r\nrdb_changes_since_last_save:594639\r\nrdb_bgsave_in_progress:0\r\nrdb_last_save_time:1700483766\r\nrdb_last_bgsave_status:ok\r\nrdb_last_bgsave_time_sec:15\r\nrdb_current_bgsave_time_sec:-1\r\nrdb_saves:132\r\nrdb_last_cow_size:112558080\r\nrdb_last_load_keys_expired:0\r\nrdb_last_load_keys_loaded:4528642\r\naof_enabled:0\r\naof_rewrite_in_progress:0\r\naof_rewrite_scheduled:0\r\naof_last_rewrite_time_sec:-1\r\naof_current_rewrite_time_sec:-1\r\naof_last_bgrewrite_status:ok\r\naof_rewrites:0\r\naof_rewrites_consecutive_failures:0\r\naof_last_write_status:ok\r\naof_last_cow_size:0\r\nmodule_fork_in_progress:0\r\nmodule_fork_last_cow_size:0\r\n\r\n# Stats\r\ntotal_connections_received:110338\r\ntotal_commands_processed:208696312\r\ninstantaneous_ops_per_sec:16425\r\ntotal_net_input_bytes:25081856113\r\ntotal_net_output_bytes:70074403043\r\ntotal_net_repl_input_bytes:10317696255\r\ntotal_net_repl_output_bytes:68010887862\r\ninstantaneous_input_kbps:1483.00\r\ninstantaneous_output_kbps:5972.07\r\ninstantaneous_input_repl_kbps:0.00\r\ninstantaneous_output_repl_kbps:5792.23\r\nrejected_connections:0\r\nsync_full:7\r\nsync_partial_ok:4\r\nsync_partial_err:7\r\nexpired_keys:358322\r\nexpired_stale_perc:8.37\r\nexpired_time_cap_reached_count:0\r\nexpire_cycle_cpu_milliseconds:9590\r\nevicted_keys:0\r\nevicted_clients:0\r\ntotal_eviction_exceeded_time:0\r\ncurrent_eviction_exceeded_time:0\r\nkeyspace_hits:40088603\r\nkeyspace_misses:3758315\r\npubsub_channels:1\r\npubsub_patterns:47\r\npubsubshard_channels:0\r\nlatest_fork_usec:241026\r\ntotal_forks:147\r\nmigrate_cached_sockets:0\r\nslave_expires_tracked_keys:0\r\nactive_defrag_hits:0\r\nactive_defrag_misses:0\r\nactive_defrag_key_hits:0\r\nactive_defrag_key_misses:0\r\ntotal_active_defrag_time:0\r\ncurrent_active_defrag_time:0\r\ntracking_total_keys:0\r\ntracking_total_items:0\r\ntracking_total_prefixes:0\r\nunexpected_error_replies:0\r\ntotal_error_replies:34396\r\ndump_payload_sanitizations:0\r\ntotal_reads_processed:22124514\r\ntotal_writes_processed:61381759\r\nio_threaded_reads_processed:0\r\nio_threaded_writes_processed:7858076\r\nreply_buffer_shrinks:119429\r\nreply_buffer_expands:11937\r\neventloop_cycles:19983213\r\neventloop_duration_sum:2739313376\r\neventloop_duration_cmd_sum:1313002872\r\ninstantaneous_eventloop_cycles_per_sec:1278\r\ninstantaneous_eventloop_duration_usec:115\r\nacl_access_denied_auth:2\r\nacl_access_denied_cmd:0\r\nacl_access_denied_key:0\r\nacl_access_denied_channel:0\r\n\r\n# Replication\r\nrole:master\r\nconnected_slaves:4\r\nslave0:ip=x.x.x.158,port=6379,state=online,offset=30411739454607,lag=1\r\nslave1:ip=x.x.x.154,port=6379,state=online,offset=30411739711643,lag=1\r\nslave2:ip=x.x.x.157,port=6379,state=online,offset=30411739702090,lag=1\r\nslave3:ip=x.x.x.155,port=6379,state=online,offset=30411741371519,lag=0\r\nmaster_failover_state:no-failover\r\nmaster_replid:bf0f70b9a1592a3516fe5cfe52bcb2c29ab85841\r\nmaster_replid2:3b185c49ed5edf6f3be0e605578303dd4384ae9f\r\nmaster_repl_offset:30411741418999\r\nsecond_repl_offset:30399789302774\r\nrepl_backlog_active:1\r\nrepl_backlog_size:536870912\r\nrepl_backlog_first_byte_offset:30411204535235\r\nrepl_backlog_histlen:536883765\r\n\r\n# CPU\r\nused_cpu_sys:722.041706\r\nused_cpu_user:5928.805942\r\nused_cpu_sys_children:483.488326\r\nused_cpu_user_children:1503.562858\r\nused_cpu_sys_main_thread:2.313608\r\nused_cpu_user_main_thread:0.598206\r\n\r\n# Modules\r\nmodule:name=ReJSON,ver=999999,api=1,filters=0,usedby=[search],using=[],options=[handle-io-errors]\r\nmodule:name=search,ver=999999,api=1,filters=0,usedby=[],using=[ReJSON],options=[]\r\n\r\n# Commandstats\r\ncmdstat_command:calls=0,usec=0,usec_per_call=0.00,rejected_calls=6,failed_calls=0\r\ncmdstat_command|docs:calls=0,usec=0,usec_per_call=0.00,rejected_calls=6,failed_calls=0\r\ncmdstat_expireat:calls=34960342,usec=27372525,usec_per_call=0.78,rejected_calls=0,failed_calls=0\r\ncmdstat_sadd:calls=15,usec=366,usec_per_call=24.40,rejected_calls=0,failed_calls=0\r\ncmdstat_exists:calls=70150,usec=69602,usec_per_call=0.99,rejected_calls=0,failed_calls=0\r\ncmdstat_monitor:calls=1,usec=0,usec_per_call=0.00,rejected_calls=0,failed_calls=0\r\ncmdstat_exec:calls=450,usec=13616,usec_per_call=30.26,rejected_calls=0,failed_calls=442\r\ncmdstat_multi:calls=12809094,usec=715650,usec_per_call=0.06,rejected_calls=0,failed_calls=0\r\ncmdstat_slaveof:calls=8,usec=816,usec_per_call=102.00,rejected_calls=442,failed_calls=0\r\ncmdstat_del:calls=150020,usec=2509648,usec_per_call=16.73,rejected_calls=0,failed_calls=0\r\ncmdstat_info:calls=27955,usec=1770259,usec_per_call=63.33,rejected_calls=2,failed_calls=0\r\ncmdstat_module|list:calls=3,usec=10,usec_per_call=3.33,rejected_calls=0,failed_calls=0\r\ncmdstat_FT._LIST:calls=3,usec=250,usec_per_call=83.33,rejected_calls=0,failed_calls=0\r\ncmdstat_hmget:calls=311395,usec=1296654,usec_per_call=4.16,rejected_calls=0,failed_calls=0\r\ncmdstat_ping:calls=496669,usec=472628,usec_per_call=0.95,rejected_calls=4373,failed_calls=0\r\ncmdstat_config|rewrite:calls=8,usec=9455,usec_per_call=1181.88,rejected_calls=0,failed_calls=0\r\ncmdstat_config|get:calls=1,usec=9,usec_per_call=9.00,rejected_calls=0,failed_calls=0\r\ncmdstat_hincrby:calls=35708499,usec=89126566,usec_per_call=2.50,rejected_calls=0,failed_calls=0\r\ncmdstat_publish:calls=93599,usec=735862,usec_per_call=7.86,rejected_calls=0,failed_calls=0\r\ncmdstat_json.del:calls=3,usec=286,usec_per_call=95.33,rejected_calls=0,failed_calls=0\r\ncmdstat_pexpireat:calls=12612584,usec=7362243,usec_per_call=0.58,rejected_calls=0,failed_calls=0\r\ncmdstat_acl|users:calls=2,usec=7,usec_per_call=3.50,rejected_calls=0,failed_calls=0\r\ncmdstat_acl|getuser:calls=6,usec=1818,usec_per_call=303.00,rejected_calls=0,failed_calls=0\r\ncmdstat_acl|list:calls=1,usec=9,usec_per_call=9.00,rejected_calls=0,failed_calls=0\r\ncmdstat_acl|setuser:calls=4,usec=621,usec_per_call=155.25,rejected_calls=0,failed_calls=0\r\ncmdstat_hmset:calls=47559,usec=7206050,usec_per_call=151.52,rejected_calls=0,failed_calls=0\r\ncmdstat_psync:calls=61,usec=1152,usec_per_call=18.89,rejected_calls=0,failed_calls=50\r\ncmdstat_hset:calls=53372583,usec=39902160,usec_per_call=0.75,rejected_calls=0,failed_calls=0\r\ncmdstat_zrevrange:calls=3199,usec=16668,usec_per_call=5.21,rejected_calls=0,failed_calls=0\r\ncmdstat_select:calls=312,usec=701,usec_per_call=2.25,rejected_calls=0,failed_calls=0\r\ncmdstat_discard:calls=12808644,usec=1097151,usec_per_call=0.09,rejected_calls=0,failed_calls=0\r\ncmdstat_latency:calls=0,usec=0,usec_per_call=0.00,rejected_calls=1,failed_calls=0\r\ncmdstat_latency|doctor:calls=1,usec=204278,usec_per_call=204278.00,rejected_calls=0,failed_calls=0\r\ncmdstat_latency|latest:calls=1,usec=1,usec_per_call=1.00,rejected_calls=0,failed_calls=0\r\ncmdstat_latency|help:calls=1,usec=10,usec_per_call=10.00,rejected_calls=1,failed_calls=0\r\ncmdstat_json.set:calls=1963,usec=379191,usec_per_call=193.17,rejected_calls=0,failed_calls=0\r\ncmdstat_expire:calls=29,usec=155,usec_per_call=5.34,rejected_calls=0,failed_calls=0\r\ncmdstat_set:calls=364098,usec=533075,usec_per_call=1.46,rejected_calls=2664,failed_calls=0\r\ncmdstat_FT.INFO:calls=135,usec=12141,usec_per_call=89.93,rejected_calls=0,failed_calls=0\r\ncmdstat_keys:calls=782,usec=1008826325,usec_per_call=1290059.25,rejected_calls=0,failed_calls=0\r\ncmdstat_hincrbyfloat:calls=40430858,usec=108784200,usec_per_call=2.69,rejected_calls=0,failed_calls=0\r\ncmdstat_auth:calls=109822,usec=275117,usec_per_call=2.51,rejected_calls=0,failed_calls=2\r\ncmdstat_json.get:calls=525330,usec=6220199,usec_per_call=11.84,rejected_calls=0,failed_calls=842\r\ncmdstat_client|setinfo:calls=338,usec=687,usec_per_call=2.03,rejected_calls=0,failed_calls=0\r\ncmdstat_client|setname:calls=240,usec=235,usec_per_call=0.98,rejected_calls=0,failed_calls=0\r\ncmdstat_client|id:calls=1,usec=1,usec_per_call=1.00,rejected_calls=0,failed_calls=0\r\ncmdstat_client|kill:calls=16,usec=2909,usec_per_call=181.81,rejected_calls=0,failed_calls=0\r\ncmdstat_quit:calls=98361,usec=135376,usec_per_call=1.38,rejected_calls=0,failed_calls=0\r\ncmdstat_subscribe:calls=90,usec=135,usec_per_call=1.50,rejected_calls=0,failed_calls=0\r\ncmdstat_replconf:calls=47434,usec=45528,usec_per_call=0.96,rejected_calls=0,failed_calls=0\r\ncmdstat_psubscribe:calls=7234,usec=26484,usec_per_call=3.66,rejected_calls=0,failed_calls=0\r\ncmdstat_FT.SEARCH:calls=77,usec=6867385,usec_per_call=89186.82,rejected_calls=0,failed_calls=0\r\ncmdstat_get:calls=3636331,usec=1019870,usec_per_call=0.28,rejected_calls=25563,failed_calls=0\r\n\r\n# Errorstats\r\nerrorstat_ERR:count=846\r\nerrorstat_EXECABORT:count=442\r\nerrorstat_LOADING:count=4752\r\nerrorstat_NOAUTH:count=28304\r\nerrorstat_NOMASTERLINK:count=50\r\nerrorstat_WRONGPASS:count=2\r\n\r\n# Latencystats\r\nlatency_percentiles_usec_expireat:p50=1.003,p99=2.007,p99.9=9.023\r\nlatency_percentiles_usec_sadd:p50=9.023,p99=209.919,p99.9=209.919\r\nlatency_percentiles_usec_exists:p50=0.001,p99=10.047,p99.9=41.215\r\nlatency_percentiles_usec_monitor:p50=0.001,p99=0.001,p99.9=0.001\r\nlatency_percentiles_usec_exec:p50=0.001,p99=856.063,p99.9=4521.983\r\nlatency_percentiles_usec_multi:p50=0.001,p99=1.003,p99.9=1.003\r\nlatency_percentiles_usec_slaveof:p50=59.135,p99=238.591,p99.9=238.591\r\nlatency_percentiles_usec_del:p50=10.047,p99=138.239,p99.9=405.503\r\nlatency_percentiles_usec_info:p50=34.047,p99=477.183,p99.9=1261.567\r\nlatency_percentiles_usec_module|list:p50=3.007,p99=4.015,p99.9=4.015\r\nlatency_percentiles_usec_FT._LIST:p50=91.135,p99=110.079,p99.9=110.079\r\nlatency_percentiles_usec_hmget:p50=3.007,p99=19.071,p99.9=74.239\r\nlatency_percentiles_usec_ping:p50=1.003,p99=8.031,p99.9=38.143\r\nlatency_percentiles_usec_config|rewrite:p50=675.839,p99=3014.655,p99.9=3014.655\r\nlatency_percentiles_usec_config|get:p50=9.023,p99=9.023,p99.9=9.023\r\nlatency_percentiles_usec_hincrby:p50=2.007,p99=15.039,p99.9=62.207\r\nlatency_percentiles_usec_publish:p50=2.007,p99=78.335,p99.9=321.535\r\nlatency_percentiles_usec_json.del:p50=112.127,p99=112.127,p99.9=112.127\r\nlatency_percentiles_usec_pexpireat:p50=1.003,p99=2.007,p99.9=7.007\r\nlatency_percentiles_usec_acl|users:p50=2.007,p99=5.023,p99.9=5.023\r\nlatency_percentiles_usec_acl|getuser:p50=107.007,p99=884.735,p99.9=884.735\r\nlatency_percentiles_usec_acl|list:p50=9.023,p99=9.023,p99.9=9.023\r\nlatency_percentiles_usec_acl|setuser:p50=126.463,p99=317.439,p99.9=317.439\r\nlatency_percentiles_usec_hmset:p50=116.223,p99=643.071,p99.9=1409.023\r\nlatency_percentiles_usec_psync:p50=6.015,p99=102.399,p99.9=112.127\r\nlatency_percentiles_usec_hset:p50=1.003,p99=3.007,p99.9=14.015\r\nlatency_percentiles_usec_zrevrange:p50=3.007,p99=45.055,p99.9=184.319\r\nlatency_percentiles_usec_select:p50=2.007,p99=15.039,p99.9=22.015\r\nlatency_percentiles_usec_discard:p50=0.001,p99=1.003,p99.9=3.007\r\nlatency_percentiles_usec_latency|doctor:p50=204472.319,p99=204472.319,p99.9=204472.319\r\nlatency_percentiles_usec_latency|latest:p50=1.003,p99=1.003,p99.9=1.003\r\nlatency_percentiles_usec_latency|help:p50=10.047,p99=10.047,p99.9=10.047\r\nlatency_percentiles_usec_json.set:p50=96.255,p99=1531.903,p99.9=5275.647\r\nlatency_percentiles_usec_expire:p50=2.007,p99=38.143,p99.9=38.143\r\nlatency_percentiles_usec_set:p50=1.003,p99=10.047,p99.9=33.023\r\nlatency_percentiles_usec_FT.INFO:p50=56.063,p99=933.887,p99.9=1105.919\r\nlatency_percentiles_usec_keys:p50=1002438.655,p99=1002438.655,p99.9=1002438.655\r\nlatency_percentiles_usec_hincrbyfloat:p50=2.007,p99=8.031,p99.9=30.079\r\nlatency_percentiles_usec_auth:p50=2.007,p99=15.039,p99.9=73.215\r\nlatency_percentiles_usec_json.get:p50=5.023,p99=119.295,p99.9=634.879\r\nlatency_percentiles_usec_client|setinfo:p50=1.003,p99=19.071,p99.9=31.103\r\nlatency_percentiles_usec_client|setname:p50=0.001,p99=15.039,p99.9=17.023\r\nlatency_percentiles_usec_client|id:p50=1.003,p99=1.003,p99.9=1.003\r\nlatency_percentiles_usec_client|kill:p50=63.231,p99=1490.943,p99.9=1490.943\r\nlatency_percentiles_usec_quit:p50=1.003,p99=13.055,p99.9=54.015\r\nlatency_percentiles_usec_subscribe:p50=1.003,p99=11.007,p99.9=18.047\r\nlatency_percentiles_usec_replconf:p50=0.001,p99=12.031,p99.9=37.119\r\nlatency_percentiles_usec_psubscribe:p50=2.007,p99=34.047,p99.9=89.087\r\nlatency_percentiles_usec_FT.SEARCH:p50=1187.839,p99=1002438.655,p99.9=1002438.655\r\nlatency_percentiles_usec_get:p50=0.001,p99=3.007,p99.9=11.007\r\n\r\n# Cluster\r\ncluster_enabled:0\r\n\r\n# Keyspace\r\ndb0:keys=4543953,expires=134720,avg_ttl=13214743\r\ndb1:keys=2,expires=0,avg_ttl=0\r\n\r\n------ MODULES INFO OUTPUT ------\r\n# ReJSON_trace\r\nReJSON_trace:   0: redis_module::base_info_func\r\n   1: rejson::__info_func\r\n   2: modulesCollectInfo\r\n             at /data/clickhouse/redis-work/redis_sentinel_prod/redis-7.2.3/src/module.c:10293:9\r\n   3: logModulesInfo\r\n             at /data/clickhouse/redis-work/redis_sentinel_prod/redis-7.2.3/src/debug.c:1900:22\r\n      printCrashReport\r\n             at /data/clickhouse/redis-work/redis_sentinel_prod/redis-7.2.3/src/debug.c:2183:5\r\n   4: _serverAssert\r\n             at /data/clickhouse/redis-work/redis_sentinel_prod/redis-7.2.3/src/debug.c:1062:9\r\n   5: unlinkClient\r\n             at /data/clickhouse/redis-work/redis_sentinel_prod/redis-7.2.3/src/networking.c:1480:5\r\n   6: freeClient\r\n             at /data/clickhouse/redis-work/redis_sentinel_prod/redis-7.2.3/src/networking.c:1636:5\r\n   7: RM_FreeThreadSafeContext\r\n             at /data/clickhouse/redis-work/redis_sentinel_prod/redis-7.2.3/src/module.c:8450:5\r\n   8: thread_do\r\n   9: start_thread\r\n  10: clone\r\n\r\n\r\n# search_version\r\nsearch_version:99.99.99\r\nsearch_redis_version:7.2.3 - oss\r\n\r\n# search_index\r\nsearch_number_of_indexes:45\r\n\r\n# search_fields_statistics\r\nsearch_fields_text:Text=211,Sortable=14,NoIndex=35\r\nsearch_fields_numeric:Numeric=96,Sortable=55\r\nsearch_fields_tag:Tag=13,Sortable=9\r\n\r\n# search_dialect_statistics\r\nsearch_dialect_1:1\r\nsearch_dialect_2:0\r\nsearch_dialect_3:0\r\nsearch_dialect_4:0\r\n\r\n# search_runtime_configurations\r\nsearch_concurrent_mode:OFF\r\nsearch_enableGC:ON\r\nsearch_minimal_term_prefix:2\r\nsearch_maximal_prefix_expansions:200\r\nsearch_query_timeout_ms:30000\r\nsearch_timeout_policy:return\r\nsearch_cursor_read_size:1000\r\nsearch_cursor_max_idle_time:300000\r\nsearch_max_doc_table_size:1000000\r\nsearch_max_search_results:-1\r\nsearch_max_aggregate_results:-1\r\nsearch_search_pool_size:20\r\nsearch_index_pool_size:8\r\nsearch_gc_scan_size:100\r\nsearch_min_phonetic_term_length:3\r\n\r\n------ CONFIG DEBUG OUTPUT ------\r\nreplica-read-only yes\r\nlazyfree-lazy-expire no\r\nlazyfree-lazy-user-del no\r\nslave-read-only yes\r\nlazyfree-lazy-eviction no\r\nlazyfree-lazy-server-del no\r\nlazyfree-lazy-user-flush no\r\nio-threads 3\r\nsanitize-dump-payload no\r\nproto-max-bulk-len 512mb\r\nclient-query-buffer-limit 1gb\r\nlist-compress-depth 0\r\nactivedefrag no\r\nrepl-diskless-sync yes\r\nio-threads-do-reads no\r\nrepl-diskless-load disabled\r\n\r\n------ FAST MEMORY TEST ------\r\n537283:M 20 Nov 2023 12:36:42.478 # main thread terminated\r\n537283:M 20 Nov 2023 12:36:42.479 # Bio worker thread #0 terminated\r\n537283:M 20 Nov 2023 12:36:42.479 # Bio worker thread #1 terminated\r\n537283:M 20 Nov 2023 12:36:42.482 # Bio worker thread #2 terminated\r\n537283:M 20 Nov 2023 12:36:42.485 # IO thread(tid:140737057978112) terminated\r\n537283:M 20 Nov 2023 12:36:42.486 # IO thread(tid:140737049585408) terminated\r\n\r\nFast memory test PASSED, however your memory can still be broken. Please run a memory test for several hours if possible.\r\n\r\n=== REDIS BUG REPORT END. Make sure to include from START to END. ===\r\n\r\n       Please report the crash by opening an issue on github:\r\n\r\n           http://github.com/redis/redis/issues\r\n\r\n  If a Redis module was involved, please open in the module's repo instead.\r\n\r\n  Suspect RAM error? Use redis-server --test-memory to verify it.\r\n\r\n  Some other issues could be detected by redis-server --check-system\r\n744888:C 20 Nov 2023 12:36:43.714 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.\r\n744889:C 20 Nov 2023 12:36:43.715 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\r\n```\r\n\r\n**Additional information**\r\n\r\nRedis Version: 7.2.3\r\nOS: Rocky Linux release 8.8\r\nRedis Mode: Standalone\r\nConfiguration: 5 nodes with sentinel running on each\r\n",
  "state": "closed",
  "created_at": "2023-11-20T12:51:55Z",
  "updated_at": "2024-01-19T13:12:51Z",
  "closed_at": "2024-01-19T13:12:51Z",
  "labels": [],
  "comments_data": [
    {
      "id": 1819015016,
      "user": "rishinair19",
      "created_at": "2023-11-20T12:55:53Z",
      "body": "Is this because of redisearch module? "
    },
    {
      "id": 1820096113,
      "user": "sundb",
      "created_at": "2023-11-21T02:03:31Z",
      "body": "@rishinair19 Thx, It's possible that it's not due to redisearch module.\r\nLike follow example:\r\nThe example has reproduced the crash locally.\r\n\r\nconfig\r\n```\r\nio-threads 4\r\n```\r\n\r\n```diff\r\ndiff --git a/tests/modules/blockedclient.c b/tests/modules/blockedclient.c\r\nindex 92060fd33..7d0e044b9 100644\r\n--- a/tests/modules/blockedclient.c\r\n+++ b/tests/modules/blockedclient.c\r\n@@ -30,21 +30,21 @@ void *sub_worker(void *arg) {\r\n \r\n void *worker(void *arg) {\r\n     // Retrieve blocked client\r\n-    RedisModuleBlockedClient *bc = (RedisModuleBlockedClient *)arg;\r\n+    // RedisModuleBlockedClient *bc = (RedisModuleBlockedClient *)arg;\r\n \r\n     // Get Redis module context\r\n-    RedisModuleCtx *ctx = RedisModule_GetThreadSafeContext(bc);\r\n+    RedisModuleCtx *ctx = RedisModule_GetThreadSafeContext(NULL);\r\n \r\n     // Acquire GIL\r\n     RedisModule_ThreadSafeContextLock(ctx);\r\n \r\n     // Create another thread which will try to acquire the GIL\r\n-    pthread_t tid;\r\n-    int res = pthread_create(&tid, NULL, sub_worker, ctx);\r\n-    assert(res == 0);\r\n+    // pthread_t tid;\r\n+    // int res = pthread_create(&tid, NULL, sub_worker, ctx);\r\n+    // assert(res == 0);\r\n \r\n     // Wait for thread\r\n-    pthread_join(tid, NULL);\r\n+    // pthread_join(tid, NULL);\r\n \r\n     // Release GIL\r\n     RedisModule_ThreadSafeContextUnlock(ctx);\r\n@@ -53,7 +53,7 @@ void *worker(void *arg) {\r\n     RedisModule_ReplyWithSimpleString(ctx, \"OK\");\r\n \r\n     // Unblock client\r\n-    RedisModule_UnblockClient(bc, NULL);\r\n+    // RedisModule_UnblockClient(bc, NULL);\r\n \r\n     // Free the Redis module context\r\n     RedisModule_FreeThreadSafeContext(ctx);\r\n@@ -66,7 +66,7 @@ int acquire_gil(RedisModuleCtx *ctx, RedisModuleString **argv, int argc)\r\n     UNUSED(argv);\r\n     UNUSED(argc);\r\n \r\n-    int flags = RedisModule_GetContextFlags(ctx);\r\n+    /* int flags = RedisModule_GetContextFlags(ctx);\r\n     int allFlags = RedisModule_GetContextFlagsAll();\r\n     if ((allFlags & REDISMODULE_CTX_FLAGS_MULTI) &&\r\n         (flags & REDISMODULE_CTX_FLAGS_MULTI)) {\r\n@@ -78,19 +78,20 @@ int acquire_gil(RedisModuleCtx *ctx, RedisModuleString **argv, int argc)\r\n         (flags & REDISMODULE_CTX_FLAGS_DENY_BLOCKING)) {\r\n         RedisModule_ReplyWithSimpleString(ctx, \"Blocked client is not allowed\");\r\n         return REDISMODULE_OK;\r\n-    }\r\n+    } */\r\n \r\n     /* This command handler tries to acquire the GIL twice\r\n      * once in the worker thread using \"RedisModule_ThreadSafeContextLock\"\r\n      * second in the sub-worker thread\r\n      * using \"RedisModule_ThreadSafeContextTryLock\"\r\n      * as the GIL is already locked. */\r\n-    RedisModuleBlockedClient *bc = RedisModule_BlockClient(ctx, NULL, NULL, NULL, 0);\r\n+    // RedisModuleBlockedClient *bc = RedisModule_BlockClient(ctx, NULL, NULL, NULL, 0);\r\n \r\n     pthread_t tid;\r\n-    int res = pthread_create(&tid, NULL, worker, bc);\r\n+    int res = pthread_create(&tid, NULL, worker, NULL);\r\n     assert(res == 0);\r\n \r\n+    RedisModule_ReplyWithSimpleString(ctx, \"OK\");\r\n     return REDISMODULE_OK;\r\n }\r\n```\r\n\r\n```\r\n./src/redis-benchmark -n 100000000 acquire_gil\r\n```\r\n\r\nwhen call `RedisModule_FreeThreadSafeContext()` and free client after `RedisModule_ThreadSafeContextUnlock()`,\r\nit's possible to trigger this assertion.\r\nping @oranagra "
    },
    {
      "id": 1820256105,
      "user": "rishinair19",
      "created_at": "2023-11-21T05:16:49Z",
      "body": "I had increased 'io-threads' in redis.conf to 3, reverted that change to default value and so far issue has not re-occurred. "
    },
    {
      "id": 1820261780,
      "user": "sundb",
      "created_at": "2023-11-21T05:24:01Z",
      "body": "@rishinair19 Yes, it will only occur when` io-threads` is turned on."
    },
    {
      "id": 1820288564,
      "user": "rishinair19",
      "created_at": "2023-11-21T05:58:08Z",
      "body": "Should I close this request then? I'm okay with io-thread being 1, not seeing issue anymore."
    },
    {
      "id": 1820294079,
      "user": "sundb",
      "created_at": "2023-11-21T06:04:55Z",
      "body": "@rishinair19 Please leave this issue open and we will fix it.\r\n"
    },
    {
      "id": 1820684453,
      "user": "oranagra",
      "created_at": "2023-11-21T10:54:08Z",
      "body": "the thing is that the (fake) client is freed while the GIL is not locked, so this assertion could happen, and we can probably easily fix it by moving the code that handles clients_pending_write_node and pending_read_list_node into the `if (c->conn)` block (since this fake client will never have any IO anyway).\r\nbut also other portions of unlinkClient can be dangerous to run.\r\nlike removing it from unblocked_clients, the code in disableTracking, stat_clients_type_memory, etc.\r\n@MeirShpilraien PTAL"
    },
    {
      "id": 1821230272,
      "user": "MeirShpilraien",
      "created_at": "2023-11-21T16:14:01Z",
      "body": "So I was looking at the code and I believe the only issue is if allocating a thread safe ctx which is not attached to a blocked client. If the thread safe context is attached to a blocked client then when releasing it, the `freeClient` will not be called. After a discussion with @oranagra we believe that the right thing to do is to refactor `freeClient` function in such way that all the server data structures will only be touched if there is actually a connection attached to the client. If there are not connection attached to the client, it can safely be freed without touching any server globals and so it will be safe to free it without holding the GIL."
    },
    {
      "id": 1821524864,
      "user": "oranagra",
      "created_at": "2023-11-21T19:12:11Z",
      "body": "few additional points:\r\n1. stressing again that this is not just a but with IO threads (the assertion), there are multi-threading issues here with module threads touching server globals without a lock.\r\n2. some of the actions in freeClient or unlinkClient can depend on c->conn being present, others can depend on the value of c->flags (doing things only if a certain flag is on, which could never happen for these fake clients).\r\n3. i'd like to try to understand when this problem started, which version, or under which conditions.\r\n\r\n@sundb maybe you want to try to tackle it?"
    },
    {
      "id": 1821939219,
      "user": "sundb",
      "created_at": "2023-11-22T01:17:15Z",
      "body": "@oranagra i'll take it."
    },
    {
      "id": 1823893343,
      "user": "sundb",
      "created_at": "2023-11-23T06:59:58Z",
      "body": "FYI @oranagra @MeirShpilraien \r\nwhy `RedisModule_Reply*()` family API is out of lock, it is not thread-safe.\r\nwhen the thread safe context is attached to a blocked client, `RedisModule_Reply*()` will end up in the following call chain:\r\n `RedisModule_Reply*()` -> `_addReplyProtoToList()` -> `closeClientOnOutputBufferLimitReached()`\r\nwe will touch `server.stat_client_outbuf_limit_disconnections`,`server.client_obuf_limits` and `server.repl_backlog_size`."
    },
    {
      "id": 1823903465,
      "user": "MeirShpilraien",
      "created_at": "2023-11-23T07:13:45Z",
      "body": "@sundb I believe that `closeClientOnOutputBufferLimitReached` will only do that if there is an actual connection on the client which I believe this is not the case with blocked client, am I missing something?"
    },
    {
      "id": 1823921877,
      "user": "sundb",
      "created_at": "2023-11-23T07:35:55Z",
      "body": "@MeirShpilraien Ooh, I missed the `if (!c->conn)` in the `closeClientOnOutputBufferLimitReached()`, thanks."
    },
    {
      "id": 1827138498,
      "user": "bossajie",
      "created_at": "2023-11-27T05:16:17Z",
      "body": "is this also related to this error? \r\nWhen we turned it off, this error is not showing anymore.\r\n\r\n<img width=\"945\" alt=\"image\" src=\"https://github.com/redis/redis/assets/36364052/548aa19d-7d89-4ab2-9dbc-d889950cc9d9\">\r\n"
    },
    {
      "id": 1827139588,
      "user": "sundb",
      "created_at": "2023-11-27T05:17:57Z",
      "body": "@bossajie Yes, this fix is still in progress."
    }
  ]
}