{
  "issue_number": 8706.0,
  "title": "[BUG] Active defrag may cause cluster node failover",
  "body": "**Describe the bug**\r\n\r\nAfter enabling activedefrag on our Redis cluster, a few days later all 6 of the primary nodes failed over to the replicas. They failed over one at a time over about an hour or so. Each primary failed over about 3-5 seconds after an active defrag was started on its replica node. The nodes had completed successful defragmentations over the previous days with no issues. This happened on 2 of the 4 clusters we enabled activedefrag on before we disabled the feature. The clusters all have 6 primary nodes, each with one replica.\r\n\r\n**To reproduce**\r\n\r\nEnable active defrag on all nodes in a cluster and wait a while. Not sure how easily this can be reproduced, but it happened on multiple clusters in our environment.\r\n\r\n**Expected behavior**\r\n\r\nActive defrag on a replica node would not cause a node failover.\r\n\r\n**Additional information**\r\n\r\nVerbose level logs from one of the replicas:\r\n```\r\nS 26 Mar 2021 00:08:20.636 - Starting active defrag, frag=10%, frag_bytes=601371816, cpu=1%\r\nS 26 Mar 2021 00:08:21.509 - Node 083f030577e5bd5dc9504036c471a67f197bc739 reported node 0ac9ecb7177550972f884075ce4a34bb5d8840 as not reachable.\r\nS 26 Mar 2021 00:08:22.506 - Node 51e234032a86d5027757efec2db6894f949d08b3 reported node 0ac9ecb7177550972f884075ce4a34bb5d8840 as not reachable.\r\nS 26 Mar 2021 00:08:22.659 - Node 96c11a34a6ef42a964aa738a858e418a78d1b7be reported node 0ac9ecb7177550972f884075ce4a34bb5d8840 as not reachable.\r\nS 26 Mar 2021 00:08:22.700 - Node 198f6f23e017962ba2372062ec5a2bea994cd732 reported node 0ac9ecb7177550972f884075ce4a34bb5d8840 as not reachable.\r\nS 26 Mar 2021 00:08:22.700 * Marking node ce0ac9ecb7177550972f884075ce4a34bb5d8840 as failing (quorum reached).\r\nS 26 Mar 2021 00:08:22.703 - Node db77608ae6b80d23664e4e6c2e916452704df585 reported node 0ac9ecb7177550972f884075ce4a34bb5d8840 as not reachable.\r\nS 26 Mar 2021 00:08:22.743 # Start of election delayed for 923 milliseconds (rank #0, offset 66060669216).\r\nS 26 Mar 2021 00:08:23.671 # Starting a failover election for epoch 18.\r\nS 26 Mar 2021 00:08:23.680 # Failover election won: I'm the new master.\r\n```\r\n\r\nLogs from the corresponding primary node. It looks like this log that was happening every 5 seconds previously stopped happening around the time the active defrag started on the replica, and then there were a bunch of new client connections. It seems like the node slowed down or got stuck, which caused our clients to open up new connections/max out their connection pools to the node.\r\n```\r\nM 26 Mar 2021 00:07:58.107 - DB 0: 12258508 keys (8388278 volatile) in 16777216 slots HT.\r\nM 26 Mar 2021 00:08:03.121 - DB 0: 12258704 keys (8388444 volatile) in 16777216 slots HT.\r\nM 26 Mar 2021 00:08:08.134 - DB 0: 12258614 keys (8388393 volatile) in 16777216 slots HT.\r\nM 26 Mar 2021 00:08:13.148 - DB 0: 12258780 keys (8388520 volatile) in 16777216 slots HT.\r\nM 26 Mar 2021 00:08:23.160 - Accepted 10.74.241.4:41246\r\nM 26 Mar 2021 00:08:23.160 - Accepted 10.74.241.4:41248\r\nM 26 Mar 2021 00:08:23.160 - Accepted 10.74.241.4:41250\r\nM 26 Mar 2021 00:08:23.160 - Accepted 10.74.241.4:41252\r\nM 26 Mar 2021 00:08:23.160 - Accepted 10.74.177.111:35586\r\n...\r\nS 26 Mar 2021 00:08:31.005 * Full resync from master: da5414463626bb6ea13fb931641f32829481e515:66063381306\r\n```\r\n\r\nRedis info/cluster info from a primary node in the cluster\r\n```\r\n# Server\r\nredis_version:6.0.5\r\nredis_git_sha1:00000000\r\nredis_git_dirty:0\r\nredis_build_id:db63ea56716d515f\r\nredis_mode:cluster\r\nos:Linux 3.10.0-1160.6.1.el7.x86_64 x86_64\r\narch_bits:64\r\nmultiplexing_api:epoll\r\natomicvar_api:atomic-builtin\r\ngcc_version:8.3.0\r\nprocess_id:1\r\nrun_id:03a7f2a70bd8601000f37377b20b0251a429bc56\r\ntcp_port:6379\r\nuptime_in_seconds:5521854\r\nuptime_in_days:63\r\nhz:10\r\nconfigured_hz:10\r\nlru_clock:6172592\r\nexecutable:/data/redis-server\r\nconfig_file:/etc/redis/redis.conf\r\n\r\n# Clients\r\nconnected_clients:677\r\nclient_recent_max_input_buffer:4\r\nclient_recent_max_output_buffer:0\r\nblocked_clients:0\r\ntracking_clients:0\r\nclients_in_timeout_table:0\r\n\r\n# Memory\r\nused_memory:5997093992\r\nused_memory_human:5.59G\r\nused_memory_rss:6562820096\r\nused_memory_rss_human:6.11G\r\nused_memory_peak:6098125536\r\nused_memory_peak_human:5.68G\r\nused_memory_peak_perc:98.34%\r\nused_memory_overhead:974546390\r\nused_memory_startup:3869608\r\nused_memory_dataset:5022547602\r\nused_memory_dataset_perc:83.80%\r\nallocator_allocated:5997794368\r\nallocator_active:6484668416\r\nallocator_resident:6585536512\r\ntotal_system_memory:12428709888\r\ntotal_system_memory_human:11.58G\r\nused_memory_lua:41984\r\nused_memory_lua_human:41.00K\r\nused_memory_scripts:216\r\nused_memory_scripts_human:216B\r\nnumber_of_cached_scripts:1\r\nmaxmemory:6000000000\r\nmaxmemory_human:5.59G\r\nmaxmemory_policy:allkeys-lru\r\nallocator_frag_ratio:1.08\r\nallocator_frag_bytes:486874048\r\nallocator_rss_ratio:1.02\r\nallocator_rss_bytes:100868096\r\nrss_overhead_ratio:1.00\r\nrss_overhead_bytes:-22716416\r\nmem_fragmentation_ratio:1.09\r\nmem_fragmentation_bytes:565095840\r\nmem_not_counted_for_evict:0\r\nmem_replication_backlog:1048576\r\nmem_clients_slaves:16986\r\nmem_clients_normal:11482636\r\nmem_aof_buffer:0\r\nmem_allocator:jemalloc-5.1.0\r\nactive_defrag_running:0\r\nlazyfree_pending_objects:0\r\n\r\n# Persistence\r\nloading:0\r\nrdb_changes_since_last_save:527374991\r\nrdb_bgsave_in_progress:0\r\nrdb_last_save_time:1611263474\r\nrdb_last_bgsave_status:ok\r\nrdb_last_bgsave_time_sec:-1\r\nrdb_current_bgsave_time_sec:-1\r\nrdb_last_cow_size:1196924928\r\naof_enabled:0\r\naof_rewrite_in_progress:0\r\naof_rewrite_scheduled:0\r\naof_last_rewrite_time_sec:-1\r\naof_current_rewrite_time_sec:-1\r\naof_last_bgrewrite_status:ok\r\naof_last_write_status:ok\r\naof_last_cow_size:0\r\nmodule_fork_in_progress:0\r\nmodule_fork_last_cow_size:0\r\n\r\n# Stats\r\ntotal_connections_received:218523\r\ntotal_commands_processed:711196011\r\ninstantaneous_ops_per_sec:984\r\ntotal_net_input_bytes:78717853978\r\ntotal_net_output_bytes:110573258327\r\ninstantaneous_input_kbps:118.74\r\ninstantaneous_output_kbps:250.05\r\nrejected_connections:0\r\nsync_full:1\r\nsync_partial_ok:0\r\nsync_partial_err:1\r\nexpired_keys:1188098\r\nexpired_stale_perc:3.31\r\nexpired_time_cap_reached_count:0\r\nexpire_cycle_cpu_milliseconds:22711\r\nevicted_keys:823034\r\nkeyspace_hits:138469946\r\nkeyspace_misses:10153408\r\npubsub_channels:0\r\npubsub_patterns:0\r\nlatest_fork_usec:142799\r\nmigrate_cached_sockets:0\r\nslave_expires_tracked_keys:0\r\nactive_defrag_hits:9112043\r\nactive_defrag_misses:173621778\r\nactive_defrag_key_hits:7654474\r\nactive_defrag_key_misses:35920459\r\ntracking_total_keys:0\r\ntracking_total_items:0\r\ntracking_total_prefixes:0\r\nunexpected_error_replies:0\r\n\r\n# Replication\r\nrole:master\r\nconnected_slaves:1\r\nslave0:ip=10.74.178.168,port=6379,state=online,offset=70772608236,lag=0\r\nmaster_replid:1cd856df66e3af4e38cc13179026d1e751b8fa0a\r\nmaster_replid2:c2511e4504865c8ad3fcf705a753687db04a69b9\r\nmaster_repl_offset:70772658623\r\nsecond_repl_offset:66028590979\r\nrepl_backlog_active:1\r\nrepl_backlog_size:1048576\r\nrepl_backlog_first_byte_offset:70771610048\r\nrepl_backlog_histlen:1048576\r\n\r\n# CPU\r\nused_cpu_sys:30752.745015\r\nused_cpu_user:21255.508407\r\nused_cpu_sys_children:8.530935\r\nused_cpu_user_children:83.261017\r\n\r\n# Modules\r\n\r\n# Cluster\r\ncluster_enabled:1\r\n\r\n# Keyspace\r\ndb0:keys=12234916,expires=8345678,avg_ttl=150404561681\r\n```\r\n```\r\ncluster_state:ok\r\ncluster_slots_assigned:16384\r\ncluster_slots_ok:16384\r\ncluster_slots_pfail:0\r\ncluster_slots_fail:0\r\ncluster_known_nodes:12\r\ncluster_size:6\r\ncluster_current_epoch:18\r\ncluster_my_epoch:13\r\ncluster_stats_messages_ping_sent:19561837\r\ncluster_stats_messages_pong_sent:19698184\r\ncluster_stats_messages_meet_sent:1\r\ncluster_stats_messages_fail_sent:11\r\ncluster_stats_messages_auth-req_sent:11\r\ncluster_stats_messages_auth-ack_sent:5\r\ncluster_stats_messages_update_sent:6\r\ncluster_stats_messages_sent:39260055\r\ncluster_stats_messages_ping_received:19698173\r\ncluster_stats_messages_pong_received:19561767\r\ncluster_stats_messages_fail_received:11\r\ncluster_stats_messages_auth-req_received:5\r\ncluster_stats_messages_auth-ack_received:5\r\ncluster_stats_messages_received:39259961\r\n```",
  "state": "open",
  "created_at": "2021-03-26T19:09:36Z",
  "updated_at": "2021-03-27T23:18:41Z",
  "closed_at": null,
  "labels": [],
  "comments_data": [
    {
      "id": 808766878,
      "user": "oranagra",
      "created_at": "2021-03-27T17:34:09Z",
      "body": "@nosammai did you happen to have latency monitor (LATENCY LATEST) enabled on these servers? \r\nDid you change any of the threshold of active defrag (other than enabling it)? "
    },
    {
      "id": 808816121,
      "user": "nosammai",
      "created_at": "2021-03-27T23:18:41Z",
      "body": "Sadly we didn't have latency monitor enabled on those clusters at the time. I didn't change any of the default settings for the active defrag, just turned it on.\r\n\r\n```\r\n['active-defrag-cycle-min', '1', 'active-defrag-cycle-max', '25', 'active-defrag-threshold-lower', '10', 'active-defrag-threshold-upper', '100', 'active-defrag-max-scan-fields', '1000', 'active-defrag-ignore-bytes', '104857600']\r\n```\r\n\r\n\r\n"
    }
  ]
}