{
  "issue_number": 1014,
  "title": "panic at 'slice out of range'",
  "body": "tikv log as follow:\n\n```\n2016-09-01 10:51:33,484 snap.rs:188 - INFO  - saving snapshot to /home/pingcap/work/poc/jinpeng/data2/snap/rev_188_6_114237.snap\n2016-09-01 10:51:33,484 raft.rs:1265 - INFO  - [region 188] 190 [commit: 0, lastindex: 0, lastterm: 0] starts to restore snapshot [index: 114237, term: 6]\n2016-09-01 10:51:33,484 raft.rs:1278 - INFO  - [region 188] 190 restored progress of 189 [Progress { matched: 0, next_idx: 1, state: Probe, paused: false, pending_snapshot: 0, recent_active: false, ins: Inflights { start: 0, count: 0, buffer: [] } }]\n2016-09-01 10:51:33,484 raft.rs:1278 - INFO  - [region 188] 190 restored progress of 190 [Progress { matched: 0, next_idx: 1, state: Probe, paused: false, pending_snapshot: 0, recent_active: false, ins: Inflights { start: 0, count: 0, buffer: [] } }]\n2016-09-01 10:51:33,484 raft.rs:1278 - INFO  - [region 188] 190 restored progress of 191 [Progress { matched: 0, next_idx: 1, state: Probe, paused: false, pending_snapshot: 0, recent_active: false, ins: Inflights { start: 0, count: 0, buffer: [] } }]\n2016-09-01 10:51:33,484 raft_log.rs:385 - INFO  - log [committed=0, applied=0, unstable.offset=1, unstable.entries.len()=0] starts to restore snapshot [index: 114237, term: 6]\n2016-09-01 10:51:33,484 raft.rs:1226 - INFO  - [region 188] 190 [commit: 114237] restored snapshot [index: 114237, term: 6]\n2016-09-01 10:51:33,484 peer_storage.rs:378 - INFO  - [region 188] 190 begin to apply snapshot\n2016-09-01 10:51:33,484 peer_storage.rs:407 - INFO  - [region 188] 190 apply snapshot for region id: 188 start_key: \"t\\200\\000\\000\\000\\000\\000\\000\\377V_r\\200\\000\\000\\000\\000\\377\\213+\\365\\000\\000\\000\\000\\000\\372\" end_key: \"\" region_epoch {conf_ver: 3 version: 33} peers {id: 189 store_id: 1} peers {id: 190 store_id: 4} peers {id: 191 store_id: 5} with state applied_index: 114237 truncated_state {index: 114237 term: 6} ok\n2016-09-01 10:51:33,484 store.rs:736 - INFO  - [region 188] snapshot for region id: 188 start_key: \"t\\200\\000\\000\\000\\000\\000\\000\\377V_r\\200\\000\\000\\000\\000\\377\\213+\\365\\000\\000\\000\\000\\000\\372\" end_key: \"\" region_epoch {conf_ver: 3 version: 33} peers {id: 189 store_id: 1} peers {id: 190 store_id: 4} peers {id: 191 store_id: 5} is applied\n2016-09-01 10:51:35,471 peer.rs:434 - WARN  - [region 152] 154 handle ready, entries 4, committed entries 1, messages 5, snapshot false, hard state changed true [takes Duration { secs: 1, nanos: 105315091 }]\n2016-09-01 10:51:35,471 store.rs:588 - WARN  - on 1 regions raft ready [takes Duration { secs: 1, nanos: 105386342 }]\n2016-09-01 10:51:35,784 snap.rs:186 - INFO  - apply new data takes Duration { secs: 1, nanos: 903820903 }\n2016-09-01 10:51:35,795 mod.rs:51 - WARN  - handle task Snap apply for 180 [takes Duration { secs: 2, nanos: 453035375 }]\n2016-09-01 10:51:35,795 snap.rs:122 - INFO  - begin apply snap data for 188\n2016-09-01 10:51:35,953 snap.rs:188 - INFO  - saving snapshot to /home/pingcap/work/poc/jinpeng/data2/snap/rev_188_6_132154.snap\n2016-09-01 10:51:35,953 raft.rs:1265 - INFO  - [region 188] 190 [commit: 114237, lastindex: 114237, lastterm: 6] starts to restore snapshot [index: 132154, term: 6]\n2016-09-01 10:51:35,953 raft.rs:1278 - INFO  - [region 188] 190 restored progress of 189 [Progress { matched: 0, next_idx: 114238, state: Probe, paused: false, pending_snapshot: 0, recent_active: false, ins: Inflights { start: 0, count: 0, buffer: [] } }]\n2016-09-01 10:51:35,953 raft.rs:1278 - INFO  - [region 188] 190 restored progress of 190 [Progress { matched: 114237, next_idx: 114238, state: Probe, paused: false, pending_snapshot: 0, recent_active: false, ins: Inflights { start: 0, count: 0, buffer: [] } }]\n2016-09-01 10:51:35,953 raft.rs:1278 - INFO  - [region 188] 190 restored progress of 191 [Progress { matched: 0, next_idx: 114238, state: Probe, paused: false, pending_snapshot: 0, recent_active: false, ins: Inflights { start: 0, count: 0, buffer: [] } }]\n2016-09-01 10:51:35,953 raft_log.rs:385 - INFO  - log [committed=114237, applied=114237, unstable.offset=114238, unstable.entries.len()=0] starts to restore snapshot [index: 132154, term: 6]\n2016-09-01 10:51:35,953 raft.rs:1226 - INFO  - [region 188] 190 [commit: 132154] restored snapshot [index: 132154, term: 6]\nthread 'raftstore-4' panicked at 'slice[132155,136093] out of bound[132155,132154]', src/raft/raft_log.rs:320\nstack backtrace:\n   1:     0x7f51cca2c02f - std::sys::backtrace::tracing::imp::write::h29f5fdb9fc0a7395\n   2:     0x7f51cca3318b - std::panicking::default_hook::_{{closure}}::h2cc84f0378700526\n   3:     0x7f51cca3159a - std::panicking::default_hook::hbbe7fa36a995aca0\n   4:     0x7f51cc73a870 - tikv::util::panic_hook::set_exit_hook::_{{closure}}::h16f63522ff4f3661\n                        at /home/pingcap/go/src/github.com/pingcap/tikv2/tikv/src/util/panic_hook.rs:68\n   5:     0x7f51cca31bda - std::panicking::rust_panic_with_hook::h105c3d42fcd2fb5e\n   6:     0x7f51cca31a41 - std::panicking::begin_panic::hbf62ea4a5ff3f9de\n   7:     0x7f51cca3196a - std::panicking::begin_panic_fmt::h20f5943904e5791d\n   8:     0x7f51cc51d303 - _<tikv..raft..raft_log..RaftLog<T>>::slice::hdad97e47cd6fba85\n                        at /home/pingcap/go/src/github.com/pingcap/tikv2/tikv/<std macros>:8\n                        at /home/pingcap/go/src/github.com/pingcap/tikv2/tikv/src/raft/raft_log.rs:339\n   9:     0x7f51cc57badc - _<tikv..raftstore..store..store..Store<T, C>>::on_raft_ready::h32f5112b543f8d85\n                        at /home/pingcap/go/src/github.com/pingcap/tikv2/tikv/src/raft/raft_log.rs:292\n                        at /home/pingcap/go/src/github.com/pingcap/tikv2/tikv/src/raft/raw_node.rs:112\n                        at /home/pingcap/go/src/github.com/pingcap/tikv2/tikv/src/raft/raw_node.rs:282\n                        at /home/pingcap/go/src/github.com/pingcap/tikv2/tikv/src/raftstore/store/peer.rs:408\n                        at /home/pingcap/go/src/github.com/pingcap/tikv2/tikv/src/raftstore/store/store.rs:568\n  10:     0x7f51cc58e199 - _<tikv..raftstore..store..store..Store<T, C>>::run::h5721048789679458\n                        at /home/pingcap/go/src/github.com/pingcap/tikv2/tikv/src/raftstore/store/store.rs:1336\n                        at /home/pingcap/.cargo/git/checkouts/mio-e79f285e2377d154/master/src/event_loop.rs:315\n                        at /home/pingcap/.cargo/git/checkouts/mio-e79f285e2377d154/master/src/event_loop.rs:261\n                        at /home/pingcap/go/src/github.com/pingcap/tikv2/tikv/src/raftstore/store/store.rs:242\n  11:     0x7f51cc5deeb4 - _<tikv..server..node..Node<C>>::start_store::_{{closure}}::h3c432f9f1415293b\n  12:     0x7f51cc5dc07a - std::panicking::try::_{{closure}}::_{{closure}}::hcf50daaf458fd93a\n  13:     0x7f51cca3a99b - __rust_try\n  14:     0x7f51cca3a87e - __rust_maybe_catch_panic\n  15:     0x7f51cc5db2ae - std::thread::Builder::spawn::_{{closure}}::h23348431e6569cfb\n  16:     0x7f51cc568aed - _<F as alloc..boxed..FnBox<A>>::call_box::h638735dc521642c1\n                        at /buildslave/rust-buildbot/slave/nightly-dist-rustc-linux/build/obj/../src/liballoc/boxed.rs:587\n  17:     0x7f51cca2fc64 - std::sys::thread::Thread::new::thread_start::h8f3bd45211e9f5ea\n  18:     0x7f51cb151183 - start_thread\n  19:     0x7f51cb96b37c - clone\n  20:                0x0 - <unknown>\n```\n",
  "state": "closed",
  "created_at": "2016-09-01T03:29:26Z",
  "updated_at": "2018-08-07T00:46:04Z",
  "closed_at": "2016-09-07T08:01:45Z",
  "labels": [
    "type/bug"
  ],
  "comments_data": [
    {
      "id": 243981944,
      "user": "BusyJay",
      "created_at": "2016-09-01T05:45:35Z",
      "body": "The bug is triggered by applying two snapshots for the same peer at the same time. It may calculate the wrong last index if the first snapshot in the queue takes too long time to be applied.\n"
    },
    {
      "id": 243992830,
      "user": "siddontang",
      "created_at": "2016-09-01T06:58:24Z",
      "body": "If we don't apply the first region, the region status is applying, can we receive another snapshot? \n"
    },
    {
      "id": 243998148,
      "user": "BusyJay",
      "created_at": "2016-09-01T07:27:35Z",
      "body": "Yes. applying status just prevent applying the new snapshot data immediately, but the new snapshot meta can still be applied to raft state machine.\n"
    }
  ]
}