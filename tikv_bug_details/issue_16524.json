{
  "issue_number": 16524,
  "title": "Failure with booting up TiKV with API v2 causes Fatal crash, which is probably a bit too severe.",
  "body": "## Bug Report\r\n\r\n### Context\r\n\r\nWe are exploring the usage of TiKV as a pure key value store within the presence of TiDB servers in the same cluster.\r\n\r\n### What version of TiKV are you using?\r\n7.5.0\r\n\r\n### What operating system and CPU are you using?\r\nCPU:\r\n```\r\nprocessor\t: 0\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 85\r\nmodel name\t: Intel Xeon Processor (Skylake, IBRS)\r\nstepping\t: 4\r\nmicrocode\t: 0x1\r\ncpu MHz\t\t: 2000.028\r\ncache size\t: 16384 KB\r\nphysical id\t: 0\r\nsiblings\t: 2\r\ncore id\t\t: 0\r\ncpu cores\t: 1\r\napicid\t\t: 0\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti ssbd ibrs ibpb tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat pku ospke md_clear\r\nvmx flags\t: vnmi preemption_timer posted_intr invvpid ept_x_only ept_ad ept_1gb flexpriority apicv tsc_offset vtpr mtf vapic ept vpid unrestricted_guest vapic_reg vid pml\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit mmio_stale_data retbleed\r\nbogomips\t: 4000.05\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 40 bits physical, 48 bits virtual\r\npower management:\r\n```\r\n\r\nOS\r\n```\r\nNAME=\"Rocky Linux\"\r\nVERSION=\"9.2 (Blue Onyx)\"\r\nID=\"rocky\"\r\nID_LIKE=\"rhel centos fedora\"\r\nVERSION_ID=\"9.2\"\r\nPLATFORM_ID=\"platform:el9\"\r\nPRETTY_NAME=\"Rocky Linux 9.2 (Blue Onyx)\"\r\nANSI_COLOR=\"0;32\"\r\nLOGO=\"fedora-logo-icon\"\r\nCPE_NAME=\"cpe:/o:rocky:rocky:9::baseos\"\r\nHOME_URL=\"https://rockylinux.org/\"\r\nBUG_REPORT_URL=\"https://bugs.rockylinux.org/\"\r\nSUPPORT_END=\"2032-05-31\"\r\nROCKY_SUPPORT_PRODUCT=\"Rocky-Linux-9\"\r\nROCKY_SUPPORT_PRODUCT_VERSION=\"9.2\"\r\nREDHAT_SUPPORT_PRODUCT=\"Rocky Linux\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"9.2\"\r\n```\r\n\r\n### Steps to reproduce\r\n\r\n1. Prepare a TiKV, TiDB cluster using the TiDB operator in kubernetes. (We are using Grafana Tanka to manage our deployments)\r\n2. Run the following code to write some key using the tikv-txnkv Go API (v1 API):\r\n\r\n```\r\n\t\"github.com/tikv/client-go/v2/txnkv\"\r\n\tgoctx \"golang.org/x/net/context\"\r\n)\r\n\r\ntype KV struct {\r\n  K, V []byte\r\n}\r\n\r\n...\r\n\r\nvar client *txnkv.Client\r\nvar pdAddr = flag.String(\"pd\", \"{IP ADDRESS HERE}, \"pdAddr\")\r\n\r\nfunc initStore() {\r\n  var err error\r\n  client, err = txnkv.NewClient([]string{*pdAddr})\r\n  if err != nil {\r\n    panic(err)\r\n  }\r\n}\r\n\r\nfunc puts(args ...[]byte) error {\r\n  tx, err := client.Begin()\r\n  if err != nil {\r\n    return err\r\n  }\r\n\r\n  for i := 0; i < len(args); i +=2 {\r\n    key, val := args[i], args[i+1]\r\n    err := tx.Set(key, val)\r\n    if err != nil {\r\n      return err\r\n    }\r\n  }\r\n  return tx.Commit(goctx.Background())\r\n}\r\n\r\n...\r\n\r\nfunc main(){\r\n    err := puts([]byte(\"key1\"), []byte(\"value1\"))\r\n    if err != nil {\r\n      panic(err)\r\n    }\r\n}\r\n\r\n```\r\n\r\n3. Upgrade tikv Stateful Set to use API version 2 with the following TiKV config\r\n\r\n```\r\n      config: |||\r\n        [backup]\r\n          num-threads = 1\r\n\r\n        [log]\r\n          level = \"info\"\r\n\r\n        [raftstore]\r\n          store-io-pool-size = 2\r\n\r\n        [readpool]\r\n          [readpool.unified]\r\n            auto-adjust-pool-size = true\r\n       \r\n        [storage]\r\n           api-version = 2\r\n           enable-ttl = true\r\n      |||,\r\n\r\n```\r\n\r\n### What did you expect?\r\n\r\nSince using  a TiKV client in the same cluster with TiDB is still a relatively new use case, I did not really expect the pod to crash. At most, I was hoping it would just raise a warning that there was an issue with switching over and continue to use the V1 API. \r\n\r\nI am willing to work on this issue to turn this into a warning instead of a critical error, if the community is okay with it. \r\n\r\n### What did happened?\r\n\r\nPod ended up in a crashloopbackoff state.\r\n\r\n```\r\n│ tikv [2024/02/16 22:51:17.468 +09:00] [WARN] [worker.rs:348] [\"adjust duration too small, skip adjustment.\"] [dur=0ns] [thread_id=0x4]                                                                             │\r\n│ tikv [2024/02/16 22:51:17.468 +09:00] [INFO] [service.rs:201] [\"load controller config\"] [config=\"RequestUnitConfig { read_base_cost: 0.25, read_cost_per_byte: 1.52587890625e-5, write_base_cost: 1.0, write_cost │\r\n│ _per_byte: 0.0009765625, read_cpu_ms_cost: 0.3333333333333333 }\"] [thread_id=0x4]                                                                                                                                  │\r\n│ tikv [2024/02/16 22:51:17.468 +09:00] [INFO] [resource_group.rs:182] [\"add resource group\"] [ru=2147483647] [name=default] [thread_id=0x4]                                                                         │\r\n│ tikv [2024/02/16 22:51:17.468 +09:00] [INFO] [resource_group.rs:182] [\"add resource group\"] [ru=10000] [name=stg_ameblo_features_rg] [thread_id=0x4]                                                               │\r\n│ tikv [2024/02/16 22:51:17.468 +09:00] [INFO] [resource_group.rs:182] [\"add resource group\"] [ru=100000] [name=stg_hyuga_rg] [thread_id=0x4]                                                                        │\r\n│ tikv [2024/02/16 22:51:17.469 +09:00] [INFO] [resource_group.rs:182] [\"add resource group\"] [ru=1000] [name=stg_korobov_rg] [thread_id=0x4]                                                                        │\r\n│ tikv [2024/02/16 22:51:17.469 +09:00] [INFO] [resource_group.rs:182] [\"add resource group\"] [ru=1000] [name=stg_ownd_access_rg] [thread_id=0x4]                                                                    │\r\n│ tikv [2024/02/16 22:51:17.469 +09:00] [INFO] [resource_group.rs:182] [\"add resource group\"] [ru=300] [name=stg_test_rg] [thread_id=0x4]                                                                            │\r\n│ tikv [2024/02/16 22:51:17.469 +09:00] [INFO] [resource_group.rs:182] [\"add resource group\"] [ru=5000] [name=stg_trend_rg] [thread_id=0x4]                                                                          │\r\n│ tikv [2024/02/16 22:51:17.469 +09:00] [INFO] [client.rs:413] [\"[global_config] start watch global config\"] [revision=9475666] [path=resource_group/settings] [thread_id=0x4]                                       │\r\n│ tikv [2024/02/16 22:51:17.469 +09:00] [INFO] [server.rs:403] [\"Causal timestamp provider startup.\"] [thread_id=0x4]                                                                                                │\r\n│ tikv [2024/02/16 22:51:17.474 +09:00] [INFO] [mod.rs:132] [\"encryption: none of key dictionary and file dictionary are found.\"] [thread_id=0x4]                                                                    │\r\n│ tikv [2024/02/16 22:51:17.474 +09:00] [INFO] [mod.rs:551] [\"encryption is disabled.\"] [thread_id=0x4]                                                                                                              │\r\n│ tikv [2024/02/16 22:51:17.699 +09:00] [INFO] [engine.rs:93] [\"Recovering raft logs takes 224.548871ms\"] [thread_id=0x4]                                                                                            │\r\n│ tikv [2024/02/16 22:51:17.876 +09:00] [INFO] [mod.rs:298] [\"Storage started.\"] [thread_id=0x4]                                                                                                                     │\r\n│ tikv [2024/02/16 22:51:17.879 +09:00] [FATAL] [server.rs:791] [\"failed to bootstrap node id: \\\"[src/server/node.rs:309]: unable to switch `storage.api_version` from V1 to V2 because found data key that is not w │\r\n│ ritten by TiDB: \\\\\\\"6B65793100000000FBF9CBF1153D4BFFFC\\\\\\\"\\\"\"] [thread_id=0x4]\r\n\r\n```\r\n\r\n### Further Questions\r\n\r\nI was wondering if there was any way to rectify this situation where we have a cluster with both TiDB and TiKV and we have already written some keys with the v1 client and we want to switch over the v2 client? Perhaps we might want to have an option to remove all keys written with the previous V1 client?\r\n",
  "state": "closed",
  "created_at": "2024-02-16T15:00:41Z",
  "updated_at": "2024-02-23T07:14:49Z",
  "closed_at": "2024-02-23T07:11:24Z",
  "labels": [
    "type/bug"
  ],
  "comments_data": [
    {
      "id": 1960830447,
      "user": "pregenRobot",
      "created_at": "2024-02-23T07:11:24Z",
      "body": "Issue was I had to wait some time for MVCC handle the physical deletion of the key. Restarting the tikv server with Api v2 the following day solved the issue."
    }
  ]
}