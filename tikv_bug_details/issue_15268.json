{
  "issue_number": 15268,
  "title": "[Dynamic Regions] QPS drop to zero during injection io hang to one of tikv",
  "body": "## Bug Report\r\n\r\n<!-- Thanks for your bug report! Don't worry if you can't fill out all the sections. -->\r\n\r\n### What version of TiKV are you using?\r\n./tikv-server -V\r\nTiKV\r\nRelease Version: 7.3.0-alpha\r\nEdition: Community\r\nGit Commit Hash: https://github.com/tikv/tikv/commit/284ff3a9ca144619f5efaa357d6c287f1c669982\r\nGit Commit Branch: heads/refs/tags/v7.3.0-alpha\r\nUTC Build Time: 2023-07-31 11:44:07\r\nRust Version: rustc 1.67.0-nightly (96ddd32c4 2022-11-14)\r\nEnable Features: pprof-fp jemalloc mem-profiling portable sse test-engine-kv-rocksdb test-engine-raft-raft-engine cloud-aws cloud-gcp cloud-azure\r\nProfile: dist_release\r\n2023-08-01T21:58:35.004+0800\r\n\r\n### What operating system and CPU are you using?\r\n8c/16g\r\n\r\n### Steps to reproduce\r\n1、run tpcc with 2w warehouse and 32 thread\r\n2、inject one of tikv io hang last for 40m\r\n\r\n### What did you expect?\r\nqps can recover within 5mins\r\n\r\n### What did happened?\r\nQPS drop to zero during injection io hang to one of tikv\r\n![image](https://github.com/tikv/tikv/assets/84712107/cb50fcdd-3221-4546-80f1-057a5c0a2ee4)\r\n\r\n",
  "state": "open",
  "created_at": "2023-08-02T02:05:26Z",
  "updated_at": "2023-10-23T22:39:43Z",
  "closed_at": null,
  "labels": [
    "type/bug",
    "affects-7.5"
  ],
  "comments_data": [
    {
      "id": 1661376444,
      "user": "Lily2025",
      "created_at": "2023-08-02T02:05:36Z",
      "body": "/severity major\r\n/assign LykxSassinator"
    },
    {
      "id": 1661376469,
      "user": "ti-chi-bot[bot]",
      "created_at": "2023-08-02T02:05:38Z",
      "body": "@Lily2025: GitHub didn't allow me to assign the following users: LykxSassinator.\n\nNote that only [tikv members](https://github.com/orgs/tikv/people) with read permissions, repo collaborators and people who have commented on this issue/PR can be assigned. Additionally, issues/PRs can only have 10 assignees at the same time.\nFor more information please see [the contributor guide](https://git.k8s.io/community/contributors/guide/first-contribution.md#issue-assignment-in-github)\n\n<details>\n\nIn response to [this](https://github.com/tikv/tikv/issues/15268#issuecomment-1661376444):\n\n>/severity major\r\n>/assign LykxSassinator\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"
    },
    {
      "id": 1667594183,
      "user": "LykxSassinator",
      "created_at": "2023-08-07T10:22:14Z",
      "body": "/assign @LykxSassinator"
    },
    {
      "id": 1668963566,
      "user": "LykxSassinator",
      "created_at": "2023-08-08T06:07:42Z",
      "body": "/remove-label may-affects-5.2\r\n/remove-label may-affects-5.3\r\n/remove-label may-affects-5.4\r\n/remove-label may-affects-6.1\r\n/remove-label may-affects-6.5\r\n/remove-label may-affects-7.1"
    },
    {
      "id": 1696803100,
      "user": "Lily2025",
      "created_at": "2023-08-29T05:41:48Z",
      "body": "The phenomenon still exists"
    },
    {
      "id": 1698650496,
      "user": "LykxSassinator",
      "created_at": "2023-08-30T07:36:04Z",
      "body": "This bug can be closed as the behavior in TiKV side is what we expected.\r\n\r\nCurrently, in dynamic region (multi-rocksdb) mode, TiKV can detect this abnormal scenario and making related reports to PD side as expected. And from e-2-e metrics, PD can also correctly make `evict leader` scheduling after it receives the message.\r\n![img_v2_4bf994ba-62bc-4555-b644-18dbec9af6cg](https://github.com/tikv/tikv/assets/18441614/422a3ef1-917e-485e-b872-b25ee650ec1c)\r\n\r\n![image](https://github.com/tikv/tikv/assets/18441614/43d97304-bb6d-496d-8ae0-06d949860038)\r\n\r\nAs for the dropping of QPS although all of the leaders on the slow node has been evicted out is that:\r\n+ The TPCC workload exhibits transactional continuity, meaning that if previous transactions are slow, they can block the execution of subsequent transactions.\r\n+ The scheduling of slow nodes is not coordinated with TiDB/Client-go. Currently, our scheduling is still conducted on the PD and TiKV sides. In other words, TiKV reports its own status, PD consolidates and makes decisions based on the information, determining the presence of slow nodes and triggering scheduling. This leads to the following consequences:\r\n  -  For long-lived connections, requests that have already been sent or historical operations (SQL sent to the client and the client forwarding the request to a slow node) will continue to execute on the slow node until a timeout occurs.\r\n  - The client might not promptly perceive updates to leader information (in other words, refreshing the regionCache might not happen in time), which can result in the client still sending requests to the slow node.\r\n\r\nThese two factors can lead to delays or timeouts in the execution of certain transactions. In monitoring from the TiDB side, you can observe prolonged heartbeat delays for transactions. This indicates that transactions are still waiting for execution returns on the slow nodes.\r\n![image](https://github.com/tikv/tikv/assets/18441614/f2ff7e72-2931-46c1-9b7c-fae807e8e9fc)\r\n\r\n"
    },
    {
      "id": 1698650785,
      "user": "LykxSassinator",
      "created_at": "2023-08-30T07:36:16Z",
      "body": "It's not a block issue for release. There is still ongoing long-term work to be done:\r\n\r\n- From the TiDB/Client side: The awareness from the upstream (client side) also needs to be included, allowing clients to detect slow nodes as early as possible instead of relying on passive timeouts to ensure this.\r\n- From the TiKV side, there might also be continued efforts to advance the previously proposed coordinated solution for restarting gRPC (essentially, when PD detects a slow node, forcefully terminating all ongoing RPC requests, compelling clients to quickly redirect requests to other nodes for attempts)."
    },
    {
      "id": 1705977631,
      "user": "tonyxuqqi",
      "created_at": "2023-09-05T05:43:56Z",
      "body": "grpc restart and raft-engine hedge could help on this issue too.\r\n"
    },
    {
      "id": 1774711404,
      "user": "Lily2025",
      "created_at": "2023-10-23T08:46:13Z",
      "body": "> It's not a block issue for release. There is still ongoing long-term work to be done:\r\n> \r\n> * From the TiDB/Client side: The awareness from the upstream (client side) also needs to be included, allowing clients to detect slow nodes as early as possible instead of relying on passive timeouts to ensure this.\r\n> * From the TiKV side, there might also be continued efforts to advance the previously proposed coordinated solution for restarting gRPC (essentially, when PD detects a slow node, forcefully terminating all ongoing RPC requests, compelling clients to quickly redirect requests to other nodes for attempts).\r\n\r\ncc @cfzjywxk please concern the improvement from TiDB/Client side  cc @tonyxuqqi "
    }
  ]
}