{
  "issue_number": 10225,
  "title": "apply term should be assigned when applying snapshot",
  "body": "## Bug Report\r\n\r\n<!-- Thanks for your bug report! Don't worry if you can't fill out all the sections. -->\r\n\r\n### What version of TiKV are you using?\r\nv5.0.1, though v4.0.x should also be affected.\r\n\r\n### What operating system and CPU are you using?\r\nDoesn't matter.\r\n\r\n### Steps to reproduce\r\nSuppose a region contains 3 replicas a, b and c, a is leader, c is isolated and has not been initialized. a and b decides to add a new replica d, and promote it as voter before it's initialized. After a has applied to last log, it sends a snapshot to d and initialize it with latest data. And then a transfer leader to d. Right after d wins the election, c is recovered and requests snapshot from d before d commits any entry.\r\n\r\n### What did you expect?\r\nc will be initialized properly.\r\n\r\n### What did happened?\r\nc will panic with following stack.\r\n\r\n```\r\n[2021/05/20 19:02:04.640 +00:00] [FATAL] [lib.rs:465] [\"to_commit 41 is out of range [last_index 0], raft_id: 149208, region_id: 149205\"] [backtrace=\"stack backtrace:\r\n   0: tikv_util::set_panic_hook::{{closure}}\r\n             at /tikv/components/tikv_util/src/lib.rs:464\r\n   1: std::panicking::rust_panic_with_hook\r\n             at /rustc/bc39d4d9c514e5fdb40a5782e6ca08924f979c35//library/std/src/panicking.rs:595\r\n   2: std::panicking::begin_panic_handler::{{closure}}\r\n             at /rustc/bc39d4d9c514e5fdb40a5782e6ca08924f979c35//library/std/src/panicking.rs:497\r\n   3: std::sys_common::backtrace::__rust_end_short_backtrace\r\n             at /rustc/bc39d4d9c514e5fdb40a5782e6ca08924f979c35//library/std/src/sys_common/backtrace.rs:141\r\n   4: rust_begin_unwind\r\n             at /rustc/bc39d4d9c514e5fdb40a5782e6ca08924f979c35//library/std/src/panicking.rs:493\r\n   5: std::panicking::begin_panic_fmt\r\n             at /rustc/bc39d4d9c514e5fdb40a5782e6ca08924f979c35//library/std/src/panicking.rs:435\r\n   6: raft::raft_log::RaftLog<T>::commit_to\r\n             at /rust/git/checkouts/raft-rs-42b8049ef2e3af07/91a60ce/src/raft_log.rs:252\r\n   7: raft::raft::Raft<T>::restore\r\n             at /rust/git/checkouts/raft-rs-42b8049ef2e3af07/91a60ce/src/raft.rs:2389\r\n      raft::raft::Raft<T>::handle_snapshot\r\n             at /rust/git/checkouts/raft-rs-42b8049ef2e3af07/91a60ce/src/raft.rs:2307\r\n   8: raft::raft::Raft<T>::step_follower\r\n             at /rust/git/checkouts/raft-rs-42b8049ef2e3af07/91a60ce/src/raft.rs:2135\r\n      raft::raft::Raft<T>::step\r\n             at /rust/git/checkouts/raft-rs-42b8049ef2e3af07/91a60ce/src/raft.rs:1440\r\n   9: raft::raw_node::RawNode<T>::step\r\n             at /rust/git/checkouts/raft-rs-42b8049ef2e3af07/91a60ce/src/raw_node.rs:378\r\n      raftstore::store::peer::Peer<EK,ER>::step\r\n             at /tikv/components/raftstore/src/store/peer.rs:1139\r\n      raftstore::store::fsm::peer::PeerFsmDelegate<EK,ER,T>::on_raft_message\r\n             at /tikv/components/raftstore/src/store/fsm/peer.rs:1228\r\n  10: raftstore::store::fsm::peer::PeerFsmDelegate<EK,ER,T>::handle_msgs\r\n             at /tikv/components/raftstore/src/store/fsm/peer.rs:484\r\n  11: <raftstore::store::fsm::store::RaftPoller<EK,ER,T> as batch_system::batch::PollHandler<raftstore::store::fsm::peer::PeerFsm<EK,ER>,raftstore::store::fsm::store::StoreFsm<EK>>>::handle_normal\r\n             at /tikv/components/raftstore/src/store/fsm/store.rs:829\r\n  12: batch_system::batch::Poller<N,C,Handler>::poll\r\n             at /tikv/components/batch-system/src/batch.rs:295\r\n  13: batch_system::batch::BatchSystem<N,C>::spawn::{{closure}}\r\n             at /tikv/components/batch-system/src/batch.rs:399\r\n      std::sys_common::backtrace::__rust_begin_short_backtrace\r\n             at /rustc/bc39d4d9c514e5fdb40a5782e6ca08924f979c35/library/std/src/sys_common/backtrace.rs:125\r\n  14: std::thread::Builder::spawn_unchecked::{{closure}}::{{closure}}\r\n             at /rustc/bc39d4d9c514e5fdb40a5782e6ca08924f979c35/library/std/src/thread/mod.rs:474\r\n      <std::panic::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once\r\n             at /rustc/bc39d4d9c514e5fdb40a5782e6ca08924f979c35/library/std/src/panic.rs:322\r\n      std::panicking::try::do_call\r\n             at /rustc/bc39d4d9c514e5fdb40a5782e6ca08924f979c35/library/std/src/panicking.rs:379\r\n      std::panicking::try\r\n             at /rustc/bc39d4d9c514e5fdb40a5782e6ca08924f979c35/library/std/src/panicking.rs:343\r\n      std::panic::catch_unwind\r\n             at /rustc/bc39d4d9c514e5fdb40a5782e6ca08924f979c35/library/std/src/panic.rs:396\r\n      std::thread::Builder::spawn_unchecked::{{closure}}\r\n             at /rustc/bc39d4d9c514e5fdb40a5782e6ca08924f979c35/library/std/src/thread/mod.rs:473\r\n      core::ops::function::FnOnce::call_once{{vtable.shim}}\r\n             at /rustc/bc39d4d9c514e5fdb40a5782e6ca08924f979c35/library/core/src/ops/function.rs:227\r\n  15: <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once\r\n             at /rustc/bc39d4d9c514e5fdb40a5782e6ca08924f979c35/library/alloc/src/boxed.rs:1484\r\n      <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once\r\n             at /rustc/bc39d4d9c514e5fdb40a5782e6ca08924f979c35/library/alloc/src/boxed.rs:1484\r\n      std::sys::unix::thread::Thread::new::thread_start\r\n             at /rustc/bc39d4d9c514e5fdb40a5782e6ca08924f979c35//library/std/src/sys/unix/thread.rs:71\r\n  16: start_thread\r\n  17: clone\r\n\"] [location=/rust/git/checkouts/raft-rs-42b8049ef2e3af07/91a60ce/src/raft_log.rs:252] [thread_name=raftstore-1-1]\r\n```\r\n\r\nAfter v4.0.0, snapshot are generated using apply index and apply term from apply worker. Apply worker initializes these fields when a peer fsm has applied the snapshot. But applying snapshot only updates apply index and leave apply term untouched, so apply worker will set apply term to a stale value until it applies next entry. If a snapshot is generated at that time, the snapshot will be set to a wrong log term.\r\n\r\nBecause raft-rs return 0 for any term query on entries beyond its logs, so term check can succeed. Receiver will fast-forward the snapshot and commit the index. Hence panic as receiver has no such log at all. Generally, PD only promotes initialized learner to voter, so the learner has to apply at least one log before it starts to campaign, apply term will be assigned to correct value before sending snapshots.\r\n\r\nBut if a long time isolated follower becomes leader right after accepting snapshot, it can still generate snapshots with wrong metadata. In this case, the stale term may not be zero and receiver can apply the data with wrong metadata. Though receiver should be able to find conflict when accepting next empty entry and request a new snapshot again. There seems to be no harm except wrong metadata itself.",
  "state": "closed",
  "created_at": "2021-05-20T20:21:25Z",
  "updated_at": "2021-12-01T08:28:13Z",
  "closed_at": "2021-06-29T19:03:26Z",
  "labels": [
    "type/bug",
    "sig/raft",
    "severity/critical",
    "affects-4.0",
    "affects-5.0",
    "affects-5.1"
  ],
  "comments_data": [
    {
      "id": 850040602,
      "user": "cosven",
      "created_at": "2021-05-28T01:07:43Z",
      "body": "/severity critical"
    }
  ]
}