{
  "issue_number": 14224,
  "title": "pipe fd leaks in tikv-6.1.0 with cpu profiler running",
  "body": "## Bug Report\r\n\r\n<!-- Thanks for your bug report! Don't worry if you can't fill out all the sections. -->\r\n### The problem description\r\n+ **See **\"What did you expect\"** and **\"What did happen\"** sections**\r\n\r\n### What version of TiKV are you using?\r\n<!-- You can run `tikv-server --version` -->\r\ntikv-6.1.0\r\n### What operating system and CPU are you using?\r\n<!-- If you're using Linux, you can run `cat /proc/cpuinfo` -->\r\nCentOS-7, Linux-3.10.0-514.16.1.el7.x86_64\r\n### Steps to reproduce\r\n<!-- If possible, provide a recipe for reproducing the error. A complete runnable program is good. -->\r\n\r\nNote: If one finds this section difficult to read, he/she might go to **\"Conclusion - Root cause\"** and **\"Explanation of the root cause\"** sections and then return to this one.\r\n\r\n#### Basic Environment in tidb/tikv.\r\n**(1) quite a certain amount of write requests on the tidb cluster;**\r\n(This is to trigger frequent rocksdb compaction)\r\n**(2) configuration rocksdb.max-sub-compactions >= 2;**\r\n(Enable parallel subcompactions from L0 to deeper-level, in order to create `short-term` threads.)\r\n**(3) promethus for `tidb-6.1.0` is running, with `continuous-profiling.enable = true`;**\r\n(step-(3) is optional, see Way-2)\r\n\r\n#### Two ways to reproduce:\r\nBased on the Basic Environment, two ways are proposed;\r\n+ **Way-1. In a tidb/tikv-6.1.0 cluster, with the following conditions:**\r\n**(1) Deploy promethus servers(including ng-monitoring-server) and Keep them running;**\r\n**(2) In at least one ng-monitoring-server, set continuous-profiling.enable = true (by default in 6.1.0);**\r\n(step-(2) is to activate the cpu profiling in tikv from promethus.)\r\nThis is the kind of environment in which we met the case and took troubleshooting to find the root cause.\r\n\r\n**Note: Way-1 can be generalized as Way-2, in which promethus is not requied but just the tidb/tikv/pd components.**\r\n\r\n+ **Way-2. A more general way is to send http requests to tikv-status-server (6.1.0) with conditions:**\r\n**(1) Make sure that tidb/tikv servers receive quite a certain amount of write requests from user/sysbench;**\r\n**(2) Send http requests to tikv-status-server with uri \"/debug/pprof/profile\";**\r\n\r\nfor example, if the tikv status-server port is 20180 (by default), execute the following shell command on a tikv host:\r\n```\r\n#### Activate the cpu profiler in tikv once.\r\n#### One can execute this multiple times.\r\n#f=99; time curl  -X GET  \"http://127.0.0.1:20180/debug/pprof/profile?seconds=10&frequency=${f}\"`; echo \"response-length: `echo -e \\\"${out}\\\" | wc -c`\";\r\n```\r\n\r\nAt the same time, one can monitor the count of pipe fd(s) opened in tikv-server:\r\n```\r\n#### commands for monitoring the number of pipe fds for a period:\r\n#### You should change the pid for tikv-server. You could redirect the output into a file.\r\n#pid=13579; while true; do dt=`date '+%Y-%m-%dT%H:%M:%S'`; echo \"======== ${dt} ========\"; ls  -l  /proc/${pid}/fd | grep -c 'pipe'; sleep 0.8; done\r\n```\r\n\r\n### What did you expect?\r\nWe are running tidb clusters with quite a certain amount of QPS/TPS, in which tikv plays an important role (also with promethus servers running). One tidb cluster, including all the components such as tidb/tikv/pd/promethus/..., was upgraded to v6.1.0 from v5.4.1.\r\n### What did happen?\r\nOne day, one instance in the `tikv-6.1.0` cluster (3 nodes) crashed with **`\"Panic ... Too many open files\"`** error in `tikv.log`. And we found **fd leaks** on all the three tikv-6.1.0 instances in the cluster, where the leaked fds were **(anonymous) pipes**.\r\n\r\ntikv-server would crash once the number of opened fd(s) in tikv is over the limit (ulimit -f) due to continuous fd leaks.\r\n\r\nSo the chain is:\r\n**`tikv-6.1.0-crashes (panic) <== \"too many open files\" error <== open files over limit <== anonymous pipe fd leaks in tikv-6.1.0.`**\r\n\r\n#### Some detail in our case\r\nThe speed of pipe fd leaks is about ***1000 per hour***, which in other words is about _20,000 ~ 25,000 fd(s) per day_.\r\nThe open file limit (got by `ulimit -f`) is _1,000,000 (1 million)_. \r\nSo a `tikv-6.1.0` instance would crash due to the above error in about _50 days_.\r\n\r\nActually, we found that the tikv instance crashed 54 days after its upgrading from 5.4.1 to 6.1.0.\r\nAnd this indicated possible relation between the upgrading to tikv-6.1.0 and pipe fd leaks.\r\n\r\n### Conclusion - Root cause\r\n\r\nIn simplicity, the root cause in tikv is that\r\n+ **`the pprof module used in tikv-6.1.0 causes pipe fd leaks during cpu profiling with short-term threads.`**\r\n> Short-term threads here means the threads created to do their jobs and then exit in the middle of the owner process life-time.\r\n\r\nThe cause in pprof-rs is that\r\n+ **`pprof integrated in tikv would leave the fds opened in short-term threads after they exit.**\r\n\r\n+ To see the details of pprof, please refer to this issue:\r\ntikv/pprof-rs#197\r\n\r\n#### others to say\r\n+ `Logically, no such problem in tikv-5.4.1`? (the version before upgrading to 6.1.0).\r\nIt is supposed to be **_NO_** (no such problem). We read the relevent code, but we did not test it yet.\r\n\r\n### Possible solutions\r\n+ Use tikv version other than 6.1.0 or any one without this problem. (deploy-level)\r\n+ In promethus (deploy-level), configure ng-monitoring-server such that continuous-profiling.enable = false and restart it, i.e. disable the cpu profiling.\r\n(I have no resource to build a new environment right now to test this. Anyway, this is probably a workaround)\r\n+ In tikv (project-level), use a pprof-rs version that does not introduce addr_validate module or do not use `frame-pointer` feature.\r\n+ Wait for the issue to be solved. (source-code-level)\r\ntikv/pprof-rs#197\r\n",
  "state": "closed",
  "created_at": "2023-02-14T12:10:20Z",
  "updated_at": "2023-04-26T05:37:53Z",
  "closed_at": "2023-04-26T05:37:53Z",
  "labels": [
    "type/bug",
    "severity/major",
    "affects-6.1",
    "affects-6.2",
    "affects-6.3",
    "affects-6.4",
    "affects-6.5",
    "affects-6.6",
    "affects-7.0",
    "affects-7.1"
  ],
  "comments_data": [
    {
      "id": 1430706340,
      "user": "alston111111",
      "created_at": "2023-02-15T03:41:50Z",
      "body": "### Explanation of the case and root cause\r\nNote: the term tikv / tikv-server / tikv-status-server / tikv-instance below mean the same thing: A tikv process.\r\n\r\n```\r\n    tidb-servers\r\n        ||\r\n        ||\r\n        \\/\r\n    tikv-servers (and pd-servers)\r\n        /\\\r\n        ||\r\n        || activate pprof module in tikv\r\n        || and do cpu profiling.\r\n        ||\r\n    promethus' ng-monitoring-server(s)\r\n```\r\n\r\nNow we demonstrate the case in the logical order.\r\n\r\n+ **promethus to tikv**\r\n\r\nIn our case, `promethus servers` are deployed in a `tidb-6.1.0` cluster. The `ng-monitoring servers`, as one component in promethus, would send http requests to tikv-(status-)servers periodically (once per minute) to start cpu profiling.\r\n\r\nIn tikv-6.1.0, `continous-profiling.enable = true` by default, while `false` in tikv-5.4.1. This is why only after the upgrading we encountered this problem.\r\n\r\n+ **tikv starts its cpu profiler**\r\n\r\nWhen a tikv instance receives the http request, it starts the cpu profiler in pprof module by do two things:\r\n**(1) `setitimer(ITIMER_PROF, .../* frequency of 99% by default */)`, activate periodical signal sending;**\r\n**(2) `signal(SIGPROF, perf_signal_handler)`, hook a signal handler to SIGPROF;**\r\n\r\nIn this way, OS kernel would send a `SIGPROF` signal to tikv-server about once every 1.01ms and the `perf_signal_handler` function would be called in Tikv-server on a CPU, say CPU-X, which is selected to receive and handle the signal.\r\n\r\n+ **cpu profiling and SIGPROF handler**\r\n\r\nIn `perf_signal_handler` function (in pprof module), `pprof` would extract the call stack in user space of the thread (say thread THD-T) currently running on CPU-X. During extracting each frame on the call stack, pprof for tikv-6.1.0 uses a feature called `frame-pointer` to validate the frame pointer address.\r\n\r\n+ **address validation and pipe opening**\r\n\r\nThe validation sub-module in pprof would do the work by opening an anonymous pipe whose two fds are stored in thread-local variable `MEM_VALIDATE_PIPE`. Once the pipe for a thread is invalid, pprof would close the two old fds first and then re-open a new pipe and store the two new fds into `MEM_VALIDATE_PIPE`.\r\n\r\nSo there should be at most two fds opened for each thread in a tikv-server. Thus the number of pipe fds opened in a tikv-server should be at most about (2 * \\#threads).\r\n\r\nIn our case, **\\#threads is about 350 which indicates at most ~700 pipe fds opened.**\r\n\r\nHowever, it used to reach the limit **1 million**. So here comes the following section.\r\n\r\n+ **But where do leaked fd(s) come from?**\r\n\r\nMany/Most of the threads running in tikv are `long-term` which are created and maintained in some thread-pool(s). Their life-times are the same as the tikv process. However, `short-term` threads are created, do their job and then exit, all the way. **`Leaked fd(s)` would be opend but not closed for `short-term` threads after their exit. And `short-term` threads are created constantly in tikv, such as threads for `(e.g. L0) subcompactions` in rocksdb.** So this results in continuous increasing of `#opened-fds`.\r\n\r\n+ **Typical short-term threads in tikv/rocksdb**\r\n\r\nActually in our case, we found that almost all leaked fd(s) came from threads named `rocksdb:lowN`. This was captured by using `stap` to print execname() in the probe of `sys_pipe2` syscall , after we found the type of leaked fd(s) is anonymous pipe.\r\n> rocksdb:lowNs are the names of the threads in rocksdb's low-priority thread-pool.\r\n\r\nDetails are here:\r\nOnce a compaction from L0 to a deeper level is triggered with at least two input files \\@ L0, then new threads for subcompactions would be created in the current compaction thread, say named `rocksdb:low3` for example. The `long-term` thread named `rocksdb:low3` should be in the low-priority thread pool of rocksdb. However, the short-term threads created for subcompactions by thread `rocksdb:low3` would derive the thread name `rocksdb:low3`. This is why nearly all interesting threads calling `sys_pipe2` are named in the form of `rocksdb:lowN`.\r\n\r\n(Almost all the long-term threads are created at the start of tikv, so almost what can be captured about `sys_pipe2` and `sys_clone` during tikv running are short-term threads.)\r\n\r\n+ **That's it!**\r\n\r\n#### Summary\r\n1. promethus -- ng-monitoring-server -- continuous-profiling.{enable = true, interval = 60s, seconds = 10s}\r\n2. ng-monitoring-server --> tikv-status-server with http requests @ \"/debug/pprof/profile?seconds=SECONDS\", once every minute.\r\n3. requests -> tikv ==> tikv starts cpu profiler and takes profiling for SECONDS seconds.\r\n4. OS kernel sends SIGPROF signal to tikv-server once every 1.01ms.\r\n5. SIGPROF -> tikv-server ==> the thread running on a CPU is interrupted and perf_signal_handler() in pprof::profiler module is called.\r\n6. perf_signal_handler: extracts frames on the call stack of the thread.\r\n7. During extracting each frame, pprof validates its validity by addr_validate::validate().\r\n8. During the memory validation of the frame pointer, a pipe would be opened and the fds are stored in thread-local MEM_VALIDATE_PIPE.\r\n9. Short-term threads are created to do their jobs and then exit, leaving pipe fds opened.\r\n10. In tikv, typical short-term threads are the ones created for parallel subcompactions in rocksdb.\r\n11. The more writes on tidb/tikv servers, the more probability that rocksdb would do compactions and short-term threads for subcompactions would be created.\r\n12. So the short-term threads named like `rocksdb:lowN` would leave pipe fds opened after they exit, which is the fd leaks.\r\n"
    },
    {
      "id": 1430801199,
      "user": "Connor1996",
      "created_at": "2023-02-15T06:05:03Z",
      "body": "Thanks for your reporting! By the way, do you observe increasing memory usage with pipe fd leaking?"
    },
    {
      "id": 1430998214,
      "user": "alston111111",
      "created_at": "2023-02-15T09:19:39Z",
      "body": "> By the way, do you observe increasing memory usage with pipe fd leaking?\r\n\r\nWell, I did not pay attention to the changes of the memory usage. And no resource now to build an environment to test it. So I'm not sure about that.\r\nBut I think it's yes, because memory would be allocated for opened pipes in OS kernel.\r\n\r\n+ Memory exhausted?\r\nSince in the case, the limit of #open-files is 1 million, thus the memory allocated was about (1M/2)* mem-per-pipe. Even if mem-per-pipe is 1KB or a few KBs, the increased memory size for pipes is only a few GBs which is much less than the host memory limit (several tens of GBs or 128GB or sth.). So no alerts on memory usage or OOM occurred.\r\n\r\nAnyway, it's just that \"too many open files\" error was encountered first. "
    }
  ]
}