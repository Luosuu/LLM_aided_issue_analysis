{
  "issue_number": 16809,
  "title": "upstream and downstream data are inconsistent after running some fault test and then pitr restore and checksum",
  "body": "## Bug Report\r\n\r\n<!-- Thanks for your bug report! Don't worry if you can't fill out all the sections. -->\r\n\r\n### What version of TiKV are you using?\r\n./tikv-server -V\r\n TiKV \r\nRelease Version:   8.1.0-alpha\r\nEdition:           Community\r\nGit Commit Hash:   d5e1ce68bb7d7782d54276dbba3a7966072e1a5d\r\nGit Commit Branch: heads/refs/tags/v8.1.0-alpha\r\nUTC Build Time:    2024-04-09 11:37:34\r\nRust Version:      rustc 1.77.0-nightly (89e2160c4 2023-12-27)\r\nEnable Features:   pprof-fp jemalloc mem-profiling portable sse test-engine-kv-rocksdb test-engine-raft-raft-engine trace-async-tasks openssl-vendored\r\nProfile:           dist_release\r\n2024-04-11T12:13:36.939+0800\r\n\r\n### What operating system and CPU are you using?\r\n8c/32g\r\n\r\n### Steps to reproduce\r\n1、br restore data to upstream and downstream \r\n2、start pitr\r\n3、run sysbench\r\n4、run some chaos，such as pd fault、tikv fault、tidb fault、s3 fault\r\n6、br log restore\r\n5、compare data consistency between upstream and downstream\r\n\r\n### What did you expect?\r\nthe upstream and downstream data are consistent\r\n\r\n### What did happened?\r\nthe upstream and downstream data are inconsistent\r\n\r\nsync diff logs：\r\n[summary.txt](https://github.com/tikv/tikv/files/14944835/summary.txt)\r\n[sync_diff.tar.gz](https://github.com/tikv/tikv/files/14944870/sync_diff.tar.gz)\r\n\r\n\r\n",
  "state": "closed",
  "created_at": "2024-04-11T10:33:41Z",
  "updated_at": "2024-04-17T10:04:38Z",
  "closed_at": "2024-04-17T10:04:38Z",
  "labels": [
    "type/bug",
    "component/backup-restore",
    "severity/critical",
    "affects-6.5",
    "affects-7.1",
    "affects-7.5",
    "affects-8.1"
  ],
  "comments_data": [
    {
      "id": 2049402054,
      "user": "Lily2025",
      "created_at": "2024-04-11T10:36:52Z",
      "body": "/assign YuJuncen\r\n/component backup-restore"
    },
    {
      "id": 2051238042,
      "user": "Lily2025",
      "created_at": "2024-04-12T08:08:07Z",
      "body": "/severity critical "
    },
    {
      "id": 2054995483,
      "user": "YuJuncen",
      "created_at": "2024-04-15T04:28:10Z",
      "body": "This is because we have two concurrent procedures:\r\n- one for advancing the progress (resolved ts, or watermark), this is a daemon and it periodically fetch ts from PD.\r\n- one for pushing the files to the downstream.\r\n\r\nOnce we are going to \"flush\" a batch of backup to the external storage, **the background progress may concurrently advance the progress, with data not in the batch to be flushed.** Hence we advanced the watermark not properly. \r\n\r\nIn normal cases, this won't hurt much. They will be sent to the external storage in next flush. But if:\r\n- TiKV restarts.\r\n- The TiKV restarted is who defined the current global checkpoint ts. (i.e. where the region with lowest checkpoint ts is.)\r\n\r\nThe data used to calculate the progress but not yet flushed will be lost."
    },
    {
      "id": 2055107608,
      "user": "YuJuncen",
      "created_at": "2024-04-15T04:50:29Z",
      "body": "We should \"freeze\" the progress firstly when we are about to do flush. That is, we will change the `CheckpointManager` with:\r\n\r\n- All normal `resolve` will update the `resolved_ts`(progress not yet frozen).\r\n- When we are going to flush, call a new API `freeze` after we fetch the `min_ts`(before requeue the `min_ts`). This API will move the current `resolved_ts` to `checkpoint_ts`(the frozen progress).\r\n- When flush done, we can \"merge\" the resolved result generated by the flush (the latest progress of all current leaders) with the frozen progress (maybe stale progress of all observed regions).\r\n  - Also, before generate the resolve result for flushing, we need to sync the endpoint's queue to make sure all writes are recorded."
    }
  ]
}