{
  "issue_number": 17278,
  "title": "Reduce `apply_ctx.commit(self)` if there are ingest SST commands applying",
  "body": "## Enhancement requirement\r\n\r\nRecently, when we tested the ingest sst performance, we observed that the number of batch ingest sst was mostly low. \r\nAs shown in the figure, the left panel shows the total number of all ingest sst in an apply batch poll, which is the ideal goal; the right panel shows the actual number of ingest sst each time it is flushed to disk by `write_to_db`.\r\n![image](https://github.com/user-attachments/assets/b74bbfb3-3328-4fff-940c-9b44109e7867)\r\n\r\nWe added some monitoring indicators and found two places where the applied but unflushed data would be persist to the disk **midway**. \r\n![image](https://github.com/user-attachments/assets/6d279c73-e36b-4c97-a0db-59be18cd237a)\r\n\r\n1. the left panel recorded the number of `apply_ctx.pending_ssts` ingested which is triggered by `apply_ctx.commit(self)`:\r\nhttps://github.com/tikv/tikv/blob/0b97a39d520afcd82a02d0f5c0966d6ccab2cd74/components/raftstore/src/store/fsm/apply.rs#L1251-L1269\r\n\r\n2. the right panel recorded the number of `apply_ctx.pending_ssts` ingested which is triggered by `apply_ctx.commit(self)`:\r\nhttps://github.com/tikv/tikv/blob/0b97a39d520afcd82a02d0f5c0966d6ccab2cd74/components/raftstore/src/store/fsm/apply.rs#L1270-L1275\r\n\r\n### For Right Panel: Directly remove the `apply_ctx.commit(self)`\r\nLet's start with the right panel. To reduce this `apply_ctx.commit(self)`, we prefer to directly remove it. Here are the reason:\r\nWhen the process run here, the result of `apply_ctx.kv_wb().should_write_to_engine()` must be false (otherwise `apply_ctx.commit(self)` has been triggered because of the left panel). Therefore, it doesn't need to **immediately** persist the applied but unflushed data(which also includes other delegates'). We don't know what the benefit of calling `apply_ctx.commit(self)` immediately is, because we think, in any case, the delegate will wait until the current batch poll is finished(end() done) before being rescheduled to the poller whose priority is equal to the delegate's priority. \r\n\r\n### For Left Panel: Reschedule the command with normal priority to the normal priority poller when delegate has pending sst if possible\r\nWe also added another monitoring indicator and it showed that most of `apply_ctx.commit(self)` are triggered by the condition `has_unflushed_data && should_write_to_engine(!apply_ctx.kv_wb().is_empty(), &cmd)`.\r\nWhen the process called `apply_ctx.commit(self)` and make the `apply_ctx.pending_ssts` ingested, we can infer that the current poller is low priority, and the `unflushed data` is **normal priority**! (well, maybe it is `Delete Range`, but actually there is no `Delete Range` happened in our test), because if the `unflushed data` is an `Ingest` command, the current entry will be yield because\r\nhttps://github.com/tikv/tikv/blob/0b97a39d520afcd82a02d0f5c0966d6ccab2cd74/components/raftstore/src/store/fsm/apply.rs#L1246-L1250\r\n\r\nSo we wonder why the normal priority command was applied by the low priority poller. Finally, we found the delegate didn't set itself's priority to Normal before calling `handle_raft_committed_entries`.\r\nhttps://github.com/tikv/tikv/blob/0b97a39d520afcd82a02d0f5c0966d6ccab2cd74/components/raftstore/src/store/fsm/apply.rs#L4160-L4170\r\nwhich is different from\r\nhttps://github.com/tikv/tikv/blob/0b97a39d520afcd82a02d0f5c0966d6ccab2cd74/components/raftstore/src/store/fsm/apply.rs#L4064-L4069\r\n\r\nLet's assume a scenario like this for one delegate which has a msg receiver containing [L1, N, L2, ...] (L indicates an low priority **Ingest** command and N indicates an normal priority command. Notice that we don't discuss other low priority command (`Delete Range`) for now).\r\n#### Deal with L1: low priority command **Ingest**\r\n1. Now the normal priority poller meets the delegate, and then starts to calls `handle_normal` for the delegate. Then the delegate finds the low priority priority command Ingest, so it will set itself's priority to `Priority::Low`. \r\nhttps://github.com/tikv/tikv/blob/0b97a39d520afcd82a02d0f5c0966d6ccab2cd74/components/raftstore/src/store/fsm/apply.rs#L1243-L1245\r\n\r\n2. But the current poller is normal priority, so it will be yield.\r\nhttps://github.com/tikv/tikv/blob/0b97a39d520afcd82a02d0f5c0966d6ccab2cd74/components/raftstore/src/store/fsm/apply.rs#L1270-L1275\r\n\r\n3. After this batch poll is end, the delegate will be reschedule to the low priority poller.\r\n\r\n4. Now the low priority poller meets the delegate, and then starts to calls `handle_normal` for the delegate. Then the delegate deals with the yield pending entries.\r\nhttps://github.com/tikv/tikv/blob/0b97a39d520afcd82a02d0f5c0966d6ccab2cd74/components/raftstore/src/store/fsm/apply.rs#L4160-L4170\r\n\r\n5. The L1 Ingest entry is succeeded to be applied, so now `delegate.has_pending_sst` is `true` and `delegate.priority` is `Priority::Low`.\r\n\r\n#### Deal with N: normal priority command\r\n6. The delegate starts to deal with the next entry, which is a normal priority command. It will be yield just because `delegate.has_pending_sst` is `true`.\r\nhttps://github.com/tikv/tikv/blob/0b97a39d520afcd82a02d0f5c0966d6ccab2cd74/components/raftstore/src/store/fsm/apply.rs#L1246-L1250\r\n\r\n7. After this batch poll is end, the delegate won't be reschedule to another priority poller because `delegate.priority` is still `Priority::Low`. Besides, because the pending ssts are ingested so the `delegate.has_pending_sst` is `false` now.\r\n\r\n8. Now the low priority poller continues to deal with the delegate, and then starts to call `handle_normal` for the delegate. Then the delegate deals with the yield pending entries.\r\nhttps://github.com/tikv/tikv/blob/0b97a39d520afcd82a02d0f5c0966d6ccab2cd74/components/raftstore/src/store/fsm/apply.rs#L4160-L4170\r\n\r\n9. The N entry is succeeded to be applied, so now `delegate.has_pending_sst` is `false` and `delegate.priority` is `Priority::Low`.\r\n\r\n#### Deal with L2: low priority command **Ingest**\r\n10. The delegate starts to deal with the next entry, which is a low priority command Ingest. The delegate finds that there is  unflushed data so it will call `apply_ctx.commit(self)`, which immediately ingests the pending ssts.\r\nhttps://github.com/tikv/tikv/blob/0b97a39d520afcd82a02d0f5c0966d6ccab2cd74/components/raftstore/src/store/fsm/apply.rs#L1251-L1269\r\n\r\nFrom the step 7, the normal priority command is yield because `delegate.has_pending_sst` is `true`. It actually can be rescheduled to the normal priority poller **by the way**. However, the `deletegate.priority` isn't changed and is still be `Priority::Low`. `Reschedule to the normal poller` or `still run the normal command in the low priority poller which still has many heavy other low priority commands from other delegates`? We think it is inevitable to choose the former. (What if the yield pending entries are [L, N, N, N, N*n,....]?)",
  "state": "closed",
  "created_at": "2024-07-16T11:02:27Z",
  "updated_at": "2024-08-13T12:52:34Z",
  "closed_at": "2024-08-13T12:52:33Z",
  "labels": [
    "type/bug",
    "component/backup-restore",
    "severity/moderate"
  ],
  "comments_data": [
    {
      "id": 2236147965,
      "user": "glorv",
      "created_at": "2024-07-18T10:18:23Z",
      "body": "I think the reason that we do handle normal priority messages in the low-priority thread is to reduce the latency of normal messages. While the currently apply fsm is running in the low-priority thread pool, if we directly trigger a yield(and then reschedule) when find a high priority task, it will need so time the actually apply this fsm again(in the high priority pool) after this round of pool is finished, this will introduce some latency. So, as we always perfer to ensure the latency of high-priority tasks, so just continue to handle these tasks in the current thread is our choice. \r\n\r\nBTW, to optimize you scenario, I think:\r\n- Remove the eager `commit` you mensioned in step 2 is doable. This commit is not useful as an implicit commit while be triggered at the end of this turn.\r\n- The currently `has_unflushed_data` check is not accurate, ideally, we should only trigger a commit if there is unflushed kv data whose range is conflict with the incoming sst.  So you may consider optimize this check so if there are only sst ingestion, eager flushed will not be triggered."
    },
    {
      "id": 2238076730,
      "user": "Leavrth",
      "created_at": "2024-07-19T04:08:34Z",
      "body": "> While the currently apply fsm is running in the low-priority thread pool, if we directly trigger a yield(and then reschedule) when find a high priority task, it will need so time the actually apply this fsm again(in the high priority pool) after this round of pool is finished, this will introduce some latency.\r\n\r\nIn our test scenario, one thing to note is that\r\n\r\n> From the step 7, the normal priority command is yield because delegate.has_pending_sst is true. It actually can be rescheduled to the normal priority poller by the way.\r\n\r\nThe following is the process of argumentation for the scenario [Ingest, N, Ingest, ...] (The command `Delete Range` is ignored):\r\n```\r\nSuppose: there is a normal priority command <N> is applied by the low priority poller.        (1)\r\n\r\nFrom (1) we can infer that the normal priority command <N> is from delegate's yield_pending_entries. See [1].      (2)\r\nOtherwise, it will be yield because \r\n1. if the first entry is the normal priority command <N>, the delegate's priority will be set to Priority::Normal, and the command is yield because priority mismatch. See [2].     (3)\r\n2. if the first entry is the low priority command <Ingest>, the delegate's has_pending_sst will be set to true, and the next entry <N> will be yield because the delegate has pending ssts. See [3].      (4)\r\n\r\nFor (3), the delegate can be rescheduled to a normal priority poller.       (end 5)\r\nFor (4), the delegate cannot be rescheduled to a normal priority poller because the delegate's priority is set to Priority::Low by the first entry <Ingest>.    (end 6)\r\n\r\nFor (2), there are 2 situations:\r\n1. if the first pending entry is not the normal priority command <N>, but is <Ingest> instead, it is the same as (4).       (4)\r\n2. if the first pending entry is the normal priority command <N>, we can infer that the command <N> has been yield.     (7)\r\n\r\nFor (7), the delegate, which has the yield command <N>, couldn't be rescheduled to a normal priority poller.       (end 8)\r\n\r\nTherefore, the behavior (end 5) is different from (end 6) and (end 8). We think for (end 6) and (end 8), the delegate should be rescheduled to normal priority poller.\r\n\r\nActually, if the delegate can be reschedule to normal poller in (end 6) and (end 8), the first commit also can be skipped.\r\n```\r\n\r\n\r\n### Code Reference\r\n[1]\r\nhttps://github.com/tikv/tikv/blob/0b97a39d520afcd82a02d0f5c0966d6ccab2cd74/components/raftstore/src/store/fsm/apply.rs#L4160-L4170\r\n\r\n[2]\r\nhttps://github.com/tikv/tikv/blob/0b97a39d520afcd82a02d0f5c0966d6ccab2cd74/components/raftstore/src/store/fsm/apply.rs#L4064-L4069\r\n\r\n[3]\r\nhttps://github.com/tikv/tikv/blob/0b97a39d520afcd82a02d0f5c0966d6ccab2cd74/components/raftstore/src/store/fsm/apply.rs#L1246-L1250\r\n\r\n"
    },
    {
      "id": 2238156203,
      "user": "Leavrth",
      "created_at": "2024-07-19T04:52:33Z",
      "body": "Besides, the scenario [Ingest, N, Ingest, ...], can be wrapped by BR client into [{Ingest, Ingest}, N, ...], and then the first commit doesn't need to modified."
    }
  ]
}