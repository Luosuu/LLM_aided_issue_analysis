{
  "issue_number": 17469,
  "title": "raftstore: TiKV panics due to self was destroyed from meta during receiving snapshot",
  "body": "## Bug Report\r\n\r\n<!-- Thanks for your bug report! Don't worry if you can't fill out all the sections. -->\r\n\r\n### What version of TiKV are you using?\r\nv6.5.6\r\n### What operating system and CPU are you using?\r\n`aarch64-unknown-linux-gnu`\r\n\r\n### Steps to reproduce\r\nRun a normal cluster. With adding / removing peers.\r\n\r\n### What did you expect?\r\nTiKV shouldn't panic.\r\n\r\n### What did happened?\r\nTiKV panicked with the following stack:\r\n```\r\n   0: tikv_util::set_panic_hook::{{closure}}\r\n             at /home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tikv/compo nents/tikv_util/src/lib.rs:508:18\r\n   1: <alloc::boxed::Box<F,A> as core::ops::function::Fn<Args>>::call\r\n             at /rust/toolchains/nightly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:2032 :9\r\n      std::panicking::rust_panic_with_hook\r\n             at /rust/toolchains/nightly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:692:13\r\n   2: std::panicking::begin_panic_handler::{{closure }}\r\n             at /rust/toolchains/nightly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:579:13\r\n   3: std::sys_common::backtrace::__rust_end_short_backtrace\r\n             at /rust/toolchains/n ightly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys_common/backtrace.rs:137:18\r\n   4: rust_begin_unwind\r\n             at /rust/toolchains/nightly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust /library/std/src/panicking.rs:575:5\r\n   5: core::panicking::panic_fmt\r\n             at /rust/toolchains/nightly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panicking.rs:65:14\r\n   6: core::panicking::panic_ display\r\n             at /rust/toolchains/nightly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panicking.rs:138:5\r\n   7: core::panicking::panic_str\r\n             at /rust/toolchains/nightly-2022-11-15-aarch 64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panicking.rs:122:5\r\n   8: core::option::expect_failed\r\n             at /rust/toolchains/nightly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/option .rs:1879:5\r\n   9: core::option::Option<T>::expect\r\n             at /rust/toolchains/nightly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/option.rs:741:21\r\n      <std::collections::hash::map::HashMap<K,V,S> as core::ops::index::Index<&Q>>::index\r\n             at /rust/toolchains/nightly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/collections/hash/map.rs:1340:9\r\n      raftstore::store::fsm::peer::PeerFsmDelegat e<EK,ER,T>::check_snapshot\r\n             at /home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tikv/components/raftstore/src/store/fsm/peer.rs:3103:12\r\n  10: raftstore::store::fsm::peer::PeerFsmDelegate<EK,ER,T>::on_ra ft_message\r\n             at /home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tikv/components/raftstore/src/store/fsm/peer.rs:2541:40\r\n      raftstore::store::fsm::peer::PeerFsmDelegate<EK,ER,T>::handle_msgs\r\n      at /home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tikv/components/raftstore/src/store/fsm/peer.rs:620:37\r\n  11: <raftstore::store::fsm::store::RaftPoller<EK,ER,T> as batch_system::batch::PollHandler<raftstore: :store::fsm::peer::PeerFsm<EK,ER>,raftstore::store::fsm::store::StoreFsm<EK>>>::handle_normal\r\n             at /home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tikv/components/raftstore/src/store/fsm/store.rs:971:9\r\n   12: batch_system::batch::Poller<N,C,Handler>::poll\r\n             at /home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tikv/components/batch-system/src/batch.rs:430:27\r\n  13: raftstore::store::worker::refresh_config: :PoolController<N,C,H>::increase_by::{{closure}}\r\n             at /home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tikv/components/raftstore/src/store/worker/refresh_config.rs:78:21\r\n      <std::thread::Builder as ti kv_util::sys::thread::StdThreadBuildWrapper>::spawn_wrapper::{{closure}}\r\n             at /home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tikv/components/tikv_util/src/sys/thread.rs:415:23\r\n      std::sys_common::ba cktrace::__rust_begin_short_backtrace\r\n             at /rust/toolchains/nightly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys_common/backtrace.rs:121:18\r\n  14: std::thread::Builder::spawn_unchecked_::{{cl osure}}::{{closure}}\r\n             at /rust/toolchains/nightly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:551:17\r\n      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function:: FnOnce<()>>::call_once\r\n             at /rust/toolchains/nightly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panic/unwind_safe.rs:271:9\r\n      std::panicking::try::do_call\r\n             at /rust/toolchains /nightly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:483:40\r\n      std::panicking::try\r\n             at /rust/toolchains/nightly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/librar y/std/src/panicking.rs:447:19\r\n      std::panic::catch_unwind\r\n             at /rust/toolchains/nightly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137:14\r\n      std::thread::Builder::spawn_uncheck ed_::{{closure}}\r\n             at /rust/toolchains/nightly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:550:30\r\n      core::ops::function::FnOnce::call_once{{vtable.shim}}\r\n             at /rus t/toolchains/nightly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:513:5\r\n  15: <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once\r\n             at /rust/toolchains/nigh tly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:2000:9\r\n      <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once\r\n             at /rust/toolchains/nightly-2022-11-15-aarch64 -unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:2000:9\r\n      std::sys::unix::thread::Thread::new::thread_start\r\n             at /rust/toolchains/nightly-2022-11-15-aarch64-unknown-linux-gnu/lib/rustlib/src/rust/libra ry/std/src/sys/unix/thread.rs:108:17\r\n  16: <unknown>\r\n  17: <unknown>\r\n```\r\n\r\nHere is a redacted log about a suspected region: [a-panic-redacted.log](https://github.com/user-attachments/files/16835781/a-panic-redacted.log)\r\n\r\nThe full log may contain sensitive data hence cannot be uploaded here.\r\n\r\n\r\n\r\n",
  "state": "closed",
  "created_at": "2024-09-02T09:52:40Z",
  "updated_at": "2024-11-13T16:00:03Z",
  "closed_at": "2024-09-14T10:23:36Z",
  "labels": [
    "type/bug",
    "severity/major",
    "affects-5.4",
    "affects-6.1",
    "affects-6.5",
    "affects-7.1",
    "affects-7.5",
    "affects-8.1",
    "report/customer",
    "impact/panic"
  ],
  "comments_data": [
    {
      "id": 2333054803,
      "user": "YuJuncen",
      "created_at": "2024-09-06T02:13:47Z",
      "body": "A unit test case that tries to reproduce this issue. But didn't success. The last `MsgSnapshot` was dropped due to our FSM was stopped.\r\n\r\n<details><summary>Case</summary>\r\n<p>\r\n\r\n```rust\r\n// This case will delay the `MsgSnapshot` in flight and \r\n// then send them after the the destroy procedure begins.\r\n#[test]\r\nfn test_stale_peer_before_snapshot() {\r\n    #[derive(Default, Clone)]\r\n    struct DelaySnapFilter(Arc<Mutex<Vec<RaftMessage>>>);\r\n\r\n    impl Filter for DelaySnapFilter {\r\n        fn before(&self, msgs: &mut Vec<RaftMessage>) -> raftstore::Result<()> {\r\n            let mut snaps = self.0.lock().unwrap();\r\n            msgs.retain(|msg| {\r\n                println!(\r\n                    \"{} -> {}: {:?} {}\",\r\n                    msg.get_from_peer().store_id,\r\n                    msg.get_to_peer().store_id,\r\n                    msg.get_message().get_msg_type(),\r\n                    msg.get_message().has_snapshot() && snaps.is_empty(),\r\n                );\r\n                if msg.get_message().has_snapshot() && snaps.is_empty() {\r\n                    snaps.push(msg.clone());\r\n                    false\r\n                } else {\r\n                    true\r\n                }\r\n            });\r\n            Ok(())\r\n        }\r\n    }\r\n\r\n    impl DelaySnapFilter {\r\n        fn msgs(&self) -> Vec<RaftMessage> {\r\n            self.0.lock().unwrap().clone()\r\n        }\r\n    }\r\n\r\n    test_util::init_log_for_test();\r\n    let mut cluster = test_raftstore::new_node_cluster(0, 2);\r\n    cluster.cfg.raft_store.store_batch_system.pool_size = 1;\r\n    let pd_client = cluster.pd_client.clone();\r\n    pd_client.disable_default_operator();\r\n    cluster.run();\r\n    let fp = \"raftstore_before_destroy_peer\";\r\n\r\n    pd_client.must_remove_peer(1, new_peer(1, 1));\r\n\r\n    for i in 0..30 {\r\n        let b = format!(\"k{}\", i).into_bytes();\r\n        cluster.must_put(&b, &b);\r\n    }\r\n    must_get_equal(&cluster.get_engine(2), b\"k29\", b\"k29\");\r\n    cluster.wait_log_truncated(1, 2, 30);\r\n    let flt = DelaySnapFilter::default();\r\n\r\n    cluster.add_recv_filter_on_node(1, Box::new(flt.clone()));\r\n    pd_client.must_add_peer(1, new_peer(1, 3));\r\n    fail::cfg(fp, \"pause\").unwrap();\r\n    pd_client.must_remove_peer(1, new_peer(1, 3));\r\n    cluster.must_put(b\"Storia\", b\"\");\r\n    cluster.clear_recv_filter_on_node(1);\r\n\r\n    for msg in dbg!(flt.msgs()) {\r\n        cluster.sim.wl().send_raft_msg(msg).unwrap();\r\n        sleep_ms(500);\r\n    }\r\n    fail::remove(fp);\r\n\r\n    // Give it enough time to receive the snapshot...\r\n    sleep_ms(500);\r\n    cluster.shutdown()\r\n}\r\n```\r\n\r\n</p>\r\n</details> \r\n\r\n<details><summary>With this patch: </summary>\r\n<p>\r\n\r\n```diff\r\ndiff --git a/components/raftstore/src/store/fsm/peer.rs b/components/raftstore/src/store/fsm/peer.rs\r\nindex 4131dcae6..d43cdca72 100644\r\n--- a/components/raftstore/src/store/fsm/peer.rs\r\n+++ b/components/raftstore/src/store/fsm/peer.rs\r\n@@ -3924,6 +3924,8 @@ where\r\n             return false;\r\n         }\r\n \r\n+        fail_point!(\"raftstore_before_destroy_peer\", |_| { true });\r\n+\r\n         let region_id = self.region_id();\r\n         let is_peer_initialized = self.fsm.peer.is_initialized();\r\n         // We can't destroy a peer which is handling snapshot.\r\n```\r\n\r\n</p>\r\n</details> \r\n"
    },
    {
      "id": 2333188803,
      "user": "seiya-annie",
      "created_at": "2024-09-06T04:00:09Z",
      "body": "/report customer"
    },
    {
      "id": 2346554038,
      "user": "hbisheng",
      "created_at": "2024-09-12T15:03:52Z",
      "body": "Created https://github.com/tikv/tikv/pull/17535 which has a test that reproduced this issue. See the PR description for more details."
    },
    {
      "id": 2348345113,
      "user": "hbisheng",
      "created_at": "2024-09-13T08:26:34Z",
      "body": "The following diagram illustrated the key events and their timestamps. \r\n<img width=\"994\" alt=\"image\" src=\"https://github.com/user-attachments/assets/168271a1-314e-41df-9e1d-d432c4a25bba\">\r\n1. There was a split operation on the leader peer 25433195 at 10:14:04 but on the learner, it was only applied 3 seconds later at 10:14:07. This indicated replication was slow or log apply was slow on learner 25435688. This was one of the key conditions of the issue. \r\n2. As a result, Peer 25435693 was created twice, once by replicate and once by split. The second one replaced the first one. \r\n3. The second one was removed immediately after the split is done (a common operation issued by PD). What was unusual was that the removal happened right before the first one finished handling its snapshot message (a rare race condition). Then, the first one ran into panic when processing snapshot. \r\n\r\n\r\n**The detailed log analysis for reference:**\r\n1. Split operation was applied on the leader peer 25433195.\r\n```\r\n[2024/08/31 10:14:04.374 +08:00] [Info] [apply.rs:1611] [\"execute admin command\"] [command=\"cmd_type: BatchSplit splits { requests { split_key: >>SPLIT_KEY<< new_region_id: 25435689 new_peer_ids: 25435690 new_peer_ids: 25435691 new_peer_ids: 25435692 new_peer_ids: 25435693 } right_derive: true share_source_region_size: true }\"] [index=14749] [term=7] [peer_id=25433195] [region_id=25433193]\r\n```\r\n\r\n2. Peer 25435693 was created for the first time by raft message, let's call it PEER_FIRST.  \r\n```\r\n[2024/08/31 10:14:06.379 +08:00] [Info] [peer.rs:315] [\"replicate peer\"] [create_by_peer_store_id=3] [create_by_peer_id=25435690] [store_id=4] [peer_id=25435693] [region_id=25435689]\r\n```\r\n\r\n3. A snapshot message was sent to PEER_FIRST, but it wasn't processed yet. \r\n```\r\n[2024/08/31 10:14:07.945 +08:00] [Info] [snap.rs:271] [\"saving all snapshot files\"] [takes=275.11375ms] [snap_key=25435689_6_6]\r\n```\r\n\r\n4. The split operation was applied on the learner peer 25435688. This created Peer 25435693 for a second time. Let's call it PEER_SECOND. Note that PEER_SECOND would replace PEER_FIRST. The mailbox of PEER_FIRST was stopped so it couldn't receive new messages, but it may still have existing messages to process. \r\n```\r\n[2024/08/31 10:14:07.945 +08:00] [Info] [apply.rs:1611] [\"execute admin command\"] [command=\"cmd_type: BatchSplit splits { requests { split_key: >>SPLIT_KEY<< new_region_id: 25435689 new_peer_ids: 25435690 new_peer_ids: 25435691 new_peer_ids: 25435692 new_peer_ids: 25435693 } right_derive: true share_source_region_size: true }\"] [index=14749] [term=7] [peer_id=25435688] [region_id=25433193]`\r\n[2024/08/31 10:14:07.946 +08:00] [Info] [router.rs:287] [\"shutdown mailbox\"] [region_id=25435689]\r\n[2024/08/31 10:14:07.946 +08:00] [Info] [peer.rs:4093] [\"insert new region\"] [store_id=4] [is_uninitialized_peer_exist=true] [region=\"id: 25435689 start_key: >>START_KEY<< end_key: >>SPLIT_KEY<< region_epoch { conf_ver: 138690 version: 1077687 } peers { id: 25435690 store_id: 3 } peers { id: 25435691 store_id: 18 } peers { id: 25435692 store_id: 1 } peers { id: 25435693 store_id: 4 role: Learner }\"] [region_id=25435689]\r\n[2024/08/31 10:14:07.946 +08:00] [Info] [peer.rs:263] [\"create peer\"] [peer_id=25435693] [region_id=25435689]\r\n```\r\n\r\n5. PEER_SECOND was removed immediately after the split succeeds. During the destroy of the peer, the region metadata was removed.\r\n```\r\n[2024/08/31 10:14:07.947 +08:00] [Info] [apply.rs:1611] [\"execute admin command\"] [command=\"cmd_type: ChangePeerV2 change_peer_v2 { changes { change_type: RemoveNode peer { id: 25435693 store_id: 4 role: Learner } } }\"] [index=7] [term=6] [peer_id=25435693] [region_id=25435689]\r\n[2024/08/31 10:14:07.947 +08:00] [Info] [peer.rs:3621] [\"starts destroy\"] [is_latest_initialized=true] [is_peer_initialized=true] [merged_by_target=false] [peer_id=25435693] [region_id=25435689]\r\n```\r\n\r\n6. Panic occurred when PEER_FIRST continued to process the Raft snapshot message, expecting the region meta to exist.  \r\n```\r\n[2024/08/31 10:14:08.284 +08:00] [UNKNOWN] [lib.rs:509] [\"no entry found for key\"]...\r\n```\r\n"
    }
  ]
}