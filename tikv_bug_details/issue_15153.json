{
  "issue_number": 15153,
  "title": "[Dynamic Regions] async-read-worker may get stuck in generating snapshot when a region is destroying",
  "body": "## Bug Report\r\n\r\n<!-- Thanks for your bug report! Don't worry if you can't fill out all the sections. -->\r\n\r\nasync-read-worker may get stuck in generating snapshot when a region is destroying.\r\n\r\n![image](https://github.com/tikv/tikv/assets/2150711/4c5aee87-c81d-4133-89b4-30c47d74ff51)\r\n\r\nExpression\r\n```\r\nsum(rate(tikv_thread_cpu_seconds_total{k8s_cluster=\"$k8s_cluster\", tidb_cluster=~\"$tidb_cluster.*\", instance=~\"$instance\", name=~\"async.*\"}[1m])) by (instance, name)\r\n```\r\n\r\n@3pointer pointed out that region 388 is the last one generating snapshot before got stuck.\r\n\r\n```shell\r\n[root@tc-tikv-0 data]# ls -lash tablet_snap/\r\ntotal 80K\r\n 20K drwxr-xr-x 16 root root  20K Jul 18 14:34 .\r\n4.0K drwxr-xr-x  6 root root 4.0K Jul 18 18:09 ..\r\n4.0K drwxr-xr-x  2 root root 4.0K Jul 18 13:17 gen_137934_137937_6_214\r\n4.0K drwxr-xr-x  2 root root 4.0K Jul 18 12:37 gen_15682_15685_6_636\r\n4.0K drwxr-xr-x  2 root root 4.0K Jul 18 12:39 gen_28932_28935_6_142\r\n4.0K drwxr-xr-x  2 root root 4.0K Jul 18 12:41 gen_29184_29187_6_302\r\n4.0K drwxr-xr-x  2 root root 4.0K Jul 18 12:42 gen_29852_29854_6_392\r\n4.0K drwxr-xr-x  2 root root 4.0K Jul 18 12:39 gen_30160_30163_6_134\r\n4.0K drwxr-xr-x  2 root root 4.0K Jul 18 12:39 gen_30196_30199_6_204\r\n4.0K drwxr-xr-x  2 root root 4.0K Jul 18 12:40 gen_30368_30371_6_239\r\n4.0K drwxr-xr-x  2 root root 4.0K Jul 18 12:42 gen_30524_30527_6_382\r\n4.0K drwxr-xr-x  2 root root 4.0K Jul 18 12:41 gen_30560_30563_6_356\r\n4.0K drwxr-xr-x  2 root root 4.0K Jul 18 13:55 gen_388_793_11_147539.tmp     <-- the last one\r\n4.0K drwxr-xr-x  2 root root 4.0K Jul 18 13:36 gen_444_820_8_127711\r\n4.0K drwxr-xr-x  2 root root 4.0K Jul 18 13:43 gen_444_820_9_135599\r\n4.0K drwxr-xr-x  2 root root 4.0K Jul 18 14:17 rev_732_138780_8_247402.tmp\r\n[root@tc-tikv-0 data]#\r\n```\r\n\r\nRegion 388 log:\r\n```log\r\n[2023/07/18 13:55:03.750 +08:00] [INFO] [region.rs:258] [\"try to transfer leader\"] [to_peers=\"[]\"] [to_peer=\"id: 138013 store_id: 1 role: IncomingVoter\"] [from_peer=\"id: 12610 store_id: 4 role: DemotingVoter\"] [region_id=388]\r\n[2023/07/18 13:55:03.751 +08:00] [INFO] [transfer_leader.rs:177] [\"reject to transfer leader\"] [last_index=147469] [index=147468] [reason=\"pending snapshot\"] [to=\"id: 138013 store_id: 1 role: IncomingVoter\"] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:04.722 +08:00] [INFO] [region.rs:258] [\"try to transfer leader\"] [to_peers=\"[]\"] [to_peer=\"id: 138013 store_id: 1 role: IncomingVoter\"] [from_peer=\"id: 12610 store_id: 4 role: DemotingVoter\"] [region_id=388]\r\n[2023/07/18 13:55:04.724 +08:00] [INFO] [transfer_leader.rs:177] [\"reject to transfer leader\"] [last_index=147492] [index=147491] [reason=\"pending snapshot\"] [to=\"id: 138013 store_id: 1 role: IncomingVoter\"] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.052 +08:00] [INFO] [tablet_snap.rs:965] [\"sent snapshot\"] [duration=1679.861224686s] [size=6201390796] [snap_key=388_793_10_81334] [region_id=388]\r\n[2023/07/18 13:55:07.052 +08:00] [INFO] [snapshot.rs:419] [\"requesting snapshot\"] [state=\"Sending(data: 0A6B08840312247480000000000000FF635F720380000000FF0000183C03800000FF0000006B50000000FC1A247480000000000000FF635F720380000000FF0000196403800000FF0000001A20000000FC2205081D10B9012A0508870310052A0508990610062A0508C262100420032A00 metadata { conf_state { voters: 391 voters: 793 voters: 12610 } index: 81334 term: 10 })\"] [request_peer=793] [request_index=0] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.052 +08:00] [INFO] [snapshot.rs:528] [\"snapshot epoch is stale\"] [latest_epoch=\"conf_ver: 32 version: 185\"] [snap_epoch=\"conf_ver: 29 version: 185\"] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.052 +08:00] [INFO] [snapshot.rs:464] [\"requesting new snapshot\"] [request_peer=793] [request_index=0] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.052 +08:00] [INFO] [snapshot.rs:248] [\"report snapshot status\"] [status=Finish] [to=\"id: 793 store_id: 6\"] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.190 +08:00] [INFO] [snapshot.rs:464] [\"requesting new snapshot\"] [request_peer=793] [request_index=0] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.197 +08:00] [INFO] [snapshot.rs:419] [\"requesting snapshot\"] [state=\"Generating { canceled: false, index: 147539 }\"] [request_peer=793] [request_index=0] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.222 +08:00] [INFO] [region.rs:258] [\"try to transfer leader\"] [to_peers=\"[]\"] [to_peer=\"id: 138013 store_id: 1 role: IncomingVoter\"] [from_peer=\"id: 12610 store_id: 4 role: DemotingVoter\"] [region_id=388]\r\n[2023/07/18 13:55:07.223 +08:00] [INFO] [snapshot.rs:419] [\"requesting snapshot\"] [state=\"Generating { canceled: false, index: 147539 }\"] [request_peer=793] [request_index=0] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.223 +08:00] [INFO] [transfer_leader.rs:211] [\"transfer leader\"] [peer=\"id: 138013 store_id: 1 role: IncomingVoter\"] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.223 +08:00] [INFO] [raft.rs:1916] [\"[term 11] starts to transfer leadership to 138013\"] [lead_transferee=138013] [term=11] [raft_id=12610] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.223 +08:00] [INFO] [raft.rs:1929] [\"sends MsgTimeoutNow to 138013 immediately as 138013 already has up-to-date log\"] [lead_transferee=138013] [raft_id=12610] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.224 +08:00] [INFO] [raft.rs:1376] [\"received a message with higher term from 138013\"] [\"msg type\"=MsgRequestVote] [message_term=12] [term=11] [from=138013] [raft_id=12610] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.224 +08:00] [INFO] [raft.rs:1132] [\"became follower at term 12\"] [term=12] [raft_id=12610] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.224 +08:00] [INFO] [raft.rs:1577] [\"[logterm: 11, index: 147541, vote: 0] cast vote for 138013 [logterm: 11, index: 147541] at term 12\"] [\"msg type\"=MsgRequestVote] [term=12] [msg_index=147541] [msg_term=11] [from=138013] [vote=0] [log_index=147541] [log_term=11] [raft_id=12610] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.224 +08:00] [INFO] [endpoint.rs:416] [\"deregister observe region\"] [observe_id=ObserveId(1077)] [region_id=388] [store_id=Some(4)]\r\n[2023/07/18 13:55:07.224 +08:00] [INFO] [raft.rs:1445] [\"ignored a message with lower term from 391\"] [\"msg term\"=11] [\"msg type\"=MsgHeartbeatResponse] [term=12] [from=391] [raft_id=12610] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.237 +08:00] [INFO] [conf_change.rs:264] [\"exec ConfChangeV2\"] [index=147543] [epoch=\"conf_ver: 32 version: 185\"] [legacy=false] [kind=LeaveJoint] [peer_id=12610] [region_id=388][2023/07/18 13:55:07.237 +08:00] [INFO] [conf_change.rs:374] [\"leave joint state successfully\"] [region=\"id: 388 start_key: 7480000000000000FF635F720380000000FF0000183C03800000FF0000006B50000000FC end_key: 7480000000000000FF635F720380000000FF0000196403800000FF0000001A20000000FC region_epoch { conf_ver: 34 version: 185 } peers { id: 391 store_id: 5 } peers { id: 793 store_id: 6 } peers { id: 12610 store_id: 4 role: Learner } peers { id: 138013 store_id: 1 }\"] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.237 +08:00] [INFO] [conf_change.rs:294] [\"conf change successfully\"] [\"current region\"=\"id: 388 start_key: 7480000000000000FF635F720380000000FF0000183C03800000FF0000006B50000000FC end_key: 7480000000000000FF635F720380000000FF0000196403800000FF0000001A20000000FC region_epoch { conf_ver: 34 version: 185 } peers { id: 391 store_id: 5 } peers { id: 793 store_id: 6 } peers { id: 12610 store_id: 4 role: Learner } peers { id: 138013 store_id: 1 }\"] [\"original region\"=\"id: 388 start_key: 7480000000000000FF635F720380000000FF0000183C03800000FF0000006B50000000FC end_key: 7480000000000000FF635F720380000000FF0000196403800000FF0000001A20000000FC region_epoch { conf_ver: 32 version: 185 } peers { id: 391 store_id: 5 } peers { id: 793 store_id: 6 } peers { id: 12610 store_id: 4 role: DemotingVoter } peers { id: 138013 store_id: 1 role: IncomingVoter }\"] [legacy=false] [changes=\"[]\"] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.238 +08:00] [INFO] [raft.rs:2668] [\"switched to configuration\"] [config=\"Configuration { voters: Configuration { incoming: Configuration { voters: {138013, 391, 793} }, outgoing: Configuration { voters: {} } }, learners: {12610}, learners_next: {}, auto_leave: false }\"] [raft_id=12610] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.241 +08:00] [INFO] [conf_change.rs:264] [\"exec ConfChangeV2\"] [index=147544] [epoch=\"conf_ver: 34 version: 185\"] [legacy=false] [kind=Simple] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.241 +08:00] [INFO] [conf_change.rs:294] [\"conf change successfully\"] [\"current region\"=\"id: 388 start_key: 7480000000000000FF635F720380000000FF0000183C03800000FF0000006B50000000FC end_key: 7480000000000000FF635F720380000000FF0000196403800000FF0000001A20000000FC region_epoch { conf_ver: 35 version: 185 } peers { id: 391 store_id: 5 } peers { id: 793 store_id: 6 } peers { id: 138013 store_id: 1 }\"] [\"original region\"=\"id: 388 start_key: 7480000000000000FF635F720380000000FF0000183C03800000FF0000006B50000000FC end_key: 7480000000000000FF635F720380000000FF0000196403800000FF0000001A20000000FC region_epoch { conf_ver: 34 version: 185 } peers { id: 391 store_id: 5 } peers { id: 793 store_id: 6 } peers { id: 12610 store_id: 4 role: Learner } peers { id: 138013 store_id: 1 }\"] [legacy=false] [changes=\"[change_type: RemoveNode peer { id: 12610 store_id: 4 role: Learner }]\"] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.241 +08:00] [INFO] [raft.rs:2668] [\"switched to configuration\"] [config=\"Configuration { voters: Configuration { incoming: Configuration { voters: {138013, 391, 793} }, outgoing: Configuration { voters: {} } }, learners: {}, learners_next: {}, auto_leave: false }\"] [raft_id=12610] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.242 +08:00] [INFO] [life.rs:792] [\"peer destroyed\"] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.242 +08:00] [INFO] [router.rs:348] [\"shutdown mailbox\"] [region_id=388]\r\n```\r\n\r\n\r\n### What version of TiKV are you using?\r\n<!-- You can run `tikv-server --version` -->\r\n\r\nc27b43018ca83bd4103abf627c50293b841396bd\r\n\r\n### Steps to reproduce\r\n<!-- If possible, provide a recipe for reproducing the error. A complete runnable program is good. -->\r\n\r\nUnknown.\r\n\r\n### What did you expect?\r\n\r\nNo stuck.\r\n\r\n### What did happened?\r\n\r\nGet stuck.",
  "state": "closed",
  "created_at": "2023-07-18T11:13:36Z",
  "updated_at": "2023-07-31T05:08:40Z",
  "closed_at": "2023-07-31T05:08:40Z",
  "labels": [
    "type/bug",
    "severity/critical",
    "feature/developing",
    "may-affects-5.2",
    "may-affects-5.3",
    "may-affects-5.4",
    "may-affects-6.1",
    "may-affects-6.5",
    "may-affects-7.1"
  ],
  "comments_data": [
    {
      "id": 1640023877,
      "user": "Lily2025",
      "created_at": "2023-07-18T11:15:03Z",
      "body": "/severity critical "
    },
    {
      "id": 1643417183,
      "user": "3pointer",
      "created_at": "2023-07-20T07:27:25Z",
      "body": "From these logs\r\n```\r\n[2023/07/18 13:55:07.052 +08:00] [INFO] [snapshot.rs:464] [\"requesting new snapshot\"] [request_peer=793] [request_index=0] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.052 +08:00] [INFO] [snapshot.rs:248] [\"report snapshot status\"] [status=Finish] [to=\"id: 793 store_id: 6\"] [peer_id=12610] [region_id=388]\r\n[2023/07/18 13:55:07.190 +08:00] [INFO] [snapshot.rs:464] [\"requesting new snapshot\"] [request_peer=793] [request_index=0] [peer_id=12610] [region_id=388]\r\n```\r\n\r\nIt may have a race in `self.snap_states` and generate two snapshot task in a short time. it maybe have pontential issue. but since read_scheduler execute task in a single thread. it shouldn't get stuck in rocksdb checkpoint."
    },
    {
      "id": 1643618971,
      "user": "3pointer",
      "created_at": "2023-07-20T09:50:12Z",
      "body": "It seems the order of destroy peer and create checkpoint result in this issue.\r\n\r\n\r\nWhen a peer starts destroy -> `record_tombstone_tablet` schedule a `PrepareDestroy` task -> `prepare_destroy` call `pause_background_work` make `bg_work_paused_` increased.(**happen first**)\r\n\r\nAt same time `read_scheduler` schedule a `ReadTask::GenTabletSnapshot` -> gen_snapshot call rocksDB `CreateCheckpoint` -> `FlushMemTable` will wait until Flush finished by default. \r\nBut it never be able to schedule a flush task due to [paused](https://github.com/facebook/rocksdb/blob/86634885ebb59bd7a7950db9a540d80a3ac4ad7e/db/db_impl/db_impl_compaction_flush.cc#L2648-L2650)."
    },
    {
      "id": 1643701589,
      "user": "overvenus",
      "created_at": "2023-07-20T10:50:52Z",
      "body": "Maybe we can cancel un-started snapshot tasks and wait for on-going snapshot tasks before destroying peer."
    },
    {
      "id": 1657609060,
      "user": "overvenus",
      "created_at": "2023-07-31T05:08:40Z",
      "body": "Fixed by #15174"
    }
  ]
}