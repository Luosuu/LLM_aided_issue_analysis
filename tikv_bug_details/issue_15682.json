{
  "issue_number": 15682,
  "title": "[Dynamic Regions] panic with \"region corrupted\"",
  "body": "## Bug Report\r\n\r\nSimilar to issue #15242, even though PR #15625 resolves the problem in the\r\ntransfer leader scenario, there are still cases that result in a \"region corrupted\" error. \r\n\r\nThe main cause of this issue is that commit merge and rollback merge are not\r\ndeterministic, meaning that any transient error during propose could lead to\r\nthe problem occurring again. E.g., proposal failure by following lines:\r\n\r\nhttps://github.com/tikv/tikv/blob/b95f5cd0353506d728d0a50b7a898b503de072e1/components/raftstore-v2/src/operation/command/admin/mod.rs#L112-L122\r\n\r\nTo fix such issue, TiKV must only propose rollback merge when target region is\r\nnot found or epoch has increased.\r\n\r\n### What version of TiKV are you using?\r\n<!-- You can run `tikv-server --version` -->\r\n\r\nv7.3.0\r\n\r\n### What operating system and CPU are you using?\r\n<!-- If you're using Linux, you can run `cat /proc/cpuinfo` -->\r\n\r\n### Steps to reproduce\r\n<!-- If possible, provide a recipe for reproducing the error. A complete runnable program is good. -->\r\n\r\n```rust\r\n#[test]\r\nfn test_deterministic_commit_rollback_merge() {\r\n    use test_raftstore_v2::*;\r\n    let mut cluster = new_node_cluster(0, 3);\r\n    configure_for_merge(&mut cluster.cfg);\r\n    // Use a large election tick to stable test.\r\n    configure_for_lease_read(&mut cluster.cfg, None, Some(1000));\r\n    // Use 2 threads for polling peers, so that they can run concurrently.\r\n    cluster.cfg.raft_store.store_batch_system.pool_size = 2;\r\n    cluster.cfg.raft_store.store_batch_system.max_batch_size = Some(1);\r\n    cluster.run();\r\n\r\n    let pd_client = Arc::clone(&cluster.pd_client);\r\n    let region = pd_client.get_region(b\"k1\").unwrap();\r\n    cluster.must_split(&region, b\"k2\");\r\n\r\n    let left = pd_client.get_region(b\"k1\").unwrap();\r\n    let right = pd_client.get_region(b\"k3\").unwrap();\r\n    let right_1 = find_peer(&right, 1).unwrap().clone();\r\n    cluster.must_transfer_leader(right.get_id(), right_1);\r\n    let left_2 = find_peer(&left, 2).unwrap().clone();\r\n    cluster.must_transfer_leader(left.get_id(), left_2);\r\n\r\n    cluster.must_put(b\"k1\", b\"v1\");\r\n    cluster.must_put(b\"k3\", b\"v3\");\r\n    for i in 0..3 {\r\n        must_get_equal(&cluster.get_engine(i + 1), b\"k1\", b\"v1\");\r\n        must_get_equal(&cluster.get_engine(i + 1), b\"k3\", b\"v3\");\r\n    }\r\n\r\n    // Delay 1003 apply by dropping append response, so that proposal will fail\r\n    // due to applied_term != current_term.\r\n    let target_region_id = left.get_id();\r\n    cluster.add_recv_filter_on_node(\r\n        1,\r\n        Box::new(DropMessageFilter::new(Arc::new(move |m| {\r\n            if m.get_region_id() == target_region_id {\r\n                return m.get_message().get_msg_type() != MessageType::MsgAppendResponse;\r\n            }\r\n            true\r\n        }))),\r\n    );\r\n\r\n    let left_1 = find_peer(&left, 1).unwrap().clone();\r\n    cluster.must_transfer_leader(left.get_id(), left_1);\r\n\r\n    // left(1000) <- right(1).\r\n    let (tx1, rx1) = channel();\r\n    let (tx2, rx2) = channel();\r\n    let rx2_ = Mutex::new(rx2);\r\n    fail::cfg_callback(\"on_propose_commit_merge_fail_store_1\", move || {\r\n        tx1.send(()).unwrap();\r\n        rx2_.lock().unwrap().recv().unwrap();\r\n    })\r\n    .unwrap();\r\n    cluster.merge_region(right.get_id(), left.get_id(), Callback::None);\r\n\r\n    // Wait for target fails to propose commit merge.\r\n    rx1.recv_timeout(Duration::from_secs(5)).unwrap();\r\n    // Let target apply continue, and new AskCommitMerge messages will propose\r\n    // commit merge successfully.\r\n    cluster.clear_recv_filter_on_node(1);\r\n\r\n    // Trigger a CheckMerge tick, so source will send a AskCommitMerge again.\r\n    fail::cfg(\"ask_target_peer_to_commit_merge_store_1\", \"pause\").unwrap();\r\n    let router = cluster.get_router(1).unwrap();\r\n    router\r\n        .check_send(1, PeerMsg::Tick(PeerTick::CheckMerge))\r\n        .unwrap();\r\n\r\n    // Send RejectCommitMerge to source.\r\n    tx2.send(()).unwrap();\r\n    fail::remove(\"on_propose_commit_merge_fail_store_1\");\r\n\r\n    // Wait for target applies to current term.\r\n    cluster.must_put(b\"k1\", b\"v11\");\r\n\r\n    // By remove the failpoint, CheckMerge tick sends a AskCommitMerge again.\r\n    fail::remove(\"ask_target_peer_to_commit_merge_store_1\");\r\n    // At this point, source region will propose rollback merge.\r\n\r\n    // Wait for source handle RejectCommitMerge and commit rollback merge.\r\n    let timer = Instant::now();\r\n    loop {\r\n        if left.get_region_epoch().get_version()\r\n            == cluster.get_region_epoch(left.get_id()).get_version()\r\n        {\r\n            if timer.saturating_elapsed() > Duration::from_secs(5) {\r\n                panic!(\"region {:?} is still not merged.\", left);\r\n            }\r\n        } else {\r\n            break;\r\n        }\r\n        sleep_ms(10);\r\n    }\r\n\r\n    // No matter commit merge or rollback merge, cluster must be available to\r\n    // process requests\r\n    cluster.must_put(b\"k0\", b\"v0\");\r\n    cluster.must_put(b\"k4\", b\"v4\");\r\n}\r\n```\r\n\r\nWith new failpoints below: \r\n\r\n```diff\r\ndiff --git a/components/raftstore-v2/src/operation/command/admin/merge/commit.rs b/components/raftstore-v2/src/operation/command/admin/merge/commit.rs\r\nindex 8e55f89a7..6203183f7 100644\r\n--- a/components/raftstore-v2/src/operation/command/admin/merge/commit.rs\r\n+++ b/components/raftstore-v2/src/operation/command/admin/merge/commit.rs\r\n@@ -177,6 +177,11 @@ impl<EK: KvEngine, ER: RaftEngine> Peer<EK, ER> {\r\n             self.region_id() == 2,\r\n             |_| {}\r\n         );\r\n+        fail::fail_point!(\r\n+            \"ask_target_peer_to_commit_merge_store_1\",\r\n+            store_ctx.store_id == 1,\r\n+            |_| {}\r\n+        );\r\n         let state = self.applied_merge_state().unwrap();\r\n         let target = state.get_target();\r\n         let target_id = target.get_id();\r\n@@ -345,6 +345,11 @@ impl<EK: KvEngine, ER: RaftEngine> Peer<EK, ER> {\r\n                         return;\r\n                     }\r\n                 }\r\n+                fail::fail_point!(\r\n+                    \"on_propose_commit_merge_fail_1\",\r\n+                    self.peer().get_store_id() == 1,\r\n+                    |_| {}\r\n+                );\r\n                 let _ = store_ctx\r\n                     .router\r\n                     .force_send(source_id, PeerMsg::RejectCommitMerge { index });\r\n\r\n```\r\n\r\n### What did you expect?\r\n\r\nNo panic.\r\n\r\n### What did happened?\r\n\r\nPanic.",
  "state": "closed",
  "created_at": "2023-09-25T12:29:40Z",
  "updated_at": "2023-10-11T07:09:27Z",
  "closed_at": "2023-10-11T07:09:27Z",
  "labels": [
    "type/bug",
    "severity/critical",
    "affects-7.1",
    "affects-7.2",
    "affects-7.3"
  ],
  "comments_data": []
}