{
  "issue_number": 12238,
  "title": "v3.0.11 upgrade to v6.0.0 fail with \"critical config check failed: raft engine dir have been changed, former is ''",
  "body": "## Bug Report\r\n\r\n<!-- Thanks for your bug report! Don't worry if you can't fill out all the sections. -->\r\n\r\n### What version of TiKV are you using?\r\nv6.0.0                                               2022-03-22T19:50:34+08:00 \r\n\r\n### What operating system and CPU are you using?\r\n<!-- If you're using Linux, you can run `cat /proc/cpuinfo` -->\r\n\r\n### Steps to reproduce\r\n<!-- If possible, provide a recipe for reproducing the error. A complete runnable program is good. -->\r\ninstall v3.0.11 with default config \r\nupgrade to v6.0.0\r\n### What did you expect?\r\nupgrade successfully\r\n\r\n### What did happened?\r\nupgrade fail with:\r\nError: init config failed: tikv1-peer:30160: executor.ssh.execute_failed: Failed to execute command over SSH for 'tidb@tikv1-peer:22' {ssh_stderr: critical config check failed: raft engine dir have been changed, former is '', current is '/tiup/data/tikv-30160/raft-engine', please check if it is expected.\r\n, ssh_stdout: , ssh_command: export LANG=C; PATH=$PATH:/bin:/sbin:/usr/bin:/usr/sbin /tiup/deploy/tikv-30160/bin/tikv-server --config-check --config=/tiup/deploy/tikv-30160/conf/tikv.toml --pd \"\" --data-dir \"/tiup/data/tikv-30160\"}, cause: Process exited with status 1: check config failed\r\n",
  "state": "closed",
  "created_at": "2022-03-23T04:33:29Z",
  "updated_at": "2022-03-28T06:46:29Z",
  "closed_at": "2022-03-28T06:46:29Z",
  "labels": [
    "type/bug",
    "severity/moderate"
  ],
  "comments_data": [
    {
      "id": 1075905580,
      "user": "seiya-annie",
      "created_at": "2022-03-23T04:35:20Z",
      "body": "please help check which config item should be set for this error, and please add compatibility description in doc"
    },
    {
      "id": 1075925211,
      "user": "Lily2025",
      "created_at": "2022-03-23T05:22:41Z",
      "body": "/type bug\r\n/severity Critical"
    },
    {
      "id": 1077404313,
      "user": "tabokie",
      "created_at": "2022-03-24T09:15:30Z",
      "body": "I cannot reproduce this issue. My steps:\r\n```\r\ntiup cluster deploy test v3.0.11 /home/tabokie/control/topology_singlekv.yaml --password --user tabokie\r\ntiup cluster start test\r\ntiup cluster upgrade test nightly\r\n```\r\nTiKV config in `topology_singlekv.yaml` is left empty."
    },
    {
      "id": 1077628122,
      "user": "you06",
      "created_at": "2022-03-24T13:28:04Z",
      "body": "It's fine to upgrade to nightly, but failed when upgrading to v6.0.0."
    },
    {
      "id": 1077786227,
      "user": "tabokie",
      "created_at": "2022-03-24T16:00:26Z",
      "body": "@you06 What's the version code for v6.0.0 alpha build? My tiup can't find \"v6.0.0\" with various suffix."
    },
    {
      "id": 1077795484,
      "user": "you06",
      "created_at": "2022-03-24T16:10:31Z",
      "body": "> It's fine to upgrade to nightly, but failed when upgrading to v6.0.0.\r\n\r\n@tabokie Sorry that's not accurate, the tiup's check(actually it uses `--config-check` of TiKV) failed inside k8s but work in normal tiup cluster, I'll do further investigation."
    },
    {
      "id": 1077800649,
      "user": "tabokie",
      "created_at": "2022-03-24T16:16:41Z",
      "body": "Well, if it helps, my theory in #12250 is that the v3.0.11 config didn't pass the validation logic in v6.0.0. It can be verified if you copied the `data/last_tikv.toml` from a v3.0.11 cluster to `conf/tikv.toml`, then do the upgrade and check if there's any error."
    },
    {
      "id": 1078706344,
      "user": "you06",
      "created_at": "2022-03-25T06:39:33Z",
      "body": "@tabokie I added some log and it shows that the config check actually failed in the following part.\r\n\r\nIn v3.0.11, `storage.scheduler-worker-pool-size` is set to 8.\r\n\r\nhttps://github.com/tikv/tikv/blob/1b35044a1910043bbf60905ceb6ce34e34879a0c/src/storage/config.rs#L112-L121\r\n\r\nAfter this failure, it returns with an error and skipped the init for `self.raft_engine.config.dir` in line [#2692](https://github.com/tikv/tikv/blob/1b35044a1910043bbf60905ceb6ce34e34879a0c/src/config.rs#L2692), so we get the error in the description of this issue.\r\n\r\nhttps://github.com/tikv/tikv/blob/1b35044a1910043bbf60905ceb6ce34e34879a0c/src/config.rs#L2677-L2692\r\n\r\nAnd you've mentioned in #12250 that we skipped the error check for `validate` of last config.\r\n\r\nhttps://github.com/tikv/tikv/blob/5ee7d037d4e2e63d710410daac81baca7e9efe69/src/config.rs#L3185\r\n\r\n-----\r\n\r\nThe reason we cannot reproduce it in our environments is that `storage.scheduler-worker-pool-size` is allowed to set up to CPU nums, so `storage.scheduler-worker-pool-size` with 8 will pass the check. In k8s test environment, both `cpu set cores` and `cpu quota` are set to 4, which fail the validation.\r\n"
    },
    {
      "id": 1078708028,
      "user": "you06",
      "created_at": "2022-03-25T06:43:14Z",
      "body": "The validation for configuration of storage is introduced in #12068, I think this looks OK. But when TikV v3.0.11 bootstraps, it choose `storage.scheduler-worker-pool-size` without consideration of cgroup limitations. @glorv PTAL"
    },
    {
      "id": 1078712301,
      "user": "tabokie",
      "created_at": "2022-03-25T06:52:05Z",
      "body": "@you06 Nice finding! I think it's because v3.0.11 doesn't have cgroup integration yet. Anyway, https://github.com/tikv/tikv/pull/12250 will fix this issue."
    },
    {
      "id": 1078881409,
      "user": "glorv",
      "created_at": "2022-03-25T10:28:58Z",
      "body": "> The validation for configuration of storage is introduced in #12068, I think this looks OK. But when TikV v3.0.11 bootstraps, it choose `storage.scheduler-worker-pool-size` without consideration of cgroup limitations. @glorv PTAL\r\n\r\nThis is truely a breaking change. Since it make little sense to config  `storage.scheduler-worker-pool-size` to bigger than cpu quota limit, I think this won't be a problem in production environment."
    }
  ]
}