{
  "issue_number": 17258,
  "title": "tpcc report \"failed Error 8133: data inconsistency in table: customer, index: PRIMARY, index-count:1 != record-count:0\\r\\n\" during tikv balance",
  "body": "## Bug Report\r\n\r\n<!-- Thanks for your bug report! Don't worry if you can't fill out all the sections. -->\r\n\r\n### What version of TiKV are you using?\r\n./tikv-server -V\r\n TiKV \r\nRelease Version:   8.2.0-alpha\r\nEdition:           Community\r\nGit Commit Hash:   212d2d51de0abb9382057aca71f7dde257044c8d\r\nGit Commit Branch: heads/refs/tags/v8.2.0-alpha\r\nUTC Build Time:    2024-07-05 11:37:37\r\nRust Version:      rustc 1.77.0-nightly (89e2160c4 2023-12-27)\r\nEnable Features:   memory-engine pprof-fp jemalloc mem-profiling portable sse test-engine-kv-rocksdb test-engine-raft-raft-engine trace-async-tasks openssl-vendored\r\nProfile:           dist_release\r\n\r\n### What operating system and CPU are you using?\r\n8c/32g\r\n\r\n### Steps to reproduce\r\n1、run tpcc\r\n2、config set max-store-down-time 10m\r\n2、down one of tikv last for 20mins\r\n\r\n### What did you expect?\r\nno unexpected error\r\n\r\n### What did happened?\r\ntpcc report error during tikv balance after fault recover\r\n[2024/07/07 07:45:51.990 +08:00] [INFO] [workload.go:680] [\"[2024-07-06 23:45:51] execute run failed, err exec SELECT c_discount, c_last, c_credit, w_tax FROM customer, warehouse WHERE w_id = ? AND c_w_id = w_id AND c_d_id = ? AND c_id = ?(wID=863,dID=5,cID=2731) failed Error 8133: data inconsistency in table: customer, index: PRIMARY, index-count:1 != record-count:0\\r\\n\"]\r\n\r\n![d655b8a0-c443-402e-bcd8-5893afe693bb](https://github.com/tikv/tikv/assets/84712107/a0982dca-caac-483f-9f47-3591db05ab46)\r\n\r\n\r\n",
  "state": "closed",
  "created_at": "2024-07-09T10:07:23Z",
  "updated_at": "2024-07-17T03:46:55Z",
  "closed_at": "2024-07-17T01:24:01Z",
  "labels": [
    "type/bug",
    "sig/raft",
    "severity/critical",
    "impact/inconsistency",
    "affects-8.2"
  ],
  "comments_data": [
    {
      "id": 2219340781,
      "user": "Lily2025",
      "created_at": "2024-07-10T02:08:15Z",
      "body": "/assign cfzjywxk"
    },
    {
      "id": 2220049641,
      "user": "cfzjywxk",
      "created_at": "2024-07-10T09:49:12Z",
      "body": "From the log report\r\n\r\n{\"log\":\"[reporter.go:213] [\\\"indexLookup found data inconsistency\\\"] [conn=1216348402] [session_alias=] [table_name=customer] [index_name=PRIMARY] [index_cnt=1] [table_cnt=0] [missing_handles=\\\"[19994168]\\\"] [total_handles=\\\"[19994168]\\\"] [\r\nrow_mvcc_0=\\\"{\\\\\\\"key\\\\\\\":\\\\\\\"74800000000000006A5F728000000001311638\\\\\\\",\\\\\\\"mvcc\\\\\\\":{\\\\\\\"info\\\\\\\":{\\\\\\\"lock\\\\\\\":{\\\\\\\"type\\\\\\\":5,\\\\\\\"start_ts\\\\\\\":451036894803263708,\\\\\\\"primary\\\\\\\":\\\\\\\"dIAAAAAAAABsX3KAAAAAARUdAQ==\\\\\\\",\\\\\\\"versions_to_last_change\\\\\\\":1}}},\\\\\\\"regionID\\\\\\\":39520172}\\\"] \r\n\r\n[index_mvcc_0=\\\"{\\\\\\\"decoded\\\\\\\":{\\\\\\\"421185361647239169\\\\\\\":{\\\\\\\"handle\\\\\\\":\\\\\\\"19994168\\\\\\\"}},\\\\\\\"key\\\\\\\":\\\\\\\"74800000000000006A5F69800000000000000103800000000000029B038000000000000005038000000000000878\\\\\\\",\\\\\\\"mvcc\\\\\\\":{\\\\\\\"info\\\\\\\":{\\\\\\\"lock\\\\\\\":{\\\\\\\"type\\\\\\\":5,\\\\\\\"start_ts\\\\\\\":451036894803263708,\\\\\\\"primary\\\\\\\":\\\\\\\"dIAAAAAAAABsX3KAAAAAARUdAQ==\\\\\\\",\\\\\\\"last_change_ts\\\\\\\":421185361673453569,\\\\\\\"versions_to_last_change\\\\\\\":1},\\\\\\\"writes\\\\\\\":[{\\\\\\\"start_ts\\\\\\\":**421185361647239169**,\\\\\\\"commit_ts\\\\\\\":**421185361673453569**,\\\\\\\"short_value\\\\\\\":\\\\\\\"AAAAAAExFjg=\\\\\\\"}]}},\\\\\\\"regionID\\\\\\\":5038}\\\"]\r\n\r\n\r\nThe handle record with commit_ts `421185361673453569` could not be found, it's weird as the commit_ts is a very old timestamp indicating it's written from the backuped data by the restore process."
    },
    {
      "id": 2228073781,
      "user": "cfzjywxk",
      "created_at": "2024-07-15T09:32:11Z",
      "body": "After reproducing disabling GC, there are `default not found` error reported from reading.\r\n\r\n\r\nThere are logs like\r\n```\r\ntikv0.log:[2024/07/12 03:22:45.660 +08:00] [INFO] [[region.rs:472](http://region.rs:472/)] [\"delete data in range because of stale\"] [end_key=7A7480000000000000FF705F698000000000FF0000010380000000FF0000001003800000FF0000000009038000FF0000000008EE0000FD] [start_key=7A7480000000000000FF6F5F728000000001FFC9B61E0000000000FA] [region_id=10987410] [thread_id=107]\r\n\r\ntikv0.log:[2024/07/12 03:31:04.769 +08:00] [INFO] [[region.rs:389](http://region.rs:389/)] [\"delete data in range because of overlap\"] [end_key=7A7480000000000000FF6F5F728000000001FF3CD1C50000000000FA] [start_key=7A7480000000000000FF6F5F728000000001FF3C97AC0000000000FA] [region_id=31402223] [thread_id=88]\r\n```\r\n\r\nA possible cause is incorrect range destroy with newly indroduced https://github.com/tikv/tikv/pull/17121 destroy logic.\r\n"
    },
    {
      "id": 2230134665,
      "user": "hbisheng",
      "created_at": "2024-07-16T06:39:12Z",
      "body": "@overvenus pointed out a potential cause of the issue: https://github.com/tikv/tikv/pull/17121#discussion_r1677644280\r\n\r\nWhen the issue happened, data can be deleted from a snapshot that's just applied. In the TiKV log, I found some evidence that suggested this might have happened. The following logs showed that data with overlapping range was deleted 2 seconds after a snapshot was applied. So it's quite likely the root cause of the missing data. \r\n```\r\n[2024/07/12 04:42:33.274 +08:00] [INFO] [peer_storage.rs:731] [\"apply snapshot with state ok\"] [for_witness=false] [state=\"applied_index: 18535 truncated_state { index: 18535 term: 7 }\"] [region=\"id: 12989474 start_key: 7480000000000000FF6F5F728000000000FFDCB0080000000000FA end_key: 7480000000000000FF6F5F728000000000FFDCB3590000000000FA region_epoch { conf_ver: 78 version: 1250 } peers { id: 12989476 store_id: 1 } peers { id: 12989477 store_id: 5 } peers { id: 31069195 store_id: 4 } peers { id: 31424858 store_id: 6 role: Learner }\"] [peer_id=31424858] [region_id=12989474] [thread_id=110]\r\n[2024/07/12 04:42:35.776 +08:00] [INFO] [region.rs:472] [\"delete data in range because of stale\"] [end_key=7A7480000000000000FF6F5F728000000000FFDCC42D0000000000FA] [start_key=7A7480000000000000FF6F5F728000000000FFDCB0080000000000FA] [region_id=31417702] [thread_id=107]\r\n```"
    },
    {
      "id": 2230391149,
      "user": "crazycs520",
      "created_at": "2024-07-16T09:04:29Z",
      "body": "tidb error logs:\r\n\r\n```log\r\n[2024/07/12 05:03:22.729 +08:00] [ERROR] [reporter.go:213] [\"indexLookup found data inconsistency\"] [conn=3573547388] [session_alias=] [table_name=customer] [index_name=PRIMARY] [index_cnt=1] [table_cnt=0] [missing_handles=\"[17087983]\"] [total_handles=\"[17087983]\"] ...\r\n```\r\n\r\nAfter check,  found a lot of rows data of table `customer` are missing:\r\n\r\n```sql\r\ntpcc> select count(1) from customer use index(primary);\r\n+----------+\r\n| count(1) |\r\n+----------+\r\n| 30000000 |\r\n+----------+\r\ntpcc> select count(1) from customer use index(idx_customer);\r\n+----------+\r\n| count(1) |\r\n+----------+\r\n| 30000000 |\r\n+----------+\r\ntpcc> select count(1) from customer ignore index(primary, idx_customer);\r\n+----------+\r\n| count(1) |\r\n+----------+\r\n| 29986907 |\r\n+----------+\r\n\r\n-- query the missing row's _tidb_rowid\r\ntpcc> select _tidb_rowid, c_w_id, c_d_id, c_id from customer use index(primary) where _tidb_rowid not in (select _tidb_rowid from customer ignore index(primary, idx_customer)) order by _tidb_rowid;\r\n+-------------+--------+--------+------+\r\n| _tidb_rowid | c_w_id | c_d_id | c_id |\r\n+-------------+--------+--------+------+\r\n| 17082740    | 570    | 5      | 740  |\r\n| 17082741    | 570    | 5      | 741  |\r\n| 17082742    | 570    | 5      | 742  |\r\n\r\n...\r\n...\r\n...\r\n| 17096438    | 570    | 9      | 2438 |\r\n| 17096439    | 570    | 9      | 2439 |\r\n| 17096440    | 570    | 9      | 2440 |\r\n+-------------+--------+--------+------+\r\n13093 rows in set\r\n```\r\n\r\nBased on the missing row data _tidb_rowid range [17082740, 17096440], locate Region 31434961, which is on Store 5, which is tikv-2:\r\n\r\n```sql\r\n>  show table customer regions;\r\n...\r\n 31434959  | t_111_r_17076872 | t_111_r_17108437 | 31434961  | 5 | 31434960, 31434961, 31434962 | 0\r\n```\r\n\r\nLooking for the tikv-2 logs around the time `2024/07/12 05:03:22`, we found that there is an apply snapshot and 2 delete data logs, and the `tid: 111` behind the log is the table id 111 parsed according to the key, that is, the table id of the `customer` table, start_handle and end_handle are also decode from the key. The following three log messages are:\r\n\r\n- 05:02:33.138,apply snapshot, the corresponding data is the row data of the customer table: [17082740,17096441) \r\n- 05:02:57.400, delete data, the corresponding data is the row data of the customer table: [17082740, 17087300)\r\n- 05:02:57.400, delete data, The corresponding data is the row data of the customer table: [17087897, 17096441) \r\n\r\nNote that the missing row data _tidb_rowid range of the table `customer` is [17082740, 17096440], which is exactly the data range of the deleted data above. I suspect that delete data mistakenly deleted the data just applied above.\r\n\r\n```log\r\n[2024/07/12 05:02:33.138 +08:00] [INFO] [peer_storage.rs:731] [\"apply snapshot with state ok\"] [for_witness=false] [state=\"applied_index: 142 truncated_state { index: 142 term: 7 }\"] [region=\"id: 31416981 start_key: 7480000000000000FF6F5F728000000001FF04A9740000000000FA end_key: 7480000000000000FF6F5F728000000001FF04DEF90000000000FA region_epoch { conf_ver: 96 version: 1223 } peers { id: 31416982 store_id: 1 } peers { id: 31416984 store_id: 4 } peers { id: 31425373 store_id: 6 } peers { id: 31426272 store_id: 5 role: Learner }\"] [peer_id=31426272] [region_id=31416981] [thread_id=103] tid: 111, start_handle: 17082740, end_handle: 17096441, contain_suspect: true\r\n[2024/07/12 05:02:57.400 +08:00] [INFO] [region.rs:472] [\"delete data in range because of stale\"] [end_key=7A7480000000000000FF6F5F728000000001FF04BB440000000000FA] [start_key=7A7480000000000000FF6F5F728000000001FF04A9740000000000FA] [region_id=31405673] [thread_id=99] tid: 111, start_handle: 17082740, end_handle: 17087300, contain_suspect: true\r\n[2024/07/12 05:02:57.400 +08:00] [INFO] [region.rs:472] [\"delete data in range because of stale\"] [end_key=7A7480000000000000FF6F5F728000000001FF04DEF90000000000FA] [start_key=7A7480000000000000FF6F5F728000000001FF04BD990000000000FA] [region_id=31406521] [thread_id=99] tid: 111, start_handle: 17087897, end_handle: 17096441, contain_suspect: true\r\n```"
    },
    {
      "id": 2232293381,
      "user": "kennedy8312",
      "created_at": "2024-07-17T03:22:47Z",
      "body": "/impact inconsistency"
    }
  ]
}