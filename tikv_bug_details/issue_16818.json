{
  "issue_number": 16818,
  "title": "tikv panic for \"assertion failed: row.lock.is_none()\"",
  "body": "## Bug Report\r\n\r\n<!-- Thanks for your bug report! Don't worry if you can't fill out all the sections. -->\r\n\r\n### What version of TiKV are you using?\r\n[\"Git Commit Hash:   b7103fab92af010ce709238a2e243dc8ab7eba56\"] [thread_id=1]\r\n\r\n### What operating system and CPU are you using?\r\n<!-- If you're using Linux, you can run `cat /proc/cpuinfo` -->\r\n\r\n### Steps to reproduce\r\n<!-- If possible, provide a recipe for reproducing the error. A complete runnable program is good. -->\r\nbank-tbls-pessimistic-inplace-follower-neterr\r\ntxn-mode=pessimistic, follower-read=true\r\nrun bank workload, \r\ninjuct network fail\r\nparams: \r\n          --txn-mode=pessimistic --force-reinstall=true --update-in-place=true\r\n          --follower-read=true\r\n          --nemesis=schedules,partition-pd-leader,partition-half,partition-ring\r\n          --read-lock=update --os=image --time-limit=600 --version=master\r\n          --workload=bank-multitable --init-sql='set\r\n          @@tidb_enable_mutation_checker=1, @@tidb_txn_assertion_level=strict,\r\n          @@tidb_constraint_check_in_place_pessimistic=off'\r\n### What did you expect?\r\npass\r\n\r\n### What did happened?\r\n```\r\n[2024/04/11 19:01:19.274 +00:00] [FATAL] [lib.rs:477] [\"assertion failed: row.lock.is_none()\"] [backtrace=\"   0: tikv_util::set_panic_hook::{{closure}}\r\n             at /workspace/source/tikv/components/tikv_util/src/lib.rs:476:18\r\n   1: <alloc::boxed::Box<F,A> as core::ops::function::Fn<Args>>::call\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:2029:9\r\n      std::panicking::rust_panic_with_hook\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:783:13\r\n   2: std::panicking::begin_panic_handler::{{closure}}\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:649:13\r\n   3: std::sys_common::backtrace::__rust_end_short_backtrace\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys_common/backtrace.rs:171:18\r\n   4: rust_begin_unwind\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:645:5\r\n   5: core::panicking::panic_fmt\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panicking.rs:72:14\r\n   6: core::panicking::panic\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panicking.rs:144:5\r\n   7: resolved_ts::cmd::group_row_changes\r\n             at /workspace/source/tikv/components/resolved_ts/src/cmd.rs:224:25\r\n      resolved_ts::cmd::ChangeLog::encode_change_log::{{closure}}\r\n             at /workspace/source/tikv/components/resolved_ts/src/cmd.rs:62:57\r\n      core::iter::adapters::map::map_fold::{{closure}}\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/iter/adapters/map.rs:85:28\r\n      core::iter::traits::iterator::Iterator::fold\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/iter/traits/iterator.rs:2640:21\r\n      <core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/iter/adapters/map.rs:125:9\r\n      core::iter::traits::iterator::Iterator::for_each\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/iter/traits/iterator.rs:858:9\r\n      alloc::vec::Vec<T,A>::extend_trusted\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/vec/mod.rs:2923:17\r\n      <alloc::vec::Vec<T,A> as alloc::vec::spec_extend::SpecExtend<T,I>>::spec_extend\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/vec/spec_extend.rs:26:9\r\n      <alloc::vec::Vec<T> as alloc::vec::spec_from_iter_nested::SpecFromIterNested<T,I>>::from_iter\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/vec/spec_from_iter_nested.rs:62:9\r\n      alloc::vec::in_place_collect::<impl alloc::vec::spec_from_iter::SpecFromIter<T,I> for alloc::vec::Vec<T>>::from_iter\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/vec/in_place_collect.rs:233:20\r\n      <alloc::vec::Vec<T> as core::iter::traits::collect::FromIterator<T>>::from_iter\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/vec/mod.rs:2791:9\r\n      core::iter::traits::iterator::Iterator::collect\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/iter/traits/iterator.rs:2054:9\r\n      resolved_ts::cmd::ChangeLog::encode_change_log\r\n             at /workspace/source/tikv/components/resolved_ts/src/cmd.rs:76:14\r\n   8: resolved_ts::endpoint::Endpoint<T,E,S>::handle_change_log\r\n             at /workspace/source/tikv/components/resolved_ts/src/endpoint.rs:883:32\r\n      <resolved_ts::endpoint::Endpoint<T,E,S> as tikv_util::worker::pool::Runnable>::run\r\n             at /workspace/source/tikv/components/resolved_ts/src/endpoint.rs:1132:46\r\n      tikv_util::worker::pool::Worker::start_with_timer_impl::{{closure}}\r\n             at /workspace/source/tikv/components/tikv_util/src/worker/pool.rs:517:25\r\n   9: <tracker::tls::TrackedFuture<F> as core::future::future::Future>::poll::{{closure}}\r\n             at /workspace/source/tikv/components/tracker/src/tls.rs:64:23\r\n      std::thread::local::LocalKey<T>::try_with\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/local.rs:270:16\r\n      std::thread::local::LocalKey<T>::with\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/local.rs:246:9\r\n      <tracker::tls::TrackedFuture<F> as core::future::future::Future>::poll\r\n             at /workspace/source/tikv/components/tracker/src/tls.rs:62:27\r\n      <futures_util::future::future::map::Map<Fut,F> as core::future::future::Future>::poll\r\n             at /workspace/.cargo/registry/src/index.crates.io-6f17d22bba15001f/futures-util-0.3.15/src/future/future/map.rs:55:37\r\n      <futures_util::future::future::Map<Fut,F> as core::future::future::Future>::poll\r\n             at /workspace/.cargo/registry/src/index.crates.io-6f17d22bba15001f/futures-util-0.3.15/src/lib.rs:93:13\r\n      yatp::task::future::RawTask<F>::poll\r\n             at /workspace/.cargo/git/checkouts/yatp-e704b73c3ee279b6/793be4d/src/task/future.rs:59:9\r\n  10: yatp::task::future::TaskCell::poll\r\n             at /workspace/.cargo/git/checkouts/yatp-e704b73c3ee279b6/793be4d/src/task/future.rs:103:9\r\n      <yatp::task::future::Runner as yatp::pool::runner::Runner>::handle\r\n             at /workspace/.cargo/git/checkouts/yatp-e704b73c3ee279b6/793be4d/src/task/future.rs:387:20\r\n  11: <tikv_util::yatp_pool::YatpPoolRunner<T> as yatp::pool::runner::Runner>::handle\r\n             at /workspace/source/tikv/components/tikv_util/src/yatp_pool/mod.rs:198:24\r\n      yatp::pool::worker::WorkerThread<T,R>::run\r\n             at /workspace/.cargo/git/checkouts/yatp-e704b73c3ee279b6/793be4d/src/pool/worker.rs:48:13\r\n      yatp::pool::builder::LazyBuilder<T>::build::{{closure}}\r\n             at /workspace/.cargo/git/checkouts/yatp-e704b73c3ee279b6/793be4d/src/pool/builder.rs:114:25\r\n      std::sys_common::backtrace::__rust_begin_short_backtrace\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys_common/backtrace.rs:155:18\r\n  12: std::thread::Builder::spawn_unchecked_::{{closure}}::{{closure}}\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:529:17\r\n      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panic/unwind_safe.rs:272:9\r\n      std::panicking::try::do_call\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:552:40\r\n      std::panicking::try\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:516:19\r\n      std::panic::catch_unwind\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:142:14\r\n      std::thread::Builder::spawn_unchecked_::{{closure}}\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:528:30\r\n      core::ops::function::FnOnce::call_once{{vtable.shim}}\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:250:5\r\n  13: <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:2015:9\r\n      <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:2015:9\r\n      std::sys::unix::thread::Thread::new::thread_start\r\n             at /root/.rustup/toolchains/nightly-2023-12-28-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys/unix/thread.rs:108:17\r\n  14: start_thread\r\n  15: clone\r\n\"] [location=components/resolved_ts/src/cmd.rs:224] [thread_name=resolved-ts-0] [thread_id=75]\r\n\r\n```",
  "state": "open",
  "created_at": "2024-04-12T03:25:30Z",
  "updated_at": "2024-11-01T10:03:39Z",
  "closed_at": null,
  "labels": [
    "type/bug",
    "sig/transaction",
    "severity/major",
    "affects-8.0",
    "affects-8.1",
    "impact/crash",
    "affects-8.5"
  ],
  "comments_data": [
    {
      "id": 2054569606,
      "user": "cfzjywxk",
      "created_at": "2024-04-15T02:57:22Z",
      "body": "~~It seems to be related to the pipelined dml implementation, which breaks the constraint `Prewrite Lock` would be written at most one time.~~ Need more investigations.\r\n\r\nVerfied the `pipelined-dml` is not enabled in this case"
    },
    {
      "id": 2070977446,
      "user": "zhangjinpeng87",
      "created_at": "2024-04-22T21:24:17Z",
      "body": "@cfzjywxk @seiya-annie Did you tested \"follwer-read=false\" case? Is this because follower-read?"
    },
    {
      "id": 2071265382,
      "user": "cfzjywxk",
      "created_at": "2024-04-23T01:52:31Z",
      "body": "> @cfzjywxk @seiya-annie Did you tested \"follwer-read=false\" case? Is this because follower-read?\r\n\r\n@zhangjinpeng87  \r\n\r\nThis issue has also occurred in non-follower-read Jepsen test jobs. The panic occurs during the observation of the apply result process in `resolved-ts`. It appears that the `Requests` of `RaftCmdRequest` involving a lock write for the same key twice which does not comply with the transaction protocol, indicating a correctness issue.\r\n\r\nCurrently, we have added logging before the assertion. After reproducing the issue, we can further examine the logs to determine which transaction's write operation caused this situation."
    },
    {
      "id": 2128714651,
      "user": "cfzjywxk",
      "created_at": "2024-05-24T06:55:41Z",
      "body": "A recent reproducing, with more logs:\r\n\r\n\"there is already \r\nlock=Put(None, [83, 38, 116, 128, 0, 0, 0, 0, 0, 0, 198, 95, 114, 128, 0, 0, 0, 0, 0, 0, 3, 129, 128, 224, 206, 157, 222, 153, 159, 6, 154, 214, 2, 102, 6, 62, 102, 241, 217, 216, 0, 1, 99, 6, 62, 102, 241, 217, 216, 0, 2, 108, 6, 62, 102, 241, 118, 160, 0, 33, 2]) \r\n\r\non \r\n\r\nrow=RowChange { write: None, lock: Some(Put(None, [83, 38, 116, 128, 0, 0, 0, 0, 0, 0, 198, 95, 114, 128, 0, 0, 0, 0, 0, 0, 3, 129, 128, 224, 206, 157, 222, 153, 159, 6, 154, 214, 2, 102, 6, 62, 102, 241, 217, 216, 0, 1, 99, 6, 62, 102, 241, 217, 216, 0, 2, 108, 6, 62, 102, 241, 118, 160, 0, 33, 2])), default: None }\"\r\n\r\nwhich is\r\n\r\nLock { lock_type: Pessimistic, primary_key: 7480000000000000C65F728000000000000003, start_ts: TimeStamp(449910201711591425), ttl: 43802, short_value: , for_update_ts: TimeStamp(449910201711591425), txn_size: 0, min_commit_ts: TimeStamp(449910201711591426), use_async_commit: false, secondaries: [], rollback_ts: [], last_change: Exist { last_change_ts: TimeStamp(449910200046977057), estimated_versions_to_last_change: 2 }, txn_source: 0, is_locked_with_conflict: false, generation: 0 }\r\n\r\nThe related query is \r\n```\r\ninsert into txn6 (id, sk, val) values (3, 3, 13) on duplicate key update val = 13; // with pessimistic mode\r\n```\r\n\r\n\r\nBut the to be inserted `key` and `value` are not printted in the last log enhancement PR..\r\n\r\nIt's still not clear why there could be duplicate keys in a single apply batch."
    },
    {
      "id": 2136993715,
      "user": "cfzjywxk",
      "created_at": "2024-05-29T09:44:07Z",
      "body": "After some investigations, one of the possbility is that pessimistic and prewrite locks with the same key are applied in a same apply batch and it is observed by the `resolved-ts` component, then the assertion is break.\r\n\r\nConsider a leader transfer situation\r\n```\r\nregion with p1, p2, p3, p1 is the leader peer\r\n1. Pessimistic lock k0 on p1, use in-memory lock, return success\r\n2. P1 attempts to transfer the leader to p3. According to the in-memory lock requirement, before officially initiating, propose an in-memory lock. Here, the lock has not been deleted yet. Propose pessimistic lock to raft log, assuming index = 10. Afterwards, propose transfer leader command index = 11.\r\n3. User prewrite k1 on p1, propose prewrite k0, index = 12 \r\n```\r\n\r\nIn this case\r\n```\r\nIn this case,\r\nlog index = 10 pessimistic lock\r\nlog index = 11 transfer leader admin command\r\nlog index = 12 prewrite lock\r\n```\r\n\r\nWhen the follower peer P3 apply logs, the pessimistic lock and prewrite locks could be consumed in a same apply batch, like\r\n\r\napply.rs:646: [INFO] [for debug] on store_id=3 cmd_batch=[CmdBatch { level: None, cdc_id: ObserveId(0), rts_id: ObserveId(0), pitr_id: ObserveId(0), region_id: 1, cmds: [**Cmd { index: 10, term: 6, request: header { region_id: 1 peer { id: 1 store_id: 1 } region_epoch { conf_ver: 3 version: 1 } } requests { cmd_type: Put put { cf: \"lock\" key: 6B30000000000000F9 value: 53046B300A0066000000000000000A6C000000000000000001 }** }, response: header { current_term: 6 } }, Cmd { index: 11, term: 6, request: header { region_id: 1 peer { id: 1 store_id: 1 } region_epoch { conf_ver: 3 version: 1 } flags: 4 } admin_request { cmd_type: TransferLeader transfer_leader { peer { id: 3 store_id: 3 } } }, response: header { current_term: 6 } admin_response { cmd_type: TransferLeader } }, **Cmd { index: 12, term: 6, request: header { region_id: 1 peer { id: 1 store_id: 1 } region_epoch { conf_ver: 3 version: 1 } term: 6 } requests { cmd_type: Put put { cf: \"lock\" key: 6B30000000000000F9 value: 50046B300AB8177602763066000000000000000A63000000000000000B } }**, response: header { current_term: 6 } }] }], thread_id: 486\r\n\r\n\r\nIt's still not clear how the `resolved-ts` could observe such situations, as the region would be registerred to the `resolved-ts` after the new leader has applied to its current term.\r\n"
    },
    {
      "id": 2137331972,
      "user": "cfzjywxk",
      "created_at": "2024-05-29T12:47:10Z",
      "body": "> After some investigations, one of the possbility is that pessimistic and prewrite locks with the same key are applied in a same apply batch and it is observed by the `resolved-ts` component, then the assertion is break.\r\n> \r\n> Consider a leader transfer situation\r\n> \r\n> ```\r\n> region with p1, p2, p3, p1 is the leader peer\r\n> 1. Pessimistic lock k0 on p1, use in-memory lock, return success\r\n> 2. P1 attempts to transfer the leader to p3. According to the in-memory lock requirement, before officially initiating, propose an in-memory lock. Here, the lock has not been deleted yet. Propose pessimistic lock to raft log, assuming index = 10. Afterwards, propose transfer leader command index = 11.\r\n> 3. User prewrite k1 on p1, propose prewrite k0, index = 12 \r\n> ```\r\n> \r\n> In this case\r\n> \r\n> ```\r\n> In this case,\r\n> log index = 10 pessimistic lock\r\n> log index = 11 transfer leader admin command\r\n> log index = 12 prewrite lock\r\n> ```\r\n> \r\n> When the follower peer P3 apply logs, the pessimistic lock and prewrite locks could be consumed in a same apply batch, like\r\n> \r\n> apply.rs:646: [INFO] [for debug] on store_id=3 cmd_batch=[CmdBatch { level: None, cdc_id: ObserveId(0), rts_id: ObserveId(0), pitr_id: ObserveId(0), region_id: 1, cmds: [**Cmd { index: 10, term: 6, request: header { region_id: 1 peer { id: 1 store_id: 1 } region_epoch { conf_ver: 3 version: 1 } } requests { cmd_type: Put put { cf: \"lock\" key: 6B30000000000000F9 value: 53046B300A0066000000000000000A6C000000000000000001 }** }, response: header { current_term: 6 } }, Cmd { index: 11, term: 6, request: header { region_id: 1 peer { id: 1 store_id: 1 } region_epoch { conf_ver: 3 version: 1 } flags: 4 } admin_request { cmd_type: TransferLeader transfer_leader { peer { id: 3 store_id: 3 } } }, response: header { current_term: 6 } admin_response { cmd_type: TransferLeader } }, **Cmd { index: 12, term: 6, request: header { region_id: 1 peer { id: 1 store_id: 1 } region_epoch { conf_ver: 3 version: 1 } term: 6 } requests { cmd_type: Put put { cf: \"lock\" key: 6B30000000000000F9 value: 50046B300AB8177602763066000000000000000A63000000000000000B } }**, response: header { current_term: 6 } }] }], thread_id: 486\r\n> \r\n> It's still not clear how the `resolved-ts` could observe such situations, as the region would be registerred to the `resolved-ts` after the new leader has applied to its current term.\r\n\r\nBut the situation here is duplicate keys in different logs, while the panic happens when there are duplicate keys within a same log which is different.."
    },
    {
      "id": 2252062303,
      "user": "cfzjywxk",
      "created_at": "2024-07-26T06:26:41Z",
      "body": "A new reproducing with more logs, two pessimsitic locks on the key are handled in a same `RaftCmdRequest`, which is unexepected.\r\n\r\n---\r\n\r\n\r\n\r\n[2024/07/20 06:29:42.467 +00:00] [ERROR] [cmd.rs:243] [\"there is already row=RowChange { write: None, lock: Some(Put(ts:None, value:53267480000000000000985F7280000000000000298B8090ECA5EACEA106F7DF026606433B525D84000F6306433B525D84000C6C06433B525D84000F0146)), default: None } with same key processing key=7480000000000000FF985F728000000000FF0000290000000000FA value=53267480000000000000985F728000000000000029978090ECA5EACEA106FADA026606433B525D8400176306433B525D8400186C06433B525D84000F01\"] [thread_id=99]\r\n\r\nexisting lock=Lock { lock_type: Pessimistic, primary_key: 7480000000000000985F728000000000000029, start_ts: TimeStamp(**451269612609470475**), ttl: 45047, short_value: , for_update_ts: TimeStamp(451269612609470479), txn_size: 0, min_commit_ts: TimeStamp(451269612609470476), use_async_commit: false, secondaries: [], rollback_ts: [], last_change: Exist { last_change_ts: TimeStamp(451269612609470479), estimated_versions_to_last_change: 1 }, txn_source: 0, is_locked_with_conflict: true, generation: 0 }\r\n\r\nnew lock=Lock { lock_type: Pessimistic, primary_key: 7480000000000000985F728000000000000029, start_ts: TimeStamp(**451269612609470487**), ttl: 44410, short_value: , for_update_ts: TimeStamp(451269612609470487), txn_size: 0, min_commit_ts: TimeStamp(451269612609470488), use_async_commit: false, secondaries: [], rollback_ts: [], last_change: Exist { last_change_ts: TimeStamp(451269612609470479), estimated_versions_to_last_change: 1 }, txn_source: 0, is_locked_with_conflict: false, generation: 0 }\r\n\r\n\r\n\r\n\r\ntxn1 451269612609470475\r\n```\r\n# Time: 2024-07-20T06:29:42.471178774Z\r\n# Txn_start_ts: 451269612609470475\r\n# User@Host: root[root] @ 10.200.65.161 [10.200.65.161]\r\n# Conn_ID: 4125097998\r\n# Exec_retry_time: 25.164334311 Exec_retry_count: 1\r\n# Query_time: 25.167019728\r\n# Parse_time: 0.000026797\r\n# Compile_time: 0.000044644\r\n# Rewrite_time: 0.00000323\r\n# Optimize_time: 0.000030689\r\n# Wait_TS: 0.00023222\r\n# LockKeys_time: 25.166502883\r\n# DB: test\r\n# Is_internal: false\r\n# Digest: dd03a9b946aecdfd38bc3013e57e8d0f2a7a2c7b0941559966463bf30b8b4b9c\r\n# Stats: txn6:pseudo[10000;0]\r\n# Num_cop_tasks: 0\r\n# Mem_max: 4096\r\n# Prepared: false\r\n# Plan_from_cache: false\r\n# Plan_from_binding: false\r\n# Has_more_results: false\r\n# KV_total: 25.162398889\r\n# PD_total: 0.000228617\r\n# Backoff_total: 0\r\n# Write_sql_response_total: 0.000035325\r\n# Result_rows: 1\r\n# Succ: true\r\n# IsExplicitTxn: true\r\n# IsSyncStatsFailed: false\r\n# Plan: tidb_decode_plan('4ALIMAkzNl8xCTAJMQl0YWJsZTp0eG42LCBoYW5kbGU6NDEsIGxvY2sJMQl0aW1lOjI1LjJzARMQb3BzOjMBCSRja19rZXlzOiB7CSFALjQybXMsIHJlZ2lvbjoxLCAFHjQxLCBzbG93ZXN0X3JwYwEtJG90YWw6IDAuMDABUQkvWF9pZDogNTIsIHN0b3JlOiBub2RlLTEuBQeQcGVlci5qZXBzZW4tdHBzLTc2MDU1MDQtMS02Mjc6MjAxNjAsIAGDGF9kZXRhaWwBZCBpa3Zfd2FsbF8BGDg6IDEuMzNtc30sIHNjYW4ZJyxnZXRfc25hcHNob3QNKgwzNcK1AcVYb2Nrc2RiOiB7YmxvY2s6IHt9fX0sIH0N+DBycGM6Mi4zNjg5MzZtAS9McGNfY291bnQ6MX0JTi9BCU4vQQo=')\r\n# Plan_digest: 0f8e0de76ece9905c88e8fd971f9269e1d6fcdaf898bb4f2d05c6d0aa66950b0\r\n# Binary_plan: tidb_decode_binary_plan('iQNECoQDCgtQb2ludF9HZXRfMSkABQHQ8D8wATgBQAFKDgoMCgR0ZXN0EgR0eG42Ug9oYW5kbGU6NDEsIGxvY2taE3RpbWU6MjUuMnMBEhxvcHM6M2KZAgEcHF9rZXlzOiB7CSJALjQybXMsIHJlZ2lvbjoxLCAFHjQxLCBzbG93ZXN0X3JwYwEtJG90YWw6IDAuMDABUgkvWF9pZDogNTIsIHN0b3JlOiBub2RlLTEuBQeQcGVlci5qZXBzZW4tdHBzLTc2MDU1MDQtMS02Mjc6MjAxNjAsIAGDGF9kZXRhaWwBZCBpa3Zfd2FsbF8BGDg6IDEuMzNtc30sIHNjYW4ZJyxnZXRfc25hcHNob3QNKgwzNcK1AcVYb2Nrc2RiOiB7YmxvY2s6IHt9fX0sIH0hAgRjawXKIDIuMzY4OTM2bQEvMHBjX2NvdW50OjF9cP8RATQBeP///////////wEYAQ==')\r\n# Resource_group: default\r\n# Request_unit_write: 142.79999999999993\r\nselect val from txn6 where id = 41 FOR UPDATE;\r\n```\r\n\r\ntxn2 451269612609470487 \r\n```\r\n# Time: 2024-07-20T06:29:42.467100628Z\r\n# Txn_start_ts: 451269612609470487\r\n# User@Host: root[root] @ 10.200.65.161 [10.200.65.161]\r\n# Conn_ID: 3152019494\r\n# Query_time: 25.141969218\r\n# Parse_time: 0.000066793\r\n# Compile_time: 0.000188155\r\n# Rewrite_time: 0.000048971\r\n# Optimize_time: 0.000062031\r\n# Wait_TS: 0.000682948\r\n# LockKeys_time: 25.139729056\r\n# DB: test\r\n# Is_internal: false\r\n# Digest: a51559580d4ab4b95aae2fdd415c78296f0f772e7b6d9b1539bb7d1c16e5d79e\r\n# Num_cop_tasks: 0\r\n# Mem_max: 8408\r\n# Prepared: false\r\n# Plan_from_cache: false\r\n# Plan_from_binding: false\r\n# Has_more_results: false\r\n# KV_total: 25.137000108\r\n# PD_total: 0.00067564\r\n# Backoff_total: 0\r\n# Write_sql_response_total: 0\r\n# Result_rows: 0\r\n# Warnings: [{\"Level\":\"Warning\",\"Message\":\"Pipelined DML can only be used in autocommit mode. Fallback to standard mode\"}]\r\n# Succ: true\r\n# IsExplicitTxn: true\r\n# IsSyncStatsFailed: false\r\n# Plan: tidb_decode_plan('zwXwTDAJMjdfMQkwCTAJTi9BCTAJdGltZToyNS4xcywgbG9vcHM6MiwgcHJlcGFyZTogMjkuNMK1cywgY2hlY2tfaW5zZXJ0OiB7dG90YWxfBTwMIDkzNg0lCG1lbQ0jAF8JGww4My41BT84cHJlZmV0Y2g6IDg1Mi45BRRIcnBjOntCYXRjaEdldDp7bnVtXwETCDEsIB1eKDc4OS42wrVzfSwgAVMYX2RldGFpbBWCLGt2X3JlYWRfd2FsbA10EDM0MC4xBWEMdGlrdjIaAAQ1OAUaGH0sIHNjYW4+TAA0cHJvY2Vzc19rZXlzOiAVhy4XABxfc2l6ZTogNBUdFSwoZ2V0X3NuYXBzaG8R/RAyOTkuMgWJSHJvY2tzZGI6IHtibG9jazoge30BAQQsIAEODUQAewHfMYkUcmVnaW9uIQsFYSgxLCByZXNvbHZlXwEyNDoxOG1zLCBzbG93ZXN0JTUEIHslNBw6IDAuNzMxcwEwAUIsX2lkOiA1Miwgc3RvIc8Ybm9kZS00LgUHiHBlZXIuamVwc2VuLXRwcy03NjA1NTA0LTEtNjI3OjIwMTYwQnQBOlEBGDczMC4ybXNCUAFKDQEEMTNuDAEELCAxDUECQY4cMjEzNzU2NjkBy2hwY19jb3VudDoyNX0JMjE2IEJ5dGVzCU4vQQo=')\r\n# Binary_plan: tidb_decode_binary_plan('2QXwRgrUBQoISW5zZXJ0XzE4AUABUgNOL0FaE3RpbWU6MjUuMXMsIGxvb3BzOjJi5gJwcmVwYXJlOiAyOS40wrVzLCBjaGVja19pBUEgOiB7dG90YWxfBT0MIDkzNg0lEG1lbV9pBSMNGww4My41BT84cHJlZmV0Y2g6IDg1Mi45BRRIcnBjOntCYXRjaEdldDp7bnVtXwETCDEsIB1eKDc4OS42wrVzfSwgAW4YX2RldGFpbBWCKGt2X3JlYWRfd2FsEY8QMzQwLjEFYQx0aWt2MhoABDU4BRoYfSwgc2Nhbj5MADRwcm9jZXNzX2tleXM6IBWHLhcAHF9zaXplOiA0FR0VLChnZXRfc25hcHNobxH9EDI5OS4yBYlIcm9ja3NkYjoge2Jsb2NrOiB7fQEBCGKyAgEPDUUAewHgMYsUcmVnaW9uIQwFYigxLCByZXNvbHZlXwEyNDoxOG1zLCBzbG93ZXN0JTYtkxw6IDAuNzMxcwEwAUIsX2lkOiA1Miwgc3RvIdAYbm9kZS00LgUHiHBlZXIuamVwc2VuLXRwcy03NjA1NTA0LTEtNjI3OjIwMTYwQnUBOlIBGDczMC4ybXNCUQFKDgEEMTNuDQEQLCB9LCAB2wXNQZAcMjEzNzU2NjkBy2xwY19jb3VudDoyNX1w2AF4////////////ARgB')\r\n# Resource_group: default\r\n# Request_unit_read: 0.4756256103515625\r\n# Request_unit_write: 127.49999999999994\r\ninsert into txn6 (id, sk, val) values (41, 41, 18) on duplicate key update val = 18;\r\n```"
    }
  ]
}