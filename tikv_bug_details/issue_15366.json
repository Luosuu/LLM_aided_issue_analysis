{
  "issue_number": 15366,
  "title": "[dr-autosync] v6.5.4 QPS drop to 0 during switch sync_recovery  to sync",
  "body": "## Bug Report\r\n\r\n<!-- Thanks for your bug report! Don't worry if you can't fill out all the sections. -->\r\n\r\n### What version of TiKV are you using?\r\n<!-- You can run `tikv-server --version` -->\r\n```\r\n[2023/08/19 15:39:12.244 +08:00] [INFO] [lib.rs:85] [\"Welcome to TiKV\"]\r\n[2023/08/19 15:39:12.244 +08:00] [INFO] [lib.rs:90] [\"Release Version:   6.5.4\"]\r\n[2023/08/19 15:39:12.244 +08:00] [INFO] [lib.rs:90] [\"Edition:           Community\"]\r\n[2023/08/19 15:39:12.244 +08:00] [INFO] [lib.rs:90] [\"Git Commit Hash:   e033d337f373481ada457ebd7537f568283ef42b\"]\r\n[2023/08/19 15:39:12.244 +08:00] [INFO] [lib.rs:90] [\"Git Commit Branch: heads/refs/tags/v6.5.4\"]\r\n[2023/08/19 15:39:12.244 +08:00] [INFO] [lib.rs:90] [\"UTC Build Time:    Unknown (env var does not exist when building)\"]\r\n[2023/08/19 15:39:12.244 +08:00] [INFO] [lib.rs:90] [\"Rust Version:      rustc 1.67.0-nightly (96ddd32c4 2022-11-14)\"]\r\n[2023/08/19 15:39:12.244 +08:00] [INFO] [lib.rs:90] [\"Enable Features:   pprof-fp jemalloc mem-profiling portable sse test-engine-kv-rocksdb test-engine-raft-raft-engine cloud-aws cloud-gcp cloud-azure\"]\r\n```\r\n\r\n### What operating system and CPU are you using?\r\n<!-- If you're using Linux, you can run `cat /proc/cpuinfo` -->\r\n\r\n### Steps to reproduce\r\n<!-- If possible, provide a recipe for reproducing the error. A complete runnable program is good. -->\r\n1. Inject network partition during primary dc and backup dc for 1 hour;\r\n2. Delete the chaos of network partition in step 1;\r\n3. After about 15min, QPS drop more than 30%. \r\n\r\n```\r\n[2023/08/19 17:15:43.432 +08:00] [INFO] [cluster.go:520] [\"network partition will be inject\"] [source=\"[tiup,tikv1,tikv2,tikv3,tidb1,tidb3,pd1,pd2,benchtoolset,br,workload-ecasdb]\"] [target=\"[pd3,tikv4,tikv5,tikv6,tidb2,tidb4]\"]\r\n[2023/08/19 18:15:43.518 +08:00] [INFO] [cluster.go:907] [\"Delete chaos tiup &{e2e-dr-auto-sync-func-thin-tps-2070499-1-384 network-partition-qkhwnhaa 0xc0039dc8c0}\"]\r\n```\r\n\r\n### What did you expect?\r\nNo QPS drop, or drop less than 30%.\r\n\r\n\r\n\r\n### What did happened?\r\n![image](https://github.com/tikv/tikv/assets/9443637/0b558221-aa5f-4483-b59e-97b9c464fb32)\r\n\r\n",
  "state": "closed",
  "created_at": "2023-08-21T04:23:15Z",
  "updated_at": "2023-09-01T07:28:15Z",
  "closed_at": "2023-09-01T07:28:15Z",
  "labels": [
    "type/bug",
    "severity/major",
    "may-affects-5.2",
    "may-affects-5.3",
    "may-affects-5.4",
    "may-affects-6.1",
    "affects-6.5",
    "affects-7.1"
  ],
  "comments_data": [
    {
      "id": 1685619681,
      "user": "mayjiang0203",
      "created_at": "2023-08-21T04:23:50Z",
      "body": "/assign @nolouch \r\n/severity critical\r\n"
    },
    {
      "id": 1687552724,
      "user": "mayjiang0203",
      "created_at": "2023-08-22T06:38:31Z",
      "body": "By reducing workload pressure and set tikv config \"{raftstore: {\\\"store-io-pool-size: 2\\\",\\\"apply-pool-size: 4\\\"}}\" , didn't hit this again.\r\n/remove-severity critical\r\n/severity major"
    },
    {
      "id": 1702298010,
      "user": "nolouch",
      "created_at": "2023-09-01T07:28:15Z",
      "body": "need to adjust the config, closed."
    }
  ]
}