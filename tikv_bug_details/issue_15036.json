{
  "issue_number": 15036,
  "title": "tikv exit \"IO error: No space left on device: While appending to file: /var/lib/tikv/data/db/2355792.log: No space left on device\"",
  "body": "## Bug Report\r\n\r\n<!-- Thanks for your bug report! Don't worry if you can't fill out all the sections. -->\r\n\r\n### What version of TiKV are you using?\r\n<!-- You can run `tikv-server --version` -->\r\n```\r\nTiKV\r\nRelease Version:   7.2.0\r\nEdition:           Community\r\nGit Commit Hash:   12ce5540f9e8f781f14d3b3a58fb9442f03b6b29\r\nGit Commit Branch: heads/refs/tags/v7.2.0\r\nUTC Build Time:    2023-06-19 09:15:38\r\nRust Version:      rustc 1.67.0-nightly (96ddd32c4 2022-11-14)\r\nEnable Features:   pprof-fp jemalloc mem-profiling portable sse test-engine-kv-rocksdb test-engine-raft-raft-engine cloud-aws cloud-gcp cloud-azure\r\nProfile:           dist_release\r\n```\r\n\r\n### What operating system and CPU are you using?\r\n<!-- If you're using Linux, you can run `cat /proc/cpuinfo` -->\r\nlinux, 16c64g\r\n\r\ntidb * 3\r\ntikv * 12\r\npd * 3\r\n\r\n### Steps to reproduce\r\n<!-- If possible, provide a recipe for reproducing the error. A complete runnable program is good. -->\r\nstart data import using IMPORT INTO, after some time, tikv start using space rapidly(might due to data compaction)\r\nwe have confirmed in another issue that those space are not taken by `import` dir on tikv, see https://github.com/pingcap/tidb/issues/44986#issuecomment-1609653360\r\n![image](https://github.com/tikv/tikv/assets/3312245/3bebb85f-8b4b-49f7-85ef-e62f1e35d746)\r\n\r\nduring those time period, there're many pending compaction data and it's in flow control\r\n![image](https://github.com/tikv/tikv/assets/3312245/3de2661d-be0f-4399-8f87-eea487028240)\r\n![image](https://github.com/tikv/tikv/assets/3312245/b079a63d-427a-4a44-a7b1-8bcbe15fdf45)\r\n\r\n```\r\n[2023/06/27 15:25:59.967 +00:00] [FATAL] [lib.rs:497] [\"rocksdb background error. db: kv, reason: write_callback, error: IO error: No space left on device: While appending to file: /var/lib/tikv/data/db/2355792.log: No space left on device\"] [backtrace=\"   0: tikv_util::set_panic_hook::{{closure}}\\n             at home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tikv/components/tikv_util/src/lib.rs:496:18\\n   1: <alloc::boxed::Box<F,A> as core::ops::function::Fn<Args>>::call\\n             at rust/toolchains/nightly-2022-11-15-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:2032:9\\n      std::panicking::rust_panic_with_hook\\n             at rust/toolchains/nightly-2022-11-15-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:692:13\\n   2: std::panicking::begin_panic_handler::{{closure}}\\n             at rust/toolchains/nightly-2022-11-15-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:579:13\\n   3: std::sys_common::backtrace::__rust_end_short_backtrace\\n             at rust/toolchains/nightly-2022-11-15-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys_common/backtrace.rs:137:18\\n   4: rust_begin_unwind\\n             at rust/toolchains/nightly-2022-11-15-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:575:5\\n   5: core::panicking::panic_fmt\\n             at rust/toolchains/nightly-2022-11-15-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panicking.rs:65:14\\n   6: <engine_rocks::event_listener::RocksEventListener as rocksdb::event_listener::EventListener>::on_background_error\\n             at home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tikv/components/engine_rocks/src/event_listener.rs:154:13\\n      rocksdb::event_listener::on_background_error\\n             at rust/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/2096b9a/src/event_listener.rs:366:5\\n   7: _ZN24crocksdb_eventlistener_t17OnBackgroundErrorEN7rocksdb21BackgroundErrorReasonEPNS0_6StatusE\\n             at rust/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/2096b9a/librocksdb_sys/crocksdb/c.cc:2535:24\\n   8: _ZN7rocksdb12EventHelpers23NotifyOnBackgroundErrorERKSt6vectorISt10shared_ptrINS_13EventListenerEESaIS4_EENS_21BackgroundErrorReasonEPNS_6StatusEPNS_17InstrumentedMutexEPb\\n             at rust/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/2096b9a/librocksdb_sys/rocksdb/db/event_helpers.cc:67:32\\n   9: _ZN7rocksdb12ErrorHandler10SetBGErrorERKNS_6StatusENS_21BackgroundErrorReasonE\\n             at rust/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/2096b9a/librocksdb_sys/rocksdb/db/error_handler.cc:337:42\\n  10: _ZN7rocksdb6DBImpl16WriteStatusCheckERKNS_6StatusE\\n             at rust/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/2096b9a/librocksdb_sys/rocksdb/db/db_impl/db_impl_write.cc:1242:76\\n      _ZN7rocksdb6DBImpl16WriteStatusCheckERKNS_6StatusE\\n             at rust/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/2096b9a/librocksdb_sys/rocksdb/db/db_impl/db_impl_write.cc:1234:6\\n  11: _ZN7rocksdb6DBImpl19MultiBatchWriteImplERKNS_12WriteOptionsEOSt6vectorIPNS_10WriteBatchESaIS6_EEPNS_13WriteCallbackEPmmSC_PNS_17PostWriteCallbackE\\n             at rust/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/2096b9a/librocksdb_sys/rocksdb/db/db_impl/db_impl_write.cc:287:23\\n  12: _ZN7rocksdb6DBImpl15MultiBatchWriteERKNS_12WriteOptionsEOSt6vectorIPNS_10WriteBatchESaIS6_EEPNS_17PostWriteCallbackE\\n             at rust/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/2096b9a/librocksdb_sys/rocksdb/db/db_impl/db_impl_write.cc:143:72\\n  13: crocksdb_write_multi_batch_callback\\n             at rust/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/2096b9a/librocksdb_sys/crocksdb/c.cc:1203:37\\n  14: rocksdb::rocksdb::DB::multi_batch_write_callback\\n             at rust/git/checkouts/rust-rocksdb-a9a28e74c6ead8ef/2096b9a/src/rocksdb.rs:910:17\\n      engine_rocks::write_batch::RocksWriteBatchVec::write_impl\\n             at home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tikv/components/engine_rocks/src/write_batch.rs:106:13\\n      <engine_rocks::write_batch::RocksWriteBatchVec as engine_traits::write_batch::WriteBatch>::write_opt\\n             at home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tikv/components/engine_rocks/src/write_batch.rs:126:9\\n  15: engine_traits::write_batch::WriteBatch::write\\n             at home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tikv/components/engine_traits/src/write_batch.rs:85:9\\n      engine_rocks::misc::<impl engine_rocks::engine::RocksEngine>::delete_all_in_range_cf_by_key\\n             at home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tikv/components/engine_rocks/src/misc.rs:115:17\\n  16: engine_rocks::misc::<impl engine_traits::misc::MiscExt for engine_rocks::engine::RocksEngine>::delete_ranges_cf\\n             at home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tikv/components/engine_rocks/src/misc.rs:248:21\\n  17: tikv::server::gc_worker::gc_worker::GcRunner<E>::unsafe_destroy_range\\n             at home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tikv/src/server/gc_worker/gc_worker.rs:760:17\\n      <tikv::server::gc_worker::gc_worker::GcRunner<E> as tikv_util::worker::pool::Runnable>::run\\n             at home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tikv/src/server/gc_worker/gc_worker.rs:1019:21\\n  18: tikv_util::worker::pool::Worker::start_impl::{{closure}}\\n             at home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tikv/components/tikv_util/src/worker/pool.rs:475:25\\n      <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll\\n             at rust/toolchains/nightly-2022-11-15-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/future/mod.rs:91:19\\n      yatp::task::future::RawTask<F>::poll\\n             at rust/git/checkouts/yatp-e704b73c3ee279b6/5523a9a/src/task/future.rs:59:9\\n  19: yatp::task::future::TaskCell::poll\\n             at rust/git/checkouts/yatp-e704b73c3ee279b6/5523a9a/src/task/future.rs:103:9\\n      <yatp::task::future::Runner as yatp::pool::runner::Runner>::handle\\n             at rust/git/checkouts/yatp-e704b73c3ee279b6/5523a9a/src/task/future.rs:387:20\\n  20: <tikv_util::yatp_pool::YatpPoolRunner<T> as yatp::pool::runner::Runner>::handle\\n             at home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tikv/components/tikv_util/src/yatp_pool/mod.rs:193:24\\n      yatp::pool::worker::WorkerThread<T,R>::run\\n             at rust/git/checkouts/yatp-e704b73c3ee279b6/5523a9a/src/pool/worker.rs:48:13\\n      yatp::pool::builder::LazyBuilder<T>::build::{{closure}}\\n             at rust/git/checkouts/yatp-e704b73c3ee279b6/5523a9a/src/pool/builder.rs:114:25\\n      std::sys_common::backtrace::__rust_begin_short_backtrace\\n             at rust/toolchains/nightly-2022-11-15-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys_common/backtrace.rs:121:18\\n  21: std::thread::Builder::spawn_unchecked_::{{closure}}::{{closure}}\\n             at rust/toolchains/nightly-2022-11-15-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:551:17\\n      <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once\\n             at rust/toolchains/nightly-2022-11-15-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panic/unwind_safe.rs:271:9\\n      std::panicking::try::do_call\\n             at rust/toolchains/nightly-2022-11-15-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:483:40\\n      std::panicking::try\\n             at rust/toolchains/nightly-2022-11-15-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:447:19\\n      std::panic::catch_unwind\\n             at rust/toolchains/nightly-2022-11-15-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137:14\\n      std::thread::Builder::spawn_unchecked_::{{closure}}\\n             at rust/toolchains/nightly-2022-11-15-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:550:30\\n      core::ops::function::FnOnce::call_once{{vtable.shim}}\\n             at rust/toolchains/nightly-2022-11-15-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:513:5\\n  22: <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once\\n             at rust/toolchains/nightly-2022-11-15-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:2000:9\\n      <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once\\n             at rust/toolchains/nightly-2022-11-15-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:2000:9\\n      std::sys::unix::thread::Thread::new::thread_start\\n             at rust/toolchains/nightly-2022-11-15-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys/unix/thread.rs:108:17\\n  23: start_thread\\n  24: clone3\\n\"] [location=components/engine_rocks/src/event_listener.rs:154] [thread_name=gc-worker-0]\r\n[2023/06/27 15:26:22.115 +00:00] [FATAL] [server.rs:1466] [\"failed to create kv engine: Storage Engine Status { code: IoError, sub_code: None, sev: NoError, state: \\\"IO error: No space left on device: While appending to file: /var/lib/tikv/data/db/MANIFEST-2355795: No space left on device\\\" }\"]\r\n[2023/06/27 15:26:42.882 +00:00] [FATAL] [server.rs:1466] [\"failed to create kv engine: Storage Engine Status { code: IoError, sub_code: None, sev: NoError, state: \\\"IO error: No space left on device: While appending to file: /var/lib/tikv/data/db/MANIFEST-2355796: No space left on device\\\" }\"]\r\n[2023/06/27 15:27:17.949 +00:00] [FATAL] [server.rs:1466] [\"failed to create kv engine: Storage Engine Status { code: IoError, sub_code: None, sev: NoError, state: \\\"IO error: No space left on device: While appending to file: /var/lib/tikv/data/db/MANIFEST-2355797: No space left on device\\\" }\"]\r\n[2023/06/27 15:28:04.855 +00:00] [FATAL] [server.rs:1466] [\"failed to create kv engine: Storage Engine Status { code: IoError, sub_code: None, sev: NoError, state: \\\"IO error: No space left on device: While appending to file: /var/lib/tikv/data/db/MANIFEST-2355798: No space left on device\\\" }\"]\r\n[2023/06/27 15:29:39.930 +00:00] [FATAL] [server.rs:1466] [\"failed to create kv engine: Storage Engine Status { code: IoError, sub_code: None, sev: NoError, state: \\\"IO error: No space left on device: While appending to file: /var/lib/tikv/data/db/MANIFEST-2355799: No space left on device\\\" }\"]\r\n[2023/06/27 15:32:35.948 +00:00] [FATAL] [server.rs:1466] [\"failed to create kv engine: Storage Engine Status { code: IoError, sub_code: None, sev: NoError, state: \\\"IO error: No space left on device: While appending to file: /var/lib/tikv/data/db/MANIFEST-2355800: No space left on device\\\" }\"]\r\n[2023/06/27 15:37:51.071 +00:00] [FATAL] [server.rs:1466] [\"failed to create kv engine: Storage Engine Status { code: IoError, sub_code: None, sev: NoError, state: \\\"IO error: No space left on device: While appending to file: /var/lib/tikv/data/db/MANIFEST-2355801: No space left on device\\\" }\"]\r\n[2023/06/27 15:43:03.991 +00:00] [FATAL] [server.rs:1466] [\"failed to create kv engine: Storage Engine Status { code: IoError, sub_code: None, sev: NoError, state: \\\"IO error: No space left on device: While appending to file: /var/lib/tikv/data/db/MANIFEST-2355802: No space left on device\\\" }\"]\r\n[2023/06/27 15:48:20.922 +00:00] [FATAL] [server.rs:1466] [\"failed to create kv engine: Storage Engine Status { code: IoError, sub_code: None, sev: NoError, state: \\\"IO error: No space left on device: While appending to file: /var/lib/tikv/data/db/MANIFEST-2355803: No space left on device\\\" }\"]\r\n```\r\n\r\n\r\n### What did you expect?\r\ntikv can continue run\r\n### What did happened?\r\ntikv exit, and cannot start again",
  "state": "closed",
  "created_at": "2023-06-29T09:47:36Z",
  "updated_at": "2023-10-24T06:37:18Z",
  "closed_at": "2023-10-23T23:13:36Z",
  "labels": [
    "type/bug",
    "severity/major",
    "may-affects-5.2",
    "may-affects-5.3",
    "may-affects-5.4",
    "may-affects-6.1",
    "may-affects-6.5",
    "may-affects-7.1",
    "affects-7.5"
  ],
  "comments_data": [
    {
      "id": 1612739344,
      "user": "D3Hunter",
      "created_at": "2023-06-29T09:52:13Z",
      "body": "please ask me for logs during this period, it's too big to upload"
    },
    {
      "id": 1615264169,
      "user": "tonyxuqqi",
      "created_at": "2023-06-30T23:06:46Z",
      "body": "@D3Hunter Is there any region balance issue in this issue, such that most of the import goes to a few nodes only? \r\nAnd of course TiKV itself should not allow the import service to continue when space is not enough. "
    },
    {
      "id": 1615338699,
      "user": "D3Hunter",
      "created_at": "2023-07-01T01:26:07Z",
      "body": "![image](https://github.com/tikv/tikv/assets/3312245/bec071cc-daaf-4628-b921-2e4654804f9b)\r\n\r\nquite balanced\r\n\r\n> And of course TiKV itself should not allow the import service to continue when space is not enough.\r\n\r\nthe issue here is that TiKV itself exist with error due to fast drop disk space（might be caused by compaction) and unable to start again"
    },
    {
      "id": 1617819661,
      "user": "lance6716",
      "created_at": "2023-07-03T10:17:42Z",
      "body": "I remember one of our users meets a similar problem. The TiKV itself starts a huge compaction that free disk can't afford. We opened https://github.com/facebook/rocksdb/issues/11502 at that time.\r\n\r\nDo we have a plan to solve it? For example, change some compaction setting of rocksdb, or add precheck about overlapping before lightning ingest SST. @tonyxuqqi "
    },
    {
      "id": 1618901936,
      "user": "tonyxuqqi",
      "created_at": "2023-07-03T17:09:36Z",
      "body": "Two issues in this case:\r\n1) Region balance is not working properly in this case. TikV-9's store size is 300~400GB more than some other TiKV nodes.\r\n<img width=\"581\" alt=\"Screen Shot 2023-07-03 at 9 59 16 AM\" src=\"https://github.com/tikv/tikv/assets/9442422/3676e0c9-cfe4-41cb-832e-997797124065\">\r\n\r\n2) There're large compactions that takes long time (the biggest compaction takes 2048s to finish and during this compaction all related SSTs cannot be deleted).\r\n<img width=\"1784\" alt=\"Screen Shot 2023-07-03 at 9 59 49 AM\" src=\"https://github.com/tikv/tikv/assets/9442422/649af7a5-5a92-4068-b821-93dfc889cde3\">\r\n\r\nThe root cause is that there're too many L0 files (9.08K) being ingested to RocksDB and this triggers large compaction.  But this is totally up to the workload and if lightning can somehow make SSTs not overlap with each other then it can be mitigated.  \r\nAnd for workaround,  reducing the  https://docs.pingcap.com/tidb/dev/tikv-configuration-file#compaction-guard-min-output-file-size from 1MB to 512KB (or even smaller) can reduce the compaction job size. "
    },
    {
      "id": 1619224414,
      "user": "lance6716",
      "created_at": "2023-07-03T22:39:39Z",
      "body": " currently, checking number of files at L0 and applying the ingestion is at two places, so lots of time passed between the checking time and using time. should we add another check for import service before ingestion?"
    },
    {
      "id": 1619503363,
      "user": "YuJuncen",
      "created_at": "2023-07-04T05:12:39Z",
      "body": "Is it possible to dump the RocksDB log (at `$tikv_data_dir/rocksdb.info`.) and metrics? I'm not sure what happened. It seems that many L0 files compacts with a huge set of L6 files. (In normal cases, it should be compacted with L4 or even lower levels.)\r\n\r\nI think this case is different from https://github.com/facebook/rocksdb/issues/11502. The latter is triggered by a TTL compaction in a cluster with few writes. And (I guess) this case relates with the blob write to L0 and L6. (Why L0 doesn't been compacted with lower levels? Are they huge too? Or the L0 files grows too huge by themselves solely?)"
    },
    {
      "id": 1619659381,
      "user": "D3Hunter",
      "created_at": "2023-07-04T07:19:51Z",
      "body": "> L0 files (9.08K)\r\n\r\nyour screen shot seems not the cluster in the issue, here's what i got\r\n\r\n![image](https://github.com/tikv/tikv/assets/3312245/796acb7a-c388-4303-becd-f49ad2851dee)\r\n"
    },
    {
      "id": 1619663104,
      "user": "D3Hunter",
      "created_at": "2023-07-04T07:22:42Z",
      "body": "> Is it possible to dump the RocksDB log (at $tikv_data_dir/rocksdb.info.)\r\n\r\nthis part is gone since the cluster is scaled-in. the metric is still there"
    },
    {
      "id": 1620960108,
      "user": "YuJuncen",
      "created_at": "2023-07-05T03:23:52Z",
      "body": "I added some comments at https://github.com/tikv/tikv/issues/15058#issuecomment-1619789754, I'm afraid this case is non-trivial..."
    },
    {
      "id": 1738377697,
      "user": "tonyxuqqi",
      "created_at": "2023-09-28T03:19:43Z",
      "body": "https://github.com/tikv/tikv/pull/15379 should be able to fix it. "
    },
    {
      "id": 1776170902,
      "user": "tonyxuqqi",
      "created_at": "2023-10-23T23:13:37Z",
      "body": "The issue should be fixed by https://github.com/tikv/tikv/pull/15379"
    }
  ]
}