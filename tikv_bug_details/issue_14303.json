{
  "issue_number": 14303,
  "title": "[Dynamic Regions] coprocessor's hardcode timeout may not work when bucket is big",
  "body": "## Bug Report\r\n\r\n<!-- Thanks for your bug report! Don't worry if you can't fill out all the sections. -->\r\n\r\n### What version of TiKV are you using?\r\n<!-- You can run `tikv-server --version` -->\r\nNightly\r\n### What operating system and CPU are you using?\r\n<!-- If you're using Linux, you can run `cat /proc/cpuinfo` -->\r\n\r\n### Steps to reproduce\r\n<!-- If possible, provide a recipe for reproducing the error. A complete runnable program is good. -->\r\nSet end-point-request-max-handle-duration to a large duration like 2 hour.\r\nWith partitioend-raft-kv enabled, ingest 500GB data (table1), with load based split off to ensure the region size is large.\r\nRun select count(*) from table1;\r\n### What did you expect?\r\nIt returns the number. \r\n### What did happened?\r\nIt may return \"context deadline exceeded\".\r\n\r\nToday the tikv-client's code hardcoded copNextMaxBackoff to 20s, which may not be enough when the bucket is big and bucket split just did not trigger yet. ",
  "state": "closed",
  "created_at": "2023-02-28T06:43:08Z",
  "updated_at": "2023-08-10T08:26:18Z",
  "closed_at": "2023-08-10T08:26:18Z",
  "labels": [
    "type/bug",
    "severity/major",
    "feature/developing",
    "may-affects-5.1",
    "may-affects-5.2",
    "may-affects-5.3",
    "may-affects-5.4",
    "may-affects-6.1",
    "may-affects-6.5",
    "affects-7.1"
  ],
  "comments_data": [
    {
      "id": 1447666861,
      "user": "tonyxuqqi",
      "created_at": "2023-02-28T06:43:57Z",
      "body": "This copNextMaxBackoff should be able to set by TiDB global/session var."
    },
    {
      "id": 1613857630,
      "user": "tonyxuqqi",
      "created_at": "2023-06-29T21:59:14Z",
      "body": "https://github.com/tikv/tikv/pull/15029"
    }
  ]
}