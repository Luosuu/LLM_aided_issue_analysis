{
  "issue_number": 15846,
  "title": "gc can not work when disk capacity is near full",
  "body": "## Bug Report\r\n\r\n<!-- Thanks for your bug report! Don't worry if you can't fill out all the sections. -->\r\n\r\n### What version of TiKV are you using?\r\n./tikv-server -V\r\n TiKV \r\nRelease Version:   6.5.4\r\nEdition:           Community\r\nGit Commit Hash:   20ccb6ebe5b738ca9a86bcce8e38caaa9079960a\r\nGit Commit Branch: heads/refs/tags/v6.5.0\r\nUTC Build Time:    2023-09-04 02:44:07\r\nRust Version:      rustc 1.67.0-nightly (96ddd32c4 2022-11-14)\r\nEnable Features:   pprof-fp jemalloc mem-profiling portable sse test-engine-kv-rocksdb test-engine-raft-raft-engine cloud-aws cloud-gcp cloud-azure\r\nProfile:           dist_release\r\n\r\n### What operating system and CPU are you using?\r\n8c/32g\r\n\r\n### Steps to reproduce\r\n1、run mussel workload with stale read and closest-replicas \r\n2、enable ttl\r\n3、disk capacity of tikv is near full\r\n![image](https://github.com/tikv/tikv/assets/84712107/6732801c-b594-4412-8313-84305a94c3b9)\r\n\r\n### What did you expect?\r\ngc can work so that release some capacity\r\n\r\n### What did happened?\r\nwrite failed and gc can not work\r\n",
  "state": "closed",
  "created_at": "2023-10-26T06:42:41Z",
  "updated_at": "2024-01-30T10:07:05Z",
  "closed_at": "2024-01-29T18:44:46Z",
  "labels": [
    "type/bug",
    "severity/major",
    "may-affects-5.3",
    "may-affects-5.4",
    "may-affects-6.1",
    "may-affects-6.5",
    "may-affects-7.1",
    "may-affects-7.5"
  ],
  "comments_data": [
    {
      "id": 1780518564,
      "user": "Lily2025",
      "created_at": "2023-10-26T06:53:56Z",
      "body": "/assign LykxSassinator"
    },
    {
      "id": 1780518604,
      "user": "ti-chi-bot[bot]",
      "created_at": "2023-10-26T06:53:58Z",
      "body": "@Lily2025: GitHub didn't allow me to assign the following users: LykxSassinator.\n\nNote that only [tikv members](https://github.com/orgs/tikv/people) with read permissions, repo collaborators and people who have commented on this issue/PR can be assigned. Additionally, issues/PRs can only have 10 assignees at the same time.\nFor more information please see [the contributor guide](https://git.k8s.io/community/contributors/guide/first-contribution.md#issue-assignment-in-github)\n\n<details>\n\nIn response to [this](https://github.com/tikv/tikv/issues/15846#issuecomment-1780518564):\n\n>/assign LykxSassinator\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"
    },
    {
      "id": 1780605236,
      "user": "LykxSassinator",
      "created_at": "2023-10-26T07:58:48Z",
      "body": "This case shows that the whole cluster has already encountered into a corner case where both the memory and the disk usage have reached the limit.\r\n\r\nThe root cause of this issue has two main reasons:\r\n* Disk almost full. Instances `tc-tikv-3`, `tc-tikv-1` and `tc-tikv-4` are both under the `DiskUsage::AlmostFull` status, which will reject the normal write operations.\r\n* Too low limitation of memory triggers the \"reject\" on raft messages. Too low `block-cache` setting with `1MB`, causing the limit of `memory-usage-limit` is ~1MB.\r\n![f8ebe888-2e41-4e71-bae7-241ceb40009e](https://github.com/tikv/tikv/assets/18441614/b4eed99c-a748-47c1-a8e0-cc7481d8dc23)\r\nAnd this makes the limit on rejecting raft messages triggered by [reject-memory-usage-ratio](https://github.com/tikv/tikv/pull/10555/files) easier to meet the requirement.\r\n![image](https://github.com/tikv/tikv/assets/18441614/4b0913c4-dbed-45f1-b025-9d13b65599a9)\r\n\r\nWith these reasons, there exists massive pending commands cannot be executed in the mentioned nodes. And when TiDB tried to trigger a `GC` request to release the stale space, it will failed on getting the basic infos from the System Catalog with the timeout message `9005: region is unavailable`.\r\n![img_v3_024j_9b250028-d2e7-415f-9a55-980abce905eg](https://github.com/tikv/tikv/assets/18441614/75b905b9-2ee6-4cac-8bf3-bb83ceffa983)\r\n\r\nSo, it keeps in a endless looping on \"failed to trigger `GC`\".\r\n"
    },
    {
      "id": 1782252927,
      "user": "LykxSassinator",
      "created_at": "2023-10-27T04:00:08Z",
      "body": "Extra references:\r\n1. tc-tikv-4 has plenty of `Running Tasks` in ReadPool:\r\n![image](https://github.com/tikv/tikv/assets/18441614/05f5f04f-ed10-425e-a724-1c03ace60937)\r\n2. reject to append raft logs:\r\n![image](https://github.com/tikv/tikv/assets/18441614/b59d33da-7570-4953-af5f-fbf3afbdfb81)\r\n"
    },
    {
      "id": 1797971117,
      "user": "Lily2025",
      "created_at": "2023-11-07T07:43:06Z",
      "body": "another test with default block cache and memory-usage-limit\r\n\r\ngc can not work when disk capacity is near full\r\n\r\n![image](https://github.com/tikv/tikv/assets/84712107/05de9bd4-e329-48ba-acab-c18d28dc7c2a)\r\n\r\n![image](https://github.com/tikv/tikv/assets/84712107/20932f35-cdd6-4041-96e8-e042acfefe6b)\r\n\r\n![image](https://github.com/tikv/tikv/assets/84712107/6652c12d-15f1-43ad-8020-13bccdfc9361)\r\n\r\n\r\n\r\n"
    },
    {
      "id": 1797988027,
      "user": "LykxSassinator",
      "created_at": "2023-11-07T07:57:30Z",
      "body": "The above case has the same root cause in this issue.\r\n\r\nIn this case:\r\n\r\n1. The memory usage in `tc-tikv-5` reached the `reject-messages-on-memory-ratio` limit, causing it reject flushing raft logs.\r\n![image](https://github.com/tikv/tikv/assets/18441614/e192a3db-27b3-42f9-afff-e00dab0c953e)\r\n![image](https://github.com/tikv/tikv/assets/18441614/0a499d6e-779e-4082-b424-6d8833cc907e)\r\n![image](https://github.com/tikv/tikv/assets/18441614/dde0b32e-0f68-4dfe-8f6b-86922097f8b4)\r\n\r\n2. The GC cannot be promoted in the first step, where `ResolveLock` cannot be applied, as tc-tikv-5 rejects to append raft logs.\r\n\r\n\r\n"
    },
    {
      "id": 1798010598,
      "user": "MyonKeminta",
      "created_at": "2023-11-07T08:15:47Z",
      "body": "For [the case posted at 20231107](https://github.com/tikv/tikv/issues/15846#issuecomment-1797971117):\r\n At 11/6 about 12:30, tikv-1 and tikv-3 entered disk-almost-full state. There found to be multiple issues occurring in this cluster:\r\n\r\n* tikv-3 produces too much logs like this:\r\n```\r\n[2023/11/07 15:39:45.269 +08:00] [WARN] [endpoint.rs:782] [error-response] [err=\"Region error (will back off and retry) message: \\\"peer 19734 is not ready, safe_ts 445446682107445300, region 19731\\\" data_is_not_ready { region_id: 19731 peer_id: 19734 safe_ts: 445446682107445300 }\"]\r\n```\r\n\r\nand this:\r\n\r\n```\r\n[2023/11/07 15:39:45.452 +08:00] [WARN] [endpoint.rs:782] [error-response] [err=\"Region error (will back off and retry) message: \\\"EpochNotMatch current epoch of region 36929 is conf_ver: 53 version: 184, but you sent conf_ver: 53 version: 185\\\" epoch_not_match { current_regions { id: 36929 end_key: 7480000000000003FF3B5F720131303030FF38363237FF300000FF0000000000F80170FF335F3233000000FCFF0419B18AF29901E2FFD200000000000000F8 region_epoch { conf_ver: 53 version: 184 } peers { id: 36930 store_id: 7 } peers { id: 36931 store_id: 6 } peers { id: 36932 store_id: 5 } } }\"]\r\n```\r\n\r\nAs the region in tikv-3 cannot apply raft logs in disk-almost-full state, it has too old region state and cannot handle requests. We may consider avoiding follower-read requests to be sent to full TiKV nodes.\r\n\r\n* tikv-1 and tikv-3's being full made tikv-5's memory usage by raftstore significantly increased and it starts to reject appending new raft logs, which may make some write operations fail. Detail mentioned in [the comment above](https://github.com/tikv/tikv/issues/15846#issuecomment-1797988027).\r\n* TiDB produces too much logs like:\r\n```\r\n[2023/11/06 18:06:57.152 +08:00] [WARN] [region_request.go:630] [\"unable to find stores with given labels\"] [region=36253] [leader-epoch-stale=false] [leader-unreachable=false] [leader-invalid=false] [stale-read=true] [labels=\"[{\\\"key\\\":\\\"zone\\\",\\\"value\\\":\\\"zone2\\\"}]\"]\r\n```\r\nNote that the TiKV nodes labeled `zone2` are tikv-1 and tikv-3, which is exactly the full nodes.\r\nPerhaps this can also be avoided by avoiding sending follower read requests to full nodes.\r\n\r\n* A few transactions run for too long (over 1h). The reason is not sure yet, but possibly related to that tikv-5 cannot handle some writes. This is *one of* the reasons that GC is being blocked.\r\n* When GC failed to run, the latest GC-ed safepoint seems to be still recorded. When there's something blocking GC (such as an active session) so that the next turn of GC is using the same safepoint, GCWorker may believe that it has already done GC with that safepoint and skip the next GC. This is not yet confirmed according to the code. If this is true, it can also be improved."
    },
    {
      "id": 1838063856,
      "user": "Lily2025",
      "created_at": "2023-12-04T08:32:53Z",
      "body": "drop db failed if disk full\r\n![image](https://github.com/tikv/tikv/assets/84712107/6e1b8e16-ddf2-47ba-a0fb-1714b933234e)\r\n log from tidb:\r\n\r\n[2023/12/06 11:54:35.893 +08:00] [WARN] [backoff.go:179] [\"tikvDiskFull backoffer.maxSleep 80000ms is exceeded, errors:\\ntikv disk full: store_id:6 reason:\\\"propose failed: tikv disk full, cmd diskFullOpt=AllowedOnAlmostFull, leader diskUsage=AlreadyFull\\\"  ctx: region ID: 1259, meta: id:1259 end_key:\\\"t\\\\200\\\\000\\\\000\\\\000\\\\000\\\\000\\\\003=_r\\\\0014809455\\\\000\\\\376\\\\001e6_45\\\\000\\\\000\\\\000\\\\374\\\\004\\\\031\\\\261\\\\002\\\\022F\\\\n\\\\216~\\\" region_epoch:<conf_ver:57 version:680 > peers:<id:1262 store_id:6 > peers:<id:81357 store_id:9 > peers:<id:81358 store_id:10 > , peer: id:1262 store_id:6 , addr: thirdly-tc-tikv-0.thirdly-tc-tikv-peer.glh-airbnb-h5vdj.svc:20160, idx: 0, reqStoreType: TiKvOnly, runStoreType: tikv at 2023-12-06T11:54:20.888746723+08:00\\ntikv disk full: store_id:6 reason:\\\"propose failed: tikv disk full, cmd diskFullOpt=AllowedOnAlmostFull, leader diskUsage=AlreadyFull\\\"  ctx: region ID: 1259, meta: id:1259 end_key:\\\"t\\\\200\\\\000\\\\000\\\\000\\\\000\\\\000\\\\003=_r\\\\0014809455\\\\000\\\\376\\\\001e6_45\\\\000\\\\000\\\\000\\\\374\\\\004\\\\031\\\\261\\\\002\\\\022F\\\\n\\\\216~\\\" region_epoch:<conf_ver:57 version:680 > peers:<id:1262 store_id:6 > peers:<id:81357 store_id:9 > peers:<id:81358 store_id:10 > , peer: id:1262 store_id:6 , addr: thirdly-tc-tikv-0.thirdly-tc-tikv-peer.glh-airbnb-h5vdj.svc:20160, idx: 0, reqStoreType: TiKvOnly, runStoreType: tikv at 2023-12-06T11:54:25.889508417+08:00\\ntikv disk full: store_id:6 reason:\\\"propose failed: tikv disk full, cmd diskFullOpt=AllowedOnAlmostFull, leader diskUsage=AlreadyFull\\\"  ctx: region ID: 1259, meta: id:1259 end_key:\\\"t\\\\200\\\\000\\\\000\\\\000\\\\000\\\\000\\\\003=_r\\\\0014809455\\\\000\\\\376\\\\001e6_45\\\\000\\\\000\\\\000\\\\374\\\\004\\\\031\\\\261\\\\002\\\\022F\\\\n\\\\216~\\\" region_epoch:<conf_ver:57 version:680 > peers:<id:1262 store_id:6 > peers:<id:81357 store_id:9 > peers:<id:81358 store_id:10 > , peer: id:1262 store_id:6 , addr: thirdly-tc-tikv-0.thirdly-tc-tikv-peer.glh-airbnb-h5vdj.svc:20160, idx: 0, reqStoreType: TiKvOnly, runStoreType: tikv at 2023-12-06T11:54:30.891470113+08:00\\ntotal-backoff-times: 20, backoff-detail: tikvDiskFull:19, regionMiss:1, maxBackoffTimeExceeded: true, maxExcludedTimeExceeded: false\\nlongest sleep type: tikvDiskFull, time: 82500ms\"]\r\n[2023/12/06 11:54:35.989 +08:00] [INFO] [region_request.go:1317] [\"send request meet region error without retry\"] [req-ts=446125780061913094] [req-type=Prewrite] [region=\"{ region id: 1259, ver: 680, confVer: 57 }\"] [region-is-valid=true] [retry-times=7] [replica-read-type=leader] [replica-selector-state=accessKnownLeader] [stale-read=false] [replica-status=\"peer: 1262, store: 6, isEpochStale: false, attempts: 8, replica-epoch: 0, store-epoch: 0, store-state: resolved, store-liveness-state: reachable; peer: 81357, store: 9, isEpochStale: false, attempts: 0, replica-epoch: 0, store-epoch: 0, store-state: resolved, store-liveness-state: reachable; peer: 81358, store: 10, isEpochStale: false, attempts: 0, replica-epoch: 3, store-epoch: 3, store-state: resolved, store-liveness-state: reachable\"] [total-backoff-ms=82502] [total-backoff-times=20] [max-exec-timeout-ms=20000] [total-region-errors=disk_full:8]\r\n[2023/12/06 11:54:35.990 +08:00] [WARN] [backoff.go:179] [\"regionMiss backoffer.maxSleep 80000ms is exceeded, errors:\\ntikv disk full: store_id:6 reason:\\\"propose failed: tikv disk full, cmd diskFullOpt=AllowedOnAlmostFull, leader diskUsage=AlreadyFull\\\"  ctx: region ID: 1259, meta: id:1259 end_key:\\\"t\\\\200\\\\000\\\\000\\\\000\\\\000\\\\000\\\\003=_r\\\\0014809455\\\\000\\\\376\\\\001e6_45\\\\000\\\\000\\\\000\\\\374\\\\004\\\\031\\\\261\\\\002\\\\022F\\\\n\\\\216~\\\" region_epoch:<conf_ver:57 version:680 > peers:<id:1262 store_id:6 > peers:<id:81357 store_id:9 > peers:<id:81358 store_id:10 > , peer: id:1262 store_id:6 , addr: thirdly-tc-tikv-0.thirdly-tc-tikv-peer.glh-airbnb-h5vdj.svc:20160, idx: 0, reqStoreType: TiKvOnly, runStoreType: tikv at 2023-12-06T11:54:20.888746723+08:00\\ntikv disk full: store_id:6 reason:\\\"propose failed: tikv disk full, cmd diskFullOpt=AllowedOnAlmostFull, leader diskUsage=AlreadyFull\\\"  ctx: region ID: 1259, meta: id:1259 end_key:\\\"t\\\\200\\\\000\\\\000\\\\000\\\\000\\\\000\\\\003=_r\\\\0014809455\\\\000\\\\376\\\\001e6_45\\\\000\\\\000\\\\000\\\\374\\\\004\\\\031\\\\261\\\\002\\\\022F\\\\n\\\\216~\\\" region_epoch:<conf_ver:57 version:680 > peers:<id:1262 store_id:6 > peers:<id:81357 store_id:9 > peers:<id:81358 store_id:10 > , peer: id:1262 store_id:6 , addr: thirdly-tc-tikv-0.thirdly-tc-tikv-peer.glh-airbnb-h5vdj.svc:20160, idx: 0, reqStoreType: TiKvOnly, runStoreType: tikv at 2023-12-06T11:54:25.889508417+08:00\\ntikv disk full: store_id:6 reason:\\\"propose failed: tikv disk full, cmd diskFullOpt=AllowedOnAlmostFull, leader diskUsage=AlreadyFull\\\"  ctx: region ID: 1259, meta: id:1259 end_key:\\\"t\\\\200\\\\000\\\\000\\\\000\\\\000\\\\000\\\\003=_r\\\\0014809455\\\\000\\\\376\\\\001e6_45\\\\000\\\\000\\\\000\\\\374\\\\004\\\\031\\\\261\\\\002\\\\022F\\\\n\\\\216~\\\" region_epoch:<conf_ver:57 version:680 > peers:<id:1262 store_id:6 > peers:<id:81357 store_id:9 > peers:<id:81358 store_id:10 > , peer: id:1262 store_id:6 , addr: thirdly-tc-tikv-0.thirdly-tc-tikv-peer.glh-airbnb-h5vdj.svc:20160, idx: 0, reqStoreType: TiKvOnly, runStoreType: tikv at 2023-12-06T11:54:30.891470113+08:00\\ntotal-backoff-times: 20, backoff-detail: regionMiss:1, tikvDiskFull:19, maxBackoffTimeExceeded: true, maxExcludedTimeExceeded: false\\nlongest sleep type: tikvDiskFull, time: 82500ms\"]\r\n[2023/12/06 11:54:35.990 +08:00] [WARN] [session.go:1014] [\"can not retry txn\"] [label=internal] [error=\"tikv disk full\"] [IsBatchInsert=false] [IsPessimistic=false] [InRestrictedSQL=true] [tidb_retry_limit=10] [tidb_disable_txn_auto_retry=true]\r\n[2023/12/06 11:54:35.990 +08:00] [WARN] [session.go:1030] [\"commit failed\"] [\"finished txn\"=\"Txn{state=invalid}\"] [error=\"tikv disk full\"]\r\n[2023/12/06 11:54:35.990 +08:00] [WARN] [session.go:2261] [\"run statement failed\"] [schemaVersion=48] [error=\"tikv disk full\"] [session=\"{\\n  \\\"currDBName\\\": \\\"\\\",\\n  \\\"id\\\": 0,\\n  \\\"status\\\": 2,\\n  \\\"strictMode\\\": true,\\n  \\\"user\\\": null\\n}\"]\r\n[2023/12/06 11:54:35.990 +08:00] [WARN] [gc_worker.go:288] [\"[gc worker] check leader\"] [error=\"tikv disk full\"]\r\n[2023/12/06 11:54:36.877 +08:00] [INFO] [http_handler.go:2220] [\"update server labels\"] [labels=\"{\\\"zone\\\":\\\"zone1\\\"}\"]\r\n[2023/12/06 11:54:37.046 +08:00] [INFO] [http_handler.go:2220] [\"update server labels\"] [labels=\"{\\\"zone\\\":\\\"zone1\\\"}\"]\r\n\r\n"
    },
    {
      "id": 1915347448,
      "user": "tonyxuqqi",
      "created_at": "2024-01-29T18:44:46Z",
      "body": "@Lily2025 It's by design. When disk is full, tikv will not process write command.  In TiDB Cloud, we will increase the disk capacity before it becomes disk full. "
    }
  ]
}