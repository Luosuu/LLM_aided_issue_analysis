{
  "issue_number": 17213,
  "title": "Raft snapshot blocked when peer is removed right after requesting snapshot",
  "body": "## Bug Report\r\n\r\n<!-- Thanks for your bug report! Don't worry if you can't fill out all the sections. -->\r\n\r\nRaft snapshots get blocked when a peer is removed immediately after requesting a snapshot. This occurs because the leader must wait for `MsgSnapGenPrecheckResponse` before generating snapshots. As a result, not only is the snapshot blocked, but subsequent conf changes (such as adding a node) are also blocked.\r\n\r\nTo resolve this issue, TiKV needs to cancel the snapshot generation task if a peer becomes unreachable.\r\n\r\nhttps://github.com/tikv/tikv/blob/5229f839c48d3b3f3f1218e7a465e13f37e864e8/components/raftstore/src/store/peer.rs#L2658-L2676\r\n\r\nCc #17019 \r\n\r\n### What version of TiKV are you using?\r\n<!-- You can run `tikv-server --version` -->\r\n\r\nNightly, 2024-06-26\r\n\r\n### Steps to reproduce\r\n<!-- If possible, provide a recipe for reproducing the error. A complete runnable program is good. -->\r\n\r\n```rust\r\n#[test_case(test_raftstore::new_node_cluster)]\r\nfn test_remove_peer_before_receiving_snapshot() {\r\n    let mut cluster = new_cluster(0, 3);\r\n    let pd_client = cluster.pd_client.clone();\r\n    pd_client.disable_default_operator();\r\n\r\n    // Use run conf change to make new node be initialized with term 0.\r\n    let r = cluster.run_conf_change();\r\n\r\n    // Add peer(3, 3) to store 3\r\n    cluster.must_put(b\"k0\", b\"v0\");\r\n    pd_client.must_add_peer(r, new_peer(3, 3));\r\n    must_get_equal(&cluster.get_engine(3), b\"k0\", b\"v0\");\r\n\r\n    // Add peer(2, 2) to store 2\r\n    //\r\n    // Drop MsgSnapGenPrecheckRequest to region 1 on store 2 before adding peer.\r\n    let (send_tx, send_rx) = mpsc::sync_channel(1);\r\n    cluster.add_send_filter(CloneFilterFactory(\r\n        RegionPacketFilter::new(r, 2)\r\n            .msg_type(MessageType::MsgSnapshot)\r\n            .drop_extra_message(ExtraMessageType::MsgSnapGenPrecheckRequest)\r\n            .direction(Direction::Recv)\r\n            .set_msg_callback(Arc::new(move |m: &RaftMessage| {\r\n                if m.get_extra_msg().get_type() == ExtraMessageType::MsgSnapGenPrecheckRequest {\r\n                    let _ = send_tx.send(());\r\n                }\r\n            })),\r\n    ));\r\n    // Make sure add peer conf change is applied on leader side.\r\n    pd_client.must_add_peer(r, new_peer(2, 2));\r\n\r\n    // Make sure peer 2 has requested snapshot.\r\n    send_rx.recv_timeout(Duration::from_secs(5)).unwrap();\r\n\r\n    // Remove peer(2, 2) from store 2\r\n    pd_client.must_remove_peer(r, new_peer(2, 2));\r\n\r\n    // Clear filters and add peer(2, 4) to store 2.\r\n    cluster.clear_send_filters();\r\n    pd_client.must_add_peer(r, new_peer(2, 4));\r\n\r\n    // Transfer leader to peer 4 to make sure it has applied a snapshot.\r\n    cluster.must_transfer_leader(1, new_peer(2, 4));\r\n    cluster.must_put(b\"k1\", b\"v1\");\r\n    must_get_equal(&cluster.get_engine(2), b\"k1\", b\"v1\");\r\n}\r\n```\r\n\r\n### What did you expect?\r\n\r\nDot not block snapshot.\r\n",
  "state": "closed",
  "created_at": "2024-06-27T10:17:20Z",
  "updated_at": "2024-07-01T03:34:28Z",
  "closed_at": "2024-07-01T03:34:28Z",
  "labels": [
    "type/bug",
    "severity/major",
    "affects-8.2"
  ],
  "comments_data": []
}