{
  "issue_number": 16892,
  "title": "ReadIndex raft message may fail to advance max_ts in the middle of write_lock",
  "body": "## Bug Report\r\n\r\n\r\n```\r\nfn concurrent_update_maxts_and_commit(read_index_in_middle: bool) {\r\n    use kvproto::{\r\n        kvrpcpb::{Mutation, Op},\r\n        raft_cmdpb::{ReadIndexRequest, ReadIndexResponse},\r\n    };\r\n    use test_raftstore::{\r\n        must_kv_commit, must_kv_prewrite, must_kv_prewrite_with, must_kv_read_equal, new_mutation,\r\n    };\r\n    let mut cluster = new_server_cluster(0, 1);\r\n    cluster.run();\r\n\r\n    let cm = cluster.sim.read().unwrap().get_concurrency_manager(1);\r\n\r\n    let region = cluster.get_region(b\"\");\r\n    let leader = region.get_peers()[0].clone();\r\n    let follower = new_learner_peer(2, 2);\r\n    let addr = cluster.sim.rl().get_addr(leader.get_store_id()).to_owned();\r\n\r\n    let env = Arc::new(Environment::new(1));\r\n    let channel = ChannelBuilder::new(env).connect(&addr);\r\n    let client = TikvClient::new(channel);\r\n\r\n    let region_id = leader.get_id();\r\n\r\n    let mut ctx = Context::default();\r\n    ctx.set_region_id(region_id);\r\n    ctx.set_peer(leader.clone());\r\n    ctx.set_region_epoch(region.get_region_epoch().clone());\r\n\r\n    let mut read_index = |ranges: &[(&[u8], &[u8])], start_ts: u64| {\r\n        let mut m = raft::eraftpb::Message::default();\r\n        m.set_msg_type(MessageType::MsgReadIndex);\r\n        let mut read_index_req = ReadIndexRequest::default();\r\n        read_index_req.set_start_ts(start_ts);\r\n        for &(start_key, end_key) in ranges {\r\n            let mut range = KeyRange::default();\r\n            range.set_start_key(start_key.to_vec());\r\n            range.set_end_key(end_key.to_vec());\r\n            read_index_req.mut_key_ranges().push(range);\r\n        }\r\n\r\n        let rctx = ReadIndexContext {\r\n            id: Uuid::new_v4(),\r\n            request: Some(read_index_req),\r\n            locked: None,\r\n        };\r\n        let mut e: raft::prelude::Entry = raft::eraftpb::Entry::default();\r\n        e.set_data(rctx.to_bytes().into());\r\n        m.mut_entries().push(e);\r\n        m.set_from(2);\r\n\r\n        let mut raft_msg = kvproto::raft_serverpb::RaftMessage::default();\r\n        raft_msg.set_region_id(region.get_id());\r\n        raft_msg.set_from_peer(follower.clone());\r\n        raft_msg.set_to_peer(leader.clone());\r\n        raft_msg.set_region_epoch(region.get_region_epoch().clone());\r\n        raft_msg.set_message(m);\r\n        cluster.send_raft_msg(raft_msg).unwrap();\r\n\r\n        (ReadIndexResponse::default(), start_ts)\r\n    };\r\n\r\n    let cli = {\r\n        let env = Arc::new(Environment::new(1));\r\n        let channel = ChannelBuilder::new(env).connect(&addr);\r\n        TikvClient::new(channel)\r\n    };\r\n\r\n    if !read_index_in_middle {\r\n        // read index -> calculate_min_commit_ts -> write lock\r\n        fail::cfg(\"before_calculate_min_commit_ts\", \"pause\").unwrap();\r\n    }\r\n    let mut prewrite_resp = {\r\n        use kvproto::kvrpcpb::PrewriteRequest;\r\n        let mut prewrite_req = PrewriteRequest::default();\r\n        prewrite_req.set_context(ctx.clone());\r\n        prewrite_req.set_mutations(\r\n            vec![new_mutation(Op::Put, &b\"key2\"[..], &b\"value1\"[..])]\r\n                .into_iter()\r\n                .collect(),\r\n        );\r\n        prewrite_req.primary_lock = b\"key2\".to_vec();\r\n        prewrite_req.start_version = 10;\r\n        prewrite_req.lock_ttl = 3000;\r\n        prewrite_req.for_update_ts = 0;\r\n        prewrite_req.min_commit_ts = prewrite_req.start_version + 1;\r\n        prewrite_req.use_async_commit = true;\r\n        prewrite_req.try_one_pc = false;\r\n        let prewrite_resp = client.kv_prewrite_async(&prewrite_req).unwrap();\r\n        prewrite_resp\r\n    };\r\n    std::thread::sleep(Duration::from_millis(2000));\r\n    if read_index_in_middle {\r\n        // calculate_min_commit_ts -> read index -> write lock\r\n        fail::cfg(\"after_calculate_min_commit_ts\", \"pause\").unwrap();\r\n    }\r\n    let prev_cm_max_ts = cm.max_ts();\r\n    let (resp, start_ts) = read_index(&[(b\"a\", b\"z\")], 1112);\r\n    std::thread::sleep(Duration::from_millis(2000));\r\n    assert_ne!(cm.max_ts(), prev_cm_max_ts);\r\n    assert_eq!(cm.max_ts().into_inner(), start_ts);\r\n    fail::remove(\"before_calculate_min_commit_ts\");\r\n    fail::remove(\"after_calculate_min_commit_ts\");\r\n\r\n    let pre_resp = prewrite_resp.receive_sync();\r\n    info!(\"pre_resp is {:?}\", pre_resp);\r\n\r\n    {\r\n        let mut leader_id = 0;\r\n        let peers = region.get_peers();\r\n        for p in peers {\r\n            if p.get_id() == leader.get_id() {\r\n                leader_id = p.get_id();\r\n                break;\r\n            }\r\n        }\r\n        info!(\"leader_id is {}\", leader_id);\r\n\r\n        let mut c = Context::default();\r\n        c.set_region_id(region.get_id());\r\n        c.set_region_epoch(region.get_region_epoch().clone());\r\n        c.set_peer(leader.clone());\r\n        c.set_replica_read(true);\r\n        let mut range = KeyRange::default();\r\n        let raw_key = b\"key2\";\r\n        let encoded_key = Key::from_raw(raw_key);\r\n        range.set_start_key(encoded_key.as_encoded().to_vec());\r\n        let snap_c = SnapContext {\r\n            pb_ctx: &c,\r\n            // start_ts: Some(1112.into()),\r\n            start_ts: None,\r\n            key_ranges: vec![range],\r\n            ..Default::default()\r\n        };\r\n\r\n        use engine_traits::KvEngine;\r\n        use tikv::storage::{kv::SnapContext, mvcc::MvccReader, Engine};\r\n        let mut engine = cluster.sim.rl().storages[&leader_id].clone();\r\n        let snapshot = engine.snapshot(snap_c).unwrap();\r\n        let mut reader = MvccReader::new(snapshot, None, true);\r\n        let lock = reader\r\n            .load_lock(&Key::from_raw(&b\"key2\"[..]))\r\n            .unwrap()\r\n            .unwrap();\r\n        info!(\"!!!!! ddddd {:?}\", lock);\r\n        // assert_eq!(lock.ts, start_ts.into());\r\n        assert!(!lock.is_pessimistic_lock());\r\n        assert_eq!(lock.min_commit_ts.into_inner(), 1113);\r\n    }\r\n\r\n    // must_kv_read_equal(&cli, ctx.clone(), b\"key2\".to_vec(),\r\n    // b\"value1\".to_vec(), 1245);\r\n}\r\n\r\n#[test]\r\nfn test_concurrent_update_maxts_and_commit_middle() {\r\n    concurrent_update_maxts_and_commit(true);\r\n}\r\n\r\n#[test]\r\nfn test_concurrent_update_maxts_and_commit_before() {\r\n    concurrent_update_maxts_and_commit(false);\r\n}\r\n\r\n```\r\n\r\nIn `test_concurrent_update_maxts_and_commit_middle`, read index raft message is handled in the middle of `write_lock`. It happens just after a `min_commit_ts` is computed and just before the lock is written.\r\n\r\nIn this case, the lock written later will have obsolete min_commit_ts.\r\n\r\n<!-- Thanks for your bug report! Don't worry if you can't fill out all the sections. -->\r\n\r\n### What version of TiKV are you using?\r\n<!-- You can run `tikv-server --version` -->\r\nALL\r\n### What operating system and CPU are you using?\r\n<!-- If you're using Linux, you can run `cat /proc/cpuinfo` -->\r\nALL\r\n### Steps to reproduce\r\n<!-- If possible, provide a recipe for reproducing the error. A complete runnable program is good. -->\r\n\r\n### What did you expect?\r\n\r\n### What did happened?\r\n",
  "state": "closed",
  "created_at": "2024-04-19T08:36:23Z",
  "updated_at": "2024-04-19T08:55:32Z",
  "closed_at": "2024-04-19T08:55:32Z",
  "labels": [
    "type/bug"
  ],
  "comments_data": [
    {
      "id": 2066132414,
      "user": "CalvinNeo",
      "created_at": "2024-04-19T08:55:32Z",
      "body": "Close to provide a better ut sample."
    }
  ]
}