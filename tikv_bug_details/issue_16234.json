{
  "issue_number": 16234,
  "title": "TiKV txn scheduler may cause OOM when raftstore runs slow",
  "body": "## Bug Report\r\n\r\n<!-- Thanks for your bug report! Don't worry if you can't fill out all the sections. -->\r\n\r\nIn an internal test that injects IO fault and stops write completely, we find TiKV memory keeps growing and OOM eventually. \r\nAfter investigation, we find most of the memory is consumed by `check_secondary_locks` RPCs, they pile up on txn latches.\r\n\r\n![image](https://github.com/tikv/tikv/assets/2150711/2f747d23-2423-481d-a6ab-e50d11c0367d)\r\n\r\n![image](https://github.com/tikv/tikv/assets/2150711/4f805c23-dd53-4fba-870d-c33b835eff78)\r\n(`check_secondary_locks` tasks are all in new stages, which means they are blocked by latch.)\r\n\r\nTo fix the OOM we should:\r\n- [x] limit the amount of memory of requests that txn latch can hold. [#16473](https://github.com/tikv/tikv/pull/16473)\r\n- [x] limit the amount of memory of pending/running task on scheduler worker pool. [#16473](https://github.com/tikv/tikv/pull/16473)\r\n- [ ] limit the amount of memory of pending/running tasks on scheduler read pool.\r\n- [x] limit the amount of memory of pending/running tasks on coprocessor read pool. See https://github.com/tikv/tikv/issues/16653\r\n\r\nCc https://github.com/tikv/tikv/issues/13731\r\nCc https://github.com/tikv/tikv/issues/15701\r\nCc #2508\r\n\r\n### Steps to reproduce\r\n<!-- If possible, provide a recipe for reproducing the error. A complete runnable program is good. -->\r\n\r\nInject IO fault and pause write.\r\n\r\n### What did you expect?\r\n\r\nNo OOM.\r\n\r\n### What did happened?\r\n\r\nOOM.",
  "state": "closed",
  "created_at": "2023-12-23T04:12:40Z",
  "updated_at": "2024-08-01T17:52:03Z",
  "closed_at": "2024-04-30T01:13:54Z",
  "labels": [
    "type/bug",
    "sig/scheduling",
    "severity/major",
    "affects-4.0",
    "affects-5.0",
    "affects-5.1",
    "affects-5.2",
    "affects-5.3",
    "affects-5.4",
    "affects-6.0",
    "affects-6.1",
    "affects-6.2",
    "affects-6.3",
    "affects-6.4",
    "affects-6.5",
    "affects-6.6",
    "affects-7.0",
    "affects-7.1",
    "affects-7.2",
    "affects-7.3",
    "affects-7.4",
    "affects-7.5"
  ],
  "comments_data": [
    {
      "id": 1913937322,
      "user": "cfzjywxk",
      "created_at": "2024-01-29T04:23:22Z",
      "body": "This was also found in a customer's production environemnt before https://github.com/tikv/tikv/issues/14838.  At that time, the focus was mainly on metrics and slow responses rather than memory. In fact, the root causes of these issues all stem from:\r\n1. Commands that have timed out cannot be canceled in a timely manner and continue to hold latches.\r\n2. Commands in a latch wait state, although they currently respond with a deadline error (https://github.com/tikv/tikv/pull/10771), the related tasks still persist in the queue until they can acquire the latch to wake up or execute them, and then they can be cleaned up because of timeout check.\r\n3. For operations received by the KV client due to being busy, resulting in a deadline error, it is necessary to control the backoff and retry frequency to reduce the possibility of accumulation.\r\n\r\nPerhaps, we may also need a similar mechanism like https://github.com/tikv/tikv/issues/14151 to avoid sending too many commands to already queued tikv nodes to make it worse and occupy lots of memory. "
    },
    {
      "id": 1914059817,
      "user": "overvenus",
      "created_at": "2024-01-29T06:44:32Z",
      "body": "Agree, most OOM issues in scheduler eventually boil down to some unknown timeout or retry bugs. The solution (memory quota) proposed here is more like the last resort to prevent OOM."
    }
  ]
}