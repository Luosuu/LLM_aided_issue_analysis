[
  {
    "bug_location": "Monitoring/Metrics",
    "severity": 2,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Incomplete thread CPU metric tracking for backup, restore, and CDC components",
    "issue_number": 10003,
    "title": "backup, restore and cdc CPU graphs should also be included in \"Thread CPU\""
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Incorrect handling of peer state during region hibernation, causing potential misreporting of peer status to PD",
    "issue_number": 10017,
    "title": "raftstore: down peer collection did not consider hibernate region"
  },
  {
    "bug_location": "RocksDB Snapshot Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Snapshot references not being properly released, causing resource leakage and preventing data cleanup in removed regions",
    "issue_number": 10027,
    "title": "rocksdb snapshot doesn't released and make space not reclaimed."
  },
  {
    "bug_location": "CDC (Change Data Capture) old value cache",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Ineffective caching mechanism for pessimistic transaction writes, preventing optimal cache hit rates",
    "issue_number": 10032,
    "title": "CDC old value cache does not work effectively for caching pessimistic txn write"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Cache implementation does not handle Put commands without previous writes by not adding Oldvalue::None to cache, causing unnecessary cache misses",
    "issue_number": 10036,
    "title": "CDC old value cache does not work effectively for caching Put commands"
  },
  {
    "bug_location": "Router/Channel Memory Management",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Memory leak in crossbeam channel sender references not being properly released when receivers are dropped",
    "issue_number": 10037,
    "title": "Memory keeps growing in long running large scale cluster"
  },
  {
    "bug_location": "Titan storage engine metrics",
    "severity": 2,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Incorrect calculation or reporting of Titan blob file size metrics during data insertion",
    "issue_number": 10052,
    "title": "Incorrect Titan blob files size metrics"
  },
  {
    "bug_location": "Region Heartbeat / Leader Transfer Scheduler",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Disk I/O bottleneck causing region heartbeat metadata collection to block leadership transfer during high disk utilization",
    "issue_number": 10065,
    "title": "transfer leader scheduler will timeout if disk load is near 100%"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "CDC sending changes for un-captured key ranges, causing unnecessary caching overhead in TiKV, especially when old value tracking is enabled",
    "issue_number": 10073,
    "title": "CDC should not send changes of un-captured key ranges"
  },
  {
    "bug_location": "raftstore/raft_log",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Inconsistent Raft log commit index causing panic when attempting to commit an index beyond the last known log index",
    "issue_number": 10078,
    "title": "Panic because the index to committed goes past the last index"
  },
  {
    "bug_location": "CDC (Change Data Capture) old value cache",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Memory leak in CDC cache management where old values continue to accumulate even after all regions are deregistered",
    "issue_number": 10091,
    "title": "CDC old value cache keeps grow after all regions are deregistered"
  },
  {
    "bug_location": "Snapshot Management Component",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Variable `applying_snap_count` is not being properly incremented during snapshot application process",
    "issue_number": 10094,
    "title": "Applying snapshot count is always 0"
  },
  {
    "bug_location": "components/raftstore/store/fsm/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Incorrect assumption about region split check conditions, preventing region from splitting when size threshold is reached",
    "issue_number": 10111,
    "title": "Region may not trigger split check no matter how large it is"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Unlimited scan task concurrency without proper memory buffering control, causing unbounded memory allocation during large data scans",
    "issue_number": 10114,
    "title": "CDC unlimited scan task concurrency may cause OOM"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "Scan task scheduling mechanism blocks concurrent changefeeds, causing checkpoint lag and potential performance degradation during initial setup and region changes",
    "issue_number": 10117,
    "title": "CDC scan tasks interferes between changefeeds"
  },
  {
    "bug_location": "CDC (Change Data Capture) endpoint",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Unhandled error condition during downstream deregistration, causing an unwrap() panic when encountering a Sink(Congest) error state",
    "issue_number": 10118,
    "title": "CDC panic on deregister downstream"
  },
  {
    "bug_location": "jemalloc memory allocator",
    "severity": 2,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Invalid or malformed jemalloc configuration string causing unnecessary error logging",
    "issue_number": 10130,
    "title": "<jemalloc>: Malformed conf string in tikv_stderr.log"
  },
  {
    "bug_location": "TiKV Profiling Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "CodeBug"
    ],
    "root_cause": "Potential resource exhaustion or race condition during high-load profiling operation causing TiKV instance disconnection",
    "issue_number": 10131,
    "title": "TiKV disconnected after profiled on TiDB Dashboard under load"
  },
  {
    "bug_location": "raft/raft_log.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Slice index out of bounds during snapshot restoration, likely caused by inconsistent log entry indexing during Raft log compaction and snapshot recovery",
    "issue_number": 1014,
    "title": "panic at 'slice out of range'"
  },
  {
    "bug_location": "jemalloc memory profiling",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Unnecessary backtrace generation during memory allocation in jemalloc profiling mechanism",
    "issue_number": 10150,
    "title": "jemalloc will fetch backtrace at every malloc"
  },
  {
    "bug_location": "Coprocessor/Aggregation Function",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Incorrect comparison logic between signed and unsigned 64-bit integers during MAX aggregation function",
    "issue_number": 10158,
    "title": "copr bug: Agg func Max/Min got bug when compare signed and unsigned int64"
  },
  {
    "bug_location": "resolved_ts endpoint",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Repeated region registration attempts during region split causing excessive logging and performance overhead",
    "issue_number": 10163,
    "title": "resolved_ts: register region busy loop prints too many logs"
  },
  {
    "bug_location": "Region Split/Scheduling Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Race condition between region split metadata propagation and client request handling, causing temporary region unavailability during high write load scenarios",
    "issue_number": 10168,
    "title": "Region not found because of access before splitting"
  },
  {
    "bug_location": "TiKV Coprocessor",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug",
      "LoadBalance"
    ],
    "root_cause": "Lack of concurrency control for analyze/checksum requests in yatp thread pool, causing excessive task queuing and request timeouts",
    "issue_number": 10172,
    "title": "Limit the concurrency of analyze/checksum like DAG does"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Missing cluster ID validation in CDC request handling, causing incorrect error reporting",
    "issue_number": 10182,
    "title": "CDC should validate request's cluster ID"
  },
  {
    "bug_location": "Monitoring/Dashboard",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect region metrics rendering in dashboard panel",
    "issue_number": 10195,
    "title": "Wrong region related panel"
  },
  {
    "bug_location": "Titan storage engine",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Missing blob handling during DeleteFilesInRange operation in Titan storage engine, causing unexpected panic when processing file deletion range",
    "issue_number": 10208,
    "title": "TiKV panic on split check due to Titan missing blob error"
  },
  {
    "bug_location": "RaftEngine/PeerStorage",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Inefficient seek operation when cleaning raft logs during peer destruction, causing high latency and potential leader drops",
    "issue_number": 10210,
    "title": "Cleaning raft log costs too much time during destroying a peer"
  },
  {
    "bug_location": "raftstore/apply_worker",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of apply term during snapshot generation, causing metadata inconsistency in Raft log replication",
    "issue_number": 10225,
    "title": "apply term should be assigned when applying snapshot"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Excessive logging during changefeed pause operation, generating unnecessary INFO level logs about region observation status",
    "issue_number": 10255,
    "title": "CDC some info level logs are very verbose"
  },
  {
    "bug_location": "BR (Backup & Restore) S3 Storage Interface",
    "severity": 3,
    "categories": [
      "Network",
      "Storage",
      "Config"
    ],
    "root_cause": "Incorrect AWS signature generation for HCP S3 compatible storage, likely due to endpoint or signing method mismatch",
    "issue_number": 10265,
    "title": "BR backup random fails \"SignatureDoesNotMatch\" for HCP S3 storage"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "CDC incremental scanner default behavior fills RocksDB block cache, causing unnecessary cache pollution and performance degradation for online read services",
    "issue_number": 10267,
    "title": "CDC incremental scan pollutes rocksdb block cache"
  },
  {
    "bug_location": "raftstore/snapshot_worker",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Message channel overflow during snapshot synchronization between nodes, preventing successful message transmission after multiple retry attempts",
    "issue_number": 1028,
    "title": "failed to send msg SnapApplyRes [region_id: 912, is_success: true] after 5 tries"
  },
  {
    "bug_location": "backup-restore component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Unnecessary thread pool recycling and potential thread leakage in backup thread management",
    "issue_number": 10287,
    "title": "Backup should not recycle threads since we set thread number via config"
  },
  {
    "bug_location": "RocksDB metrics component",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Metrics tracking implementation incorrectly preserves historical max values instead of updating with latest sample",
    "issue_number": 10294,
    "title": "RocksDB duration metrics aren't refreshed"
  },
  {
    "bug_location": "file_system/rate_limiter.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Rate limiter test assertion failure indicating potential inaccurate bandwidth throttling calculation",
    "issue_number": 10303,
    "title": "Test test_rate_limited_hybrid_flow fails"
  },
  {
    "bug_location": "file_system/rate_limiter.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Rate limiter test assertion failure indicating potential inaccurate bandwidth throttling calculation in compaction write operations",
    "issue_number": 10306,
    "title": "Test test_rate_limited_light_flow fails"
  },
  {
    "bug_location": "unified-read-pool",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential memory corruption or invalid memory access during concurrent profiling and high-load sysbench workload",
    "issue_number": 10308,
    "title": "TiKV segfaults during profiling"
  },
  {
    "bug_location": "raftstore/hibernate",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Leadership transition failure during peer hibernation, preventing leader election within expected timeframe",
    "issue_number": 10309,
    "title": "test_leader_demoted_when_hibernated failed"
  },
  {
    "bug_location": "TiKV Server Metrics",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect metric calculation for hibernate region tracking, potentially caused by race condition or improper counter management in metric reporting logic",
    "issue_number": 10330,
    "title": "TiKV Hiberate Peers metric has negative numbers"
  },
  {
    "bug_location": "raftstore metrics",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect metric calculation logic for hibernate peer tracking, potentially causing incorrect state representation of region peer hibernation status",
    "issue_number": 10331,
    "title": "raftstore: incorrect hibernate peers metrics"
  },
  {
    "bug_location": "raftstore/peer",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Assertion failure in read index handling, where expected value was 1 but actual value was 2",
    "issue_number": 10347,
    "title": "tikv panicked during tipocket-bank2 test"
  },
  {
    "bug_location": "raftstore",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Non-deterministic response handling during node shutdown causing potential transaction state inconsistency",
    "issue_number": 10353,
    "title": "response from raftstore is not deterministic"
  },
  {
    "bug_location": "TiKV Configuration",
    "severity": 4,
    "categories": [
      "Config",
      "Upgrade"
    ],
    "root_cause": "Incompatible TTL configuration during version upgrade from v4.0.3 to v5.1.0, attempting to enable TTL on an instance not originally configured for TTL",
    "issue_number": 10367,
    "title": "upgrade from v4.0.3 to v5.1.0 fail for tikv timeout,report critical config check failed: can't enable ttl on a non-ttl instance"
  },
  {
    "bug_location": "Metrics/Monitoring Component",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Metric tracking for channel fullness was inadvertently removed during version upgrade, preventing proper monitoring of channel capacity and potential bottlenecks",
    "issue_number": 10377,
    "title": "Channel full metric needs to be updated"
  },
  {
    "bug_location": "raftstore/peer_storage",
    "severity": 5,
    "categories": [
      "Storage",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Failed to retrieve snapshot after multiple retry attempts during high-concurrency TPC-C benchmark preparation",
    "issue_number": 10407,
    "title": "TiKV panic during go-tpc prepare"
  },
  {
    "bug_location": "TiKV Cluster Load Balancing",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance",
      "Network"
    ],
    "root_cause": "Inefficient node failure handling and data redistribution mechanism causing unexpected performance degradation during node failures",
    "issue_number": 10408,
    "title": "QPS drop a lot when a tikv is killed"
  },
  {
    "bug_location": "CDC sink channel",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Quota not properly freed when channel message sending fails, causing resource leak in CDC sink component",
    "issue_number": 10411,
    "title": "CDC sink quota leaks after channel is dropped"
  },
  {
    "bug_location": "Configuration Management",
    "severity": 3,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Dynamic configuration update mechanism does not immediately apply changes, causing delayed config synchronization",
    "issue_number": 10426,
    "title": "Online changing `resolved-ts.advance-ts-interval` can't take effect immediately"
  },
  {
    "bug_location": "Import/Ingest File Handler",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Race condition between file deletion and file ingestion threads causing unexpected file access attempts",
    "issue_number": 10438,
    "title": "TiKV panic because of ingest files not exist."
  },
  {
    "bug_location": "store initialization",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Extremely slow store initialization process, potentially due to large number of regions and complex recovery mechanism",
    "issue_number": 10444,
    "title": "TiKV spend more than 20 minutes to (re)start"
  },
  {
    "bug_location": "Storage/RocksDB",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Memory management inefficiency under high write concurrency, likely related to write buffer or cache allocation strategy",
    "issue_number": 10445,
    "title": "TiKV OOM under high write pressure"
  },
  {
    "bug_location": "Transaction Layer",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Inconsistent transaction ordering and visibility in optimistic transaction mode, causing causal consistency violation during concurrent append operations",
    "issue_number": 10468,
    "title": "Anomaly found by jepsen append workload"
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Memory management inefficiency under high concurrent workload, potentially causing uncontrolled memory allocation during intensive database operations",
    "issue_number": 10492,
    "title": "TiKV OOM under high pressure"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Improper handling of uninitialized peer state during region reconfiguration, allowing potential leader duplication by bypassing epoch and peer ID validation checks",
    "issue_number": 10533,
    "title": "destroy uninitialized peer can make it possible to recreate old peer"
  },
  {
    "bug_location": "storage/mod.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "LoadBalance"
    ],
    "root_cause": "Incorrect key encoding during region split, causing unrecognizable key ranges that TiDB cannot process",
    "issue_number": 10542,
    "title": "load-base-split may split regions with unencoded keys"
  },
  {
    "bug_location": "import/sst_service",
    "severity": 2,
    "categories": [
      "CodeBug"
    ],
    "root_cause": "Potential race condition or synchronization issue in SST import service test causing test instability",
    "issue_number": 10549,
    "title": "Unstable test `import::test_sst_service::test_duplicate_and_close`"
  },
  {
    "bug_location": "Transaction Layer",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Inconsistent update ordering during concurrent append operations, causing lost or incorrectly sequenced updates in distributed transaction processing",
    "issue_number": 10552,
    "title": "Lost update was detected by jepsen append"
  },
  {
    "bug_location": "Transaction Layer",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Race condition in transaction commit sequence causing stale data reads during concurrent optimistic and pessimistic transactions",
    "issue_number": 10570,
    "title": "Transaction may read staled data"
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 5,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Memory exhaustion during high-concurrency data migration workload, likely caused by inefficient memory management or resource allocation during large-scale data transfer operations",
    "issue_number": 10580,
    "title": "Tikv node oom happened while dm job working"
  },
  {
    "bug_location": "storage.io-rate-limit",
    "severity": 3,
    "categories": [
      "Performance",
      "Config"
    ],
    "root_cause": "SST importer local write operations are not being properly rate-limited by the configured IO rate limiter",
    "issue_number": 10581,
    "title": "ratelimiter: sst_importer local write IO is not limited"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Network"
    ],
    "root_cause": "Race condition during voter removal in 2-replica Raft group causing leadership transition failure",
    "issue_number": 10595,
    "title": "Remove voter from a 2 replica raft group may lead to region unavailable"
  },
  {
    "bug_location": "Profiling Component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incompatibility with libunwind library on aarch64 architecture causing signal handler failure during profiling",
    "issue_number": 10658,
    "title": "Fail to profile TiKV in aarch64"
  },
  {
    "bug_location": "RocksDB L0 File Management",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Incorrect L0 file ingestion and deletion handling causing excessive file accumulation beyond configured limits",
    "issue_number": 10680,
    "title": "L0 files are not fully respected when using ingest deletion"
  },
  {
    "bug_location": "raftstore configuration management",
    "severity": 2,
    "categories": [
      "Config",
      "CodeBug"
    ],
    "root_cause": "Configuration parameter not properly registered for query retrieval in configuration management system",
    "issue_number": 10683,
    "title": "raftstore.raft-max-inflight-msgs can not support query through show config"
  },
  {
    "bug_location": "server/storage",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Improper handling of disk space exhaustion, causing panic instead of graceful error handling when creating storage files",
    "issue_number": 10688,
    "title": "tikv panic with \"no such file or directory\" when disk space is not enough"
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Performance",
      "Storage"
    ],
    "root_cause": "Memory management issue during high-concurrency workload after version upgrade, potentially causing out-of-memory conditions and repeated TiKV crashes",
    "issue_number": 10694,
    "title": "After upgrading from 5.1.0 to the 5.2.0 master, run sysbench insert and query\uff0csome tikv crash repeatedly"
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Memory exhaustion during high-concurrency sysbench insert workload, likely due to insufficient memory management or resource allocation under extreme write pressure",
    "issue_number": 10701,
    "title": "v5.2.0: run sysbench insert abort 40min, one tikv oom"
  },
  {
    "bug_location": "Transaction Timestamp Management",
    "severity": 3,
    "categories": [
      "Transaction",
      "Performance",
      "Upgrade"
    ],
    "root_cause": "Timestamp synchronization disruption after TiKV node restarts, potentially caused by OOM-induced state inconsistency",
    "issue_number": 10702,
    "title": "after two tikv restart\uff0c The \u201cMax Resolved TS gap\u201d display is unreasonable"
  },
  {
    "bug_location": "Storage/Disk Management",
    "severity": 5,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Inadequate handling of disk full conditions causing transaction and metadata operation failures without proper error propagation",
    "issue_number": 10708,
    "title": "After the disk is full, create table\u3001drop table\u3001truncate table operators are all stuck\uff0cand no any prompt message"
  },
  {
    "bug_location": "Storage/IO",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Potential memory management issue under extreme I/O bandwidth constraints causing uncontrolled memory allocation during high-concurrency workload",
    "issue_number": 10728,
    "title": "TiKV oom when inject bandwidth limit"
  },
  {
    "bug_location": "DDL Worker",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Improper handling of disk full conditions during DDL operations, causing task blocking and retry deadlock",
    "issue_number": 10731,
    "title": "*: Another drop table blocked problem when tikv is disk full"
  },
  {
    "bug_location": "Monitoring/Alerting Component",
    "severity": 3,
    "categories": [
      "Performance",
      "Config"
    ],
    "root_cause": "Monitoring system failed to generate expected alert for slow node performance degradation",
    "issue_number": 10737,
    "title": "After a slow node appears, the alarm that prompts the user to open the slow node processing is not generated"
  },
  {
    "bug_location": "Load Balancer / Node Detection Component",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Overly sensitive slow node detection mechanism that incorrectly interprets high load during hot spot scenarios as node performance degradation",
    "issue_number": 10744,
    "title": "When the business has a hot spot, one node is mistakenly detected as a slow node"
  },
  {
    "bug_location": "storage/txn/flow_controller",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Unhandled None value during flow control tick operation, causing unexpected panic under heavy write workload",
    "issue_number": 10752,
    "title": "Panic in flow controller"
  },
  {
    "bug_location": "PD Scheduler",
    "severity": 3,
    "categories": [
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Slow leader eviction mechanism when detecting performance-degraded nodes, causing extended recovery time for distributed workloads",
    "issue_number": 10754,
    "title": "run sysbench prepare\uff0cAfter a slow node appears, QPS recover is too slow due to PD scheduler slow"
  },
  {
    "bug_location": "Load Balancing / Store Detection",
    "severity": 3,
    "categories": [
      "LoadBalance",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Slow node detection and recovery algorithm inefficiently handles performance degradation, causing extended QPS restoration time",
    "issue_number": 10756,
    "title": "run sysbench update\uff0cAfter a slow node appears, QPS recover is too slow due to detection algorithm"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Inefficient region splitting mechanism causing excessive large region generation during high-volume data insertion",
    "issue_number": 10762,
    "title": "Insert data into single table\uff0c after running for a period of time, large region are generated"
  },
  {
    "bug_location": "Placement Driver (PD) Replication Module",
    "severity": 4,
    "categories": [
      "Transaction",
      "LoadBalance",
      "Network"
    ],
    "root_cause": "Race condition during DR auto-sync state transition causing region replication state to become blocked in SIMPLE_MAJORITY",
    "issue_number": 10772,
    "title": "region cannot be INTEGRITY_OVER_LABEL in dr-auto-sync"
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Memory allocation failure during RocksDB background compaction process, likely due to insufficient memory resources or memory fragmentation during high-stress workload",
    "issue_number": 10803,
    "title": "truncate table during running sysbench insert\uff0cone tikv panic four times"
  },
  {
    "bug_location": "monitoring/error_reporting",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incomplete error tracking and dashboard reporting for aborted network connections",
    "issue_number": 10808,
    "title": "aborted connection doesn't show error in error dashboard"
  },
  {
    "bug_location": "raftstore/store/fsm/store.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Early return in snapshot garbage collection logic prevents cleanup of corrupted and stale snapshot files",
    "issue_number": 10813,
    "title": "corrupted snapshot can prevent snapshot files gc"
  },
  {
    "bug_location": "RocksDB Storage Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Memory allocation failure during RocksDB compaction process, likely due to insufficient memory resources or memory fragmentation",
    "issue_number": 10832,
    "title": "rocksdb background error. db: kv, reason: compaction, error: IO error: While appending to file: /var/lib/tikv/data/db/438734.sst: Cannot allocate memory\""
  },
  {
    "bug_location": "Transaction Commit Module",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential race condition or memory management issue in commit transaction logic during concurrent SQL operations",
    "issue_number": 1084,
    "title": "TiKV panic on `commit_to()`"
  },
  {
    "bug_location": "Logger Thread",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Single logger thread becomes CPU-bound and message queue bottleneck under high logging load, particularly during extreme coprocessor workloads",
    "issue_number": 10841,
    "title": "*:Logger thread can be bottleneck and slow down the whole TiKV instance"
  },
  {
    "bug_location": "coprocessor_plugin",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Unsafe dynamic plugin loading mechanism allowing potential runtime plugin removal",
    "issue_number": 10854,
    "title": "copr_plugin: should disallow unregistering plugins"
  },
  {
    "bug_location": "Flow Control Module",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Uneven flow control mechanism causing significant QPS degradation under high concurrency and load conditions",
    "issue_number": 10879,
    "title": "Under high pressure and hot business, QPS dropped bottom due to uneven flow control"
  },
  {
    "bug_location": "CDC (Change Data Capture)",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "Flow control mechanism preventing timely snapshot release, causing extended resource holding",
    "issue_number": 10900,
    "title": "CDC may hold snapshots for a long time when eventfeed is flowcontrolled"
  },
  {
    "bug_location": "Build/Debug Information Generation",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Incorrect DWARF debug information generation during release build process, causing invalid debug information references",
    "issue_number": 10906,
    "title": "DWARF info of dist_release binary is incorrect"
  },
  {
    "bug_location": "Raftstore",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Transaction",
      "Network"
    ],
    "root_cause": "Deadlock during region migration and cluster scaling, preventing proper leader and region redistribution across TiKV nodes",
    "issue_number": 10909,
    "title": "Raftstore deadlock during migrating regions"
  },
  {
    "bug_location": "TiKV Coprocessor",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "Network"
    ],
    "root_cause": "Server timeout during table analyze operation, likely due to resource constraints or inefficient query processing during large data import",
    "issue_number": 10924,
    "title": "Analyze table failed due to tikv server timeout error during lighting import"
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "LoadBalance"
    ],
    "root_cause": "Memory exhaustion under high concurrent sysbench workload with large number of tables, causing TiKV instances to experience out-of-memory conditions",
    "issue_number": 10927,
    "title": "the status of two store are abnormal and some tikv oom while run sysbench"
  },
  {
    "bug_location": "RegionCollector",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Incorrect leader count calculation in RegionCollector causing inaccurate metrics during rolling restart",
    "issue_number": 10942,
    "title": "evict leader timeout during rolling restart"
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 5,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential data corruption during import process causing RocksDB internal state inconsistency",
    "issue_number": 10947,
    "title": "TiKV panic during import data because of RocksDB Corruption."
  },
  {
    "bug_location": "Documentation",
    "severity": 2,
    "categories": [
      "Human"
    ],
    "root_cause": "Inaccurate test coverage reporting in project documentation",
    "issue_number": 10949,
    "title": "Correct test coverage information in README"
  },
  {
    "bug_location": "Raft Log Compaction",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Overly aggressive log compaction during rolling updates causing unnecessary snapshot generation between followers and leader",
    "issue_number": 10954,
    "title": "Periodically raft purge tick can cause unnecessary snapshots between rolling update"
  },
  {
    "bug_location": "resolved_ts/advance.rs",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "CodeBug"
    ],
    "root_cause": "Unbounded coroutine accumulation during leadership verification when one TiKV node becomes unresponsive",
    "issue_number": 10965,
    "title": "Coroutine leaking after one TiKV got stuck"
  },
  {
    "bug_location": "TiKV Configuration Parser",
    "severity": 3,
    "categories": [
      "Config",
      "CodeBug"
    ],
    "root_cause": "Configuration type parsing error for RocksDB compaction style, where string value is not correctly mapped to enum/integer representation",
    "issue_number": 11028,
    "title": "`compaction-style` setting is broken"
  },
  {
    "bug_location": "TiKV startup process",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Upgrade"
    ],
    "root_cause": "Potential filesystem or mount configuration incompatibility during TiKV initialization",
    "issue_number": 11037,
    "title": "TiKV panic when startup"
  },
  {
    "bug_location": "components/raftstore/src/store/fsm/apply.rs",
    "severity": 2,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Performance context reset occurs before writing to key-value database, causing incorrect latency measurements when multiple writes occur in a single round",
    "issue_number": 11044,
    "title": "Should reset perf context after writing to kvdb"
  },
  {
    "bug_location": "raft/batch_raft service",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Improper error handling when channel becomes full, causing premature RPC stream termination and potential raft cluster instability",
    "issue_number": 11047,
    "title": "Channel full could break the raft connection"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect event ordering during delta scan snapshot, causing real-time events to be sent out of sequence",
    "issue_number": 11055,
    "title": "cdc: Init event should be sent after all observed events have been sent"
  },
  {
    "bug_location": "Leader Election Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "LoadBalance"
    ],
    "root_cause": "Failure to properly reset leader election state during node recovery, causing leadership assignment to default/zero state",
    "issue_number": 11075,
    "title": "After a tikv failure is recovered, the leader of this tikv is always zero"
  },
  {
    "bug_location": "CDC (Change Data Capture)",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Overly aggressive message batching in CDC causing excessive memory consumption beyond default quota of 512MB",
    "issue_number": 11082,
    "title": "CDC message batching is too aggressive and may exceed the default memory quota   "
  },
  {
    "bug_location": "Backup Component",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "Storage"
    ],
    "root_cause": "Socket connection timeout during remote backup operation without proper retry mechanism",
    "issue_number": 11117,
    "title": "auto backup error"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Region epoch version mismatch during downstream registration, causing a panic when a downstream connection cannot be properly established",
    "issue_number": 11123,
    "title": "CDC panic \"region 298252 miss downstream\""
  },
  {
    "bug_location": "raftstore/peer_fsm",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Assertion failure during peer destruction while handling snapshot, indicating a race condition or improper state management during TiKV scaling operation",
    "issue_number": 11130,
    "title": "one tikv panic while scaling in tikv"
  },
  {
    "bug_location": "server/status_server/profile",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Improper state management in heap profiling activation, where `on_end` callback was not properly handled, causing an assertion failure when attempting to reactivate profiling",
    "issue_number": 11156,
    "title": "Panic when activating heap profiling"
  },
  {
    "bug_location": "Snapshot Transfer Component",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Incorrect snapshot state tracking during network interruption, causing PD to misinterpret cluster node states",
    "issue_number": 11157,
    "title": "Incorrect snapshot state count if an error occurs on receiving snapshot"
  },
  {
    "bug_location": "status_server/mod.rs",
    "severity": 4,
    "categories": [
      "Security",
      "CodeBug"
    ],
    "root_cause": "Unrestricted file access through heap profile debug endpoint allowing potential unauthorized file read",
    "issue_number": 11161,
    "title": "security issue of dumping heap profile through http api"
  },
  {
    "bug_location": "documentation",
    "severity": 2,
    "categories": [
      "Human"
    ],
    "root_cause": "Lack of maintenance and documentation update process",
    "issue_number": 11167,
    "title": "Update release note in the TiKV repo"
  },
  {
    "bug_location": "Raft Consensus Module",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Async IO changes disrupted the existing slow node detection mechanism by preventing accurate latency measurement of store loop operations",
    "issue_number": 11178,
    "title": "Slow node detection is not compatible with raft async io"
  },
  {
    "bug_location": "CDC/Transaction Layer",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Inconsistent lock timestamp handling in pessimistic transactions causing unexpected lock state during CDC scanning",
    "issue_number": 11187,
    "title": "Stale non-pessimistic lock may be incompatible with CDC and possibly affect data correctness"
  },
  {
    "bug_location": "coprocessor/statistics/analyze",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Unhandled edge case in row sampling logic where samples array is empty and max_sample_size is zero, causing an unwrap() on a None value",
    "issue_number": 11192,
    "title": "Panic in analyze sampling"
  },
  {
    "bug_location": "metrics/threads_linux.rs",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Thread metrics labels are persistently stored without proper cleanup mechanism, causing memory leak for exited threads",
    "issue_number": 11195,
    "title": "Label leaking of thread metrics"
  },
  {
    "bug_location": "Store Detection Component",
    "severity": 3,
    "categories": [
      "Performance",
      "Network"
    ],
    "root_cause": "Potential race condition or inconsistent timing in slow store detection mechanism causing test instability",
    "issue_number": 11197,
    "title": "Unstable test about slow store detection"
  },
  {
    "bug_location": "External Storage Component",
    "severity": 2,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Thread list cache ineffectiveness when creating new threads during external I/O operations, potentially causing increased memory fragmentation",
    "issue_number": 11204,
    "title": "Possible memory fragmentation risk when using external storage"
  },
  {
    "bug_location": "tikv-ctl logging system",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Asynchronous logging configuration causing incomplete log output during quick process termination",
    "issue_number": 11210,
    "title": "tikv-ctl's output is incomplete, maybe slog-async is not flushed when tikv-ctl process exits"
  },
  {
    "bug_location": "gc_worker/gc_worker.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Missing data prefix when generating key ranges for garbage collection, causing incorrect region identification and preventing delete marker cleanup",
    "issue_number": 11217,
    "title": "TiKV GcKeys task doesn't work when called with multiple keys (at least in 5.1 but I think for everything)"
  },
  {
    "bug_location": "RocksDB configuration parser",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Config",
      "Storage"
    ],
    "root_cause": "Incompatible RocksDB configuration option introduced in newer version causing parsing failure during rollback",
    "issue_number": 11226,
    "title": "TiKV rollback encounters panic \"failed to load_latest_options\""
  },
  {
    "bug_location": "configuration/resource-metering",
    "severity": 2,
    "categories": [
      "Config",
      "Human"
    ],
    "root_cause": "Default configuration parameter incorrectly set to enabled state when it should be disabled by default",
    "issue_number": 11235,
    "title": "v5.2.2 resource-metering.enabled default value is true, should be false"
  },
  {
    "bug_location": "Import/SST Ingestion Component",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Synchronous checksum verification performed in single apply-thread, causing potential performance bottleneck during SST file ingestion",
    "issue_number": 11239,
    "title": "import: TiKV verify-checksum in apply-thread and it cost a lot of time"
  },
  {
    "bug_location": "rocksdb compaction job",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Invalid data key processing during RocksDB compaction, causing unexpected panic in key handling logic",
    "issue_number": 11290,
    "title": "Panic in rocksdb compaction job"
  },
  {
    "bug_location": "coprocessor/statistics/analyze.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Unhandled Option::unwrap() on a None value during row sampling in analyze statistics operation, likely due to incomplete or unexpected data handling during benchmark dataset cleanup",
    "issue_number": 11295,
    "title": "panic when preparing tpcc/ch benchmark dataset"
  },
  {
    "bug_location": "components/tikv_util/src/sys/cgroup.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Assertion failure in cgroup parsing logic when attempting to insert key-value pairs, likely due to container environment incompatibility",
    "issue_number": 11296,
    "title": "Can't run tikv inside container"
  },
  {
    "bug_location": "gRPC metrics calculation",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Incorrect metric calculation with missing instance filter in denominator",
    "issue_number": 11299,
    "title": "Incorrect by-instance average gRPC duration"
  },
  {
    "bug_location": "Transaction Layer",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect timestamp management in distributed transaction processing causing potential data inconsistency during concurrent operations",
    "issue_number": 11313,
    "title": "hint_min_ts can cause downstream data incorrupt"
  },
  {
    "bug_location": "Leader Balance/Region Management",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Delayed leader rebalancing mechanism after TiKV node recovery, causing extended period of suboptimal cluster state",
    "issue_number": 11319,
    "title": "one tikv failure was restored after one minute, but the leader start the balance after 3 hours"
  },
  {
    "bug_location": "Raftstore",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Synchronous log entry fetching blocking raftstore processing, causing significant latency during node recovery",
    "issue_number": 11320,
    "title": "Raft should fetch log entries in async way"
  },
  {
    "bug_location": "components/cdc/tests/failpoints",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential race condition or unstable test in joint configuration change handling in Change Data Capture (CDC) component",
    "issue_number": 11325,
    "title": "test_resolve::test_joint_confchange failed"
  },
  {
    "bug_location": "GC (Garbage Collection) Module",
    "severity": 2,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Incomplete or missing data generation during garbage collection scan process, potentially causing incomplete tracking of storage cleanup details",
    "issue_number": 11340,
    "title": "GC scan detail has no data"
  },
  {
    "bug_location": "tikv_util/cgroupp.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Config",
      "Performance"
    ],
    "root_cause": "Integer parsing error in cgroup configuration, likely related to parsing system resource limits with an overflow condition",
    "issue_number": 11348,
    "title": "tidb-cluster can not run pod of basic-tikv-0"
  },
  {
    "bug_location": "logging/time_handling",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incorrect timezone handling in logging system, likely using UTC instead of local container time despite volume mount",
    "issue_number": 1135,
    "title": "log time is incorrect in docker"
  },
  {
    "bug_location": "CDC (Change Data Capture) endpoint",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Network"
    ],
    "root_cause": "Blocking wait for leader check requests when a TiKV node is disconnected, causing resolved timestamp synchronization delay",
    "issue_number": 11351,
    "title": "resolved ts lag increased after stoping a tikv"
  },
  {
    "bug_location": "Resource Management/Cgroup Module",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incorrect parsing of memory limit configuration from cgroup filesystem, potentially causing memory allocation or limit detection failures",
    "issue_number": 11353,
    "title": "cgroup code parses memory.limit_in_bytes fail"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Unable to process local Raft messages, potentially indicating a state synchronization or message handling issue in the Raft consensus protocol implementation",
    "issue_number": 11371,
    "title": "tikv error in log: Raft raft: cannot step raft local message"
  },
  {
    "bug_location": "Storage/RocksDB",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Memory management inefficiency during high concurrent sysbench prepare and query workload, causing uncontrolled memory consumption and out-of-memory condition",
    "issue_number": 11379,
    "title": "tikv oom when run sysbench prepare while query under stress"
  },
  {
    "bug_location": "tikv-ctl",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect handling of Raft database state during region state retrieval in tikv-ctl tool",
    "issue_number": 11393,
    "title": "tikv-ctl can't open raft db correctly"
  },
  {
    "bug_location": "raftstore/apply",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Inconsistent commit state in Raft region, causing state jump backward during high-stress workload",
    "issue_number": 11396,
    "title": "two tikv crash repeatly after run some stress workload"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Inefficient log truncation mechanism causing excessive RaftDB write operations and accumulation of small SST files during garbage collection tasks",
    "issue_number": 11404,
    "title": "raftstore: TiKV can not deleted the truncated raft log as soon as possible"
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Memory management inefficiency during long-running write-intensive workloads, causing gradual performance degradation and potential out-of-memory conditions",
    "issue_number": 11410,
    "title": "performance is getting worse and two tikv oom after running sysbech write-only for a long time"
  },
  {
    "bug_location": "Scaling Component",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Performance instability during horizontal scaling of TiKV nodes, causing significant QPS degradation during cluster expansion",
    "issue_number": 11424,
    "title": "QPS dropped severely many times during scale out in DBaaS"
  },
  {
    "bug_location": "Point Get Query Processing",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Potential regression in query optimization or execution path in v5.3.0 affecting point get workloads",
    "issue_number": 11431,
    "title": "QPS drop 1% ~4% for Point Get workload in v5.3.0, compared to v5.2.2"
  },
  {
    "bug_location": "Alert/Monitoring System",
    "severity": 3,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Deprecated metric used in alert rule after counter was disabled, causing ineffective low disk space detection",
    "issue_number": 11434,
    "title": "ineffective alert rule for low space"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incorrect error type renaming in coprocessor alert rules without proper migration or update of existing monitoring configurations",
    "issue_number": 11437,
    "title": "wrong coprocessor alert rules"
  },
  {
    "bug_location": "coprocessor::endpoint",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Memory access violation in synchronization channel during task polling, likely caused by race condition or memory management issue in Rust's mpsc channel",
    "issue_number": 1144,
    "title": "tikv-server: Segmentation fault"
  },
  {
    "bug_location": "Storage::scan()",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Incorrect key range handling in reverse scan prevents proper memory lock detection",
    "issue_number": 11440,
    "title": "Reverse scan can't detect memory locks"
  },
  {
    "bug_location": "CDC (Change Data Capture) Integration Tests",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "Unknown"
    ],
    "root_cause": "Potential timeout or synchronization issue in integration test environment causing test pipeline to hang",
    "issue_number": 11461,
    "title": "TiKV hangs with latest commit in integration tests"
  },
  {
    "bug_location": "raftstore/apply.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Attempting to add duplicate learner peers to regions, causing configuration change failures during peer demotion process",
    "issue_number": 11463,
    "title": "tikv error when demote follower to learner"
  },
  {
    "bug_location": "coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Invalid UTF-8 casting to real number type causing unexpected behavior in coprocessor processing",
    "issue_number": 11466,
    "title": "copr: got bug when cast invalid utf8 to real"
  },
  {
    "bug_location": "Region Management",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Inefficient region splitting and merging logic during large data import, causing temporary region fragmentation",
    "issue_number": 11470,
    "title": "Big/Fast increase of empty regions after import of tpcc 5000 warehouses"
  },
  {
    "bug_location": "raftstore/peer",
    "severity": 5,
    "categories": [
      "Upgrade",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Inconsistent region merge state during version upgrade, causing unexpected merge result and panic in Raft consensus protocol",
    "issue_number": 11475,
    "title": "One Tikv panic after upgrade from 5.0.1 to 5.3.0"
  },
  {
    "bug_location": "Upgrade/Recovery Component",
    "severity": 4,
    "categories": [
      "Upgrade",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential race condition or deadlock during cluster version transition between TiKV 5.0.1 and 5.3.0, causing upgrade process to stall after node recovery",
    "issue_number": 11484,
    "title": "Upgrade hung more than 1 hours when upgrade from 5.0.1 to 5.3.0"
  },
  {
    "bug_location": "config.rs",
    "severity": 3,
    "categories": [
      "Upgrade",
      "Config"
    ],
    "root_cause": "Configuration parsing error with unrecognized enum variant for raft-engine recovery mode during upgrade process",
    "issue_number": 11489,
    "title": "nightly upgrade panic due to unrecognized configuration for raft-engine.recovery-mode"
  },
  {
    "bug_location": "TiDB Query Expression Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Substring function not fully implemented for batch mode processing in query expression evaluation",
    "issue_number": 11495,
    "title": "Got error when substring is used in filter"
  },
  {
    "bug_location": "metrics/alertmanager/tikv.rules.yml",
    "severity": 2,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Alert rule regex pattern does not match updated apply worker thread naming convention",
    "issue_number": 11517,
    "title": "Alert rule is wrong as the name of apply worker has changed"
  },
  {
    "bug_location": "Backup and Restore (BR) component",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Potential encryption key handling or data integrity issue during backup restoration process causing data read/magic number verification failure",
    "issue_number": 11524,
    "title": "br restore encrypted backup failed \"Bad table magic number\""
  },
  {
    "bug_location": "RaftStore/PeerStorage",
    "severity": 5,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Memory allocation failure during large dataset startup, potentially caused by NUMA node memory fragmentation or insufficient memory management during high-volume data loading",
    "issue_number": 11526,
    "title": "After inserted 1 billions data in to tidb, the one of TiKV never able to startup, neither restart."
  },
  {
    "bug_location": "storage/mvcc/reader/scanner",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Unexpected panic during forward scanning of MVCC write cursor, likely due to invalid timestamp or cursor movement logic",
    "issue_number": 11541,
    "title": "Unexpected panic when scan_next"
  },
  {
    "bug_location": "storage/mvcc/reader/scanner/forward.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Invalid write cursor state during MVCC scanner operation, likely caused by unexpected cursor invalidation or incorrect cursor management in the forward scanner",
    "issue_number": 11543,
    "title": "Assertion failed: `self.cursors.write.valid()?`"
  },
  {
    "bug_location": "Memory Management",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Memory allocation imbalance across NUMA nodes causing single node memory exhaustion",
    "issue_number": 11547,
    "title": "One TiKV OOM when it exhausts memory in one numa"
  },
  {
    "bug_location": "PD Worker Thread",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Network"
    ],
    "root_cause": "Mutex deadlock in PD worker thread preventing transaction timestamp synchronization",
    "issue_number": 11549,
    "title": "TiKV PD worker thread deadlocks"
  },
  {
    "bug_location": "storage/raw_scan",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential incorrect metrics tracking during raw key scanning, possibly not reading write column family correctly",
    "issue_number": 11571,
    "title": "keys read metrics in raw_scan is wrong"
  },
  {
    "bug_location": "components/raftstore/store/worker/compact.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Manual compaction logic only checks write column family, ignoring tombstones in default column family for RawKV usage",
    "issue_number": 11575,
    "title": "Manual compaction is not enabled for raw kv usage"
  },
  {
    "bug_location": "TiKV Storage Component",
    "severity": 5,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Memory exhaustion causing out-of-memory (OOM) conditions preventing TiKV instances from starting, likely due to memory management or resource allocation issues during high-load sysbench workloads",
    "issue_number": 11581,
    "title": "v5.2.2: 2 tikvs oom and can't start after 1 hours "
  },
  {
    "bug_location": "tikv-ctl",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incorrect handling of region properties for RawKV mode, causing default/zero values to be returned instead of actual region metadata",
    "issue_number": 11583,
    "title": "tikv-ctl region-properties doesn't work on rawkv"
  },
  {
    "bug_location": "storage.api_version",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Config"
    ],
    "root_cause": "Incompatible data key version when attempting to switch storage API version with TTL enabled",
    "issue_number": 11588,
    "title": "TiKV panic on restart when `storage.enable_ttl=true`"
  },
  {
    "bug_location": "tikv-ctl logging system",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Logging mechanism fails to output debug messages in offline recovery mode",
    "issue_number": 11589,
    "title": "Some logs of tikv-ctl are not printed on offline mode"
  },
  {
    "bug_location": "RaftStore",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Potential inefficient async snapshot transfer mechanism causing unexpected high tail latency during leader transfers and read-only workloads",
    "issue_number": 11596,
    "title": "Unexpected high tail latency of async snapshot"
  },
  {
    "bug_location": "components/tikv_util/src/sys/cgroup.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incorrect assumption about cgroup file format in systemd-nspawn container environment, causing assertion failure during parsing of /proc/self/cgroup",
    "issue_number": 11609,
    "title": "tikv panicked in parse_proc_cgroup_v2"
  },
  {
    "bug_location": "Transaction Layer",
    "severity": 3,
    "categories": [
      "Transaction",
      "Performance"
    ],
    "root_cause": "High concurrency write conflicts causing transaction commit failures due to timestamp and key collision",
    "issue_number": 11612,
    "title": "Sysbench read and write workload hit 9007 Write conflict error"
  },
  {
    "bug_location": "raftstore/peer_storage.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Incorrect snapshot handling logic causing race condition between snapshot application and Raft log processing",
    "issue_number": 11618,
    "title": "Panic after snapshot is aborted"
  },
  {
    "bug_location": "rust standard library time module",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Unknown"
    ],
    "root_cause": "Insufficient debug information during time-related panic scenario",
    "issue_number": 1162,
    "title": "rust: add debug message in time panic "
  },
  {
    "bug_location": "Transaction Layer",
    "severity": 3,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Async-IO callback modification disrupting pessimistic lock tracking mechanism",
    "issue_number": 11649,
    "title": "More frequent 'pessimistic lock not found' since v5.3.0"
  },
  {
    "bug_location": "table_storage_stats",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect key counting mechanism in table storage statistics tracking, causing significant overestimation of table keys compared to actual row count",
    "issue_number": 11669,
    "title": "table_storage_stats.TABLE_KEYS is much higher than actual rows causing analyze wrong sample ratio "
  },
  {
    "bug_location": "TiKV Storage/Restore Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Excessive memory allocation during backup/restore process on ARM64 architecture, likely due to inefficient memory management or architecture-specific memory handling during large dataset restoration",
    "issue_number": 11670,
    "title": "tikv keep oom on arm64 k8s deployments during br restore"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Range cleanup operation encounters out-of-order key sequence during SST file processing",
    "issue_number": 11673,
    "title": "Can't cleanup overlapped ranges due to out-or-order keys"
  },
  {
    "bug_location": "procfs dependency parsing",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Kernel patch version parsing limitation in procfs library causing integer overflow when version exceeds 255",
    "issue_number": 11697,
    "title": "panic on 'Failed to parse patch version'"
  },
  {
    "bug_location": "Build System",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incompatible library linking on macOS due to compiler differences between GCC and Clang",
    "issue_number": 1172,
    "title": "failed to static link rocksdb on latest OS X"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Race condition between snapshot garbage collection and peer state transition during snapshot application, potentially causing file deletion before state persistence",
    "issue_number": 11746,
    "title": "raftstore: snapshot files may deleted while the peer state is still `PeerState::Applying`"
  },
  {
    "bug_location": "Memory Management",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Insufficient memory control and cgroup limit recognition during high-write workload scenarios",
    "issue_number": 11747,
    "title": "sysbench write only run makes tikv oom"
  },
  {
    "bug_location": "GC Worker",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "GcKeys tasks blocking UnsafeDestroyRange requests, preventing garbage collection progress during TiKV instance recovery",
    "issue_number": 11752,
    "title": "GcKeys tasks in GC worker block UnsafeDestroyRange requests and make GC stuck"
  },
  {
    "bug_location": "resource_metering",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Assertion failure in read keys coprocessor test, where expected value (4) does not match actual value (8)",
    "issue_number": 11765,
    "title": "Unstable test: resource_metering::test_read_keys::test_read_keys_coprocessor"
  },
  {
    "bug_location": "Resolved Timestamp (Resolved TS) Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Potential race condition or inefficient timestamp resolution mechanism during region split operations under high concurrency workload",
    "issue_number": 11767,
    "title": "Resolved ts exceed 1 hours for more than 30min without any fault inject"
  },
  {
    "bug_location": "Performance/Transaction Processing",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Potential regression in transaction processing logic introduced by commit 74cd8a, causing significant throughput reduction in YCSB workload",
    "issue_number": 11769,
    "title": "21% tps degradation on ycsb workloada from commit 74cd8a"
  },
  {
    "bug_location": "external_storage",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Compilation failure when attempting to build with cloud storage feature flags that are not properly configured in the build system",
    "issue_number": 11773,
    "title": "cloud: build failed when `grpc` or `dylib` backend enabled."
  },
  {
    "bug_location": "TiKV Network Recovery Handler",
    "severity": 5,
    "categories": [
      "Network",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Memory management failure during network fault recovery under high concurrency TPC-C workload, causing repeated out-of-memory conditions in recovered TiKV pod",
    "issue_number": 11774,
    "title": "one tikv\uff08recovered pod\uff09oom repeatedly when network fault recover while run tpcc"
  },
  {
    "bug_location": "server.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Inappropriate log level for non-critical IO snooper initialization failure when BCC is not compiled",
    "issue_number": 11775,
    "title": "Log level for IO snooper failure is too high"
  },
  {
    "bug_location": "logging_system",
    "severity": 4,
    "categories": [
      "Config",
      "Network",
      "Storage"
    ],
    "root_cause": "Kubernetes logging configuration preventing log persistence or log file generation",
    "issue_number": 11789,
    "title": "tikv have not logs saved in k8s "
  },
  {
    "bug_location": "RocksDB storage engine",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Rate limiting mechanism fails to properly throttle compaction when pending bytes exceed threshold",
    "issue_number": 11810,
    "title": "RocksDB rate limit takes no effect when pending bytes is high"
  },
  {
    "bug_location": "Resource Metering Component",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Resource metering tag attachment overhead is significantly higher than expected, causing performance degradation",
    "issue_number": 11835,
    "title": "attach resource metering tag expects to be a extremely lightweight job but not now"
  },
  {
    "bug_location": "Configuration Management",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Config"
    ],
    "root_cause": "Deprecated configuration parameter 'resource-metering.enabled' was not properly handled during version upgrade, causing configuration validation failure",
    "issue_number": 11836,
    "title": "param resource-metering.enabled  is deleted in v5.4.0, upgrade to 5.4.0 will fail if pre-upgrade version include this config"
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Network"
    ],
    "root_cause": "Memory pressure under high concurrency and IO delay injection, causing simultaneous out-of-memory conditions in multiple TiKV nodes",
    "issue_number": 11838,
    "title": "two tikv oom at the same time while run sysbench_oltp_read_write and inject IO delay"
  },
  {
    "bug_location": "Stale Read Component",
    "severity": 3,
    "categories": [
      "Network",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Network chaos disrupting resolved timestamp synchronization during stale read operations, causing excessive timestamp gap",
    "issue_number": 11847,
    "title": "resolved_ts: max resolved ts gap exceed 1 hours  while inject network chaos during run stale read"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unexpected panic during post-apply operation in Raft peer processing, likely due to an unhandled edge case or unexpected state during log application",
    "issue_number": 11852,
    "title": "Got unexpected panic during tipocket ledger test"
  },
  {
    "bug_location": "Configuration Management",
    "severity": 3,
    "categories": [
      "Upgrade",
      "Config",
      "CodeBug"
    ],
    "root_cause": "Incompatible configuration parameter value introduced in nightly version that breaks backward compatibility with earlier versions",
    "issue_number": 11861,
    "title": "downgrade from nightly to v5.2.x will fail"
  },
  {
    "bug_location": "TiKV Index Predicate Pushdown",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Inability to correctly push down predicates on bit column indexes in query optimization",
    "issue_number": 11893,
    "title": "The predicate on the bit column cannot be pushed down"
  },
  {
    "bug_location": "GC (Garbage Collection) Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Excessive seek_tombstone operations during garbage collection causing significant performance overhead when processing large datasets",
    "issue_number": 11902,
    "title": "GC Keys tasks executes slowly due to millions of seek_tombstone"
  },
  {
    "bug_location": "gc_worker/gc_worker.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Task scheduling error causing incorrect maintenance of scheduled_tasks counter, leading to false GcWorkerTooBusy state",
    "issue_number": 11903,
    "title": "False GcWorkerTooBusy caused by incorrect scheduled_tasks"
  },
  {
    "bug_location": "gc-worker",
    "severity": 2,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Incorrect tracking of pending garbage collection tasks, where the counter represents increments rather than actual current pending tasks",
    "issue_number": 11915,
    "title": "gc-worker pending tasks can not represent the current total number of real pending but the number of increments "
  },
  {
    "bug_location": "tikv_util/sys/cgroup.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Attempted to unwrap a None value when accessing cgroup CPU cores configuration, likely due to unexpected cgroup filesystem structure or parsing failure",
    "issue_number": 11920,
    "title": "tikv-server --config-check failed caused by  \u2018called Option::unwrap() on a None value\u2019 when exec tiup-cluster deploy"
  },
  {
    "bug_location": "TiKV Server",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "Transaction"
    ],
    "root_cause": "Potential memory management or concurrency issue causing unexpected server resets during high-load OLTP read-write workloads",
    "issue_number": 11923,
    "title": "tikv abnormally reset in 5.1.3 hotfix version"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Potential message processing bottleneck in Raft consensus layer causing high latency in peer message handling",
    "issue_number": 11927,
    "title": "Slow to handle Raft messages but no obvious latency source"
  },
  {
    "bug_location": "TiKV Region Management",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Time offset injection to PD leader causing region unavailability during stale read operations",
    "issue_number": 11937,
    "title": "Stale read hit data is not ready after inject some errors to pd"
  },
  {
    "bug_location": "pd_client/tso.rs and tokio_timer",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Long-running timer wheel causing index out of bounds error during timestamp request processing after extended system uptime",
    "issue_number": 11940,
    "title": "TiKV running over 2 years may panic"
  },
  {
    "bug_location": "Metrics/Load Balancing Component",
    "severity": 3,
    "categories": [
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Inconsistent sampling periods for different load splitting metrics causing potential synchronization and accuracy issues in load distribution calculations",
    "issue_number": 11941,
    "title": "metrics: load base split event with different period"
  },
  {
    "bug_location": "raftstore/peer",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Unhandled Option::unwrap() on None value during network loss scenario, causing panic in Raft message processing",
    "issue_number": 11951,
    "title": "one tikv panic after inject network-loss"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "High network packet loss (98%) causing region unavailability and transaction failure during distributed database operations",
    "issue_number": 11960,
    "title": "report \u201c9005: Region is unavailable\u201d while inject network loss fault for one tikv or down one of tikv or tikv rolling restart"
  },
  {
    "bug_location": "Profiler Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential memory or synchronization issue during CPU profiling that triggers unexpected TiKV restart",
    "issue_number": 11964,
    "title": "5.1.4 tikv restart in many testcases after start cpu profiling"
  },
  {
    "bug_location": "raftstore/peer_storage.rs",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Unexpected None value during term retrieval when scaling in TiKV nodes, likely due to race condition or incomplete state management during cluster reconfiguration",
    "issue_number": 11973,
    "title": "One tikv panic at tikv_util/src/lib.rs:465 when do scale in"
  },
  {
    "bug_location": "Memory Management",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Config"
    ],
    "root_cause": "Transparent Huge Pages (THP) causing excessive memory allocation and fragmentation on ARM architecture, leading to unexpected memory consumption during TiKV startup",
    "issue_number": 11979,
    "title": "TiKV memory usage problem when THP enabled"
  },
  {
    "bug_location": "Raft Log Replication",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Network"
    ],
    "root_cause": "Potential log synchronization failure during extended node recovery, causing persistent log replication lag after fault recovery",
    "issue_number": 11989,
    "title": "After a tikv fault (more than 30 minutes) is restored, the raft log lag has been chasing"
  },
  {
    "bug_location": "coprocessor/dag",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Response size exceeding 4GB limit during large data update operation, causing serialization failure",
    "issue_number": 11994,
    "title": "TiKV copr resp size may exceed 4GB"
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Memory management issue causing out-of-memory (OOM) during high-load OLTP workload test",
    "issue_number": 12002,
    "title": "Master: Tikv oom in daily run test plan"
  },
  {
    "bug_location": "Grafana Monitoring Dashboard",
    "severity": 2,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Incorrect metric type configuration and visualization design in monitoring dashboard",
    "issue_number": 12007,
    "title": "Some grafana expression are wrong or suboptimal"
  },
  {
    "bug_location": "RaftEngine/ApplyModule",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Apply process lagging behind leader append rate, causing entry cache eviction and log fetch overhead",
    "issue_number": 12009,
    "title": "Apply can't catch up when raft engine is enabled"
  },
  {
    "bug_location": "Raft Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Inefficient log compaction mechanism for follower Raft logs, causing excessive file accumulation and slow recovery during cluster restart",
    "issue_number": 12011,
    "title": "follower Raft logs cannot be force compacted and purged by Raft Engine"
  },
  {
    "bug_location": "split_controller.rs",
    "severity": 3,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Default ReadStats creation does not respect user-configured sample_num, potentially causing incorrect load-based splitting behavior",
    "issue_number": 12014,
    "title": "The creation of ReadStats should take into account the user configuration of sample_num"
  },
  {
    "bug_location": "backup/writer.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Unhandled empty key scenario in backup writer causing panic during raw backup operation",
    "issue_number": 12015,
    "title": "backup: TiKV panic when execute \"br backup raw\" with an empty key entry"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Assertion failure during Raft message handling, indicating a potential synchronization or state tracking issue in the RaftStore component",
    "issue_number": 12023,
    "title": "RaftStore panicked during jepsen bank test"
  },
  {
    "bug_location": "file_system/rate_limiter",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Rate limiter test cases failing due to potential timing or calculation inconsistencies in bandwidth measurement assertions",
    "issue_number": 12024,
    "title": " The two testcase test_rate_limited_light_flow,test_rate_limited_hybrid_flow failed by running `make test`"
  },
  {
    "bug_location": "Monitoring/Metrics",
    "severity": 2,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Incorrect region size calculation or reporting in monitoring metrics",
    "issue_number": 12025,
    "title": "Approximate Region size in grafana didn't correspond to the actual value"
  },
  {
    "bug_location": "raftstore/peer_storage",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Unexpected state transition in Raft log snapshot generation, causing panic when processing region state",
    "issue_number": 12026,
    "title": "tikv panic:918 unexpected state: Applying(0)"
  },
  {
    "bug_location": "RocksDB block cache",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Memory allocation inefficiency during complex query execution, potentially causing memory leak or improper memory release in sorting/aggregation operations",
    "issue_number": 1203,
    "title": "TiKV memory usage"
  },
  {
    "bug_location": "Store Read Query Component",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Potential issue with store selection algorithm when strict-picking-store is disabled, causing query convergence delays",
    "issue_number": 12032,
    "title": "Store read query is not converging after 30m when set strict-picking-store to false"
  },
  {
    "bug_location": "RaftEngine/Storage",
    "severity": 4,
    "categories": [
      "Storage",
      "Upgrade",
      "CodeBug"
    ],
    "root_cause": "Incompatible storage engine state during version transition, causing data parsing and consistency errors when switching between RocksDB and Raft Engine configurations",
    "issue_number": 12045,
    "title": "RocksDB and Raft Engine data exist simultaneously due to operational errors"
  },
  {
    "bug_location": "RaftStore/read_queue",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Out of bounds index access in ReadIndexQueue during leader reads, likely due to incorrect index management or race condition in read queue processing",
    "issue_number": 12046,
    "title": "RaftStore panicked with out of bounds access error"
  },
  {
    "bug_location": "RaftStore/PeerFsm",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Inconsistent region merge logic causing peer ID and epoch validation failure during region merge operation",
    "issue_number": 12048,
    "title": "RaftStore panicked when merging region"
  },
  {
    "bug_location": "SQL Expression Evaluation",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Improper handling of extreme numeric truncation parameters causing runtime panic",
    "issue_number": 12064,
    "title": "truncating infinit cause panic"
  },
  {
    "bug_location": "raftstore/apply",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Unexpected state during region merge operation in Raft commit merge execution, causing panic in apply delegate processing",
    "issue_number": 12073,
    "title": "raft_store: unexpected state of merging region region "
  },
  {
    "bug_location": "coprocessor/tracker.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Logging mechanism fails to capture slow requests exceeding predefined time threshold",
    "issue_number": 12088,
    "title": "tracker.rs doesn't log the slow request if it exceeds the deadline "
  },
  {
    "bug_location": "advance.rs",
    "severity": 3,
    "categories": [
      "Network",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "RPC communication failure between TiKV stores, specifically an unimplemented RPC method causing repeated leader check failures",
    "issue_number": 12092,
    "title": "v5.3.1 version, run tpch workload, print lots of [advance.rs:273] [\"check leader failed\"] "
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Memory management failure during sustained high-volume insert workload, likely related to inefficient memory allocation or memory leak in write path",
    "issue_number": 12107,
    "title": "Tikv oom after continue run oltp insert for about  4 hours"
  },
  {
    "bug_location": "Backup & Restore (BR) component",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Config"
    ],
    "root_cause": "Version compatibility check preventing restore operation between slightly mismatched TiKV cluster and BR tool versions",
    "issue_number": 12112,
    "title": "running BR in incompatible version of cluster"
  },
  {
    "bug_location": "Transaction/Pessimistic Lock",
    "severity": 3,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Incorrect assertion handling during concurrent transaction rollback and prewrite operations, causing false-positive assertion failures when checking index key versions under strict assertion level",
    "issue_number": 12113,
    "title": "Assertion false-positive on non-unique index key in pessimistic transaction that's rolled back by another transaction"
  },
  {
    "bug_location": "raft-engine",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential memory leak or incorrect entry tracking mechanism in Raft log storage causing persistent entry count growth",
    "issue_number": 12158,
    "title": "Entry count of raft-engine seems not right"
  },
  {
    "bug_location": "Raft/Replication Component",
    "severity": 5,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Memory accumulation during replica recovery with high-load workload, likely due to uncontrolled log replay or cache management when nodes are temporarily unavailable",
    "issue_number": 12159,
    "title": "TiKV OOM under TPCC workload when two tikv down for 10 minutes "
  },
  {
    "bug_location": "raftstore metrics tracking",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Integer overflow during high-load memory metrics tracking, likely due to insufficient range checking or improper metric accumulation logic",
    "issue_number": 12160,
    "title": "raftstore: Integer overflow problems on memory metrics"
  },
  {
    "bug_location": "Raft Consensus Module",
    "severity": 4,
    "categories": [
      "Transaction",
      "Network",
      "Performance"
    ],
    "root_cause": "Potential log replication synchronization issue during node recovery after extended downtime",
    "issue_number": 12161,
    "title": "raft log lag is more and more when down one tikv for 38min"
  },
  {
    "bug_location": "raft_log_engine/encryption",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Security",
      "Storage"
    ],
    "root_cause": "Invalid nonce/counter length during encryption initialization, preventing raft engine creation when encryption is enabled",
    "issue_number": 12162,
    "title": "Failed to create raft engine when encryption enabled."
  },
  {
    "bug_location": "Region Leader Election",
    "severity": 4,
    "categories": [
      "Transaction",
      "Upgrade",
      "LoadBalance"
    ],
    "root_cause": "Hibernate region election mechanism causing unexpected leader banishment during cluster rolling upgrade",
    "issue_number": 12166,
    "title": "region leaders can be banished in an election timeout when rolling upgrade a cluster"
  },
  {
    "bug_location": "Profiling/Debugging Component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect handling of heap profiling configuration when prof_active is initially set to false, preventing proper memory profiling initialization",
    "issue_number": 12180,
    "title": "Heap profiling not works when prof_active is set to false before starting"
  },
  {
    "bug_location": "gRPC SSL/TLS handshake component",
    "severity": 5,
    "categories": [
      "Network",
      "Security",
      "CodeBug"
    ],
    "root_cause": "SSL key share parsing error causing fatal TLS handshake failure and subsequent segmentation fault",
    "issue_number": 12198,
    "title": "Handshake failed with fatal error SSL_ERROR_SSL"
  },
  {
    "bug_location": "futures_util/future/map",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Attempting to poll a future after it has already returned Poll::Ready, violating future polling contract",
    "issue_number": 12202,
    "title": "Map must not be polled after it returned `Poll::Ready`"
  },
  {
    "bug_location": "Quota Limiter",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect implementation of CPU time limiting method that unnecessarily reduces configured foreground CPU time and requires additional result processing",
    "issue_number": 12218,
    "title": "Quota limiter: correct the mehod to limit cpu"
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Memory leak or inefficient memory management during long-running sysbench workload with high concurrency",
    "issue_number": 12229,
    "title": "after running sysbench read_write for a long time\uff083 days\uff09, one tikv oom"
  },
  {
    "bug_location": "raftstore/read_queue.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Unexpected mismatch in leader read UUID during Raft leader read advancement, causing an assertion failure in the read index queue",
    "issue_number": 12230,
    "title": "raftstore: unexpected panic when advance_leader_reads"
  },
  {
    "bug_location": "resolved_ts::endpoint",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Unexpected channel closure during build_scan_task in resolved timestamp endpoint, causing panic when attempting to unwrap a closed channel",
    "issue_number": 12231,
    "title": "resolved_ts: unexpected panic when build_scan_task"
  },
  {
    "bug_location": "raftstore/store/fsm/store",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Inconsistent peer ID during region merge operation, causing a mismatch between message peer ID and local peer ID",
    "issue_number": 12232,
    "title": "tikv panic:peer id increased after region is merged, message peer id 2228, local peer id 1140, region id: 649"
  },
  {
    "bug_location": "resource_metering/reporter",
    "severity": 5,
    "categories": [
      "Upgrade",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Assertion failure in resource metering records during upload process after version upgrade, indicating potential data structure incompatibility between v4.0.0 and v6.0.0",
    "issue_number": 12234,
    "title": "after upgrade from v4.0.0  to v6.0.0, tikv report \" [FATAL] [lib.rs:468] [\"assertion failed: self.others.is_empty()\"] [backtrace=\"   0: tikv_util::set_panic_hook::{{closure}}\""
  },
  {
    "bug_location": "TiKV Configuration Management",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Config"
    ],
    "root_cause": "Configuration validation during version upgrade prevents migration due to unexpected Raft engine directory change detection",
    "issue_number": 12238,
    "title": "v3.0.11 upgrade to v6.0.0 fail with \"critical config check failed: raft engine dir have been changed, former is ''"
  },
  {
    "bug_location": "raftstore/peer",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Hash verification failure during Raft consensus process, indicating potential data corruption or inconsistency in region state",
    "issue_number": 12253,
    "title": "tikv panic\uff1a[region 12786953] 12786955 hash at 226 not correct, want \\\"\\\\200\\\\207\\\\326{\\\", got \\\"5\\\\305'R\\\"!!!\""
  },
  {
    "bug_location": "components/raftstore/src/store",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect mathematical comparison and duplicate duration recording in slow score calculation logic",
    "issue_number": 12254,
    "title": "slow score calculation is not accurate"
  },
  {
    "bug_location": "TiKV Network/Memory Management",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Memory leak during network fault injection and recovery, likely caused by unhandled network interruption in connection management or data replication processes",
    "issue_number": 12255,
    "title": "two tikv oom after inject tikv network-loss and recovery for some time"
  },
  {
    "bug_location": "Raft/Apply Engine",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Async Apply CPU queue becoming saturated during network partition recovery, causing temporary processing blockage and QPS drop",
    "issue_number": 12259,
    "title": "tikv stability: QPS fell to zero for a few minutes after fault recover from minority tikv network-loss due to full of \"Async Apply CPU\""
  },
  {
    "bug_location": "RocksDB Options Parser",
    "severity": 5,
    "categories": [
      "Upgrade",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Invalid configuration file parsing during version upgrade, specifically a malformed configuration line missing an '=' sign",
    "issue_number": 12269,
    "title": "when upgrade cluster to v6.0.0, tikv throws an error."
  },
  {
    "bug_location": "Configuration Documentation",
    "severity": 2,
    "categories": [
      "Config",
      "Human"
    ],
    "root_cause": "Incomplete documentation for configuration parameters and online modification guidelines",
    "issue_number": 12302,
    "title": "6.0.0 document issue for config online modify"
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Decreased RocksDB block cache hit ratio and increased table scan duration in TiKV v6.0.0 compared to v5.4.0",
    "issue_number": 12306,
    "title": "Compared with v5.4.0, the performance of TPCH drop 4.5% in v6.0.0 with the duration of TableFullScan increases and cache_hit_count decreases"
  },
  {
    "bug_location": "DR (Disaster Recovery) Autosync Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "LoadBalance",
      "Storage"
    ],
    "root_cause": "Region unavailability during cluster switchover in disaster recovery mode, likely caused by inconsistent region metadata or synchronization issues during failover process",
    "issue_number": 12307,
    "title": "dr-autosync: After switch to backup cluster in sync mode, scan table hit 9005: Region is unavailable "
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Misconfiguration of RocksDB write stall parameters causing write buffer overflow and transaction processing blockage",
    "issue_number": 12316,
    "title": "set storage.flow-control.enable: false\uff0cthe transaction OPS drops to 0. after the load is stopped, connect to tidb cluster executes SQL statements without response"
  },
  {
    "bug_location": "pd_client",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Inconsistent store ID mapping during disaster recovery, causing PD client reconnection failures",
    "issue_number": 12320,
    "title": "dr-autosync: dr tikvs hung  if primary tikvs startup again after do disaster recovery"
  },
  {
    "bug_location": "external_storage/s3_backend",
    "severity": 2,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Incorrect handling of multi-part uploads with small file chunks during S3 object storage operations, causing upload failures for files larger than 5MB",
    "issue_number": 12325,
    "title": "external_storage: the `s3` backend cannot update data from some not-in-memory `Read` types"
  },
  {
    "bug_location": "tikv-ctl tool",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect string matching logic for SST file name validation in bad-ssts tool",
    "issue_number": 12329,
    "title": "tikv-ctl: error string match in `bad-ssts` tool"
  },
  {
    "bug_location": "Transaction/Follower Read",
    "severity": 3,
    "categories": [
      "Transaction",
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Persistent key lock conflict during full table scan with follower read configuration, preventing transaction completion",
    "issue_number": 12341,
    "title": "full scan report other error when open follower read"
  },
  {
    "bug_location": "PD Client",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Transaction"
    ],
    "root_cause": "Improper error handling in PD client causing unnecessary reconnections and request retries for invalid requests",
    "issue_number": 12345,
    "title": "PD client should handle errors properly"
  },
  {
    "bug_location": "raftstore/store/fsm/store.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Network"
    ],
    "root_cause": "Missing Raft log entry during peer creation, likely caused by race condition or incomplete log replication during network partition",
    "issue_number": 12348,
    "title": "raftstore: panicked with `no entry found for key`"
  },
  {
    "bug_location": "Transaction Recovery",
    "severity": 4,
    "categories": [
      "Transaction",
      "Network",
      "Performance"
    ],
    "root_cause": "Incomplete transaction state recovery mechanism during node fault and network partition scenarios, causing query failures and QPS degradation",
    "issue_number": 12353,
    "title": "Many failed query OPM after tikv fault recover and cause QPS falls zero"
  },
  {
    "bug_location": "Region Management",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Statistical error in region size calculation for large number of partitioned tables, causing potential load balancing and performance inconsistencies",
    "issue_number": 12365,
    "title": "Single table with 7000 partitioned tables\uff0cApproximate Region size exception"
  },
  {
    "bug_location": "raftstore/peer_fsm",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Hash verification failure during region split and network fault injection, causing TiKV panic",
    "issue_number": 12366,
    "title": "tikv panic when inject network fault repeatly and  split tables"
  },
  {
    "bug_location": "RaftStore/Compaction",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Inefficient SST ingestion strategy causing excessive L0 compactions and potential disk space exhaustion",
    "issue_number": 12367,
    "title": "too many compactions and even write stalls occur by cleaning range by ingesting SSTs"
  },
  {
    "bug_location": "raftstore/apply",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Attempting to apply a region split operation on a tombstone region that already exists in the key-value engine, causing an unexpected panic during Raft log application",
    "issue_number": 12368,
    "title": "raftstore: panicked due to trying to apply a tombstone region already existed in kv engine"
  },
  {
    "bug_location": "server startup process",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config",
      "Human"
    ],
    "root_cause": "Permission conflict when creating lock files across different user contexts, likely due to insufficient file access rights during server initialization",
    "issue_number": 12399,
    "title": "File locks for working around port conflict are sometimes inaccessible "
  },
  {
    "bug_location": "TiKV Server",
    "severity": 2,
    "categories": [
      "Network",
      "Config",
      "Performance"
    ],
    "root_cause": "Potential timeout handling issue during cluster reload causing unreachable node alarms",
    "issue_number": 12408,
    "title": "The tikv component triggers the alarm type = \"unreachable\" when reloading"
  },
  {
    "bug_location": "Memory Management",
    "severity": 5,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Memory leak or unbounded memory growth in TiKV's internal memory allocation mechanism",
    "issue_number": 12416,
    "title": "TiKV Short-term memory usage continues to rise"
  },
  {
    "bug_location": "Region Merge Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Inefficient region merge algorithm causing excessive processing time for empty/small regions after large data drop",
    "issue_number": 12421,
    "title": "Take more than 5 hours to merge empty region after drop 6T data"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Dynamic region configuration causing potential load balancing and transaction processing instability during high-concurrency insert workloads",
    "issue_number": 12423,
    "title": "\u3010Dynamic Regions\u3011QPS dropped to zero while sysbench load data"
  },
  {
    "bug_location": "Region Load Balancing",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance",
      "Storage"
    ],
    "root_cause": "Dynamic region configuration causing uneven data distribution across TiKV nodes, leading to capacity imbalance and potential storage pressure on specific nodes",
    "issue_number": 12425,
    "title": "\u3010Dynamic Regions\u3011unbalanced capacity between tikv, and a tikv generates an insufficient capacity alarm"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Improper handling of learner peer state during region configuration change, causing persistent pending peer state",
    "issue_number": 12431,
    "title": "Pending peer may exist for a long time when adding a learner peer"
  },
  {
    "bug_location": "tikv-ctl",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incomplete manifest dump implementation in TiKV's ldb subcommand, causing truncated or partial output compared to RocksDB's native ldb tool",
    "issue_number": 12438,
    "title": "tikv-ctl ldb subcommand can't print right info"
  },
  {
    "bug_location": "gRPC global timer component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Network"
    ],
    "root_cause": "Potential memory management or synchronization issue causing unexpected automatic restart of TiKV server without out-of-memory condition",
    "issue_number": 12453,
    "title": "TiKV automatic restart, non-oom"
  },
  {
    "bug_location": "RaftLogEngine",
    "severity": 5,
    "categories": [
      "Storage",
      "Config",
      "Upgrade"
    ],
    "root_cause": "Codec error during Raft log engine initialization, likely caused by configuration update or data corruption during engine startup",
    "issue_number": 12463,
    "title": "tikv can't start when running go-ycsb rawkv workload"
  },
  {
    "bug_location": "raftstore/split_check",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Assertion failure during region split check after dropping large tables, indicating a potential race condition or incorrect state tracking in dynamic region management",
    "issue_number": 12467,
    "title": "[Dynamic Regions] tikv panic after drop big table"
  },
  {
    "bug_location": "RaftStore/RaftEngine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Incomplete snapshot application causing inconsistent Raft engine state, leading to write failures and potential data integrity issues during interrupted snapshot synchronization",
    "issue_number": 12470,
    "title": "Raft Engine is in inconsistent state after snapshot apply is interrupted"
  },
  {
    "bug_location": "PD (Placement Driver) Store Registration",
    "severity": 5,
    "categories": [
      "Network",
      "Config",
      "Transaction"
    ],
    "root_cause": "Invalid store ID generation or registration failure during cluster restart, causing TiKV node to lose its store identity",
    "issue_number": 12478,
    "title": "v5.4.1: after restart all tikv/pd/tidb, one tikv continue report error \"invalid store ID 0, not found\""
  },
  {
    "bug_location": "tikv_util/sys/cgroup.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Unexpected nil value when attempting to retrieve CPU cores quota from cgroup, causing unwrap() panic during TiKV configuration initialization",
    "issue_number": 12479,
    "title": "`cpu_cores_quota` cause TiKV Panic"
  },
  {
    "bug_location": "Transaction",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Lack of constraint validation before assertion in distributed transaction processing, allowing potential inconsistent index state",
    "issue_number": 12487,
    "title": "Do constraint check before returning an assertion error"
  },
  {
    "bug_location": "causal_ts/tso.rs",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Race condition in timestamp batch renewal causing potential timestamp fallback when multiple threads concurrently request and update timestamp batches",
    "issue_number": 12489,
    "title": "Causal timestamp fall back"
  },
  {
    "bug_location": "RawKV API V2",
    "severity": 5,
    "categories": [
      "Transaction",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Timestamp causality violation in multi-threaded RawKV operations causing inconsistent key-value state",
    "issue_number": 12498,
    "title": "RawKV API V2 timestamp causality violation"
  },
  {
    "bug_location": "PD Client",
    "severity": 3,
    "categories": [
      "Network",
      "Config"
    ],
    "root_cause": "Improper handling of StoreTombstone error during store removal, causing excessive reconnection attempts",
    "issue_number": 12506,
    "title": "PD client keeps reconnecting on error StoreTombstone"
  },
  {
    "bug_location": "backup-stream/endpoint",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Inconsistent region task state during log backup task stop, causing unexpected region registration and observation mismatch",
    "issue_number": 12507,
    "title": "TiKV panic after stop PiTR log backup task"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Checkpoint synchronization failure preventing log backup progress tracking",
    "issue_number": 12508,
    "title": "Checkpoint doesn't move forward during log backup"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 5,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Memory exhaustion during log backup scanning when processing large historical log volumes",
    "issue_number": 12509,
    "title": "TiKV OOM after starting log backup when there are lots of log to be scanned"
  },
  {
    "bug_location": "Raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Config",
      "Storage"
    ],
    "root_cause": "Region splitting mechanism fails to trigger after configuration change and restart, preventing automatic region size management",
    "issue_number": 12510,
    "title": "Raftstore: tikv does not split regions whose  size is larger than the max region size after restart with config change"
  },
  {
    "bug_location": "log-backup component",
    "severity": 3,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Temporary log backup directories are not being automatically cleaned up after task completion or termination",
    "issue_number": 12512,
    "title": "Clean log-backup.temp-path after a log backup task is stopped"
  },
  {
    "bug_location": "backup-stream/event_loader",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Channel of paired_future_callback unexpectedly canceled during log backup task, causing unhandled panic in event loader",
    "issue_number": 12513,
    "title": "TiKV panic when running PiTR log backup"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Inconsistent checkpoint synchronization between TiKV stores during log backup, causing significant time gaps in checkpoint updates",
    "issue_number": 12514,
    "title": "Log backup checkpoint gap > 5 mins  during log backup"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Log backup checkpoint timestamp being incorrectly garbage collected, causing task interruption and potential data loss risk",
    "issue_number": 12515,
    "title": "log backup checkpoint_ts get smaller than gc_safepoint"
  },
  {
    "bug_location": "Region Configuration Management",
    "severity": 4,
    "categories": [
      "Config",
      "CodeBug"
    ],
    "root_cause": "Inconsistent configuration synchronization between TiKV and PD configuration management systems",
    "issue_number": 12518,
    "title": "[Dynamic Region] region size config is not inconsistent between tikv config file and pd config show "
  },
  {
    "bug_location": "RawKV Compaction Filter",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect metric tracking in compaction filter reporting mechanism, causing total count to remain zero despite filtering entries",
    "issue_number": 12522,
    "title": "RawKV Compaction filter report total is aways 0"
  },
  {
    "bug_location": "steady-timer",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential memory corruption or segmentation fault in libgcc library during timer operations",
    "issue_number": 12525,
    "title": "TiKV restarts due to segfault"
  },
  {
    "bug_location": "RawKV Compaction Filter",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect metric tracking in compaction filter reporting mechanism, causing total count to remain zero despite filtering entries",
    "issue_number": 12535,
    "title": "RawKV Compaction filter report total is aways 0"
  },
  {
    "bug_location": "Backup & Restore (BR) Component",
    "severity": 5,
    "categories": [
      "Storage",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Potential data synchronization or replay issue during point-in-time recovery (PiTR) log restore process, causing inconsistent data state between upstream and downstream clusters",
    "issue_number": 12538,
    "title": "[BR] Data inconsistency after br log restore"
  },
  {
    "bug_location": "resource_metering",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential race condition or timing instability in dynamic configuration testing of resource metering component",
    "issue_number": 12541,
    "title": "unstable resouce_metering::test_dynamic_config::test_report_interval"
  },
  {
    "bug_location": "tidb_query_expr",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unhandled UTF-8 character conversion error during cast operation, causing panic when processing invalid character sequences",
    "issue_number": 12542,
    "title": "TiKV panic when `cast` func pushdown with process invalid utf8 character"
  },
  {
    "bug_location": "storage/space_reservation",
    "severity": 2,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Incompatibility with aufs filesystem storage reservation mechanism",
    "issue_number": 12543,
    "title": "space reserve fails on aufs"
  },
  {
    "bug_location": "Backup and Restore (BR) Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Upgrade",
      "Storage"
    ],
    "root_cause": "Incompatible key format handling during cross-version backup and restore between APIv1 and APIv2 clusters",
    "issue_number": 12548,
    "title": "[rawkv backup] error key format in response files when backup from v1 to v2 and range is not empty "
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Timestamp incorrectly included in bucket keys during dynamic region and region buckets configuration",
    "issue_number": 12549,
    "title": "raftstore: bucket keys should not contain TS information"
  },
  {
    "bug_location": "TLS Metrics",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Thread-local metrics not consistently flushed across different thread pools, causing metrics aggregation inconsistencies in async/await contexts",
    "issue_number": 12562,
    "title": "tls metrics is not reliable"
  },
  {
    "bug_location": "raftstore/store/fsm/peer",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Hash verification failure in Raft region peer state synchronization, indicating potential data corruption or inconsistent state between nodes",
    "issue_number": 12564,
    "title": "6.1.0: Tikv panic and report [lib.rs:491] [\"[region 28137] 28138 hash at 2799 not correct"
  },
  {
    "bug_location": "backup module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Incompatible backup response file range handling between API versions",
    "issue_number": 12571,
    "title": "Revert \"[backup apiv2] fix file range in backup response\" #12570"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Potential race condition or memory management issue during dynamic region splitting under high concurrency workload",
    "issue_number": 12574,
    "title": "[Dynamic Region] tikv panic repeatly"
  },
  {
    "bug_location": "Apply struct in transaction processing",
    "severity": 3,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Incorrect handling of bucket_meta during batch processing, causing potential data inconsistency by using an outdated metadata reference",
    "issue_number": 12578,
    "title": "In try_batch, we should use the latest apply's bucket_meta."
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Incorrect GC safepoint calculation during log backup fatal error, using start_ts which fails to prevent potential data loss",
    "issue_number": 12579,
    "title": "[BR] gc safepoint not set correctly when there is log backup fatal error "
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Excessive logging triggered during PD error condition, potentially causing log flooding and performance overhead",
    "issue_number": 12580,
    "title": "[BR] huge logs when PD error during log backup"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Unhandled residual transaction lock preventing checkpoint timestamp progression in log backup task",
    "issue_number": 12582,
    "title": "[BR] log backup checkpoint_ts doesn't move forward when there is residual tidb lock"
  },
  {
    "bug_location": "Backup and Restore (BR) Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect global checkpoint management during log backup pause operation, causing checkpoint to continue advancing despite paused state",
    "issue_number": 12583,
    "title": "After br log pausing, the global checkpoint is still moving forward"
  },
  {
    "bug_location": "GC (Garbage Collection) Worker",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Incompatibility between TiKV API V2 and lock resolution mechanism during garbage collection process",
    "issue_number": 12595,
    "title": "when enable api v2 , TiDB resolve lock meet error"
  },
  {
    "bug_location": "Region Split Mechanism",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Config"
    ],
    "root_cause": "Region splitting configuration parameters preventing expected dynamic region size reduction, particularly with large data volumes and complex configuration settings",
    "issue_number": 12597,
    "title": "[Dynamic Region] Some large regions did not split as expected"
  },
  {
    "bug_location": "Region Split Checker",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Incorrect calculation of region_split_check_diff when region buckets are enabled, causing split ticks to be skipped in large regions",
    "issue_number": 12598,
    "title": "default size of region_split_check_diff is too large when big region is enabled"
  },
  {
    "bug_location": "storage configuration",
    "severity": 3,
    "categories": [
      "Config",
      "Upgrade",
      "Storage"
    ],
    "root_cause": "Incompatible storage API version configuration preventing startup when attempting to enable TTL on a non-TTL storage instance",
    "issue_number": 12600,
    "title": "Fail to startup when modify storage.api-version to 2 from 1"
  },
  {
    "bug_location": "Transaction Commit Protocol",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Non-idempotent prewrite handling in pessimistic async-commit transactions causing inconsistent transaction states",
    "issue_number": 12615,
    "title": "Pessimistic prewrite may be not idempotent on CommitTsTooLarge"
  },
  {
    "bug_location": "components/raftstore/src/store/peer_storage.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Incorrect snapshot last index validation during leader state transition and log compaction",
    "issue_number": 12618,
    "title": "checks for snapshot last index is wrong"
  },
  {
    "bug_location": "lock_manager",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential deadlock detection logic failure causing test panic in transaction lock management",
    "issue_number": 12619,
    "title": "test_detect_deadlock_basic panics"
  },
  {
    "bug_location": "raft/peer_storage",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Delayed state transition of removed peer during configuration change, allowing potential replica recreation and cluster split",
    "issue_number": 1262,
    "title": "Mark tombstone immediately when applying a confchagne"
  },
  {
    "bug_location": "quota_limiter",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Config",
      "Performance"
    ],
    "root_cause": "Invalid configuration causing duration conversion failure when attempting to update online configuration, resulting in unhandled panic",
    "issue_number": 12622,
    "title": "all tikv panic when update config online "
  },
  {
    "bug_location": "Region Merge Handling",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Unsafe handling of uncommitted merge regions during leader state transitions, potentially causing data loss during region merge operations",
    "issue_number": 12627,
    "title": " Handle unapplied commit merges in unavailable regions during unsafe recovery."
  },
  {
    "bug_location": "DynamicRegion/Coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Inconsistent bucket generation and reporting mechanism when recreating database, preventing proper region task splitting",
    "issue_number": 12642,
    "title": "DynamicRegion: Buckets are not working as expected."
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Incorrect timestamp tracking during log backup task initialization and cluster restart operations, causing checkpoint_ts to be incorrectly set to task start time instead of actual checkpoint",
    "issue_number": 12643,
    "title": "[br] checkpoint_ts metrics abnormal, log backup start-ts is display sometimes."
  },
  {
    "bug_location": "Online Recovery Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Network"
    ],
    "root_cause": "Timeout during unsafe recovery process when attempting to force leader election across distributed stores",
    "issue_number": 12644,
    "title": "[Online Recovery] Unsafe online recovery failed with info \u201cunsafe recovery enters exit force leader stage exceeds timeout\u201d"
  },
  {
    "bug_location": "raftstore/apply",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Inconsistent index handling during region merge operation, causing first index to exceed minimum index",
    "issue_number": 12663,
    "title": "tikv panic: 12475 first index 56 > min_index 55, skip pre merge"
  },
  {
    "bug_location": "raftstore/store/fsm/store.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Missing entry lookup in peer creation process causing unexpected panic when no entry is found for a specific key during Raft message handling",
    "issue_number": 12664,
    "title": "raftstore: tikv panicked with `no entry found for key`"
  },
  {
    "bug_location": "metrics endpoint",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Duplicate metric generation in response generation logic",
    "issue_number": 12670,
    "title": "tikv returns duplicated metrics result."
  },
  {
    "bug_location": "TiKV SQL Expression Evaluation",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Improper handling of CONV function with bit column type during SQL expression evaluation, causing unexpected crash when processing empty or invalid conversion inputs",
    "issue_number": 12673,
    "title": "tikv crash when conv empty string"
  },
  {
    "bug_location": "RawKV API V2 timestamp management",
    "severity": 5,
    "categories": [
      "Transaction",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Timestamp causality violation during region merge operations, causing inconsistent key-value state across distributed stores",
    "issue_number": 12680,
    "title": "RawKV API V2 timestamp causality violation"
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Upgrade",
      "CodeBug"
    ],
    "root_cause": "File system access error during RocksDB background compaction, possibly related to data file corruption or migration during version upgrade",
    "issue_number": 12698,
    "title": "v5.2.4 upgrade to v6.1.0, tikv report panic for \" [FATAL] [lib.rs:491] [\"rocksdb background error. db: raft, reason: compaction, error: IO ...\""
  },
  {
    "bug_location": "build system/test compilation",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Incompatible tonic transport module import or version mismatch causing compilation failure during test",
    "issue_number": 12711,
    "title": "*: `cargo test` cannot compile "
  },
  {
    "bug_location": "Change Data Capture (CDC) Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Potential race condition or incomplete handling of transaction conflicts during incremental data scanning, leading to potential data loss in change data capture process",
    "issue_number": 12717,
    "title": "cdc data lost when transaction conflicts during incremental scan"
  },
  {
    "bug_location": "MVCC Version Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Inefficient garbage collection of invisible MVCC versions leading to excessive key versions and iterator performance degradation",
    "issue_number": 12729,
    "title": "the invisible mvcc versions are not purged by gc"
  },
  {
    "bug_location": "Coprocessor DateTime Parsing",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of timezone and fractional second parsing in DateTime conversion function",
    "issue_number": 12739,
    "title": "Wrong check in time parsing"
  },
  {
    "bug_location": "Import Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Network"
    ],
    "root_cause": "Failed S3 file access during log backup restore, likely due to permissions, network, or file path configuration issue",
    "issue_number": 12750,
    "title": "No such file or directory when download and apply log backup file"
  },
  {
    "bug_location": "Backup and Restore (BR) Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Potential memory management or error handling issue during point-in-time restore operation that causes unexpected TiKV pod restart",
    "issue_number": 12751,
    "title": "TiKV panic when execute br restore point"
  },
  {
    "bug_location": "Region Split Check Component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config",
      "Performance"
    ],
    "root_cause": "Dynamic region configuration updates did not trigger split check mechanism, causing large regions to remain unsplit even after configuration changes",
    "issue_number": 12768,
    "title": "[Dynamic Regions] Some large regions did not split due to split check did not run after update region split config online"
  },
  {
    "bug_location": "log backup checkpoint mechanism",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Log backup resolver timestamp is not advancing, causing checkpoint to stall at a single timestamp",
    "issue_number": 12782,
    "title": "br log backup checkpoint doesn't move forward"
  },
  {
    "bug_location": "Monitoring/Metrics Collection",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Incorrect IO utilization metric calculation for disk devices, potentially caused by incorrect parsing or sampling of disk I/O statistics on EC2 instances",
    "issue_number": 12790,
    "title": "TiKV IOUTIL not correct on EC2(4c instance)"
  },
  {
    "bug_location": "PD (Placement Driver) Leader Election Component",
    "severity": 4,
    "categories": [
      "Network",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Unstable leader election process causing transient service disruption during PD leader replacement, potentially impacting ongoing read/write operations",
    "issue_number": 12794,
    "title": "Workload Error when killing pd Leader in HA test"
  },
  {
    "bug_location": "log-backup component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Premature GC of checkpoint timestamp when log-backup task encounters error, causing potential data loss before expected 24-hour retention period",
    "issue_number": 12802,
    "title": "log-backup: The checkpoint-ts do not keep 24h when the task status become error."
  },
  {
    "bug_location": "Memory Management",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Memory leak during chaos testing with multiple node restarts causing uncontrolled memory consumption",
    "issue_number": 12807,
    "title": "TiKV oom due to memory leak"
  },
  {
    "bug_location": "raftstore/peer_fsm",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Race condition in region recovery logic causing unexpected panic when handling delayed destroy operation",
    "issue_number": 12825,
    "title": "raftstore: tikv panicked due to trying to recover a delayed destroy"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Checkpoint synchronization failure during repeated TiKV server restarts, causing significant delay in log backup progress tracking",
    "issue_number": 12854,
    "title": "br log backup checkpoint lag > 1H after stop restarting tikv for 1H"
  },
  {
    "bug_location": "TiKV server configuration",
    "severity": 3,
    "categories": [
      "Config",
      "Network"
    ],
    "root_cause": "TLS certificate validation fails when using IP addresses instead of localhost for status_address",
    "issue_number": 12867,
    "title": "tikv should support use hostname instead of ip for status_address"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Network"
    ],
    "root_cause": "Excessive retry mechanism during S3 file flush causing redundant S3 API requests",
    "issue_number": 12869,
    "title": "Br Log backup flush file retry results in too many s3 request"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Log backup generating excessive small files in single directory, causing filesystem indexing limitations",
    "issue_number": 12885,
    "title": "br log backup too many small files and too many files in single folder"
  },
  {
    "bug_location": "Compaction Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "High thread count during cluster compaction causing excessive memory allocation and potential out-of-memory condition",
    "issue_number": 12887,
    "title": "compact-cluster using high threads  may cause oom"
  },
  {
    "bug_location": "Raft Engine Encryption Key Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Security",
      "Performance"
    ],
    "root_cause": "Lack of encryption key cleanup mechanism for WAL files in Raft Engine, causing memory and disk space leaks",
    "issue_number": 12890,
    "title": "Encryption keys for WALs are not cleaned up when Raft Engine is enabled"
  },
  {
    "bug_location": "Backup and Restore (BR) Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "Performance degradation during concurrent log backup and full backup operations, causing significant latency increase",
    "issue_number": 12896,
    "title": "[BR] Duration 99 triples  when log backup and full backup are in progress"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Network",
      "Performance"
    ],
    "root_cause": "Rusoto S3 object storage upload failure during long-running log backup process",
    "issue_number": 12902,
    "title": "PiTR log backup fails due to \"failed to put object rusoto error\""
  },
  {
    "bug_location": "Backup and Restore (BR) component",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "Coprocessor task timeout when handling large region sizes (>1GB) during restore operation",
    "issue_number": 12904,
    "title": "br restore timeout when region size more then 1GB\uff08tested 1GB and 10GB\uff09"
  },
  {
    "bug_location": "Disaster Recovery Auto-Sync Module",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Timeout handling failure during cross-datacenter leader election and query routing when backup datacenter stores are down",
    "issue_number": 12914,
    "title": "dr-auto-sync: After down tikv in backup cluster, query in primary cluster hit 9002: TiKV timeout"
  },
  {
    "bug_location": "Backup and Restore (BR) component",
    "severity": 5,
    "categories": [
      "Storage",
      "Transaction",
      "Upgrade"
    ],
    "root_cause": "Incorrect handling of partition table metadata during log backup and point-in-time recovery (PiTR) restore process",
    "issue_number": 12916,
    "title": "[BR] partition table created during log backup are not restored correctly"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Network",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Incomplete log backup synchronization during network partition and TiKV restart, causing potential data loss in point-in-time recovery scenarios",
    "issue_number": 12918,
    "title": "BR log backup data lost if tikv restart followed by tikv network partition"
  },
  {
    "bug_location": "encryption/crypter component",
    "severity": 4,
    "categories": [
      "Config",
      "Security",
      "Storage"
    ],
    "root_cause": "Invalid nonce/counter configuration during encryption initialization, causing TiKV reload failure when enabling data encryption",
    "issue_number": 12919,
    "title": "redhat8.4 OS , v6.1.0 add \"data-encryption-method\", reload tikv fail "
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Incompatible type casting behavior between TiDB and TiKV when converting strings to time, specifically in complex type conversion scenarios",
    "issue_number": 12932,
    "title": "cast string as time behaviour is incompatible with TiDB"
  },
  {
    "bug_location": "pd_client/client.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential deadlock due to nested read locks on the same RwLock, which can cause thread contention when write locks are attempted",
    "issue_number": 12933,
    "title": "components/pd_client: potential deadlocks caused by double-readlock in call_option"
  },
  {
    "bug_location": "PD (Placement Driver) Heartbeat Mechanism",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Network communication disruption during PD leader IO delay, causing TiKV nodes to fail region heartbeat reporting",
    "issue_number": 12934,
    "title": "Master: two tikv don't report region heartbeat after inject fault to pd leader"
  },
  {
    "bug_location": "Raw API (apiv2) checksum implementation",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Key encoding not properly handled in raw checksum operation for API v2",
    "issue_number": 12950,
    "title": "raw_checksum does not encode key for apiv2"
  },
  {
    "bug_location": "tikv_util/sys/thread.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Incompatible futures library version causing type resolution failure for ThreadPoolBuilder",
    "issue_number": 12963,
    "title": "Can't compile and run individual tests"
  },
  {
    "bug_location": "Network/Availability Component",
    "severity": 4,
    "categories": [
      "Network",
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Network partition causing multi-AZ communication failure, leading to temporary service unavailability",
    "issue_number": 12966,
    "title": "one az is isolated from other az networks, and the service is unavailable for 4 minutes"
  },
  {
    "bug_location": "Raft Log Replication",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Network"
    ],
    "root_cause": "Potential performance bottleneck in log synchronization mechanism preventing rapid log catch-up after node recovery",
    "issue_number": 12979,
    "title": "In version 6.1.0 , a tikv failure was restored after 13 minutes, and the raft log lag has not been able to catch up"
  },
  {
    "bug_location": "Memory Management",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Network"
    ],
    "root_cause": "Memory leak or excessive memory allocation during network recovery and high-concurrency workload",
    "issue_number": 12983,
    "title": "one tikv oom after 40 minutes of recovery from network loss"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Checkpoint information retrieval delay in log status display for non-JSON output",
    "issue_number": 12990,
    "title": "[PiTR] br log status doesn't show checkpoint info for few minutes after starting a log backup task "
  },
  {
    "bug_location": "TiKV Server",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Config"
    ],
    "root_cause": "Memory management issue causing repeated out-of-memory (OOM) conditions in TiKV server process",
    "issue_number": 12997,
    "title": "RedHat 8.5:  Tikv oom repeatedly after deploy cluster via tiup "
  },
  {
    "bug_location": "server/engine_factory",
    "severity": 5,
    "categories": [
      "Upgrade",
      "Config",
      "CodeBug"
    ],
    "root_cause": "Configuration option 'enable_pipelined_commit' not found during TiKV version upgrade, causing engine initialization failure",
    "issue_number": 13007,
    "title": "tikv restart failed\uff08upgrade new image and CrashLoopBackOff\uff09"
  },
  {
    "bug_location": "RocksDB Engine Configuration",
    "severity": 5,
    "categories": [
      "Upgrade",
      "Config",
      "CodeBug"
    ],
    "root_cause": "Configuration option mismatch during TiKV version upgrade, specifically missing 'enable_multi_thread_write' option",
    "issue_number": 13015,
    "title": "after upgrade tikv restart failed (option not match)"
  },
  {
    "bug_location": "RocksDB Block Cache",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Backup process incorrectly populating RocksDB block cache, causing potential cache pollution and eviction of important user read data",
    "issue_number": 13020,
    "title": "Should not fill cache when backup rawkv"
  },
  {
    "bug_location": "api_version component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect range validation logic for reverse scan that does not account for key order reversal",
    "issue_number": 13021,
    "title": "API v2 range check will fail in reverse scan"
  },
  {
    "bug_location": "Scheduler/Transaction Layer",
    "severity": 2,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Metrics tracking for pessimistic lock key count not properly implemented, causing incorrect monitoring of scheduled key operations",
    "issue_number": 13026,
    "title": "in-mem pessimistic lock does not record the scheduled key count"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Potential performance bottleneck in log backup checkpoint synchronization mechanism causing excessive lag during high write workloads",
    "issue_number": 13030,
    "title": "BR log backup checkpoint lag > 10 min sometimes"
  },
  {
    "bug_location": "Backup & Restore (BR) component",
    "severity": 4,
    "categories": [
      "Network",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Checksum validation failure during metadata restoration after network partition disruption between TiKV and S3 storage",
    "issue_number": 13034,
    "title": "BR restore point fails due to \"\u201cfailed to restore meta files: checksum mismatch\" after injecting network partition bw s3 and tikv for 1 hour"
  },
  {
    "bug_location": "raftstore/apply_fsm",
    "severity": 5,
    "categories": [
      "Storage",
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unimplemented engine write operation when Titan storage engine is enabled, causing fatal startup error during TiKV restart",
    "issue_number": 13038,
    "title": "tikv cannot start when open titan"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Memory inefficient handling of large number of regions, causing excessive memory consumption and potential out-of-memory conditions",
    "issue_number": 13042,
    "title": "tikv oom when tikv instance has a large number of regions such as 300k+"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Checkpoint tracking mechanism failure preventing log backup progress",
    "issue_number": 13044,
    "title": "Br log backup checkpoint lag doesn't move foward"
  },
  {
    "bug_location": "SQL Expression Evaluation",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect type conversion and boolean evaluation in XOR operation between numeric and string types",
    "issue_number": 13045,
    "title": "Confusing behavior in the UPDATE statement"
  },
  {
    "bug_location": "raftstore/StoreFsm",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "CodeBug"
    ],
    "root_cause": "Excessive broadcast of StoreUnreachable messages causing potential message processing bottleneck when a store becomes unreachable",
    "issue_number": 13054,
    "title": "Raftstore may broadcast too many messages after one peer store become unreachable"
  },
  {
    "bug_location": "Transaction Scheduler",
    "severity": 2,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Unnecessary thread CPU time collection in critical paths when quota limiter auto-tune is disabled",
    "issue_number": 13055,
    "title": "ThreadTime is collected accidentally even if QuotaLimiter auto-tune is disabled"
  },
  {
    "bug_location": "log-backup",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect timestamp validation during point-in-time recovery (PiTR) restore process, causing restore failure when restored timestamp exceeds log backup range",
    "issue_number": 13062,
    "title": "log-backup: PiTR restore failed due to restored_ts > log backup range"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Initial log backup scan generates excessive background load, causing significant performance degradation during event generation and scanning of historical data",
    "issue_number": 13068,
    "title": "br log backup initial scan downgrade cluster performance "
  },
  {
    "bug_location": "Raft commit log processing",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Network isolation recovery causing excessive commit log processing overhead, leading to significant performance degradation",
    "issue_number": 13077,
    "title": "v6.1.0: After network isolation is recovered, QPS drops more than 50% due to high commit log duration"
  },
  {
    "bug_location": "encryption/manager",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Security"
    ],
    "root_cause": "Encryption key management failure when changing encryption method from AES to plaintext, causing assertion errors during file operations",
    "issue_number": 13081,
    "title": "v6.2.0-alpha: change \"data-encryption-method\" from \"aes192-ctr\" to \"plaintext\" got failed"
  },
  {
    "bug_location": "CPU Quota Management",
    "severity": 2,
    "categories": [
      "Performance",
      "Config"
    ],
    "root_cause": "Incorrect CPU time tracking mechanism causing cross-contamination between foreground and background request CPU quotas",
    "issue_number": 13084,
    "title": "Avoid mutual contamination of cpu limitation of foreground and background cpu quota"
  },
  {
    "bug_location": "metrics/monitoring",
    "severity": 2,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Metrics collection or reporting failure for unified read pool CPU usage chart",
    "issue_number": 13086,
    "title": "metrics: the unified read pool cpu usage chart in tikv-details usage is always empty"
  },
  {
    "bug_location": "TiKV Memory Management",
    "severity": 4,
    "categories": [
      "Performance",
      "Config",
      "Storage"
    ],
    "root_cause": "Memory usage limit configuration not effectively constraining actual memory consumption, leading to potential OOM scenarios",
    "issue_number": 13090,
    "title": "Redhat 8.4: tikv restart by systemd after set memory-usage-limit less than vm message"
  },
  {
    "bug_location": "Transaction Processing Engine",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Performance regression introduced by code changes in transaction update index handling, causing measurable throughput reduction in sysbench workloads",
    "issue_number": 13095,
    "title": "commit b558d0 causing 7%~14% performance degradation for sysbench oltp_update_index workload"
  },
  {
    "bug_location": "unified read pool metrics",
    "severity": 3,
    "categories": [
      "Upgrade",
      "Performance"
    ],
    "root_cause": "Metrics reporting failure during version upgrade from v5.4.0 to v6.2.0, potentially related to changes in metrics collection or instrumentation",
    "issue_number": 13096,
    "title": "upgrade to master\uff08v6.2.0\uff09\uff0cthe metrics of unified read pool cpu does not display properly"
  },
  {
    "bug_location": "log backup storage module",
    "severity": 4,
    "categories": [
      "Storage",
      "Config",
      "Network"
    ],
    "root_cause": "Potential incompatibility or configuration issue with Google Cloud Storage (GCS) external storage integration during log backup process",
    "issue_number": 13106,
    "title": "log backup failed when use GCS as external storage"
  },
  {
    "bug_location": "Stale Read Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "Performance"
    ],
    "root_cause": "Safe timestamp (safe_ts) is consistently zero, preventing stale read operations from progressing in multi-region distributed setup",
    "issue_number": 13110,
    "title": "Stale read meet `Data not ready` for a long time"
  },
  {
    "bug_location": "pd-client",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Region split mechanism fails to distribute load effectively for small tables with limited scan operations",
    "issue_number": 13111,
    "title": "Load base split don't work in scenario of pure little table scan"
  },
  {
    "bug_location": "proc.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Failure to handle non-existent proc filesystem file gracefully, causing repeated log warnings",
    "issue_number": 13116,
    "title": "proc file read failure flooding logs"
  },
  {
    "bug_location": "raft-engine recovery module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Potential race condition or memory management issue during log recovery process",
    "issue_number": 13123,
    "title": "raft engine panic during recovery"
  },
  {
    "bug_location": "backup-restore component",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Read index failure during network partition, preventing log backup checkpoint progression due to region merge conflicts",
    "issue_number": 13126,
    "title": "br log backup checkpoint doesn't move forward due to\"read index not ready, reason can not read index due to merge, region 94812\""
  },
  {
    "bug_location": "TiKV CDC (Change Data Capture)",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Resolved timestamp advancement failure during RawKV replication, causing CDC changefeed to stall",
    "issue_number": 13144,
    "title": "RawKV CDC does not advance in some scenario"
  },
  {
    "bug_location": "RocksDB Compaction Configuration",
    "severity": 3,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Static configuration parameter preventing runtime adjustment of compaction parallelism",
    "issue_number": 13145,
    "title": "Make max_subcompactions dynamically changeable"
  },
  {
    "bug_location": "CDC (Change Data Capture) Endpoint",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect instance label expression in CPU metrics calculation",
    "issue_number": 13147,
    "title": "cdc endpoint CPU expression is not correct"
  },
  {
    "bug_location": "Apply-db mutex lock",
    "severity": 3,
    "categories": [
      "Upgrade",
      "Performance"
    ],
    "root_cause": "Potential lock contention or inefficient mutex handling during version upgrade from 5.3 to 6.2",
    "issue_number": 13153,
    "title": "Apply-db_mutex_lock_nanos up to 5min for 10min during upgrade from 5.3 to 6.2"
  },
  {
    "bug_location": "Region Split Component",
    "severity": 3,
    "categories": [
      "Performance",
      "LoadBalance",
      "CodeBug"
    ],
    "root_cause": "Overly aggressive region splitting mechanism during low-volume table scan operations causing unnecessary region fragmentation",
    "issue_number": 13154,
    "title": "Load split: split too fast during do little table scan"
  },
  {
    "bug_location": "raftstore/store/fsm",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Potential silent loss of ApplyRes messages during region split operations, causing inconsistent region state and potential system panic during data cleanup",
    "issue_number": 13160,
    "title": "raftstore: apply res may be dropped silently"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Race condition during leader transfer that can cause inconsistent transaction execution across different Raft terms",
    "issue_number": 1317,
    "title": "Make sure different stages of a kv command execute raft command in a same term."
  },
  {
    "bug_location": "engine_test and server components",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Double locking pattern causing potential deadlock or performance bottleneck in mutex synchronization",
    "issue_number": 13186,
    "title": "DoubleLock in components/engine_test and server"
  },
  {
    "bug_location": "Network/Raft Consensus",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Network isolation disrupts Raft consensus state synchronization, preventing automatic leader transfer and node recovery after network partition healing",
    "issue_number": 13191,
    "title": "Inject a minority of tikv network isolation\uff0cafter network isolation was recovered, one tikv status still show down and can not transfer leader back"
  },
  {
    "bug_location": "TiKV Transaction Layer",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Potential race condition or incorrect handling of min resolved timestamp during cluster switchover in DR auto-sync mode",
    "issue_number": 13194,
    "title": "v6.2.0: run in dr-auto-sync mode, tikv will down after reset min resovled ts"
  },
  {
    "bug_location": "Transaction Recovery System",
    "severity": 4,
    "categories": [
      "Transaction",
      "Upgrade",
      "Storage"
    ],
    "root_cause": "Inconsistent transaction state resolution during disaster recovery and version reset, causing temporary data inconsistency in distributed transactions",
    "issue_number": 13203,
    "title": "v6.2.0 dr auto sync: data is not consistent after switch to backup dc and reset version to min resolved ts"
  },
  {
    "bug_location": "Transaction Recovery/Reset Version",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance"
    ],
    "root_cause": "Potential performance bottleneck in version reset process causing excessive processing time for small data sets",
    "issue_number": 13208,
    "title": "v6.2.0 dr auto sync: reset-to-version didn't finished after 20min with only 7G data in it"
  },
  {
    "bug_location": "Tablet Registry",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Recursive lock acquisition in tablet registry without proper lock ordering or prevention mechanism",
    "issue_number": 13213,
    "title": "`open_tablet` may cause deadlock"
  },
  {
    "bug_location": "pd_client",
    "severity": 5,
    "categories": [
      "Network",
      "Transaction",
      "Config"
    ],
    "root_cause": "PD connection failure causing TiKV cluster-wide panic when PD is down for an extended period",
    "issue_number": 13240,
    "title": "all tikv panic when pd down for some time"
  },
  {
    "bug_location": "network/connection_manager",
    "severity": 3,
    "categories": [
      "Network",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incomplete handling of asynchronous socket connection state, leading to potential message buffer overflow during connection establishment",
    "issue_number": 1325,
    "title": "server: drop message when socket is still connecting "
  },
  {
    "bug_location": "PiTR (Point-in-Time Recovery) Log Backup Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Checkpoint tracking mechanism failure preventing log backup progress tracking",
    "issue_number": 13251,
    "title": "PiTR log backup checkpoint doesn't move foward"
  },
  {
    "bug_location": "QuotaLimiter",
    "severity": 4,
    "categories": [
      "Performance",
      "Config"
    ],
    "root_cause": "Coprocessor read traffic not properly controlled by foreground quota limiter's read bytes configuration",
    "issue_number": 13256,
    "title": "QuotaLimiter: coprocessor read traffic should be controlled by foreground quota limiter's read bytes"
  },
  {
    "bug_location": "Metrics/Monitoring Panel",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect metric description text in GC compaction filter panel",
    "issue_number": 13274,
    "title": "metric: description is incorrect in GC compaction filter panel"
  },
  {
    "bug_location": "storage readpool",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Performance regression in storage read concurrency handling between TiKV versions 6.1.0 and 6.2.0",
    "issue_number": 13280,
    "title": "Storage pool regression on AWS EC2 instance type i3.4xlarge"
  },
  {
    "bug_location": "Point-in-Time Recovery (PiTR) Restore Component",
    "severity": 5,
    "categories": [
      "Storage",
      "Transaction",
      "Upgrade"
    ],
    "root_cause": "Data synchronization failure during point-in-time recovery process, potentially caused by checkpoint tracking or log replay inconsistencies",
    "issue_number": 13281,
    "title": "data inconsistency after br PiTR restore "
  },
  {
    "bug_location": "Log Backup Checkpoint",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Storage"
    ],
    "root_cause": "Checkpoint tracking mechanism fails to handle long-duration transactions efficiently, causing abnormal lag in log backup progress",
    "issue_number": 13304,
    "title": "Br log backup checkpoint lag abnormal when there is long duration transaction"
  },
  {
    "bug_location": "log backup component",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Excessive log backup generation during large data preparation, causing disproportionate log backup size compared to actual data size",
    "issue_number": 13306,
    "title": "Lots of log backup generated during data preparation"
  },
  {
    "bug_location": "resolved_ts/advance.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network"
    ],
    "root_cause": "Incorrect address selection when sending check leader message to TiFlash proxy, using store.address instead of store.peer_address",
    "issue_number": 13310,
    "title": "The address of the check leader message sent to the TiFlash proxy is wrong"
  },
  {
    "bug_location": "raftstore/store/fsm/store.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Metadata corruption during region peer creation, causing inability to locate region metadata during Raft message processing",
    "issue_number": 13311,
    "title": "TiKV panicked due to corrupted meta"
  },
  {
    "bug_location": "RocksDB Compaction Component",
    "severity": 2,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Concurrent compaction scheduling limitation in RocksDB preventing parallel auto and manual compaction operations",
    "issue_number": 13338,
    "title": "Manual compaction triggers flow control/write stall"
  },
  {
    "bug_location": "RocksDB storage layer",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Manual compaction process prevents timely database closure during TiKV shutdown, causing extended reload times due to blocking references to the database object",
    "issue_number": 13339,
    "title": "Manual compaction blocks tikv shutdown"
  },
  {
    "bug_location": "resolved_ts component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "tracked_index not being updated for IngestSST and 1PC command types, causing inaccurate data synchronization progress tracking",
    "issue_number": 13353,
    "title": "resolved_ts:  IngestSST do not update the tracked_index"
  },
  {
    "bug_location": "Region Leader Election",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Intermittent leadership transfer causing temporary QPS drop and retry attempts in distributed key-value regions",
    "issue_number": 1336,
    "title": "test: QPS fails to 0 in block writer test "
  },
  {
    "bug_location": "Read Index Request Queue",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Network isolation causing read index requests to become stuck in queue, leading to significant performance degradation after network recovery",
    "issue_number": 13379,
    "title": "inject one tikv network isolated for 50m\uff0cafter recover fault\uff0cqps drop 80%+ last for few minutes due to the read index request is stuck in the queue for a long time"
  },
  {
    "bug_location": "Performance/Execution Path",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Performance regression introduced by code changes in commit 68f99, causing measurable sysbench OLTP read-write workload slowdown",
    "issue_number": 13394,
    "title": "commit 68f99 causing sysbench oltp_read_write performance degradation 4.4%- 5.3%"
  },
  {
    "bug_location": "tikv-server configuration module",
    "severity": 3,
    "categories": [
      "Config",
      "CodeBug"
    ],
    "root_cause": "Invalid address configuration handling when advertise-addr is set to an empty string, causing immediate program termination instead of falling back to default address",
    "issue_number": 1340,
    "title": "advertise-addr = \"\" causes program exit"
  },
  {
    "bug_location": "Transaction Conflict Resolution",
    "severity": 3,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Incorrect timestamp comparison logic in pessimistic lock conflict detection causing false positive error when rollback records exist on non-unique index keys",
    "issue_number": 13425,
    "title": "False positive PessimisticLockNotFound when a non-pessimistic key has newer rollbacks"
  },
  {
    "bug_location": "tikv-ctl/raft_log_printing",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Unexpected wire type error during Raft log parsing, causing panic when attempting to print log entries",
    "issue_number": 1343,
    "title": "tikv-ctl: panic when print raft log"
  },
  {
    "bug_location": "CDC (Change Data Capture) Endpoint",
    "severity": 4,
    "categories": [
      "Transaction",
      "Network",
      "Performance"
    ],
    "root_cause": "Incorrect TSO cache management causing request failures when PD leader is killed, triggered by periodic flush of causal timestamp provider",
    "issue_number": 13430,
    "title": "The QPS almost dropped to 0 when leader of PD is killed"
  },
  {
    "bug_location": "Snapshot Transfer Component",
    "severity": 4,
    "categories": [
      "Network",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Concurrent snapshot generation for multiple peers causing multiple SST file transfers to TiFlash",
    "issue_number": 13445,
    "title": "TiKV send multi SST to TiFlash"
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Unexpected region splitting behavior when inserting large records with Titan enabled, causing unnecessary empty region generation during data insertion",
    "issue_number": 13446,
    "title": "titan: unexpected increasing empty regions when inserting large records"
  },
  {
    "bug_location": "MVCC Transaction Layer",
    "severity": 5,
    "categories": [
      "Transaction",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Inconsistent MVCC state after flashback operation with reduced GC lifetime, causing key version conflicts and transaction aborts",
    "issue_number": 13448,
    "title": "change gc_life_time from 12h to 10 minutes after flashback, report [errors.rs:409] [\"txn aborts\"] [err_code=KV:Storage:DefaultNotFound]"
  },
  {
    "bug_location": "Type Casting Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Inconsistent float/decimal to datetime type casting implementation compared to MySQL/TiDB specification",
    "issue_number": 13458,
    "title": "Cast float/decimal as time behavior inconsistent with TiDB/MySQL"
  },
  {
    "bug_location": "KvEngineFactory",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Deadlock during concurrent tablet open/creation due to improper lock synchronization in root database access",
    "issue_number": 13463,
    "title": "deadlock on root_db lock by open_tablet() for tablet factory"
  },
  {
    "bug_location": "RocksDB Configuration Module",
    "severity": 4,
    "categories": [
      "Config",
      "Performance",
      "Storage"
    ],
    "root_cause": "Inflexible RocksDB write-stall configuration preventing optimal compaction triggering",
    "issue_number": 13467,
    "title": "Support configuring RocksDB write-stall settings smaller than flow control"
  },
  {
    "bug_location": "PD Client/gRPC Interface",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Network"
    ],
    "root_cause": "Incompatible gRPC method between PD v6.2.0 and TiKV v6.3.0 during snapshot recovery check",
    "issue_number": 13497,
    "title": "br: tikv startup failure when tikv v6.3.0 connect to pd v6.2.0"
  },
  {
    "bug_location": "RawKV API V2",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Timestamp causality violation causing inconsistent read/write operations in RawKV API V2 concurrent workloads",
    "issue_number": 13502,
    "title": "RawKV API V2 timestamp causality violation"
  },
  {
    "bug_location": "Regexp Function Processing",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Improper handling of empty regular expression pattern, potentially causing unexpected behavior in query processing",
    "issue_number": 13513,
    "title": "regexp function using empty pattern should report error"
  },
  {
    "bug_location": "TiKV Store/IO Scheduler",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Network"
    ],
    "root_cause": "Slow recovery mechanism during IO hang scenarios, causing extended QPS degradation beyond expected recovery time",
    "issue_number": 13524,
    "title": "qps recovery after 15min when one tikv io hang"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Log Backup Module",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "Performance"
    ],
    "root_cause": "GC safepoint timestamp conflict preventing log backup task pause operation",
    "issue_number": 13532,
    "title": "PITR: failed to pause a log-backup task with the error  BR:Backup:ErrBackupGCSafepointExceeded"
  },
  {
    "bug_location": "Region Manager",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Upgrade"
    ],
    "root_cause": "Inefficient stale range deletion mechanism during instance reboot, causing significant performance overhead when handling large numbers of stale regions",
    "issue_number": 13534,
    "title": "Slow stale range deletion at TiKV instance reboots"
  },
  {
    "bug_location": "Compaction Filter",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Expression parsing failure in garbage collection compaction filter mechanism",
    "issue_number": 13537,
    "title": "The expr for `GC in Compaction Filter` meets parse error"
  },
  {
    "bug_location": "RawKV Transaction Layer",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Race condition in atomic compare-and-swap operation causing inconsistent state updates during concurrent modifications",
    "issue_number": 13550,
    "title": "rawkv: causality violation for atomic commands"
  },
  {
    "bug_location": "tikv_util/lru.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Unsafe memory manipulation in LruCache implementation causing undefined behavior during Rust borrow checking, specifically in reuse_tail and clear methods",
    "issue_number": 13551,
    "title": "LruCache is unsound"
  },
  {
    "bug_location": "Snapshot/Region Management",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Race condition between snapshot operation and region configuration change leading to potential data incompleteness during concurrent read operations",
    "issue_number": 13553,
    "title": "Batch snapshot is unsound"
  },
  {
    "bug_location": "raftstore/log_unstable",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Out-of-bounds slice access in Raft log unstable storage, causing index range violation during log truncation and appending",
    "issue_number": 13554,
    "title": "TiKV panic repeatedly: [FATAL] [lib.rs:465] [\"unstable.slice[22, 23] out of bound[22, 22], raft_id: 115835, region_id: 115832\"]"
  },
  {
    "bug_location": "PD Client/Version Compatibility",
    "severity": 3,
    "categories": [
      "Upgrade",
      "Config"
    ],
    "root_cause": "Version compatibility mismatch between TiKV and PD client during version upgrade/migration",
    "issue_number": 13562,
    "title": "when `tikv:6.3.0` checkout to `tikv:master` in k8s, k8s pod cannot start and `crashloopbackoff`"
  },
  {
    "bug_location": "tidb_query_datatype/expr/ctx.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Inconsistent SQL mode constant definitions between TiKV and TiDB components, causing potential runtime mismatches in constant values",
    "issue_number": 13566,
    "title": "Sql mode constants are not consistent with TiDB"
  },
  {
    "bug_location": "RawKV storage layer",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Race condition during atomic compare-and-swap operation under node failure scenario, causing inconsistent state preservation during concurrent writes and node crash",
    "issue_number": 13573,
    "title": "rawkv: correctness violation for raw atomic commands"
  },
  {
    "bug_location": "RawKV Test Suite",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Region merge operation failure during test execution",
    "issue_number": 13582,
    "title": "test: cases::test_rawkv::test_region_merge is unstable"
  },
  {
    "bug_location": "TiKV Configuration Initialization",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config",
      "Performance"
    ],
    "root_cause": "Incorrect handling of background job configuration when CPU resources are limited, causing startup failure when allocated less than one full CPU core",
    "issue_number": 13586,
    "title": "TiKV start failed with less than 1 core CPU on Kubernetes"
  },
  {
    "bug_location": "RawKV Storage Test Component",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Incorrect test logic in raw key guard unit test causing unexpected value retrieval and test instability",
    "issue_number": 13599,
    "title": "rawkv: unit test \"test_storage::test_raw_put_key_guard\" is not correct and unstable"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Checkpoint Mechanism",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Upgrade"
    ],
    "root_cause": "Potential synchronization or state recovery issue during TiKV pod restart causing significant checkpoint lag",
    "issue_number": 13616,
    "title": "PITR checkpoint lag increases to ~2H after restarting TiKVs"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) component",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "Upgrade"
    ],
    "root_cause": "Potential synchronization or checkpoint tracking issue during TiDB pod restart causing extended recovery lag",
    "issue_number": 13617,
    "title": "PITR lag > 5min after TiDB failure chaos for 10 minutes for 1 of the 3 TiDB pod"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) backup component",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Conflict between move-to-better-location scheduler and PITR log backup tasks causing checkpoint lag during data redistribution",
    "issue_number": 13619,
    "title": "PITR lag > 5min when move-to-better-location scheduler running"
  },
  {
    "bug_location": "GC (Garbage Collection) module",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Incomplete storage reclamation after flashback operation, causing garbage data to persist despite GC lifecycle changes",
    "issue_number": 13620,
    "title": "after flashback, gc work normal but storage available size is not freed"
  },
  {
    "bug_location": "Log Backup/PITR Checkpoint",
    "severity": 3,
    "categories": [
      "Performance",
      "Network",
      "LoadBalance"
    ],
    "root_cause": "Checkpoint synchronization mechanism becomes unstable during single availability zone failure, causing extended lag in log backup recovery point tracking",
    "issue_number": 13622,
    "title": "PITR checkpoint lag might >5min during/right after single AZ disaster happens"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Checkpoint Mechanism",
    "severity": 3,
    "categories": [
      "Network",
      "Performance",
      "Storage"
    ],
    "root_cause": "Inefficient network recovery handling in log backup checkpoint synchronization, causing extended lag after S3 network partition",
    "issue_number": 13623,
    "title": "PITR checkpoint lag might be > 5 min for ~10 min after S3 network failure recovered"
  },
  {
    "bug_location": "TiKV Heartbeat Service",
    "severity": 4,
    "categories": [
      "Performance",
      "Config"
    ],
    "root_cause": "Delayed initial heartbeat interval prevents immediate PD scheduling after service readiness",
    "issue_number": 13627,
    "title": "Send heartbeat to PD as soon as the service is ready"
  },
  {
    "bug_location": "storage/txn/scheduler",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Failed to send write completion notification to scheduler, likely due to channel closure during shutdown process",
    "issue_number": 1363,
    "title": "Panic when stop"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Checkpoint Component",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Network partition disrupts checkpoint advancement mechanism, preventing proper synchronization between TiKV and TiDB during distributed recovery process",
    "issue_number": 13632,
    "title": "PITR checkpoint not move forward during tikv and tidb advance owner network partition"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Checkpoint Component",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction"
    ],
    "root_cause": "Network partition between TiKV node and PD leader disrupts checkpoint progress tracking mechanism, preventing normal log backup checkpoint advancement",
    "issue_number": 13637,
    "title": "PITR checkpoint doesn't move forward during one TiKV and PD leader network partition"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Checkpoint Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Upgrade",
      "Transaction"
    ],
    "root_cause": "Potential synchronization or state transfer inefficiency during rolling restart causing extended checkpoint lag",
    "issue_number": 13638,
    "title": "PITR checkpoint lag > 10min during tikv rolling restart"
  },
  {
    "bug_location": "DDL (Data Definition Language) Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Upgrade"
    ],
    "root_cause": "Race condition during PD node restart causing region epoch inconsistency during flashback operation",
    "issue_number": 13643,
    "title": "flashback ddl will hang when restart all pd nodes during flashback command"
  },
  {
    "bug_location": "TiKV codec/type handling",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incomplete implementation of geometric data type support in TiKV's type codec system",
    "issue_number": 13651,
    "title": "\"Unsupported FieldType\" for FieldTypeTp::Geometry"
  },
  {
    "bug_location": "TiKV cgroup detection module",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Assertion failure in cgroup parsing logic, likely due to unexpected cgroup configuration in Gitpod environment",
    "issue_number": 13660,
    "title": "tiup playground fails with a tikv error during execution in the Gitpod environment"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Transaction"
    ],
    "root_cause": "Corrupted Raft message processing causing unexpected panic and inability to recover",
    "issue_number": 13668,
    "title": "tikv panic due to corrupted raft message"
  },
  {
    "bug_location": "raftstore/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Bucket statistics reset interval conflicts with split check interval, preventing effective bucket re-splitting",
    "issue_number": 13671,
    "title": "Bucket re-split hardly works"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "Upgrade"
    ],
    "root_cause": "Data synchronization failure during flashback operation, causing index and record value inconsistency when concurrent restart or stop/start operations occur",
    "issue_number": 13672,
    "title": "Data inconsistency after flashback "
  },
  {
    "bug_location": "TiFlash Integration",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Network"
    ],
    "root_cause": "Performance degradation when a TiFlash node becomes unavailable, causing increased process ready duration during region processing",
    "issue_number": 13676,
    "title": "Process ready duration is high when one tiflash is down"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Backup System",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Potential backup synchronization or resource allocation issue causing progressive lag in backup processes in distributed cloud environment",
    "issue_number": 13688,
    "title": "pitr backup lag continuously increase in dbaas gcp environment"
  },
  {
    "bug_location": "Transaction Processing",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "Performance regression introduced by code changes in commit a4dc37b, causing reduced query processing speed across multiple workload types",
    "issue_number": 13692,
    "title": "commit a4dc37b causes 2%~4% performance regression in different workloads"
  },
  {
    "bug_location": "Flashback/Compaction Process",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Excessive pending compaction blocking flashback recovery process, causing data synchronization delay during point-in-time recovery",
    "issue_number": 13704,
    "title": "Flashback process is blocked for compaction pending too much data"
  },
  {
    "bug_location": "Coprocessor Plugin Test Framework",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incorrect library name reference in test configuration causing CI pipeline failure",
    "issue_number": 13708,
    "title": "Incorrect library name in coprocessor_v2 plugin tests"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) log backup component",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Resource contention and performance interference between BR restore process and log backup checkpoint mechanism",
    "issue_number": 13714,
    "title": "PITR lag increase during br restore"
  },
  {
    "bug_location": "resolved_ts component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Assertion failure during change log encoding in resolved timestamp tracking, likely due to unexpected state during flashback operation",
    "issue_number": 13723,
    "title": "flashback during workload, tikv panic lead to tidb down"
  },
  {
    "bug_location": "Snapshot Transfer Mechanism",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Improper state management during snapshot generation when channel capacity is exceeded",
    "issue_number": 1373,
    "title": "Discard scan snapshot result may lead to invalid peer state"
  },
  {
    "bug_location": "Store Thread / gRPC Channel",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Channel message backlog due to I/O hang preventing store thread from consuming messages, causing gRPC message rejection and potential memory exhaustion",
    "issue_number": 13731,
    "title": "tikv oom after this tikv io delay/hang or one of tikv network partition last for 50mins and recover"
  },
  {
    "bug_location": "DDL (Data Definition Language) Flashback Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Incorrect handling of keys with only delete type MVCC records during flashback operation, causing incomplete key processing and potential deadlock",
    "issue_number": 13743,
    "title": "flashback ddl is hang when some key's last mvcc record is delete"
  },
  {
    "bug_location": "config.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incorrect validation logic for max_background_jobs when CPU quota is less than 1",
    "issue_number": 13752,
    "title": "validate max_background_jobs will fail when cpu quota is less than 1"
  },
  {
    "bug_location": "Raft consensus/network communication layer",
    "severity": 4,
    "categories": [
      "Network",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Network partition causing TiKV node isolation prevents consensus and service recovery within expected timeframe",
    "issue_number": 13758,
    "title": "one tikv is isolated from others tikv, and services are unavailable during a failure"
  },
  {
    "bug_location": "raftstore/read_worker",
    "severity": 5,
    "categories": [
      "Network",
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unexpected None value during local read request validation, causing unwrap() panic when network fault is injected",
    "issue_number": 13764,
    "title": "tikv panic when inject network fault"
  },
  {
    "bug_location": "TiKV String Comparison/Like Function",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Charset decoding failure when new collation is disabled, causing incorrect string pattern matching in like comparisons",
    "issue_number": 13769,
    "title": "When the new collation is not enabled, `_` pattern in like misbehaved"
  },
  {
    "bug_location": "Configuration Management",
    "severity": 3,
    "categories": [
      "Upgrade",
      "Config"
    ],
    "root_cause": "Incompatible configuration parameter during version upgrade, where a deprecated or removed config option prevents successful migration",
    "issue_number": 13772,
    "title": "v6.3.0 upgrade to v6.4.0 will fail when has causal-ts.available-interval config in v6.3.0"
  },
  {
    "bug_location": "raftstore/read_worker",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Upgrade",
      "Transaction"
    ],
    "root_cause": "Unexpected None value unwrap in LocalReaderCore::validate_request during read operations after version upgrade",
    "issue_number": 13812,
    "title": "tikv fatal error after upgrading when running ch and cc workload"
  },
  {
    "bug_location": "TiKV Reset-to-Version Component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Upgrade"
    ],
    "root_cause": "Potential memory management or null pointer dereference during version reset operation",
    "issue_number": 13829,
    "title": "reset-to-version segfault"
  },
  {
    "bug_location": "Backup and Restore Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Max resolved timestamp mechanism fails to advance during data restoration process, potentially causing synchronization and tracking issues",
    "issue_number": 13838,
    "title": "TiKV Max Resolved TS stop to advance when restore / import data into tikv"
  },
  {
    "bug_location": "storage/mvcc/reader",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Assertion mismatch between expected Delete and actual Put operation during MVCC read, likely caused by inconsistent transaction state management",
    "issue_number": 13839,
    "title": "tikv restart for \"[\"assertion failed: `(left == right)`\\n  left: `Delete`,\\n right: `Put`\"] \""
  },
  {
    "bug_location": "storage/mvcc/reader",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Incorrect handling of write record scanning during flashback operation, potentially returning incorrect write records when encountering lock or rollback write types",
    "issue_number": 13844,
    "title": "Flashback may overwrite wrong write record"
  },
  {
    "bug_location": "Network Configuration",
    "severity": 4,
    "categories": [
      "Network",
      "Config"
    ],
    "root_cause": "TiKV not properly configuring IPv6 network binding, causing connection failures during restore operations",
    "issue_number": 13847,
    "title": "IPv6 support: tikv should listen on IPv6 instead of IPv4\uff0c which cause br restore failed."
  },
  {
    "bug_location": "RaftEngine log_batch",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Buffer size limitation causing panic when log batch exceeds 2GB threshold",
    "issue_number": 13848,
    "title": "TiKV panic because the buffer size in log_batch is more than 2G when append to RaftEngine."
  },
  {
    "bug_location": "Flashback Component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Improper key validation during prewrite and commit operations in Flashback, allowing potentially illegal keys to be processed",
    "issue_number": 13861,
    "title": "Flashback may prewrite and commit an illegal key"
  },
  {
    "bug_location": "import/sst_service.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "No Tokio runtime context available when attempting to execute an asynchronous operation",
    "issue_number": 13862,
    "title": "TiKV boost failed."
  },
  {
    "bug_location": "import/sst_service.rs",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Tokio runtime context not properly initialized, causing async runtime failure during SST import service operation",
    "issue_number": 13866,
    "title": "[FATAL] [lib.rs:495] [\"there is no reactor running, must be called from the context of a Tokio 1.x runtime\"]"
  },
  {
    "bug_location": "Flashback/Region Management",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "Upgrade"
    ],
    "root_cause": "Incomplete region synchronization during cluster flashback operation, causing inconsistent region states across tables",
    "issue_number": 13868,
    "title": "after flashback, admin check table report \" region 20009 is in flashback progress\""
  },
  {
    "bug_location": "PeerFSM",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Race condition between raft admin command execution and PeerFSM state update causing delayed flashback state visibility",
    "issue_number": 13870,
    "title": "Flashback state may not be visible immediately in PeerFSM"
  },
  {
    "bug_location": "sst_importer",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Attempting to start a runtime from within an existing runtime, causing a deadlock in async task execution during Point-in-Time Recovery (PITR)",
    "issue_number": 13874,
    "title": "sst_importer: TiKV panics when doing PITR"
  },
  {
    "bug_location": "storage/txn/commands/mod.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of empty end_key during flashback operation, causing incomplete key restoration",
    "issue_number": 13879,
    "title": "Flashback may miss the last region"
  },
  {
    "bug_location": "sst_importer",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Nested runtime creation during asynchronous task execution, preventing proper blocking of current thread",
    "issue_number": 13883,
    "title": "tikv panic during pitr restore "
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) checkpoint mechanism",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "Storage"
    ],
    "root_cause": "TiKV fails to advance PITR checkpoint when unable to establish connection with PD (Placement Driver) during network disruption or AZ failure",
    "issue_number": 13898,
    "title": "PITR checkpoint not advance if tikv fails to connect to PD at start"
  },
  {
    "bug_location": "Configuration Management",
    "severity": 4,
    "categories": [
      "Config",
      "Upgrade"
    ],
    "root_cause": "Deprecated configuration option 'cdc.raw-min-ts-outlier-threshold' not handled during version upgrade process",
    "issue_number": 13906,
    "title": "v6.4.0 upgrade to v6.5.0 fail when has config \"cdc.raw-min-ts-outlier-threshold\" in old version"
  },
  {
    "bug_location": "coprocessor",
    "severity": 2,
    "categories": [
      "Performance",
      "Config"
    ],
    "root_cause": "Potential mismatch between alert name and actual performance metric description",
    "issue_number": 13918,
    "title": "coprocessor cpu alert is wrong"
  },
  {
    "bug_location": "RaftKV",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unhandled error in async snapshot generation causing unexpected Result::unwrap() panic",
    "issue_number": 13926,
    "title": "called `Result::unwrap()` on an `Err` value: Canceled"
  },
  {
    "bug_location": "tikv-ctl CLI tool",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Unnecessary log directory creation before command validation, causing potential permission errors",
    "issue_number": 13927,
    "title": "`tikv-ctl` creates log directory regardless of command"
  },
  {
    "bug_location": "coprocessor_plugin_api",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Unnecessary recompilation of coprocessor_plugin_api package during each 'make dev' invocation, causing redundant build steps",
    "issue_number": 13950,
    "title": "coprocessor_plugin_api get rebuild every time running `make dev`"
  },
  {
    "bug_location": "backup-restore component",
    "severity": 4,
    "categories": [
      "Network",
      "Storage",
      "Performance"
    ],
    "root_cause": "Network interruption causing restore process to hang when network is unstable for more than 5 seconds during large cluster backup/restore operation",
    "issue_number": 13955,
    "title": "br: ebs restore could not success when network down more than 5 seconds"
  },
  {
    "bug_location": "Point-in-Time Recovery (PITR) component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of log backup and flashback operations during point-in-time recovery, causing data synchronization inconsistencies",
    "issue_number": 13958,
    "title": "PITR: data inconsistency after PITR with log containing flashback data. "
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) log backup component",
    "severity": 4,
    "categories": [
      "Security",
      "Config",
      "Network"
    ],
    "root_cause": "SSL certificate verification failure during TLS connection establishment, likely due to incorrect certificate chain or misconfigured CA trust",
    "issue_number": 13959,
    "title": "log-backup: log backup cannot work in some conditions"
  },
  {
    "bug_location": "raftstore-v2/router/response_channel.rs",
    "severity": 3,
    "categories": [
      "CodeBug"
    ],
    "root_cause": "Undefined type 'FlushChannel' causing compilation error, likely a type definition or import issue",
    "issue_number": 13968,
    "title": "raftstore-v2: exists compilation errors"
  },
  {
    "bug_location": "etcd client/network communication layer",
    "severity": 3,
    "categories": [
      "Network",
      "CodeBug"
    ],
    "root_cause": "Lack of data integrity verification mechanism during gRPC communication, making the system vulnerable to random byte corruption without detection",
    "issue_number": 1397,
    "title": "server: can not pass etcd local test with send/receive corruption enabled"
  },
  {
    "bug_location": "raft",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Raft ready processing exceeded memory or processing limits, causing panic during point-in-time recovery (PITR) operations",
    "issue_number": 13977,
    "title": "pitr panic due to raft ready too large"
  },
  {
    "bug_location": "backup-stream/subscription_track.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Excessive logging from backup stream observer when refreshing region subscriptions, potentially indicating inefficient region tracking mechanism",
    "issue_number": 14012,
    "title": "Logging from backup-stream is noisy"
  },
  {
    "bug_location": "TiKV Operator Volume Snapshot Restore",
    "severity": 3,
    "categories": [
      "Config",
      "Storage",
      "Upgrade"
    ],
    "root_cause": "Configuration validation error in background job settings when CPU request is 1 or less, causing restore process to fail",
    "issue_number": 14017,
    "title": "tidb-operator volume-snapshot restore failure when tikv request cpu less or equal to 1"
  },
  {
    "bug_location": "memory management",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Potential memory leak or inefficient memory allocation during write-heavy workloads",
    "issue_number": 14033,
    "title": "TiKV OOM itself"
  },
  {
    "bug_location": "Transaction Layer",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Pessimistic lock constraint check mechanism fails to properly validate existing locks within the same transaction",
    "issue_number": 14038,
    "title": "txn: the left pessimistic lock may break the constraint check"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Security",
      "Config",
      "Network"
    ],
    "root_cause": "TLS certification validation failure during online reload, preventing log backup task registration",
    "issue_number": 14071,
    "title": "Log backup: failed to register task after online reload certification when TLS enable"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 3,
    "categories": [
      "Network",
      "Performance"
    ],
    "root_cause": "Network communication overhead increase in change data capture mechanism introduced in v6.5.0 pull requests",
    "issue_number": 14092,
    "title": "Inter-TiKV idle network traffic increases about 100% since v6.5.0"
  },
  {
    "bug_location": "backup-restore endpoint",
    "severity": 3,
    "categories": [
      "Network",
      "Storage"
    ],
    "root_cause": "HTTP 500 error during azblob upload with timeout, indicating potential network or storage service instability",
    "issue_number": 14093,
    "title": "When azblob is used, retry for http code 500 error"
  },
  {
    "bug_location": "Backup and Restore (PiTR) Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Incorrect leadership verification when advancing resolved timestamp, potentially causing incorrect checkpoint and potential data loss",
    "issue_number": 14099,
    "title": "PiTR resolved ts is not reliable and may lost data in extreme cases"
  },
  {
    "bug_location": "coprocessor/endpoint.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect tracking and aggregation of scan details during coprocessor task processing, causing incomplete key processing metrics",
    "issue_number": 14109,
    "title": "cop: some scan details batch are mistakenly dropped"
  },
  {
    "bug_location": "engine_traits/flush.rs",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Missing default column family during RocksDB flush operation in dynamic region management",
    "issue_number": 14113,
    "title": "[dynamic regions] TiKV panics with \"default not found\""
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "LoadBalance"
    ],
    "root_cause": "Memory allocation inefficiency during large-scale data import with dynamic region splitting, causing excessive memory consumption and out-of-memory condition",
    "issue_number": 14114,
    "title": "[Dynamic Regions] TiKV OOM in tpcc 25K import data "
  },
  {
    "bug_location": "Tablet GC",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Inefficient tablet garbage collection mechanism causing task accumulation and incorrect store size reporting",
    "issue_number": 14115,
    "title": "[Dynamic Regions] Tablet GC tasks are accumulated overtime and incorrect store size reported"
  },
  {
    "bug_location": "PD Leader Balancing Component",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Config",
      "Transaction"
    ],
    "root_cause": "Witness configuration disrupts standard leader distribution algorithm, causing uneven leader placement and preventing normal load balancing across TiKV stores",
    "issue_number": 14118,
    "title": "\u3010witness\u3011after enable witness\uff0ctikv can not balance leader"
  },
  {
    "bug_location": "Stale Read Component",
    "severity": 3,
    "categories": [
      "Transaction",
      "Performance"
    ],
    "root_cause": "Slow resolved timestamp (resolved-ts) processing causing read request timeouts in distributed transaction handling",
    "issue_number": 14122,
    "title": "Stale read request timeout when resolved-ts is slow"
  },
  {
    "bug_location": "Tablet/CacheTablet",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "Performance"
    ],
    "root_cause": "CacheTablet fails to remove old peer cache after peer destruction, causing snapshot application inconsistency",
    "issue_number": 14128,
    "title": "[Dynamic Regions] TiKV scale-in failed"
  },
  {
    "bug_location": "Witness Region Management",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Storage"
    ],
    "root_cause": "Performance degradation and region unavailability when witness mode is enabled during backup/restore operations",
    "issue_number": 14130,
    "title": "\u3010witness\u3011after enable witness\uff0cbr restore very slowly and failed with error [tikv:9005]Region is unavailable"
  },
  {
    "bug_location": "Region Split Mechanism",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Dynamic regions not effectively splitting hot write regions across nodes, causing uneven write distribution and potential performance bottlenecks",
    "issue_number": 14135,
    "title": "[dynamic regions] region is not split for hot write"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Network",
      "Performance"
    ],
    "root_cause": "Witness peer snapshot retrieval failure under high latency network conditions, causing PITR task to pause and fail to automatically recover",
    "issue_number": 14137,
    "title": "\u3010witness\u3011enable witness and pitr\uff0cinject one tikv io delay 200ms last for 5m\uff0cpitr task became to paused"
  },
  {
    "bug_location": "RawKV Integration Test",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential race condition or timing-related issue in test case implementation",
    "issue_number": 14141,
    "title": "rawkv: integration test case `test_raw_put_key_guard` is flaky"
  },
  {
    "bug_location": "raftstore/peer_fsm",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Hash verification failure during region peer processing, likely caused by concurrent region operations with witness nodes",
    "issue_number": 14142,
    "title": "\u3010witness\u3011enable witness and add randomly scheduler\uff0cafter run workload some time\uff0ctikv panic"
  },
  {
    "bug_location": "Flashback Region Handler",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect error handling in flashback region request processing, returning unexpected error type that breaks client retry logic",
    "issue_number": 14143,
    "title": "Flashback should not return any unexpected error to the client"
  },
  {
    "bug_location": "Region Transfer/Scheduling",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Leader and region transfer mechanisms fail when witness nodes are enabled, preventing successful TiKV node scale-in operation",
    "issue_number": 14144,
    "title": "\u3010witness\u3011enable witness\uff0cscale in one tikv can not successfully due to the leaders and regions can not transfer successfully"
  },
  {
    "bug_location": "Change Data Capture (CDC) component",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Network",
      "Performance"
    ],
    "root_cause": "Unstable integration test case for RawKV scan operation, indicating potential race condition or timing-related synchronization issue",
    "issue_number": 14146,
    "title": "cdc: integration test case test_rawkv_sacn is unstable"
  },
  {
    "bug_location": "tidb_query_expr/src/types/expr_builder.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unsupported handling of MySQL ENUM data type in expression evaluation",
    "issue_number": 14156,
    "title": "query got tikv error \"other error: [components/tidb_query_expr/src/types/expr_builder.rs:74]: Unsupported expression type MysqlEnum\""
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Task Management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Upgrade",
      "Transaction"
    ],
    "root_cause": "Task state persistence failure during cluster-wide component restart, causing PITR tasks to become unexpectedly disabled",
    "issue_number": 14159,
    "title": "after restart all tikv pods\u3001pd pods\u3001tidb pods\uff0cpitr task became to disabled"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) log-backup component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Upgrade",
      "Network"
    ],
    "root_cause": "Failure to dynamically detect and adapt to changes in the etcd cluster membership during scaling operations",
    "issue_number": 14165,
    "title": "log-backup: PITR cannot detect new members in the etcd cluster"
  },
  {
    "bug_location": "Region Load Balancer",
    "severity": 3,
    "categories": [
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Uneven data distribution during cluster scale-out, causing single TiKV node to handle disproportionate data movement",
    "issue_number": 14178,
    "title": "[Dynamic Regions] store is not balanced during the scale-out"
  },
  {
    "bug_location": "Store/Region Management",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Incomplete region size recalculation during scale-out operations, preventing automatic store size reduction when region count decreases",
    "issue_number": 14179,
    "title": "[Dynamic Regions] store size is not reduced during the scale-out when the region count is reduced"
  },
  {
    "bug_location": "log-backup component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Storage"
    ],
    "root_cause": "Unable to advance backup progress when PD (Placement Driver) is unavailable, indicating a resilience failure in distributed state management",
    "issue_number": 14184,
    "title": "log-backup: cannot advance the progress when PD loss."
  },
  {
    "bug_location": "resource_control",
    "severity": 4,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Resource group configuration not properly detecting or registering groups during write workloads",
    "issue_number": 14191,
    "title": "resource_control: resource group is not detected "
  },
  {
    "bug_location": "read_pool",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Divide by zero error in time slice calculation during TPC workload processing",
    "issue_number": 14200,
    "title": "tikv FATAL ERROR when running tpc "
  },
  {
    "bug_location": "read_pool",
    "severity": 5,
    "categories": [
      "Network",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Divide by zero error during time slice update in background task under network partition scenario",
    "issue_number": 14205,
    "title": "tikv panic when inject network-partition "
  },
  {
    "bug_location": "log-backup module",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Dashmap's segment locking mechanism restricts concurrent log backup downloads by limiting shard-level concurrency",
    "issue_number": 14206,
    "title": "log-backup: get rid of the dashmap locks"
  },
  {
    "bug_location": "raftstore-v2/ready module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Network"
    ],
    "root_cause": "Incorrect peer cache retrieval causing peer isolation when processing peer configuration changes",
    "issue_number": 14211,
    "title": "[dynamic regions] wrong peer cache can make a peer isolated forever"
  },
  {
    "bug_location": "PD (Placement Driver) Region Scheduler",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Region balance scheduling algorithm fails to handle moderate read-write traffic, preventing optimal region distribution across TiKV nodes",
    "issue_number": 14214,
    "title": "[Dynamic Regions] PD does not schedule region-balance when there's moderate read-write traffic "
  },
  {
    "bug_location": "Raft Consensus Module",
    "severity": 4,
    "categories": [
      "Transaction",
      "LoadBalance",
      "CodeBug"
    ],
    "root_cause": "Learner peer cannot be promoted to voter during dynamic region configuration change, causing region replication configuration to stall",
    "issue_number": 14215,
    "title": "[Dynamic Regions] Scale-in is blocked because one region peer cannot be converted from learner to voter"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Potential race condition during TiKV node restart causing temporary region unavailability and transaction disruption",
    "issue_number": 14217,
    "title": "\u3010witness\u3011restart one tikv every 20min\uff0ctpcc report 9005 at  certain fault"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "Network"
    ],
    "root_cause": "Potential failure handling weakness in PITR task management during TiKV node failure scenario, causing unexpected task pause under witness configuration",
    "issue_number": 14219,
    "title": "\u3010witness\u3011inject one tikv failure for 10m, pitr task became to paused"
  },
  {
    "bug_location": "Region Transfer/Snapshot Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "LoadBalance"
    ],
    "root_cause": "Bandwidth limitation mechanism not properly implemented during region snapshot transfer, causing potential network congestion during scale operations",
    "issue_number": 14221,
    "title": "[Dynamic Regions] the limit can not work in transfer snapshot  "
  },
  {
    "bug_location": "pprof module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Upgrade"
    ],
    "root_cause": "pprof module fails to close pipe file descriptors in short-term threads, causing gradual fd leaks during CPU profiling",
    "issue_number": 14224,
    "title": "pipe fd leaks in tikv-6.1.0 with cpu profiler running"
  },
  {
    "bug_location": "Raft Log Replication",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Network loss causing persistent Raft log synchronization delays and accumulating lag during repeated network fault injection scenarios",
    "issue_number": 14228,
    "title": "\u3010witness\u3011inject one tikv network loss for in 10min repeatly\uff0cfor certain inject and after recover fault\uff0craft log lag is more and more"
  },
  {
    "bug_location": "batch_system_mailbox",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Network"
    ],
    "root_cause": "Unsafe unwrap operation on potentially None mailbox during concurrent batch system shutdown",
    "issue_number": 14233,
    "title": "[dynamic regions] potential unwrap on None when getting mailbox"
  },
  {
    "bug_location": "Backup and Restore (PITR)",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "Performance"
    ],
    "root_cause": "PD leader network partition causing extended recovery point objective (RPO) delay during point-in-time recovery process",
    "issue_number": 14241,
    "title": "pitr rpo is more than 5m last for 9min after inject pdleader network partition which trigger pd changed leader"
  },
  {
    "bug_location": "coprocessor/paging",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Paging executor not correctly terminating requests when LIMIT is applied, causing unnecessary repeated region requests",
    "issue_number": 14254,
    "title": "copr: paging not stop with limit executor"
  },
  {
    "bug_location": "raftstore-v2/snapshot",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "SST file size mismatch between manifest record and actual file size during snapshot processing",
    "issue_number": 14256,
    "title": "[dynamic regions] snapshot corruption"
  },
  {
    "bug_location": "OpenSSL dependency",
    "severity": 4,
    "categories": [
      "Security",
      "Upgrade"
    ],
    "root_cause": "Outdated OpenSSL version with known vulnerability (CVE-2023-0286)",
    "issue_number": 14257,
    "title": "upgrade openssl version to fix CVE-2023-0286"
  },
  {
    "bug_location": "Coprocessor Tracker",
    "severity": 2,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Incorrect time tracking mechanism where suspended time is merged into process time instead of being reported separately",
    "issue_number": 14262,
    "title": "suspended-time should be passed through to TiDB to avoid confusion"
  },
  {
    "bug_location": "TiKV Region Management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Transaction"
    ],
    "root_cause": "Potential region epoch mismatch or context timeout during large-scale count query across distributed regions",
    "issue_number": 14284,
    "title": "[Dynamic Regions]: select count(*) returns \"context deadline exceeded \" error"
  },
  {
    "bug_location": "Backup and Restore (PITR) Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Runtime task dispatch mechanism failure during large-scale log data restoration from S3 storage, potentially related to task scheduling or cache handling",
    "issue_number": 14285,
    "title": "PITR: dispatch task is gone: runtime dropped the dispatch task"
  },
  {
    "bug_location": "coprocessor",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Paging request truncation in group by query causing incomplete result set",
    "issue_number": 14291,
    "title": "copr: paging request stop unexpectedly"
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Inefficient storage allocation in multi-RocksDB configuration causing increased disk space consumption compared to single-RocksDB setup",
    "issue_number": 14295,
    "title": "[Dynamic Regions] Multi-RocksDB's store size is 18% more than single-rocksdb"
  },
  {
    "bug_location": "MultiRocksDB Storage Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Unexpected storage size growth in MultiRocks DB implementation causing inefficient storage management during TPC-C workload",
    "issue_number": 14301,
    "title": "[Dynamic Regions]After using MultiRocks DB, the store size of TiKV surged unexpectedly, which is not in line with expectations"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Hardcoded timeout of 20s is insufficient for large data regions, causing premature request cancellation",
    "issue_number": 14303,
    "title": "[Dynamic Regions] coprocessor's hardcode timeout may not work when bucket is big"
  },
  {
    "bug_location": "RaftStore/ReadIndexQueue",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Resource leak in index read callback handling, causing panic during resource drop in multi-RocksDB context",
    "issue_number": 14306,
    "title": "[Dynamic Regions] Multi-RocksDB's tikv panic with resource leak detected: callback of index read"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Potential race condition or inconsistent state during dynamic region partitioning in distributed transaction preparation",
    "issue_number": 14308,
    "title": "[Dynamic Regions] go-tpc prepare faild with partitioned-raft-kv"
  },
  {
    "bug_location": "pd_client/client_v2.rs",
    "severity": 3,
    "categories": [
      "Network",
      "CodeBug"
    ],
    "root_cause": "Transient gRPC channel connection failure during PD client test scenarios, causing test cases to fail unexpectedly",
    "issue_number": 14309,
    "title": "pd_client tests fail occassionary"
  },
  {
    "bug_location": "Transaction Layer",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Race condition in pessimistic lock handling with WakeUpModeForceLock mode, allowing stale lock requests to bypass version conflict checks",
    "issue_number": 14311,
    "title": "Replayed pessimistic lock requests using `WakeUpModeForceLock` may lead to correctness issue"
  },
  {
    "bug_location": "Backup and Restore (PITR) Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Raft log entry size exceeding system-defined maximum threshold during point-in-time recovery (PITR) restore operation",
    "issue_number": 14313,
    "title": "pitr restore data fail for \"failed: raft entry is too large, region 170, entry size 8616655\""
  },
  {
    "bug_location": "Dynamic Regions",
    "severity": 5,
    "categories": [
      "LoadBalance",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Performance degradation during single TiKV node failure, causing significant QPS reduction and slow recovery",
    "issue_number": 14315,
    "title": "[Dynamic Regions] QPS drop form 28k to 1k with workload error code 9005 during one tikv down "
  },
  {
    "bug_location": "raftstore-v2/raft/apply",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Attempted to unwrap a None Option during Raft apply operation, indicating a potential nil/uninitialized value handling issue in dynamic region processing",
    "issue_number": 14316,
    "title": "[Dynamic Regions] multiple rocksdb tikv panic with Option::unwrap() on a None value"
  },
  {
    "bug_location": "RocksDB Configuration Management",
    "severity": 4,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Dynamic configuration update mechanism for RocksDB parameters is not properly implemented or propagated in TiKV",
    "issue_number": 14320,
    "title": "[Dynamic Regions]  dynamic change on max-compactions, write-buffer-limit  do not work "
  },
  {
    "bug_location": "raftstore-v2",
    "severity": 2,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Incomplete metric instrumentation in new raftstore implementation",
    "issue_number": 14321,
    "title": "[Dynamic Regions]  Some metrics are missing in raftstore-v2"
  },
  {
    "bug_location": "raftstore-v2/operation/command",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unexpected nil or uninitialized apply_scheduler in Raft region processing",
    "issue_number": 14324,
    "title": "[Dynamic Regions] mutiple rocksdb tikv panic for apply_scheduler should be something"
  },
  {
    "bug_location": "Region Split Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Overly aggressive region splitting triggered by bucket flow reporting mechanism when partitioned-raft-kv is enabled",
    "issue_number": 14333,
    "title": "[Dynamic Regions] Bucket flow report triggers much faster region split than before. "
  },
  {
    "bug_location": "GC (Garbage Collection) module in partitioned-raft-kv",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "GC process malfunction in partitioned-raft-kv configuration, causing incomplete data cleanup and potential performance degradation",
    "issue_number": 14339,
    "title": "[Dynamic Regions] GC did not work properly with partitioned-raft-kv"
  },
  {
    "bug_location": "Memory Allocator / RaftStore",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Memory fragmentation in TiKV's memory allocation mechanism, potentially related to Resolved-TS and large transaction processing",
    "issue_number": 14346,
    "title": "TiKV OOM Killed"
  },
  {
    "bug_location": "raftstore-v2/region-split",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Unexpected None value during region split operation, causing panic in Rust unwrap() method",
    "issue_number": 14347,
    "title": "[Dynamic Regions] multiple rocksdb tikv panic"
  },
  {
    "bug_location": "RocksDB Engine Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Exhaustion of file descriptors during dynamic region management, causing RocksDB engine to panic when attempting to open new database files",
    "issue_number": 14352,
    "title": "[Dynamic Regions] multiple rocksdb panic with too many open files"
  },
  {
    "bug_location": "Region Split Mechanism",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Concurrent region split operations causing excessive retry and blocking of read-index requests when multiple hot write regions attempt to split simultaneously",
    "issue_number": 14355,
    "title": "[Dynamic Regions] When there's about 100 split happening roughly at same time, some splits are very slow."
  },
  {
    "bug_location": "RocksDB Memtable",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Memory pressure from excessive memtable usage during high I/O delay scenarios, causing out-of-memory conditions in TiKV nodes",
    "issue_number": 14356,
    "title": "[Dynamic Regions] tikv oom due to memtable is highly used"
  },
  {
    "bug_location": "TiKV Server",
    "severity": 4,
    "categories": [
      "Network",
      "Upgrade",
      "Transaction"
    ],
    "root_cause": "Server timeout during cluster restart, potentially related to dynamic region handling or distributed system synchronization failure",
    "issue_number": 14362,
    "title": "[Dynamic Regions] all tidbs can not start after restart all tidbs\u3001tikvs\u3001pds"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Race condition in leader election process during forced node restart with nobarrier mount, causing conflicting leader state perceptions between cluster nodes",
    "issue_number": 14368,
    "title": "[Dynamic Regions] a region fails to elect leader after restart with force (with nobarrier mount option)"
  },
  {
    "bug_location": "Resource Control Module",
    "severity": 4,
    "categories": [
      "Performance",
      "Config"
    ],
    "root_cause": "Default resource control configuration introducing performance overhead in write-intensive workloads",
    "issue_number": 14375,
    "title": "Enabling resource control by default results in 6%- 8% performance regression"
  },
  {
    "bug_location": "TiKV High Availability Component",
    "severity": 4,
    "categories": [
      "Network",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Potential failure handling issue during node shutdown/failure injection in distributed transaction processing",
    "issue_number": 14377,
    "title": "[witness] tpcc report 9005 when inject tikv down"
  },
  {
    "bug_location": "Region Management",
    "severity": 5,
    "categories": [
      "Performance",
      "Network",
      "LoadBalance"
    ],
    "root_cause": "IO hang causing region leader election and QPS recovery failure",
    "issue_number": 14378,
    "title": "[Dynamic Regions] qps drop to zero after inject one tikv io hang"
  },
  {
    "bug_location": "raftstore/entry_storage",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Attempted to unwrap a None Option during Raft log term retrieval, indicating a potential race condition or unhandled edge case in dynamic region management",
    "issue_number": 14386,
    "title": "[Dynamic Regions] multiple rocksdb tikv panic for called `Option::unwrap()` on a `None` value"
  },
  {
    "bug_location": "raftstore/read_queue",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unexpected region UUID mismatch during read index queue processing, causing a panic in the read queue advancement logic",
    "issue_number": 14388,
    "title": "[Dynamic Regions] multiple rocksdb tikv panic for unexpected uuid detected"
  },
  {
    "bug_location": "raftstore-v2/split_operation",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Failed snapshot acceptance during dynamic region split operation, causing unexpected panic in Raft state machine",
    "issue_number": 14389,
    "title": "[Dynamic Regions] multiple rocksdb tikv panic for failed to accept snapshot"
  },
  {
    "bug_location": "storage/txn/actions/check_txn_status.rs",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Unexpected transaction record found during transaction status check, causing unexpected panic in transaction rollback logic",
    "issue_number": 14390,
    "title": "[Dynamic Regions] multiple rocksdb tikv panic for txn record found but not expected"
  },
  {
    "bug_location": "Flow Controller",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Flow control mechanism fails to properly monitor and throttle when compaction pending bytes exceed expected thresholds",
    "issue_number": 14392,
    "title": "Flow controller doesn't throttle when compaction pending bytes is high"
  },
  {
    "bug_location": "Raft Log Replication",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Insufficient flow control mechanisms during network partition recovery, causing excessive log replication memory pressure leading to out-of-memory condition",
    "issue_number": 14411,
    "title": "[Dynamic Regions] tikv oom after Inject one tikv network_partition last for 50m and recover due to lack of flow control for log replication"
  },
  {
    "bug_location": "metrics/unified-read-pool",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Metrics instrumentation failure after resource control became default configuration",
    "issue_number": 14424,
    "title": "The histogram `tikv_yatp_task_poll_duration` is missing"
  },
  {
    "bug_location": "backup-stream component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Network",
      "Transaction"
    ],
    "root_cause": "Unhandled future polling state in gRPC task lifecycle, causing unexpected panic during network partition scenario",
    "issue_number": 14426,
    "title": "tikv panic with fatal error \"Resolved future is not supposed to be polled again.\" when run ha-tidb-random-to-all-network-partition-last-for-10m"
  },
  {
    "bug_location": "Monitoring/Grafana Dashboard",
    "severity": 2,
    "categories": [
      "Performance",
      "Config"
    ],
    "root_cause": "Incorrect query configuration for gRPC resource group QPS metrics display",
    "issue_number": 14427,
    "title": "Grafana display anomaly"
  },
  {
    "bug_location": "Transaction Processing",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Performance regression in write-heavy transaction processing introduced by specific code commit",
    "issue_number": 14429,
    "title": "commit bec403 causes 2%-4% in write-heavy workloads"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Region split operation blocks raftstore message processing due to memtable flush, causing write performance degradation",
    "issue_number": 14447,
    "title": "[Dynamic Regions] Split can significantly impacts write QPS"
  },
  {
    "bug_location": "log-backup component",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "CodeBug"
    ],
    "root_cause": "Excessive retry mechanism in log backup when network connection to PD is disrupted, causing high CPU consumption and log spam",
    "issue_number": 14451,
    "title": "log-backup: log backup retry may be too frequent and consuming many CPU resource"
  },
  {
    "bug_location": "log-backup/subscription_manager.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Unnecessary logging mechanism triggered even when no log backup task is active",
    "issue_number": 14453,
    "title": "log-backup: verbose log printed even there isn't a log bakcup task"
  },
  {
    "bug_location": "TiKV Configuration Parser",
    "severity": 4,
    "categories": [
      "Config",
      "Upgrade"
    ],
    "root_cause": "Configuration option 'server.snap-max-write-bytes-per-sec' is not recognized in TiKV 7.0.0, causing configuration validation failure during upgrade",
    "issue_number": 14455,
    "title": "tikv will upgrade fail with config: server.snap-max-write-bytes-per-sec"
  },
  {
    "bug_location": "Raft Scheduler Network Traffic",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "LoadBalance"
    ],
    "root_cause": "High network overhead during region scheduling in partitioned-raft-kv configuration",
    "issue_number": 14458,
    "title": "[Dynamic Regions] improve the network traffic spikes when partitioned-raft-kv is enabled"
  },
  {
    "bug_location": "TiKV Restart/Recovery Mechanism",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "Upgrade"
    ],
    "root_cause": "Extended recovery time during heavy write workloads causing restart timeout, potentially due to region synchronization and log replay bottlenecks",
    "issue_number": 14459,
    "title": "[Dynamic Regions] When tikv's under heavy write, the restart time needed exceeds tiup's timeout"
  },
  {
    "bug_location": "storage.partitioned-raft-kv",
    "severity": 5,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Performance degradation in partitioned-raft-kv storage engine during long-running TPCC workload, causing significant QPS drop compared to baseline configuration",
    "issue_number": 14460,
    "title": "[Dynamic Regions] TPCC 1K 7d stability test: QPS drops dramatically when partitioned-raft-kv is enabled compared to baseline"
  },
  {
    "bug_location": "Raft-Engine",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Performance instability during heavy read-write workloads causing QPS jitter due to Raft-Engine rewrite load",
    "issue_number": 14462,
    "title": "[Dynamic Regions] Raft-Engine rewrite load results in qps jitter. "
  },
  {
    "bug_location": "raft-engine",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Config"
    ],
    "root_cause": "Conflict between max_prefill_count and purge-threshold causing log recycling inefficiency",
    "issue_number": 14468,
    "title": "[Dynamic Regions] max_prefill_count conflicts with large purge-threshhold"
  },
  {
    "bug_location": "PD Scheduler",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Default hot region scheduler prioritization not effectively distributing write load across TiKV nodes",
    "issue_number": 14469,
    "title": "[Dynamic Regions] TiKV CPUs are not balanced under heavy write with default PD scheduler setting"
  },
  {
    "bug_location": "Dynamic Regions",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Unstable region distribution causing inconsistent query performance during high-concurrency insert workloads",
    "issue_number": 14470,
    "title": "[Dynamic Regions] qps and latency is unstable for oltp_insert"
  },
  {
    "bug_location": "Raft Log Recovery Engine",
    "severity": 3,
    "categories": [
      "Upgrade",
      "Performance",
      "Storage"
    ],
    "root_cause": "Inefficient log recovery mechanism during cluster upgrade with large pure_threshold configuration",
    "issue_number": 14472,
    "title": "[Dynamic Regions] upgrade from v6.6. to v7.0 is slow due to Recovering raft logs"
  },
  {
    "bug_location": "raft-engine recovery module",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Upgrade"
    ],
    "root_cause": "Inefficient log recovery mechanism with high overhead during restart, potentially exacerbated by log file corruption and insufficient parallel processing",
    "issue_number": 14478,
    "title": "[Dynamic Regions] Raft log recovery takes 450s after restart, even with 16 raft-engine recovery threads"
  },
  {
    "bug_location": "GC Worker",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Invalid hex encoding during garbage collection delete range operation, likely caused by corrupted key or encoding error in range deletion process",
    "issue_number": 14480,
    "title": "GC delete range report error\uff1aencoding/hex: invalid byte"
  },
  {
    "bug_location": "Raft Log Replay Engine",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Inefficient log replay mechanism causing extremely slow recovery of Raft logs during TiKV node restart, resulting in prolonged system unavailability",
    "issue_number": 14481,
    "title": "[Dynamic Regions] replay raft log after restart is too slow (> 5hour in some cases)"
  },
  {
    "bug_location": "raft-engine",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Inefficient log append flow causing performance degradation on GCP pd-ssd disk storage",
    "issue_number": 14487,
    "title": "[Dynamic Regions] raft-engine with v2 is slower than baseline due to append flow on GCP pd-ssd disk"
  },
  {
    "bug_location": "RocksDB Block Cache",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Inefficient filter and index block management during large data writes, causing block cache overwhelming under high write load",
    "issue_number": 14496,
    "title": "[Dynamic Regions] When the TiKV has a large data size, filter and index block can be too big"
  },
  {
    "bug_location": "raftstore_v2::operation::bucket",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unwrapping a None value during region bucket refresh, indicating an unexpected nil state in dynamic region handling",
    "issue_number": 14506,
    "title": "[Dynamic Regions] bucket refresh panic"
  },
  {
    "bug_location": "resource_control_module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Virtual time calculation overflow during resource group rate limit updates, causing abnormal reset frequency",
    "issue_number": 14507,
    "title": "resource_control: virtual time overflow after update_min_virtual_time"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Network",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Region unavailability during datacenter switchover in disaster recovery sync mode",
    "issue_number": 14508,
    "title": "[dr-autosync] query hit 9005 after switch to backup dc in sync mode"
  },
  {
    "bug_location": "Region Cache/Block Cache Filter",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "LoadBalance"
    ],
    "root_cause": "Performance degradation in block cache filter management when handling large data volumes (6TB) across single TiKV instance",
    "issue_number": 14514,
    "title": "[Dynamic Regions] Large block cache filter insert when Single TiKV has 6TB data"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Network",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Network partition causing region unavailability and inability to maintain consensus during distributed network isolation",
    "issue_number": 14532,
    "title": "[Dynamic Regions] workload report 9005: Region is unavailable during one tikv network partition form other tikv"
  },
  {
    "bug_location": "RocksDB Compaction Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Inefficient disk space management during RocksDB compaction process leading to potential disk space exhaustion",
    "issue_number": 14540,
    "title": "Should avoid disk explosion caused by compaction"
  },
  {
    "bug_location": "Region Leader Election",
    "severity": 4,
    "categories": [
      "Transaction",
      "LoadBalance",
      "Network"
    ],
    "root_cause": "Potential race condition or failure in leader re-election mechanism during TiKV instance failure, causing persistent leadership loss in regions",
    "issue_number": 14547,
    "title": "Seems region leaders can lose after one tikv instance fail"
  },
  {
    "bug_location": "raftstore-v2",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Transaction"
    ],
    "root_cause": "Raft tick scheduling logic fails to handle snapshot-based peer recovery, potentially blocking region availability during network partitions",
    "issue_number": 14548,
    "title": "[Dynamic Regions] raft tick may be missing"
  },
  {
    "bug_location": "storage/txn/actions/commit.rs",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect handling of stale pessimistic lock requests in force-lock mode, leading to potential data overwrite and inconsistency during transaction commit resolution",
    "issue_number": 14551,
    "title": "Stale pessimistic lock requests in force-lock mode may lead to data corruption or in consistency after resolving lock"
  },
  {
    "bug_location": "Region Split Handling",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Batch region split mechanism causing excessive latency during TTL operations, potentially due to inefficient splitting algorithm or synchronization overhead",
    "issue_number": 14566,
    "title": "[Dynamic Regions] batch split cause significant  latency jitter"
  },
  {
    "bug_location": "raftstore-v2",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Peer status update mechanism fails to synchronize quickly after region split operation",
    "issue_number": 14572,
    "title": "raftstore-v2: peer status not update in time after region split."
  },
  {
    "bug_location": "tablet_snap component",
    "severity": 5,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Uncontrolled tablet snapshot growth causing excessive disk space consumption during long-running workloads",
    "issue_number": 14581,
    "title": "[Dynamic Regions] tablet_snap takes 1.4TB and TiKV running out of space"
  },
  {
    "bug_location": "Encryption Key Management",
    "severity": 4,
    "categories": [
      "Security",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Collision in encryption key ID generation causing unintended key overwriting",
    "issue_number": 14585,
    "title": "encryption key id collision will erase old key"
  },
  {
    "bug_location": "TiKV Index Merge",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Incorrect handling of index merge query with partitioned table and complex index selection",
    "issue_number": 14619,
    "title": "TiKV reports error like \"3th column is missing value\" when add `ExtraPhysTblID` in schema"
  },
  {
    "bug_location": "Unified Read Pool",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "CodeBug"
    ],
    "root_cause": "Incorrect task tracking in unified read pool causing tasks to remain queued after workload completion",
    "issue_number": 14633,
    "title": "[Dynamic Regions] the value of metric tikv_unified_read_pool_running_tasks is wrong causing server is busy error"
  },
  {
    "bug_location": "Transaction Status Handler",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Incomplete transaction status verification mechanism when handling check_txn_status requests across different client versions",
    "issue_number": 14636,
    "title": "Support verifying whether it's primary when handling check_txn_status"
  },
  {
    "bug_location": "Backup & Restore (BR) Region Splitting",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Oneshot channel cancellation during region split operation, preventing successful data restoration for large number of tables",
    "issue_number": 14641,
    "title": "br restore fail for \"split region failed\""
  },
  {
    "bug_location": "pd_client/resolve",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Network"
    ],
    "root_cause": "Invalid handling of store ID 0 during dynamic region resolution, causing repeated failed address resolution attempts",
    "issue_number": 14644,
    "title": "[Dynamic Regions] TiKV keeps resolving store 0 address after split"
  },
  {
    "bug_location": "gRPC metrics component",
    "severity": 2,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Missing instrumentation for tracking `check_leader` request metrics in gRPC monitoring panel",
    "issue_number": 14658,
    "title": "`check_leader` requests are missing in the metrics"
  },
  {
    "bug_location": "raftstore-v2/operation/command/write/ingest",
    "severity": 5,
    "categories": [
      "Storage",
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "SST file corruption during dynamic region recovery, causing file system access errors when attempting to ingest a missing or corrupted SST file",
    "issue_number": 14663,
    "title": "[Dynamic Regions] tikv panic repeatedly after one tikv failure for 10m and recover"
  },
  {
    "bug_location": "raftstore/v2",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Incomplete implementation of stale read mechanism in raftstore v2 architecture",
    "issue_number": 14664,
    "title": "[Dynamic Regions] stale read is not working in raftstore v2"
  },
  {
    "bug_location": "Raft Engine / Write Performance",
    "severity": 4,
    "categories": [
      "Performance",
      "Config",
      "Transaction"
    ],
    "root_cause": "Configuration parameter changes in 7.1.0-rc causing increased write latency and reduced insert scenario performance compared to 7.0.0",
    "issue_number": 14667,
    "title": "[dbaas] The performance of version 7.1.0-rc has regressed by 13% in the sysbench insert scenario,compared with version 7.0.0"
  },
  {
    "bug_location": "PessimisticLock",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Performance degradation in pessimistic locking mechanism during long-running TPCC workload, causing unexpected latency spikes",
    "issue_number": 14675,
    "title": "[Dynamic Regions] latency spike caused by PessimisticLock when running tpcc stability workload with v2"
  },
  {
    "bug_location": "Build/Development Environment",
    "severity": 2,
    "categories": [
      "Config",
      "Human"
    ],
    "root_cause": "Complex test environment setup requiring manual dependency installation",
    "issue_number": 14677,
    "title": "simplify the process of running tikv test in docker"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Incompatibility between flashback feature and raftstore v2 implementation",
    "issue_number": 14684,
    "title": "[Dynamic Regions] flashback is not working in raftstore v2"
  },
  {
    "bug_location": "Region Split Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect handling of removed_records during region split operation, causing data loss during peer management and region reconfiguration",
    "issue_number": 14689,
    "title": "[Dynamic Regions] Region split removes records unexpectedly "
  },
  {
    "bug_location": "Raftstore v2",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Missing epoch validation during read requests in region split scenarios",
    "issue_number": 14699,
    "title": "[Dynamic Regions] Response read without checking epoch"
  },
  {
    "bug_location": "raftstore/v2",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Incomplete implementation of follower read functionality in raftstore v2",
    "issue_number": 14701,
    "title": "[Dynamic Regions] Follower read is not working in raftstore v2"
  },
  {
    "bug_location": "DR Auto-Sync Replication Component",
    "severity": 3,
    "categories": [
      "Transaction",
      "LoadBalance",
      "Config"
    ],
    "root_cause": "Replication state management failure when a voter node goes down in backup data center, preventing automatic mode transition from sync to async",
    "issue_number": 14704,
    "title": "[dr-autosync] after down one voter tikv in backup dc\uff0creplicatiton state didn't change to async"
  },
  {
    "bug_location": "Transaction Layer",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Incompatibility between autocommit point get read and follower read mechanisms causing potential linearizability violation",
    "issue_number": 14715,
    "title": "txn: autocommit point get read is not compatible with follower read"
  },
  {
    "bug_location": "Region Leader Election",
    "severity": 4,
    "categories": [
      "Network",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Network partition disrupts leader election and region balancing mechanism, preventing automatic recovery of distributed cluster state",
    "issue_number": 14740,
    "title": "[Dynamic Regions] tikv leader can not balance after one tikv network partition for 50m and recover"
  },
  {
    "bug_location": "raftstore-v2/apply_trace",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Unhandled None value during region recovery, likely caused by inconsistent Raft log state during dynamic region restart",
    "issue_number": 14743,
    "title": "[Dynamic Regions]: tikv panic during the restart"
  },
  {
    "bug_location": "sst_importer",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Improper SST file cleanup mechanism when region epoch changes during lightning import process",
    "issue_number": 14745,
    "title": "sst_importer may leak sst files once lightning encounters error due to wrong range loaded"
  },
  {
    "bug_location": "raftstore-v2/operation/ready",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Unimplemented functionality in Raft message handling when using TiFlash, causing a panic in dynamic region processing",
    "issue_number": 14749,
    "title": "[Dynamic Regions] tikv panic when using tiflash"
  },
  {
    "bug_location": "Region Management",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Inefficient handling of empty regions causing excessive region fragmentation and potential performance overhead",
    "issue_number": 14752,
    "title": "Empty Regions are too much"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Timeout handling mechanism in long-running distributed query processing with high CPU load on learner nodes",
    "issue_number": 14753,
    "title": "Coprocessor task terminated due to exceeding the deadline in tispark scenario"
  },
  {
    "bug_location": "raftstore-v2/apply_trace.rs",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Unable to retrieve region state during TiKV node restart, indicating potential data inconsistency or corruption in region metadata",
    "issue_number": 14756,
    "title": "[Dynamic Regions] TiKV restart fails \"failed to get region state\""
  },
  {
    "bug_location": "Read-Only Replica",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "Storage"
    ],
    "root_cause": "Memory exhaustion under network latency conditions during data export operations, likely due to inefficient resource management or memory allocation during high-latency network interactions",
    "issue_number": 14759,
    "title": "[Read-Only Replica] tikv oom when network delay between read-only instance and tispark"
  },
  {
    "bug_location": "Raft Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Log file magic header inconsistency during pause and resume operations in cloud environment",
    "issue_number": 14761,
    "title": "Raft engine encounters log file magic header mismatch"
  },
  {
    "bug_location": "Lightning Import Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "SST file leak during parallel large-scale data import, causing uncleared temporary files to accumulate in /var/lib/tikv/data/import directory",
    "issue_number": 14763,
    "title": "lightning import fails due to sst leakake in /var/lib/tikv/data/import "
  },
  {
    "bug_location": "Transaction/Write CF",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Upgrade"
    ],
    "root_cause": "Incomplete handling of lock record versioning during cluster upgrades, causing potential performance degradation due to accumulated lock records without proper version tracking",
    "issue_number": 14780,
    "title": "Clusters upgraded from <6.5 to >=6.5 may suffer additional bad performance from accumulated lock records"
  },
  {
    "bug_location": "Region Leader Transfer Component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Leader transfer blocking mechanism prevents epoch change during concurrent admin commands",
    "issue_number": 14785,
    "title": "[Dynamic Regions] transfer leader may be blocked on a slow TiKV node"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "Leader fails to mark itself as ready after serving read index request, causing unnecessary delay in follower read response",
    "issue_number": 14786,
    "title": "[Dynamic Regions] Follower read may be slow when leader is in idle."
  },
  {
    "bug_location": "Region Split/Snapshot Generation",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Inefficient handling of slow region splits in follower replica, causing unnecessary snapshot generation when split operation takes longer than expected",
    "issue_number": 14787,
    "title": "[Dynamic Regions] A slow (>1s) split in follower may cause leader to generate a snapshot"
  },
  {
    "bug_location": "RaftStore",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Memory leak in entry cache with potential race condition during dynamic region management, causing unbounded memory growth under high load",
    "issue_number": 14798,
    "title": "[Dynamic Regions] memory of entry cache keep increasing and raftstore race condition and oom"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Lack of detailed error type information in error reporting mechanism for raftstore errors",
    "issue_number": 14808,
    "title": "[storage]: Raftstore error's err_other is confusing"
  },
  {
    "bug_location": "CDC (Change Data Capture) Old Value Cache",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Memory quota management failure in old value cache during multi-table change data capture, causing memory usage to exceed configured limits",
    "issue_number": 14815,
    "title": "cdc's old value cache sometimes exceeds the quota"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential index out of bounds error in element access function causing runtime crash",
    "issue_number": 14827,
    "title": "Crashing due to index out of bounds in elt function"
  },
  {
    "bug_location": "RocksDB SST Ingestion Flow Control",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Insufficient flow control checks during multiple SST file snapshot ingestion, leading to potential L0 file accumulation and unintended stalling",
    "issue_number": 14828,
    "title": "Multiple SST files snapshot may trigger flow control"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Network",
      "Performance"
    ],
    "root_cause": "Potential synchronization issue with resolved timestamp tracking during concurrent workloads and chaos testing, causing timestamp gap to progressively increase",
    "issue_number": 14831,
    "title": "TiKV resolved ts gap keep increasing"
  },
  {
    "bug_location": "Scheduler/Metrics",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Incomplete duration tracking for write commands during high contention scenarios, causing metrics to not accurately reflect command processing times",
    "issue_number": 14838,
    "title": "metric: the scheduler and latch duration may not be recorded with the grpc duration"
  },
  {
    "bug_location": "storage",
    "severity": 3,
    "categories": [
      "Storage",
      "Transaction",
      "Upgrade"
    ],
    "root_cause": "Potential stale read issue requiring enhancement and bug fix for release 6.5",
    "issue_number": 14839,
    "title": "storage: backport the stale read enhancement and bug fix to release 6.5"
  },
  {
    "bug_location": "Raftstore v2",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Missing entry cache warm-up mechanism during leader transfer in Raftstore v2",
    "issue_number": 14845,
    "title": "[Dynamic Regions] Entry cache does not warm up before transferring leader"
  },
  {
    "bug_location": "StaleRead Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Unbounded memory growth from tracking locks in large transactions without memory quota management",
    "issue_number": 14864,
    "title": "Resolved ts in stale read may cause TiKV OOM"
  },
  {
    "bug_location": "CDC (Change Data Capture)",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Resolved timestamp synchronization issue during region split with multi-rocksdb configuration",
    "issue_number": 14870,
    "title": "CDC resolved ts has 20s+ spike after region split"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction",
      "Network"
    ],
    "root_cause": "Local message handling failure during leader eviction with high region count, causing message rejection and logging errors",
    "issue_number": 14874,
    "title": "Too many \"cannot step raft local message\" when RejectRaftAppend occurs"
  },
  {
    "bug_location": "PD (Placement Driver) Recovery Module",
    "severity": 5,
    "categories": [
      "Transaction",
      "LoadBalance",
      "Network"
    ],
    "root_cause": "Store 5 failed to report recovery status to PD during unsafe recovery process, causing recovery procedure timeout and failure",
    "issue_number": 14883,
    "title": "[dr-autosync]online recover failed due to one store have not reported to PD"
  },
  {
    "bug_location": "Compaction Guard",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Config"
    ],
    "root_cause": "Default compaction guard minimum output file size (8MB) is too large, causing ineffective region filtering and potential massive data compaction during TTL-enabled scenarios",
    "issue_number": 14888,
    "title": "compaction-guard-min-output-file-size default value 8MB is too big that could leads to huge compaction"
  },
  {
    "bug_location": "raftstore",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unexpected disabling of CheckLongUncommitted tick mechanism in raftstore v1 component",
    "issue_number": 14893,
    "title": "CheckLongUncommitted tick is missing"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Region redistribution overhead during database drop operation causing significant QPS degradation",
    "issue_number": 14898,
    "title": "[Dynamic Regions] online qps drop a lot when drop other database"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Inconsistent state between memtable and index tracking during delete range operations, causing synchronization blocking",
    "issue_number": 14904,
    "title": "[Dynamic Regions] after delete range, admin flushed index may be blocked"
  },
  {
    "bug_location": "backup-stream component",
    "severity": 5,
    "categories": [
      "Network",
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unhandled future polling error in gRPC task during network partition, causing unexpected panic in checkpoint manager",
    "issue_number": 14910,
    "title": "one tikv panic when inject one tidb network partition"
  },
  {
    "bug_location": "Dynamic Regions Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Upgrade",
      "Transaction"
    ],
    "root_cause": "Performance degradation during rolling update of TiKV nodes, causing significant QPS drop when handling large table workloads",
    "issue_number": 14912,
    "title": "[Dynamic Regions] online qps almost drop to zero when rolling update"
  },
  {
    "bug_location": "Import Configuration Component",
    "severity": 3,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Configuration parameter not dynamically reconfigurable at runtime",
    "issue_number": 14934,
    "title": "Import: num-thread is not onlineConfig"
  },
  {
    "bug_location": "resource_control/resource_group.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential integer overflow or incorrect threshold calculation in resource group virtual token reset mechanism",
    "issue_number": 14943,
    "title": "test_reset_resource_group_vt_overflow failed when executing `make test`"
  },
  {
    "bug_location": "raftstore-v2/ingest",
    "severity": 5,
    "categories": [
      "Storage",
      "Transaction",
      "Upgrade"
    ],
    "root_cause": "SST file corruption during backup and restore process, likely caused by incomplete file transfer or interrupted import operation",
    "issue_number": 14946,
    "title": "[Dynamic Regions] after br full restored to multirocksdb cluster failed and tikv restarted, tikv panic"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Performance regression in dynamic region handling causing increased delete_skipped-index operations and significant QPS reduction",
    "issue_number": 14947,
    "title": "[Dynamic Regions] v2 60% QPS drop compare with v1 with ~5x delete_skipped-index"
  },
  {
    "bug_location": "backup-stream component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Removed failpoints causing test integration failures during execution",
    "issue_number": 14958,
    "title": "backup-stream: failpoint tests may fail"
  },
  {
    "bug_location": "Dynamic Regions",
    "severity": 3,
    "categories": [
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Performance regression in key-value get operations during dynamic region handling, causing approximately 5% QPS drop compared to previous version",
    "issue_number": 14966,
    "title": "[Dynamic Regions] v2 ~5% QPS drop compare with v1, ycsb workload"
  },
  {
    "bug_location": "raft_log_engine",
    "severity": 5,
    "categories": [
      "Storage",
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Checksum mismatch in Raft log engine indicating potential data corruption or memory/storage integrity issue",
    "issue_number": 14969,
    "title": "TiFlash proxy panic with \"Corruption: Checksum expected 369222644 but got 1144459145\""
  },
  {
    "bug_location": "pd_client/tikv-ctl",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Invalid store ID handling during flashback recovery process, likely due to incomplete region metadata cleanup in etcd during online recovery",
    "issue_number": 14974,
    "title": "[dr-autosync] tikv flashback to version panic due to online recover didn't clean region meta data in etcd"
  },
  {
    "bug_location": "Region Recovery/Sync Mechanism",
    "severity": 4,
    "categories": [
      "Transaction",
      "Network",
      "LoadBalance"
    ],
    "root_cause": "Potential synchronization failure during disaster recovery process causing region unavailability when restarting workload after DC recovery",
    "issue_number": 14975,
    "title": "[dr-autosync] during sync_recover, workload report error: Region is unavailable"
  },
  {
    "bug_location": "raftstore-v2/region_merge",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Overlapping database ranges during tablet merge causing invalid argument error",
    "issue_number": 14992,
    "title": "[Dynamic Regions] panic during region merge fails to merge tablet"
  },
  {
    "bug_location": "Region Statistics Component",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect calculation or tracking of region bucket statistics during coprocessor requests, leading to zero statistics being reported",
    "issue_number": 14998,
    "title": "[Dynamic Regions]: the statistics of region buckets can't work well for coprocessor request "
  },
  {
    "bug_location": "Import Mode/Lightning Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Race condition during import job cancellation causing TiKV panic when exiting import mode while import process is still active",
    "issue_number": 15003,
    "title": "tikv panic when tikv exit the import mode but import process is still not stopped during \"import into\" command is cancelled"
  },
  {
    "bug_location": "sysinfo/linux/cpu.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Index out of bounds error when dynamically detecting CPU cores without proper array bounds checking during hardware information retrieval",
    "issue_number": 15006,
    "title": "diagnostic: non-stop cpu expansion causes index out of bounds error"
  },
  {
    "bug_location": "Region Scheduler",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Dynamic region scheduling fails to distribute write hot spots effectively during TPCC workload preparation, causing uneven data distribution across TiKV stores",
    "issue_number": 15011,
    "title": "[Dynamic Region] Write hot can't be scheduled with the tpcc preapre insert"
  },
  {
    "bug_location": "tikv_util component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Multiple compilation errors related to import conflicts, type mismatches, and missing dependencies when building on FreeBSD platform",
    "issue_number": 15015,
    "title": "tikv_util: Building on FreeBSD fails"
  },
  {
    "bug_location": "Region Distribution/Load Balancing",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Network",
      "Performance"
    ],
    "root_cause": "Uneven region distribution during network partition recovery, causing capacity imbalance between TiKV nodes",
    "issue_number": 15018,
    "title": "[Dynamic Regions] the available size is a big gap between tikvs and leading to the capacity of one tikv is insufficient "
  },
  {
    "bug_location": "storage/txn/scheduler",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Async write callback dropped during pessimistic transaction rollback, causing unexpected panic in transaction scheduler",
    "issue_number": 15020,
    "title": "Enabling failpoint `raftkv_early_error_report`, `cleanup`, `pessimistic_prewrite` sometimes leads to tikv panic"
  },
  {
    "bug_location": "Scheduling Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Upgrade"
    ],
    "root_cause": "Performance regression in scheduling mechanism causing significant latency increase during version upgrade from v4.0.8 to v6.5.2",
    "issue_number": 15021,
    "title": "Will upgrading from v4.0.8 to 6.5.2 meet this bug #14780 ?"
  },
  {
    "bug_location": "Region Restore Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Potential timeout or performance issue during database restore process, specifically in checksum verification for dynamic regions",
    "issue_number": 15022,
    "title": "[Dynamic Regions] br restore checksum failure on v2"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Checkpoint Mechanism",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Network partition disrupts synchronization of checkpoint timestamp, causing lag accumulation during recovery process",
    "issue_number": 15031,
    "title": "[Dynamic Regions] pitr checkpoint ts lag keep increasing after simulate tikv network partition for 50m and recover"
  },
  {
    "bug_location": "Raft Consensus Module",
    "severity": 5,
    "categories": [
      "Transaction",
      "Network",
      "CodeBug"
    ],
    "root_cause": "Lease renewal and leadership election race condition causing potential linearizability violation during network partition and node restart scenarios",
    "issue_number": 15035,
    "title": "Leader holds an invalid lease if one node is isolated and one node restarts"
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Insufficient disk space during data compaction and log writing, causing RocksDB background write operations to fail",
    "issue_number": 15036,
    "title": "tikv exit \"IO error: No space left on device: While appending to file: /var/lib/tikv/data/db/2355792.log: No space left on device\""
  },
  {
    "bug_location": "raftstore/peer_storage",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Snapshot application failure during I/O recovery causing repeated TiKV panics",
    "issue_number": 15044,
    "title": "tikv panic repeatedly after inject tikv io hang and recover"
  },
  {
    "bug_location": "Region Management",
    "severity": 3,
    "categories": [
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Performance regression in dynamic region handling causing uneven commit log processing across TiKV nodes",
    "issue_number": 15051,
    "title": "[Dynamic Regions] v2 ~8% QPS drop compare with v1, sysbench oltp_read_write"
  },
  {
    "bug_location": "TiKV Server/Encryption Key Management",
    "severity": 5,
    "categories": [
      "Network",
      "Storage",
      "Security"
    ],
    "root_cause": "Encryption key management failure during network isolation and recovery, causing persistent node startup corruption",
    "issue_number": 15052,
    "title": "[Dynamic Region] start ticdc changefeed and pitr task, inject one tikv network isolation 10min and recovery, 3 hours later, one tikv panic repeatedly"
  },
  {
    "bug_location": "SST Importer",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Premature deletion of SST files during region split and ingest process, causing race condition between file deletion and Raft log processing",
    "issue_number": 15053,
    "title": "tikv panic because SST files of sst_importer is deleted before raft ingest"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Potential race condition or resource allocation issue during dynamic region preparation with multiple RocksDB instances during large dataset loading",
    "issue_number": 15056,
    "title": "[Dynamic Region] region unavailable when prepare tpcc data"
  },
  {
    "bug_location": "Raft Consensus Module",
    "severity": 5,
    "categories": [
      "Network",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Network partition handling failure causing persistent log synchronization lag in distributed consensus protocol",
    "issue_number": 15059,
    "title": "[Dynamic Regions] raft log lag more and more after inject one tikv network partition for 50m and recover"
  },
  {
    "bug_location": "Region Scheduler",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Network"
    ],
    "root_cause": "Memory pressure under high IO latency and large data workload causing uncontrolled memory allocation",
    "issue_number": 15061,
    "title": "[Dynamic Region] tikv oom when inject 100ms io delay chaos"
  },
  {
    "bug_location": "Lock Manager",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "In-memory pessimistic locks are not visible during scan_lock operations due to hash table storage implementation, preventing complete lock visibility during garbage collection and resolved timestamp t",
    "issue_number": 15066,
    "title": "In-memory pessimistic locks are invisible to scan_lock requests"
  },
  {
    "bug_location": "TiFlash Proxy",
    "severity": 3,
    "categories": [
      "Network",
      "CodeBug"
    ],
    "root_cause": "Failure in region state retrieval mechanism causing proxy crash during dynamic region handling",
    "issue_number": 15074,
    "title": "[Dynamic Region] TiFlash proxy crash for failed to get regions state"
  },
  {
    "bug_location": "Build System",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Dependency resolution or compilation failure for tipb library during build process",
    "issue_number": 15075,
    "title": "build error failed to run custom build command"
  },
  {
    "bug_location": "components/encryption/src/io.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Security"
    ],
    "root_cause": "Partial write implementation in encryption writer causes data corruption during encryption process, specifically when writing only a subset of input bytes",
    "issue_number": 15080,
    "title": "encryption: encryption may corrupt data when partial write"
  },
  {
    "bug_location": "Region Resolver/Safe Timestamp Tracking",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "Limited observability of resolved timestamp and safe timestamp progression mechanisms in TiKV regions",
    "issue_number": 15082,
    "title": "Improve observability of resolved-ts and safe_ts"
  },
  {
    "bug_location": "Region Management Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Illegal response handling in multi-RocksDB configuration during region dynamic management",
    "issue_number": 15091,
    "title": "[Dynamic Region] TiDB fails to start, \"Internal error: received illegal TiKV response\""
  },
  {
    "bug_location": "Raft Consensus Module",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Network partition causing Raft log synchronization disruption without proper snapshot recovery mechanism",
    "issue_number": 15105,
    "title": "[Dynamic Regions] : raft log lag increased after network partition."
  },
  {
    "bug_location": "Unified Read Pool",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Resource exhaustion during large-scale data restore and checksum operations, causing task accumulation in unified read pool",
    "issue_number": 15106,
    "title": "[Dynamic Regions] running task in unified read pool keep increasing during checksum after restore 1TB sysbench data"
  },
  {
    "bug_location": "DR-AutoSync Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Storage"
    ],
    "root_cause": "Potential deadlock or resource exhaustion during flashback recovery process in distributed recovery scenario",
    "issue_number": 15107,
    "title": "[dr-autosync] tikv flashback hung more than 12h"
  },
  {
    "bug_location": "storage/txn/actions/acquire_pessimistic_lock",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Assertion failure in version tracking logic during pessimistic lock acquisition, indicating an unexpected state in transaction version estimation",
    "issue_number": 15109,
    "title": "Tikv panic for assertion failed: estimated_versions_to_last_change > 0"
  },
  {
    "bug_location": "DR (Disaster Recovery) Autosync Component",
    "severity": 3,
    "categories": [
      "Performance",
      "Config",
      "Storage"
    ],
    "root_cause": "Excessive logging during online recovery process in sync_recovery mode, generating multiple large log files rapidly",
    "issue_number": 15112,
    "title": "[dr-autosync] too much logs during online recovery after switching to back dc in sync_recovery mode"
  },
  {
    "bug_location": "Dynamic Regions Flow Control",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Unexpected flow control mechanism causing unstable online query per second (QPS) in distributed region management",
    "issue_number": 15113,
    "title": "[Dynamic Regions] online qps is unstable for the unexpected flow control"
  },
  {
    "bug_location": "RaftStore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Timeout during garbage collection of removed peer in Raft consensus mechanism",
    "issue_number": 15114,
    "title": "test_gc_removed_peer fails"
  },
  {
    "bug_location": "Raft Consensus/Network Partition Handling",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Stale read configuration and network partition recovery mechanism causing prolonged service disruption after network fault healing",
    "issue_number": 15120,
    "title": "enable stale read and closest-replicas\uff0csimulate one of tikv network partition last for 50min\uff0cqps drop to zero for more than 1h after fault recover"
  },
  {
    "bug_location": "snapshot_restore_module",
    "severity": 4,
    "categories": [
      "Network",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Message dropping during snapshot restoration causing potential deadlock in region election process",
    "issue_number": 15122,
    "title": "snap_restore: when there are message dropping, snap_restore may fail"
  },
  {
    "bug_location": "Region Split/Memory Management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Lack of memory pressure control during region split operations causing uncontrolled memory allocation",
    "issue_number": 15125,
    "title": "region count should not be throttled when memory usage exceeds a threshold "
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Performance regression in dynamic region handling causing significant QPS reduction during high-load uniform distribution workload",
    "issue_number": 15132,
    "title": "[Dynamic Regions] v2 62% QPS drop compare with v1, ycsb workloade"
  },
  {
    "bug_location": "GitHub Issue Template",
    "severity": 2,
    "categories": [
      "Human",
      "Config"
    ],
    "root_cause": "Incomplete issue template design preventing automatic type assignment and clear problem description",
    "issue_number": 15139,
    "title": "TiKV repo's issue template is not correct"
  },
  {
    "bug_location": "RawKV API v2 TTL management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "TTL check poll interval configuration not being properly applied in RawKV API v2, causing storage size to continuously increase instead of cleaning expired data",
    "issue_number": 15142,
    "title": "rawkv: `ttl-check-poll-interval` does not take effect on RawKV API v2"
  },
  {
    "bug_location": "Flow Controller / Network Partition Handler",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Network partition recovery mechanism fails to properly reset flow control parameters, causing persistent performance degradation",
    "issue_number": 15145,
    "title": "[Dynamic Region] QPS continues to decrease after inject one tikv network partition for 50m and recover due to flow controller"
  },
  {
    "bug_location": "Region Bucket Management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Delayed bucket updates after SST file ingestion causing metadata synchronization lag",
    "issue_number": 15147,
    "title": "[Dynamic Regions] Let TiKV update buckets ASAP after ingesting SST files"
  },
  {
    "bug_location": "raftstore-v2/merge",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Assertion failure in merge operation when availability context is missing during HTAP workload with TiFlash",
    "issue_number": 15151,
    "title": "[Dynamic Regions] tikv crash for assertion failed: resp.has_availability_context() with tiflash"
  },
  {
    "bug_location": "async-read-worker/snapshot_generation",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Race condition during region configuration change and snapshot generation, causing async-read-worker to get stuck while generating snapshot during region destruction",
    "issue_number": 15153,
    "title": "[Dynamic Regions] async-read-worker may get stuck in generating snapshot when a region is destroying"
  },
  {
    "bug_location": "Raft Consensus Module",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "Potential message flushing delay in Raft log commit process causing extended commit duration",
    "issue_number": 15175,
    "title": "Delay flushing raft message may cause high raft log commit duration"
  },
  {
    "bug_location": "import/sst_service.rs",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Region replica synchronization failure during full restore, causing specific region (197505) to be unresolvable or unavailable",
    "issue_number": 15176,
    "title": "[Dynamic Regions] br full restore meets error \"region xxx not found \""
  },
  {
    "bug_location": "TiKV Transaction Layer",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Performance regression in transaction commit and write operations between TiDB versions 7.1.0 and 7.1.1, likely due to changes in transaction processing or storage write mechanisms",
    "issue_number": 15177,
    "title": "During a large-scale table test of TiDB, a noticeable regression in insertion performance was observed for version 7.1.1, accompanied by significant tidb duration"
  },
  {
    "bug_location": "Block Cache Configuration",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Config"
    ],
    "root_cause": "Oversized block cache configuration causing excessive memory consumption during fault simulation scenarios",
    "issue_number": 15181,
    "title": "[Dynamic Regions] tikv oom when simulate fault testing due to the block cache configuration is too large"
  },
  {
    "bug_location": "PD Client Store Heartbeat Handler",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Inefficient retry mechanism and lock contention in store heartbeat processing under high load conditions",
    "issue_number": 15184,
    "title": "Store heartbeat cannot be consumed, heartbeat storm in big cluster"
  },
  {
    "bug_location": "Transaction/Stale Read Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Network",
      "LoadBalance"
    ],
    "root_cause": "Network partition handling causes significant performance degradation in multi-availability zone configurations, likely due to suboptimal replica selection and read routing during network isolation",
    "issue_number": 15187,
    "title": "enable stale read and closest-replicas\uff0cqps of all az drop to bottom during simulating network partition between two az "
  },
  {
    "bug_location": "Scheduling/Load Balancing Component",
    "severity": 4,
    "categories": [
      "Network",
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Stale read and closest-replicas configuration causing prolonged QPS degradation during network partition scenarios",
    "issue_number": 15188,
    "title": "enable stale read and closest-replicas\uff0cqps drop last for 10min on us-west-2a when injection network partition between one of tikv of us-west-2a and all other pods in cluster"
  },
  {
    "bug_location": "Follower Read Component",
    "severity": 5,
    "categories": [
      "Transaction",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Performance degradation in follower read mechanism during node recovery after simulated failure, causing sustained QPS drop",
    "issue_number": 15189,
    "title": "[Dynamic Regions] enable follower read, qps drop more than 90% lasts for 50m after fault recover\uff08one of tikv failure for 10mins\uff09"
  },
  {
    "bug_location": "RocksDB/Region Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Memory management issue during dynamic region creation and table schema updates, likely causing excessive memory allocation during concurrent schema changes",
    "issue_number": 15190,
    "title": "[Dynamic Regions] tikv oom frequently when create table"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Checkpoint Mechanism",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Resolved timestamp progression stalled during network partition, causing significant checkpoint lag",
    "issue_number": 15192,
    "title": "[Dynamic Regions] pitr checkpoint ts lag reached 88mins after fault recover when injection network partition between one of tikv and others tikv due to resolved ts do not move forward"
  },
  {
    "bug_location": "DR Auto-Sync Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance"
    ],
    "root_cause": "Resolved timestamp tracking mechanism not functioning correctly in DR auto-sync mode, preventing proper transaction progress tracking",
    "issue_number": 15194,
    "title": "[dr-autosync] set cluster to dr-autosync mode,  min-resolved-ts is still 0 after run some transactions"
  },
  {
    "bug_location": "raftstore-v2/merge",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Assertion failure during region merge operation due to missing availability context in response",
    "issue_number": 15197,
    "title": "[Dynamic Regions] tikv panic for [FATAL] [lib.rs:510] [\"assertion failed: resp.has_availability_context()\"] "
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "Network"
    ],
    "root_cause": "Snapshot retrieval failure during region merge operation, preventing read index completion",
    "issue_number": 15198,
    "title": "[Dynamic Regions] br: pitr task in error \"retry time exceeds: error failed to get initial snapshot\""
  },
  {
    "bug_location": "Transaction Scheduler",
    "severity": 3,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Unexpected response channel drop during TiKV shutdown, causing potential race condition in transaction processing",
    "issue_number": 15202,
    "title": "[Dynamic Regions] Response channel may be unexpectedly dropped during tikv stop"
  },
  {
    "bug_location": "raftstore-v2/region_merge",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Missing source peer during region merge checkpoint retrieval, indicating a potential synchronization or metadata tracking issue in dynamic region management",
    "issue_number": 15209,
    "title": "[Dynamic Regions] tikv panic for \"[FATAL] [lib.rs:510] [\"source peer is missing when getting checkpoint for merge [peer_id=6259] [region_id=6258]\""
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "LoadBalance",
      "Storage"
    ],
    "root_cause": "Inconsistent region metadata during dynamic region splitting, causing intermittent region lookup failures",
    "issue_number": 15210,
    "title": "[Dynamic Regions] region split may led PD returned no region"
  },
  {
    "bug_location": "Flashback Transaction Scanning",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incomplete handling of non-user write records during key scanning in flashback process",
    "issue_number": 15219,
    "title": "scan_latest_user_keys should skip the type of non-user write"
  },
  {
    "bug_location": "Build System",
    "severity": 2,
    "categories": [
      "CodeBug"
    ],
    "root_cause": "Potential dependency resolution or cargo configuration issue during package update process",
    "issue_number": 15221,
    "title": "cargo update error"
  },
  {
    "bug_location": "storage/txn/actions/check_txn_status",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Inconsistent transaction state handling during concurrent write operations, causing unexpected write record detection in transaction rollback logic",
    "issue_number": 15223,
    "title": "[Dynamic Regions] tikv panic for \"txn record found but not expected: Write { write_type: Put\""
  },
  {
    "bug_location": "Snapshot/Apply Module",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction"
    ],
    "root_cause": "Incomplete notification mechanism for snapshot cancellation, potentially causing synchronization or state tracking failures during snapshot application process",
    "issue_number": 15227,
    "title": "post_apply_snapshot may fail to be informed if the snapshot is cancelled"
  },
  {
    "bug_location": "storage/partitioned-raft-kv",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Memory management inefficiency during dynamic scaling of TiKV nodes, potentially causing excessive memory allocation and uncontrolled growth during region redistribution",
    "issue_number": 15240,
    "title": "[Dynamic Regions] TiKV OOM after scaling TiKV"
  },
  {
    "bug_location": "raftstore-v2/fsm/store",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Region metadata corruption during merge operation in Raft store state machine",
    "issue_number": 15242,
    "title": "[Dynamic Regions] panic with \"region corrupted\""
  },
  {
    "bug_location": "RawStorage coprocessor plugin",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect implementation of scan function in RawStorage trait, returning only Value instead of expected KvPair",
    "issue_number": 15246,
    "title": "The scan function return in RawStorage in coprocessor_plugin does not meet expectations"
  },
  {
    "bug_location": "raftstore/region_info_accessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Inconsistent region range validation during dynamic region configuration, causing assertion failure in region metadata comparison",
    "issue_number": 15253,
    "title": "[Dynamic Regions]tikv panic for \"assertion failed: `(left != right)`\\n  left: `51`,\\n right: `51`: id: 1376 start_key\""
  },
  {
    "bug_location": "Snapshot Transfer Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Insufficient memory quota management during snapshot send/receive operations, causing potential out-of-memory conditions on systems with limited memory",
    "issue_number": 15255,
    "title": "[Dynamic Regions] limit the memory quota in snapshot send/recv"
  },
  {
    "bug_location": "Region Management / Memory Management",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Memory pressure from large number of small tables and high-throughput workload causing uncontrolled memory allocation during region restoration and data processing",
    "issue_number": 15257,
    "title": "[Dynamic Regions] TiKV OOM seen after restore data of 4k tables and run workload"
  },
  {
    "bug_location": "Region Merge Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Storage"
    ],
    "root_cause": "Resolved timestamp blocking during dynamic region merge, preventing flashback and point-in-time recovery operations",
    "issue_number": 15258,
    "title": "[Dynamic Regions] resolved ts blocked after region merge"
  },
  {
    "bug_location": "Transaction Layer",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Inconsistent transaction ordering and timestamp management in distributed transaction processing, causing potential causal consistency violations",
    "issue_number": 15261,
    "title": "[Dynamic Region] transaction check inconsistent "
  },
  {
    "bug_location": "raftstore-v2/operation/command/admin/merge",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unexpected state during region merge operation, causing an assertion failure in merge commit logic",
    "issue_number": 15262,
    "title": "[Dynamic Region]tikv panic for \"assertion failed: !state.has_merge_state()\""
  },
  {
    "bug_location": "Region Scheduler",
    "severity": 4,
    "categories": [
      "Network",
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Ineffective handling of network latency in dynamic region scheduling, causing QPS instability when slow trend eviction is enabled",
    "issue_number": 15267,
    "title": "[Dynamic Regions] QPS is severe jitter after inject one of tikv network latency with enable evict-slow-trend-scheduler"
  },
  {
    "bug_location": "Region Load Balancer",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Network"
    ],
    "root_cause": "Dynamic region scheduling fails to handle prolonged I/O hang in a single TiKV node, causing system-wide QPS degradation",
    "issue_number": 15268,
    "title": "[Dynamic Regions] QPS drop to zero during injection io hang to one of tikv"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Memory exhaustion during large-scale table partitioning and region splitting, causing excessive memory allocation in region metadata handling",
    "issue_number": 15269,
    "title": "[Dynamic Regions]tikv oom when table has a large number of partition"
  },
  {
    "bug_location": "Backup and Restore (BR) Checkpoint Synchronization",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Checkpoint timestamp synchronization failure during network partition and datacenter failover scenarios, causing prolonged checkpoint lag",
    "issue_number": 15279,
    "title": "[dr-autosync] pitr checkpoint ts lag up to 15min after down backup dc "
  },
  {
    "bug_location": "Compaction/Region Management",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "LoadBalance"
    ],
    "root_cause": "Ineffective compaction threshold mechanism for regions with large MVCC versions and key-value sizes",
    "issue_number": 15282,
    "title": "Improve compaction check mechanism"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Large key size causing region size exceeding recommended limits, triggering flow control during region balancing",
    "issue_number": 15286,
    "title": "have a big key more than a half region size"
  },
  {
    "bug_location": "raftstore/entry_storage",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Invalid handling of warmup range during entry cache initialization, potentially triggered by disk space constraints",
    "issue_number": 15289,
    "title": "tikv crash for the warmup range should still be valid"
  },
  {
    "bug_location": "storage/mvcc/reader",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Missing default column family data during key retrieval operation",
    "issue_number": 1529,
    "title": "tikv-server panics due to default cf data missing."
  },
  {
    "bug_location": "raftstore/peer_storage",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Snapshot application failure during I/O recovery, causing repeated panic in Raft store component",
    "issue_number": 15292,
    "title": "tikv panic repeatedly after this tikv recover from io hang"
  },
  {
    "bug_location": "Raft Store Memory Tracing",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incomplete memory trace implementation in TiKV v2 for Raft store related entries",
    "issue_number": 15293,
    "title": "[Dynamic Regions] Memory Trace in v2 is missing some raft store related entries"
  },
  {
    "bug_location": "Raft leader election component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Network"
    ],
    "root_cause": "Unsafe vote mechanism preventing leader election during snapshot restoration process",
    "issue_number": 15296,
    "title": "snap_restore: cannot elect leader when start up."
  },
  {
    "bug_location": "resource_metering/cpu_recorder",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Memory leak in thread statistics tracking due to lack of cleanup for destroyed threads",
    "issue_number": 15304,
    "title": "CpuRecorder may leak memory for destroyed threads"
  },
  {
    "bug_location": "raft-store-v2",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Network",
      "LoadBalance"
    ],
    "root_cause": "Failure to properly detect and report peer status in distributed cluster when a node becomes unresponsive",
    "issue_number": 15316,
    "title": "raft-store-v2: tikv does not report down peers after 1 tikv is down"
  },
  {
    "bug_location": "RocksDB Compaction",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Uncontrolled memory allocation during RocksDB compaction process causing excessive memory consumption",
    "issue_number": 15324,
    "title": "3GB memory spike caused by RocksDB compaction"
  },
  {
    "bug_location": "Region Split Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Incorrect region size estimation during incremental region splits, causing inaccurate size tracking when splitting regions sequentially",
    "issue_number": 15326,
    "title": "Region approximate size maybe wrong after split"
  },
  {
    "bug_location": "Region Management",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Performance bottleneck in table creation and analysis with dynamic region configurations, potentially related to RocksDB multi-instance and table splitting strategies",
    "issue_number": 15330,
    "title": "[Dynamic Regions] create table and anlyze is slow "
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Race condition during peer state management during snapshot and destroy peer message handling, particularly affecting learner nodes during region merge operations",
    "issue_number": 15333,
    "title": "TiKV may stuck/panic if a destroy peer message is handled after apply snapshot on raftstore v1"
  },
  {
    "bug_location": "TiKV Unsafe Recovery Module",
    "severity": 5,
    "categories": [
      "Transaction",
      "Storage",
      "Performance"
    ],
    "root_cause": "Timeout during force leader stage of unsafe recovery process, preventing successful cluster recovery and leader redistribution",
    "issue_number": 15346,
    "title": "[dr-autosync] online recover time out after switching to backup cluster in sync_recover mode"
  },
  {
    "bug_location": "TiKV Region Management",
    "severity": 4,
    "categories": [
      "Transaction",
      "LoadBalance",
      "Storage"
    ],
    "root_cause": "Region unavailability during distributed transaction recovery in sync_recover mode, likely due to lock resolution or region leader election instability",
    "issue_number": 15347,
    "title": "[dr-autosync] after running in sync_recovery for 10min, hit 9005 error "
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Global mutex blocking writes during manifest synchronization, causing unnecessary performance bottleneck",
    "issue_number": 15351,
    "title": "titan: avoid to hold global mutex while syncing titan manifest"
  },
  {
    "bug_location": "storage/raw/raw_mvcc.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect prefix seek implementation with mismatched key timestamp and prefix extraction, potentially causing unexpected iterator behavior",
    "issue_number": 15354,
    "title": "RawMvccSnapshot use prefix seek in defaultcf which not setup PrefixExtrator"
  },
  {
    "bug_location": "raft_router/apply_router",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Memory trace metrics not correctly reporting alive or leak data for raft router components",
    "issue_number": 15357,
    "title": "Memory Trace does not include alive or leak data for raft_router/apply_router"
  },
  {
    "bug_location": "PD (Placement Driver) Client",
    "severity": 2,
    "categories": [
      "Config",
      "Network"
    ],
    "root_cause": "URL scheme parsing limitation preventing proper connection to local PD endpoints using full HTTP URL format",
    "issue_number": 1536,
    "title": "pd: support \"http://127.0.0.1:2379\" scheme "
  },
  {
    "bug_location": "DR-AutoSync Component",
    "severity": 4,
    "categories": [
      "Network",
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Performance degradation during network partition recovery and sync mode transition, likely due to synchronization overhead or consensus protocol instability",
    "issue_number": 15366,
    "title": "[dr-autosync] v6.5.4 QPS drop to 0 during switch sync_recovery  to sync"
  },
  {
    "bug_location": "raftstore/store/fsm/store",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Metadata corruption during region creation, likely caused by concurrent async commit and network failure scenarios",
    "issue_number": 15370,
    "title": "v6.5.4 tikv panic for \"meta corrupted: no region for 51902 7A7480000000000000FFA45F728000000000FF00192A0000000000FA \""
  },
  {
    "bug_location": "Region Merge/Split Component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Memory trace tracking mechanism fails to properly reset or clean up apply counts during repeated region merge and split operations",
    "issue_number": 15371,
    "title": "The applys count of memory trace keeps growing "
  },
  {
    "bug_location": "storage/txn/scheduler",
    "severity": 3,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Unexpected channel drop during transaction prewrite operation, likely due to asynchronous task handling failure",
    "issue_number": 15382,
    "title": "[Dynamic Regions] response channel is unexpectedly dropped"
  },
  {
    "bug_location": "RaftStoreV2/Snapshot",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Excessive file descriptor usage during dynamic region snapshot processing, leading to system file descriptor exhaustion",
    "issue_number": 15383,
    "title": "[Dynamic Regions] Jepsen test reported \"too many open files\""
  },
  {
    "bug_location": "Authentication/AzureAD",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Network"
    ],
    "root_cause": "Error handling mechanism suppresses detailed error information during Azure AD token retrieval, masking the underlying authentication failure",
    "issue_number": 15384,
    "title": "the error, which is occurred when try to get azure ad token, is hidden by upper error."
  },
  {
    "bug_location": "Region Merge Component",
    "severity": 5,
    "categories": [
      "Transaction",
      "Storage",
      "Performance"
    ],
    "root_cause": "Resolved timestamp blocking during dynamic region merge, preventing point-in-time recovery (PITR) and flashback operations",
    "issue_number": 15386,
    "title": "[Dynamic Regions] resolved ts blocked after region merge"
  },
  {
    "bug_location": "Region Split Configuration",
    "severity": 3,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Dynamic configuration update mechanism does not support online modification of load base split parameters",
    "issue_number": 15403,
    "title": "[Dynamic Regions] Load base split config can't be updated online"
  },
  {
    "bug_location": "CDC (Change Data Capture) Resolver",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Memory consumption by resolver during large transaction processing exceeds available memory",
    "issue_number": 15412,
    "title": "CDC resolver may cause TiKV OOM"
  },
  {
    "bug_location": "Monitoring/Metrics Collection",
    "severity": 2,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Instrumentation or metric collection failure in thread context switch tracking",
    "issue_number": 15413,
    "title": "No data in Thread Voluntary/Nonvoluntary Context Switches panel"
  },
  {
    "bug_location": "backup-stream resolver",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Memory consumption in resolver during large transaction processing exceeds available memory limits",
    "issue_number": 15414,
    "title": "PiTR resolver may cause TiKV OOM"
  },
  {
    "bug_location": "Transaction Layer",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Inconsistent transaction ordering and visibility across concurrent transactions, causing potential read-write conflicts and violating transaction isolation guarantees",
    "issue_number": 15420,
    "title": "[Dynamic Regions]  Jepsen reported contradiction "
  },
  {
    "bug_location": "RocksDB Configuration Management",
    "severity": 2,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Incorrect dynamic thread pool configuration update mechanism in background job management",
    "issue_number": 15424,
    "title": "can't dynamic adjust rocksdb background_compaction threads"
  },
  {
    "bug_location": "PD Client/Cluster Management",
    "severity": 4,
    "categories": [
      "Network",
      "LoadBalance",
      "Config"
    ],
    "root_cause": "Inefficient leader election recovery mechanism causing excessive RPC requests during PD leader transition",
    "issue_number": 15428,
    "title": "Too many MemberList request after PD lost leader for a while"
  },
  {
    "bug_location": "Block Cache Memory Management",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Memory leak or inefficient memory release mechanism in block cache management when region count decreases",
    "issue_number": 15430,
    "title": "TiKV's memory usage keeps increases even when region count decreases and block cache cap reaches. "
  },
  {
    "bug_location": "raftstore/config.rs",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Config",
      "CodeBug"
    ],
    "root_cause": "Configuration parsing failure during version upgrade, likely due to incompatible configuration handling between v5.2.4 and master branch",
    "issue_number": 15438,
    "title": "v5.2.4 upgrade to master with tiup\uff0ctikv report check config fail"
  },
  {
    "bug_location": "raftstore/v2",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Peer ID collision and incorrect region merge handling causing leader election failure and region state corruption",
    "issue_number": 15439,
    "title": "raftstore v2: A region's peer may be stolen by another region and cause destruction of its Leader peer"
  },
  {
    "bug_location": "Storage Engine / Titan KV Engine",
    "severity": 5,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Blob file corruption during suspend/resume cluster operation, causing multiple deletion of the same blob file which triggers an unrecoverable storage engine error",
    "issue_number": 15454,
    "title": "tikv occur `CrashLoopBackOff` after lighning import data, suspend cluster then cancel suspend cluster"
  },
  {
    "bug_location": "resolved_ts/resolver.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Memory leak in resolver hashmap not clearing entries after lock removal",
    "issue_number": 15458,
    "title": "Resolver memory is not reclaimed and may cause OOM"
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Blob file corruption during multiple deletion attempts, causing storage engine initialization failure",
    "issue_number": 15459,
    "title": "titan: tikv start fails with error"
  },
  {
    "bug_location": "Import/SST Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Upgrade"
    ],
    "root_cause": "Incomplete SST file cleanup mechanism after backup and restore operation, causing unnecessary storage overhead and potential performance degradation during cluster restart",
    "issue_number": 15461,
    "title": "[Dynamic Regions] many sst files left in the import directory after br restore "
  },
  {
    "bug_location": "raft-engine",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Performance jitter caused by log recycling mechanism in raft-engine when append-recycle files are exhausted, leading to increased write latency",
    "issue_number": 15462,
    "title": "[Dynamic Regions] periodical performance jitter due to raft-engine log recycle"
  },
  {
    "bug_location": "Region Read Path",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Race condition between read operations and region destruction, causing unnecessary back-off delays during witness node interactions",
    "issue_number": 15468,
    "title": "possible slow query when reading racing with region destroy"
  },
  {
    "bug_location": "IO Performance Layer",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "LoadBalance"
    ],
    "root_cause": "Performance degradation during simulated IO latency injection, indicating potential sensitivity in dynamic region handling under network stress conditions",
    "issue_number": 15474,
    "title": "[Dynamic Regions] qps drops more than 90% during injection one of tikv io delay 10ms "
  },
  {
    "bug_location": "regexp library",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Inconsistent regular expression implementation between TiKV and TiDB, causing unexpected matching behavior",
    "issue_number": 15478,
    "title": "[regexp lib] TiKV's regexp is inconsistent with that of Tidb"
  },
  {
    "bug_location": "raftstore v2",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Inconsistent index handling during multi-version concurrency control (MVCC) snapshot reads, causing divergent row count results between table scan and index scan",
    "issue_number": 15480,
    "title": "[Dynamic Regions] raftstore v2: TiKV has inconsistent index"
  },
  {
    "bug_location": "sst_importer",
    "severity": 4,
    "categories": [
      "Storage",
      "Upgrade",
      "CodeBug"
    ],
    "root_cause": "Potential race condition or improper file handling during repeated restoration process causing data inconsistency",
    "issue_number": 15483,
    "title": "sst_importer: file may not be rewritten properly while repeating restoration"
  },
  {
    "bug_location": "online_config component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Type conversion error when setting write-buffer-limit, causing unexpected panic during configuration update",
    "issue_number": 15503,
    "title": "panic when set write-buffer-limit "
  },
  {
    "bug_location": "raftstore/store/util.rs",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Inaccurate min resolved timestamp calculation due to uninitialized peers and new peer creation, potentially compromising transaction atomicity during distributed snapshot operations",
    "issue_number": 15506,
    "title": "Store min resolved ts may not be accurate"
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 3,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incomplete blob file garbage collection during Titan fallback mode, preventing full cleanup of live blob files despite configuration settings",
    "issue_number": 15507,
    "title": "[Titan] the number of live blob file can not be reduced to 0 when enable fallback mode"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of multi-value index data type restoration logic",
    "issue_number": 15531,
    "title": "The `need_restored_data` for multi value index should return false"
  },
  {
    "bug_location": "RocksDB Event Listener / Storage Engine",
    "severity": 5,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Synchronization issue with sealed sequence number during memtable flush, causing unexpected panic in concurrent write operations",
    "issue_number": 15534,
    "title": "[Dynamic Regions] tikv panic with error \u201csealed seqno has been flushed lock 469114 11526377 <= 11526405\\\"] [backtrace=\\\"   0: tikv_util::set_panic_hook::{{closure}}\u201d"
  },
  {
    "bug_location": "TiDB Schema Update Mechanism",
    "severity": 4,
    "categories": [
      "Transaction",
      "Network",
      "Performance"
    ],
    "root_cause": "Schema lease update failure preventing batch insert transactions from completing, likely due to network or synchronization issues between TiDB and TiKV components",
    "issue_number": 15535,
    "title": "[Dynamic Regions] Sysbench prepare fails with error \"schema failed to update in 1 lease\""
  },
  {
    "bug_location": "raftstore/peer",
    "severity": 3,
    "categories": [
      "LoadBalance",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Follower peers cannot propose region split commands, requiring leader-only proposal handling",
    "issue_number": 15539,
    "title": "Load base split cannot work with stale read if all reads occurs on the follower"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Dynamic region unavailability during large data import causing epoch mismatch and region access failures",
    "issue_number": 15542,
    "title": "[Dynamic Region] TiDB can't start for region unavailable"
  },
  {
    "bug_location": "Backup and Restore (BR) component",
    "severity": 4,
    "categories": [
      "Storage",
      "Network",
      "Performance"
    ],
    "root_cause": "Concurrent multipart upload conflict during large-scale backup to object storage (KS3), causing upload initialization errors",
    "issue_number": 15555,
    "title": "Br failed backup to ks3 for The operation generates conflict uploadId, try to initiate multipart upload later"
  },
  {
    "bug_location": "Transaction/Endpoint",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Persistent key locking issue causing transaction deadlock and store hanging during concurrent write operations",
    "issue_number": 15561,
    "title": "[Dynamic Regions] Tikv hang for Key is locked"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "Current implementation does not respect max_execution_duration_ms parameter for coprocessor tasks, causing potential timeout issues during workload processing",
    "issue_number": 15565,
    "title": "Coprocessor task use the deadline specified in kvrpcpb::Context.max_execution_duration_ms"
  },
  {
    "bug_location": "Region Request Handler",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Region epoch inconsistency causing replica unavailability during large table analyze operation, potentially due to dynamic region configuration and high data volume",
    "issue_number": 15566,
    "title": "[Dynamic Regions] Analyze table failed: [tikv:9005] Region is unavailable"
  },
  {
    "bug_location": "Collation/Character Set Handling",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of UTF8MB4 binary collation during data restoration, causing potential data integrity or compatibility issues",
    "issue_number": 15571,
    "title": "`utf8mb4_0900_bin` shouldn't try to read RestoredData"
  },
  {
    "bug_location": "Dynamic Region Analysis",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Context timeout during large dataset auto-analyze operation, likely due to excessive data processing time exceeding default timeout limits",
    "issue_number": 15575,
    "title": "[Dynamic Regions] Auto Analyze failed for context deadline exceeded"
  },
  {
    "bug_location": "Build System",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incorrect Cargo dependency reference pointing to a personal repository instead of an official/stable source",
    "issue_number": 15579,
    "title": "Cargo reference should not point to personal repo. "
  },
  {
    "bug_location": "PD Unsafe Recovery Controller",
    "severity": 4,
    "categories": [
      "Transaction",
      "Upgrade",
      "LoadBalance"
    ],
    "root_cause": "Timeout during multi-stage unsafe recovery process when switching between data centers, preventing successful store removal and cluster recovery",
    "issue_number": 15580,
    "title": "[dr-autosync] online recover time out after switching to backup cluster in sync_recover mode"
  },
  {
    "bug_location": "Log Backup Stream",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential race condition during region merge/split causing file access failure in log backup stream",
    "issue_number": 15602,
    "title": "Log Backup stopped due to \"No such file or directory\" error"
  },
  {
    "bug_location": "Store/Raft Component",
    "severity": 3,
    "categories": [
      "Network",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Potential race condition or communication failure during store disconnection with hibernate mode enabled",
    "issue_number": 15607,
    "title": "freaky test case test_store_disconnect_with_hibernate "
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Network partition causing persistent region performance degradation and QPS reduction due to potential leader election or data replication instability",
    "issue_number": 15613,
    "title": "[Dynamic Regions] Qps continuously dropped by 10% for 20 minutes in the scenario tikv network partition 50m"
  },
  {
    "bug_location": "raftstore-v2/worker/tablet",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Assertion failure in tablet destruction logic, indicating potential race condition or incorrect file path handling during tablet removal process",
    "issue_number": 15615,
    "title": "unstable test test_destroy_missing"
  },
  {
    "bug_location": "Backup and Restore (PITR) Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Network"
    ],
    "root_cause": "Excessive file read rate overwhelming S3 API rate limits during log backup restoration, causing potential throttling and restore failure",
    "issue_number": 15620,
    "title": "PiTR restore failed due to reading S3 files too frequently"
  },
  {
    "bug_location": "third-party dependencies",
    "severity": 3,
    "categories": [
      "Security",
      "Upgrade"
    ],
    "root_cause": "Outdated third-party crate versions with known security vulnerabilities",
    "issue_number": 15621,
    "title": "Update third-party crates to fix insecure issues."
  },
  {
    "bug_location": "Dynamic Regions Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Resolved timestamp advancement mechanism stalled in large-scale distributed environment with high data volume",
    "issue_number": 15623,
    "title": "[Dynamic Regions] resolved ts can't  advance for long time"
  },
  {
    "bug_location": "Unsafe Recovery Controller",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Upgrade"
    ],
    "root_cause": "Timeout during cluster recovery process in sync_recover mode, preventing successful leader election and region recovery",
    "issue_number": 15629,
    "title": "[dr-autosync] online recover time out after switching to backup cluster in sync_recover mode"
  },
  {
    "bug_location": "storage/engine/partitioned-raft-kv",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Performance regression in write-heavy workloads with partitioned-raft-kv engine, likely due to changes in region splitting or write path optimization in nightly version",
    "issue_number": 15630,
    "title": "[Dynamic Regions] Compared with 7.3.0, write-heavy workloads have 7.6% - 31% performance regression"
  },
  {
    "bug_location": "tidb_query_datatype/decimal.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect implementation of decimal division and modulo operations for zero numerator",
    "issue_number": 15631,
    "title": "The result of `0 / decimal` and `0 % decimal` is wrong."
  },
  {
    "bug_location": "raftstore-v2/operation/command/admin/merge",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Failed to retrieve merge entries due to potential log compaction or data loss during region merge operation",
    "issue_number": 15633,
    "title": "[Dynamic Regions] failed to get merge entires"
  },
  {
    "bug_location": "raftstore-v2/operation/life.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Inconsistent region peer state where a peer is reported as non-existent despite having a valid local state region",
    "issue_number": 15634,
    "title": "[Dynamic Regions] peer doesn't exist but has valid local state region"
  },
  {
    "bug_location": "Region Splitting Mechanism",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Dynamic region splitting algorithm fails to trigger bucket splits under high write load, causing inefficient data distribution and potential performance bottlenecks",
    "issue_number": 15636,
    "title": "[Dynamic Regions]: buckets maybe not split even if there are many writes."
  },
  {
    "bug_location": "dr-autosync recovery mechanism",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "Upgrade"
    ],
    "root_cause": "Data inconsistency during multi-datacenter recovery and flashback operation, causing index and record misalignment",
    "issue_number": 15639,
    "title": "[dr-autosync] after do tikv flashback to  min-resolved-ts, admin check table hit  ERROR 8223 (HY000): data inconsistency in table "
  },
  {
    "bug_location": "raftstore-v2/region_merge",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Race condition during dynamic region merging process causing unexpected state transition in merge state machine",
    "issue_number": 15642,
    "title": "[Dynamic Regions] tikv panic at \"state: Merging region\""
  },
  {
    "bug_location": "raftstore-v2/operation/life.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Followers do not report GcPeerResponse after commit merge, preventing cleanup of merged_records",
    "issue_number": 15644,
    "title": "[Dynamic Regions] `merged_records` will never be cleaned up"
  },
  {
    "bug_location": "TiKV Store Status Metadata",
    "severity": 3,
    "categories": [
      "Config",
      "Storage"
    ],
    "root_cause": "Stale label metadata not automatically cleaned when configuration key is deleted",
    "issue_number": 15648,
    "title": "information_schema.tikv_store_status  bug"
  },
  {
    "bug_location": "RaftStore merge process",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Network"
    ],
    "root_cause": "Lack of retry mechanism for MsgAvailabilityRequest during region merge, causing merge process to stall when a critical message is dropped",
    "issue_number": 15651,
    "title": "[Dynamic Regions] merge is blocked if MsgAvailabilityRequest is dropped"
  },
  {
    "bug_location": "TableScan/ExecutionEngine",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Performance regression in table full scan query execution, likely due to inefficient query plan optimization or index selection changes between commits",
    "issue_number": 15653,
    "title": "TableFullScan gets slower resulting in a 5% performance regression in TPCDS "
  },
  {
    "bug_location": "endpoint.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Incorrect handling of background control request metadata during table analysis, leading to empty request source identification",
    "issue_number": 15663,
    "title": "Background control cannot get the stats type"
  },
  {
    "bug_location": "information_schema.table_storage_stats",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect calculation of PEER_COUNT column value, potentially related to replication configuration mismatch",
    "issue_number": 15665,
    "title": "infromation_schema.table_storage_stats  bug"
  },
  {
    "bug_location": "RaftStore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Leader continues sending GC peer requests to tombstone stores instead of directly removing peers",
    "issue_number": 15669,
    "title": "[Dynamic Regions] Leader keeps sending GcPeerRequests to peers on tombstone store"
  },
  {
    "bug_location": "Region Management / Index Lookup",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Data inconsistency during dynamic region splitting or merging, causing mismatched index and table handle references",
    "issue_number": 15670,
    "title": "[Dynamic Regions] indexLookup found data inconsistency"
  },
  {
    "bug_location": "Dynamic Regions Component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Resolved timestamp advancement failure in large-scale distributed cluster with high data volume and complex workload",
    "issue_number": 15672,
    "title": "[Dynamic Regions] resolved ts can't advance in big cluster scenario"
  },
  {
    "bug_location": "Build System",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Compilation failure during release build process, likely due to incompatible build configuration or dependency issue",
    "issue_number": 15673,
    "title": "fail to build fail_release"
  },
  {
    "bug_location": "TiCDC ResolvedTs component",
    "severity": 3,
    "categories": [
      "Network",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Network partition disrupts ResolvedTs synchronization mechanism, causing lag in timestamp resolution across distributed nodes",
    "issue_number": 15679,
    "title": "ResolvedTs lag increases after a TiKV store is partitioned"
  },
  {
    "bug_location": "raftstore-v2/operation/command/admin/merge",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Non-deterministic merge commit and rollback process causing race conditions during region merge operations, specifically when proposal failures occur during transient errors",
    "issue_number": 15682,
    "title": "[Dynamic Regions] panic with \"region corrupted\""
  },
  {
    "bug_location": "Region Split Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect handling of region size metadata during region splitting, where size is set to None instead of zero",
    "issue_number": 15696,
    "title": "After spliting regions, the size of the new regions are not updated to zero but None."
  },
  {
    "bug_location": "raftstore/channel_buffer",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Unbounded channel buffer allocation across multiple peers leading to potential memory exhaustion",
    "issue_number": 15701,
    "title": "The total buffer of peers' channels is essentially unlimited and may cause OOM  "
  },
  {
    "bug_location": "Backup and Restore (BR) component",
    "severity": 3,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "File access/permission issue during SST file restoration, likely related to file path handling or temporary directory permissions when running as root user",
    "issue_number": 15705,
    "title": "Problem cannot read sst file when running br restore"
  },
  {
    "bug_location": "TiKV Flashback Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Potential race condition or state management issue during multiple flashback operations causing retry failures and hanging",
    "issue_number": 15712,
    "title": "[dr-autosync] flashback hang for more than 30min on second call"
  },
  {
    "bug_location": "backup-restore",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "Performance"
    ],
    "root_cause": "Region merge operation preventing read index, causing snapshot retrieval failure during point-in-time recovery (PITR)",
    "issue_number": 15714,
    "title": "br: pitr-task stuck and log shows \"failed to get initial snapshot: failed to get the snapshot\""
  },
  {
    "bug_location": "raft_engine",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Improper space holder file management causing disk space recovery failure",
    "issue_number": 15718,
    "title": "Need a standalone space holder file for raft engine"
  },
  {
    "bug_location": "Region Cache / Client-Go",
    "severity": 5,
    "categories": [
      "Performance",
      "LoadBalance",
      "Upgrade"
    ],
    "root_cause": "Excessive GetRegionByID requests triggered by bucket update mechanism when region size exceeds bucket size threshold",
    "issue_number": 15719,
    "title": "Bucket update triggers too many GetRegionByID requests and make PD overwhelmed "
  },
  {
    "bug_location": "store.rs",
    "severity": 5,
    "categories": [
      "Network",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "PD worker channel overload causing communication failure between TiKV stores, potentially due to high write load on large dataset",
    "issue_number": 15721,
    "title": "[Dynamic Regions] some tikv stores status are always down"
  },
  {
    "bug_location": "TiFlash Replica Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Upgrade"
    ],
    "root_cause": "Potential synchronization or metadata tracking issue during TiFlash data restoration process preventing replica availability status from being correctly updated",
    "issue_number": 15724,
    "title": "[Dynamic Regions] AVAILABLE is always 0 when query information_schema.tiflash_replica after br restore tiflash data with 2 replicas "
  },
  {
    "bug_location": "raftstore-v2/operation/command/admin/merge",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Potential race condition or improper error handling during region merge operation under network loss conditions",
    "issue_number": 15725,
    "title": "[Dynamic Regions] tikv panic after injection network loss for one of tikv repeatedly"
  },
  {
    "bug_location": "Storage/Coprocessor",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Performance regression in dynamic region handling or storage engine configuration between v7.3.0 and nightly versions",
    "issue_number": 15735,
    "title": "[Dynamic Regions] Compared with v7.3.0, TPCC has a 8% ~ 29% performance regression"
  },
  {
    "bug_location": "Storage/RocksDB Configuration",
    "severity": 4,
    "categories": [
      "Performance",
      "Config",
      "Storage"
    ],
    "root_cause": "Increased write_buffer_size and write_buffer_limit for lockcf negatively impacts mixed insert/update and scan workloads by introducing performance overhead",
    "issue_number": 15736,
    "title": "[Dynamic Regions] Compared with v7.3.0, workloads with insert/update + scan operations have a 32% ~ 77% performance regression in v7.4.0"
  },
  {
    "bug_location": "raftstore/log_unstable",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Inconsistent Raft log state during region merge when restoring from EBS snapshot, causing out-of-bounds slice access during log catch-up",
    "issue_number": 15739,
    "title": "TiKV panic due to catch up logs during ebs restore"
  },
  {
    "bug_location": "CDC/ResolvedTS",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Unstable resolved timestamp tracking during large row operations, causing significant CDC lag and timestamp tracking inconsistencies",
    "issue_number": 15741,
    "title": "TiKV resolved ts not stable when running workload with >1MB large rows"
  },
  {
    "bug_location": "backup-restore component",
    "severity": 4,
    "categories": [
      "Network",
      "Config",
      "Performance"
    ],
    "root_cause": "Lack of retry mechanism when obtaining Azure AD authentication token fails",
    "issue_number": 15744,
    "title": "need retry logic when failed to get azure ad token from azure blob server"
  },
  {
    "bug_location": "Dynamic Regions Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Inability to advance resolved timestamp and garbage collection safepoint in large-scale distributed TiKV cluster with dynamic region configuration",
    "issue_number": 15745,
    "title": "[Dynamic Regions] resolved ts and gc safepoint can't advance"
  },
  {
    "bug_location": "raftstore-v2/query_module",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Unexpected handling of stale read requests in RaftQuery message processing",
    "issue_number": 15748,
    "title": "[Dynamic Regions] stale read with raft-kv2 returns unexpected region error occasionally"
  },
  {
    "bug_location": "raft_log_engine",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Corruption in file sequence number range during disk operations, likely caused by unexpected disk state or incomplete write",
    "issue_number": 15751,
    "title": "TiKV panic for Corruption: file seqno out of range"
  },
  {
    "bug_location": "Raft message metrics component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Metrics counter for disk_full message drops is incorrectly implemented, only tracking rare MsgTimeoutNow message type instead of comprehensively capturing disk full conditions",
    "issue_number": 15753,
    "title": "dropped raft message metrics on disk_full type is misleading"
  },
  {
    "bug_location": "Raft Write Flow",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Insufficient disk space management in Raft write operations, allowing minor writes when storage is critically low",
    "issue_number": 15754,
    "title": "when disk almost full happens, there's still minor raft write flow which could further consume up tikv free space"
  },
  {
    "bug_location": "raft-engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Raft log rewrite process does not handle disk space exhaustion gracefully, causing process crash instead of controlled degradation",
    "issue_number": 15755,
    "title": "raft-engine should not start rewrite when there's no available space"
  },
  {
    "bug_location": "Profiling/Debug Module",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Concurrent profiling modes are not properly handled, causing mutual exclusion of profiling operations",
    "issue_number": 15760,
    "title": "Can't perform CPU profiling when heap profiling is activated"
  },
  {
    "bug_location": "Placement Rules / Distributed Recovery",
    "severity": 4,
    "categories": [
      "Transaction",
      "LoadBalance",
      "Storage"
    ],
    "root_cause": "Inconsistent data replication during multi-zone failure scenario, causing partial data loss or unavailability",
    "issue_number": 15762,
    "title": "[dr-autosync] After down 2 zone, got of total bank account  does not meet expectations"
  },
  {
    "bug_location": "raftstore-v2/merge_commit",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Sibling region check failure during region merge operation, likely due to inconsistent region epoch or peer configuration",
    "issue_number": 15764,
    "title": "[Dynamic Regions] sibling check failed during commit merge"
  },
  {
    "bug_location": "Encryption Key Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Security",
      "CodeBug"
    ],
    "root_cause": "Repeated TiKV node kills causing encryption key retrieval failure during node restart",
    "issue_number": 15768,
    "title": "[Dynamic Regions] tikv panic with fatal error \"Corruption: Encryption key manager get file failure: key not found\" after kill one tikv repeatedly"
  },
  {
    "bug_location": "raft-rs component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Unbounded memory consumption during log scanning before election, caused by scanning unapplied log entries without pagination",
    "issue_number": 15770,
    "title": "TiKV OOM causes by reading raft conf changes during raft.hup"
  },
  {
    "bug_location": "RaftEngine Configuration",
    "severity": 3,
    "categories": [
      "Upgrade",
      "Config"
    ],
    "root_cause": "Configuration migration failure during TiKV version upgrade, where raft-engine directory is not automatically preserved from previous configuration",
    "issue_number": 15771,
    "title": "Can't set raft-engine.dir as the same as raftstore.raftdb-path when upgrading"
  },
  {
    "bug_location": "Replication/Sync Recovery",
    "severity": 5,
    "categories": [
      "Transaction",
      "LoadBalance",
      "Config"
    ],
    "root_cause": "Race condition during complex replication mode transition between dr-auto-sync and majority, causing sync recovery process to hang when reconfiguring placement rules and node states",
    "issue_number": 15784,
    "title": "[dr-autosync] hang in sync_recover when change from majority to dr-auto-sync"
  },
  {
    "bug_location": "Lightning SST Importer Client",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Upgrade",
      "Storage"
    ],
    "root_cause": "Potential integration test compatibility issue during merge process affecting SST import functionality",
    "issue_number": 15791,
    "title": "lightning (sst_importer client) integration tests fail after merge #15013"
  },
  {
    "bug_location": "GC Worker / Compaction Filter",
    "severity": 3,
    "categories": [
      "Transaction",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Race condition in metric collection where Prometheus scrapes compaction metrics before compaction filter metrics, triggering a false 'GC not working' alert when single-version data exists",
    "issue_number": 15796,
    "title": "Avoid false \"GC can not work\" alert when there is only one version of all the data."
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Inconsistent region replica availability during placement rule reconfiguration, causing region unavailability when some TiKV nodes are down",
    "issue_number": 15799,
    "title": "Change placement rule from 5 replicas to 3 replicas with some tikv down, tpcc report error: [tikv:9005]Region is unavailable"
  },
  {
    "bug_location": "Raft Log Append Module",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Memory pressure from excessive raft log caching caused by TiFlash node disk exhaustion, leading to log append rejection",
    "issue_number": 15800,
    "title": "TiKV reports ServerIsBusy error because it can not append raft log"
  },
  {
    "bug_location": "TiDB Query Datatype Codec",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Index out of bounds error during enum datum decoding, likely caused by data corruption or incompatible schema during backup/restore process",
    "issue_number": 15801,
    "title": "Panic every 5 minute after restore from backup"
  },
  {
    "bug_location": "backup-restore",
    "severity": 5,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Memory management failure when processing large row values, causing out-of-memory condition during high-throughput workload",
    "issue_number": 15805,
    "title": "tikv oom when running large-row-values workload"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "Transaction",
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Commit log failure during multi-datacenter region state transition when peers are in joint consensus state and some stores are down",
    "issue_number": 15817,
    "title": "Commit can't advance for region in joint state when dr auto sync is enabled"
  },
  {
    "bug_location": "Type Casting Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of byte-to-time type conversion during CAST and REVERSE operations",
    "issue_number": 15820,
    "title": "Cast bytes as time gets encoding failed"
  },
  {
    "bug_location": "Monitoring/Grafana Dashboard",
    "severity": 3,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Incorrect metric labeling or dashboard configuration in cloud environment causing misrepresentation of scheduler command metrics",
    "issue_number": 15832,
    "title": "Scheduler command variables of grafana are not correct on cloud "
  },
  {
    "bug_location": "Transaction Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Config",
      "Performance"
    ],
    "root_cause": "Configuration parameter modification mechanism does not properly propagate dynamic changes to resolved timestamp advance interval",
    "issue_number": 15835,
    "title": "dynamic modify `resolved-ts.advance-ts-interval` from 5s to 2s is not work"
  },
  {
    "bug_location": "GC (Garbage Collection) Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Insufficient disk space preventing garbage collection process from executing cleanup operations",
    "issue_number": 15846,
    "title": "gc can not work when disk capacity is near full"
  },
  {
    "bug_location": "Build System",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incompatible AFL dependency configuration in DEBUG build mode on MacOS",
    "issue_number": 15847,
    "title": "Build fails due to `afl` dependency in DEBUG mode"
  },
  {
    "bug_location": "Release Management System",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Unspecified release process failure preventing successful build/packaging",
    "issue_number": 15851,
    "title": "release failed"
  },
  {
    "bug_location": "worker_pool",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Incorrect metric type usage in monitoring expression, applying rate() on a gauge metric which is semantically incorrect",
    "issue_number": 15859,
    "title": "tikv's worker pending tasks under task result is not correct"
  },
  {
    "bug_location": "RaftStore/Region Split Component",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Race condition or synchronization issue in region split test case causing inconsistent bucket range generation",
    "issue_number": 15862,
    "title": "flaky test case: test_gen_split_check_bucket_ranges"
  },
  {
    "bug_location": "region split mechanism",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Split proposal mechanism failure preventing automatic region division when size exceeds threshold",
    "issue_number": 15863,
    "title": "A 12GB region is not splitted automatically, cause TiSpark's failure due to too large coproc response. "
  },
  {
    "bug_location": "RocksDB Block Cache / Raft Log Replay",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Performance degradation during TiKV restart due to block cache warm-up and pending Raft log application causing temporary high IO burst and reduced query processing capability",
    "issue_number": 15864,
    "title": "Write QPS drops to 0 after TiKV Restart"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Blocking semaphore acquisition preventing timely task cancellation during region split or merge events",
    "issue_number": 15866,
    "title": "cdc incremental scan tasks can't be canceld by region split or merge correctly"
  },
  {
    "bug_location": "Leader Transfer Component",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance",
      "Network"
    ],
    "root_cause": "Inefficient leader transfer mechanism under high IO latency conditions, causing prolonged leadership transition and performance degradation",
    "issue_number": 15875,
    "title": "transfer leader too slow when inject one of tikv io delay with \u201cevict-slow-trend-scheduler\u201d or \u201cevict-slow-store-scheduler\u201d"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect string comparison handling of different collation types in field function",
    "issue_number": 15878,
    "title": "`field` function doesn't consider the collation"
  },
  {
    "bug_location": "endpoint.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Overly verbose logging at inappropriate log level for expected transactional lock scenarios",
    "issue_number": 15881,
    "title": "server: reduce unnecesary logs"
  },
  {
    "bug_location": "Transaction/Encoding",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Potential key encoding corruption during transaction operations, causing invalid byte sequences in key generation or scanning process",
    "issue_number": 15896,
    "title": "We have some bad encoded-keys like: aaaa FF bbbb FF cccc 06 dd00 FE"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Storage"
    ],
    "root_cause": "Excessive snapshot generation when resuming changefeed with a historical timestamp, likely due to inefficient MVCC data traversal and snapshot recovery mechanism",
    "issue_number": 15910,
    "title": "Number of snapshots  increases tremendously and TiKV OOM when resuming changefeed with a ts 6 days before."
  },
  {
    "bug_location": "TiKV Lightning Import Component",
    "severity": 5,
    "categories": [
      "Storage",
      "Upgrade",
      "CodeBug"
    ],
    "root_cause": "SST file persistence and recovery mechanism failure during TiKV node restart, causing imported data files to be lost or unrecoverable",
    "issue_number": 15912,
    "title": "lightning Import sst lost after tikv restart"
  },
  {
    "bug_location": "Transaction Resolver",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Storage"
    ],
    "root_cause": "Resolved timestamp advancement blocked during batch insert operations with wide tables, likely due to transaction metadata management limitations in high-volume write scenarios",
    "issue_number": 15916,
    "title": "resolved ts can't advance in the batch insert wide table scenario"
  },
  {
    "bug_location": "Scheduler/ResourceGroup",
    "severity": 3,
    "categories": [
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Inefficient task scheduling across resource groups with uncontrolled priority queue resource consumption",
    "issue_number": 15917,
    "title": "Priority Scheduling Optimization"
  },
  {
    "bug_location": "raftstore/pd_worker",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Incomplete implementation of peer removal logic during region merge, causing stale peers to persist indefinitely",
    "issue_number": 15919,
    "title": "Stale peers are left forever after region merge"
  },
  {
    "bug_location": "TiKV-PD Connection Module",
    "severity": 4,
    "categories": [
      "Network",
      "Config"
    ],
    "root_cause": "Potential network configuration or authentication issue preventing TiKV from establishing connection to PD, despite network connectivity being functional",
    "issue_number": 15920,
    "title": "tikv connecto to pd failed, but pd_ctl is ok"
  },
  {
    "bug_location": "Transaction Processing",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Performance regression specific to ARM64 architecture, likely introduced by commit cb27f2 affecting transaction processing efficiency",
    "issue_number": 15926,
    "title": "cb27f2 causes 6%- 10% QPS regression in CH benchmark on ARM64"
  },
  {
    "bug_location": "Follower Read Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Disk capacity exhaustion causing region read errors during stale read operations with closest-replicas strategy",
    "issue_number": 15932,
    "title": "tikv always report \u201cRegion error (will back off and retry)\u201d when disk capacity is near full with follower read"
  },
  {
    "bug_location": "Diagnostics/Profiling",
    "severity": 3,
    "categories": [
      "Performance",
      "Unknown"
    ],
    "root_cause": "Lack of built-in memory profiling and diagnostic tools for tracking heap memory usage in TiKV",
    "issue_number": 15958,
    "title": "Can't investigate OOM issues easily"
  },
  {
    "bug_location": "Raft Peer Management",
    "severity": 3,
    "categories": [
      "CodeBug",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Inconsistent peer state tracking between TiKV and PD, causing persistent pending peer condition without actual synchronization issue",
    "issue_number": 15969,
    "title": "high pending peer duration"
  },
  {
    "bug_location": "Titan storage metrics",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Inaccurate size calculation for Titan storage metrics, potentially caused by incorrect metric tracking or aggregation logic",
    "issue_number": 15971,
    "title": "Titan store size metric is not accurate"
  },
  {
    "bug_location": "snapshot_transfer_module",
    "severity": 3,
    "categories": [
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Snapshot transfer limit reached, causing snapshot generation and transfer failures when multiple regions are being balanced simultaneously",
    "issue_number": 15972,
    "title": "TiKV should delay request snapshot if it exceeds receiving snapshot  limit"
  },
  {
    "bug_location": "engine_rocks/cf_options.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Invalid configuration validation for Titan blob run mode, causing incorrect argument handling in TiKV configuration modification",
    "issue_number": 15978,
    "title": "tikv-ctl modify titan config Invalid argument error"
  },
  {
    "bug_location": "Makefile build scripts",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incorrect usage of `basename` command without providing a required argument in shell script/Makefile",
    "issue_number": 15980,
    "title": "make: `basename` prints errors."
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Block-level CRC checksum verification failure during SST file ingestion, causing unhandled corruption detection leading to system panic",
    "issue_number": 15986,
    "title": "Isolate the damaged ssts and regions and avoid tikv panic when block crc mismatches "
  },
  {
    "bug_location": "Titan configuration component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Invalid configuration argument for blob_run_mode, indicating a validation or configuration parsing error in the Titan storage engine configuration handling",
    "issue_number": 15987,
    "title": "fail to update titan config by tikv-ctl or tikv status API"
  },
  {
    "bug_location": "Transaction Layer",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Uncommitted transaction memory not being properly released, causing memory leak during high concurrency scenarios",
    "issue_number": 15992,
    "title": "TiKV OOM"
  },
  {
    "bug_location": "Performance/Transaction Processing",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Performance regression in transaction processing, likely due to code changes in commit 076b79c54b9e7656f9119865d4ed75bec3bd58d6 affecting read/write and insert workloads under high concurrency",
    "issue_number": 15994,
    "title": "release 7.5.0 Compared with the previous commit, oltp_read_write has a 3.65% regression under 200 concurrency and oltp_insert has a 1.5+% regression under 100 concurrency"
  },
  {
    "bug_location": "Titan storage engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Config"
    ],
    "root_cause": "Inconsistent compression rates across TiKV nodes when Titan is enabled, leading to uneven data distribution and storage size variations",
    "issue_number": 15996,
    "title": "TiKV store size among TiKV nodes are very different after Titan is enabled"
  },
  {
    "bug_location": "Store Leader Check Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Potential cascading performance degradation during leader check process causing unexpected resolve timestamp delays across multiple stores",
    "issue_number": 15999,
    "title": "1 store check_leader slow may affect may store to advance resolve_ts"
  },
  {
    "bug_location": "sst_importer",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Incomplete cleanup of empty SST files during snapshot restoration, leading to unnecessary file accumulation",
    "issue_number": 16005,
    "title": "sst_importer needs to clean the sst if no kv need to be saved and rewritten"
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Memory leak or inefficient garbage collection in Titan storage engine when enabled, causing unbounded storage growth during long-running workloads",
    "issue_number": 16006,
    "title": "TiKV store size can't converges stablly after titan enable"
  },
  {
    "bug_location": "Performance Scoring Component",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Incorrect performance metric calculation causing false positive slow score evaluation under high workload conditions",
    "issue_number": 16011,
    "title": "slow score sometimes has false positive which prevent its adoption in production environment"
  },
  {
    "bug_location": "Performance Layer",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Performance regression between TiKV commits, likely due to code changes affecting workload efficiency",
    "issue_number": 16015,
    "title": "Daily run , compared with the previous tikv commit  , commit 7be1b1 leads ycsb workloada/workloadc has a 3% ~ 5% performance regression under 200 concurrency"
  },
  {
    "bug_location": "Raft Apply Queue",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Write hotspot causing excessive apply wait latency in Raft log processing, potentially due to queue contention or inefficient scheduling of log application",
    "issue_number": 16016,
    "title": "High apply wait tail latency  "
  },
  {
    "bug_location": "raftstore/store",
    "severity": 3,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "File system access error during SST file cleanup, potentially caused by missing or deleted file during import process",
    "issue_number": 16018,
    "title": "tikv report \"cleanup import sst failed\" during import into command"
  },
  {
    "bug_location": "coprocessor/endpoint.rs",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect deadline check implementation that does not respect client-side timeout configuration",
    "issue_number": 16021,
    "title": "Coprocessor deadlink check ignore `max_execution_duration_ms`"
  },
  {
    "bug_location": "Expression Evaluation Engine",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect implementation of integer division for decimal data types, causing potential computational errors",
    "issue_number": 16024,
    "title": "intdiv for decimal is not handle correctly"
  },
  {
    "bug_location": "unified-read-pool",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug",
      "LoadBalance"
    ],
    "root_cause": "Async quota-limiter incorrectly calculates task concurrency, causing false positive queue-is-full errors when high-priority tasks are pending",
    "issue_number": 16026,
    "title": "unified-read-pool may return false positive queue-is-full error"
  },
  {
    "bug_location": "storage/txn/actions/prewrite",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Assertion failure in transaction prewrite logic related to last change timestamp tracking, causing panic during concurrent index addition and workload execution",
    "issue_number": 16030,
    "title": "more than one tikv panic repeatedly and service is unavailable when run workload and add index"
  },
  {
    "bug_location": "CDC (Change Data Capture) initial scan component",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Memory management inefficiency during large-scale initial data synchronization, causing excessive memory consumption and out-of-memory condition",
    "issue_number": 16035,
    "title": "TiKV OOM during CDC initial scan"
  },
  {
    "bug_location": "RocksDB Compaction Thread",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Internal key comparison failure during RocksDB compaction process, causing thread panic and server abort",
    "issue_number": 1604,
    "title": "rocksdb compaction thread panic when stop "
  },
  {
    "bug_location": "Store Balancer / Titan Storage Engine",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Storage",
      "Performance"
    ],
    "root_cause": "Imbalanced data distribution when scaling out TiKV stores with Titan storage engine enabled, causing uneven data placement across new nodes",
    "issue_number": 16055,
    "title": "The scaled out tikv store size can't balance when titan is enable"
  },
  {
    "bug_location": "log-backup component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Transaction"
    ],
    "root_cause": "Incomplete error handling during task initialization with partial network connectivity issues, causing task to enter an unrecoverable state",
    "issue_number": 16056,
    "title": "log-backup: log backup task may silently broken by a failed initialization"
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Config"
    ],
    "root_cause": "Manual cluster compaction triggers excessive disk space consumption when Titan storage engine is enabled, potentially causing unexpected disk full conditions during compaction process",
    "issue_number": 16062,
    "title": "Manual  compaction raises diskfull when titan is enable"
  },
  {
    "bug_location": "raftstore/apply/snapshot_handling",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Race condition between peer removal and snapshot processing causing stale apply state and potential linearizability violation",
    "issue_number": 16069,
    "title": "raftstore: linearibilizy is broken when follower read is used"
  },
  {
    "bug_location": "log-backup module",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Memory not properly deallocated after stopping log backup task, causing memory leak in template file handling",
    "issue_number": 16070,
    "title": "log-backup: memory leak after stopping a task."
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 5,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Compaction filter mechanism failure preventing garbage collection when Titan storage engine is enabled",
    "issue_number": 16091,
    "title": "Titan can't execute compaction filter"
  },
  {
    "bug_location": "tikv-ctl encryption metadata handling",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Missing data directory configuration causing panic when attempting to access encryption metadata without explicit data directory specification",
    "issue_number": 16094,
    "title": "(encryption) tikv-ctl dump encryption metadata panic without data-dir in config.toml"
  },
  {
    "bug_location": "GarbageCollection",
    "severity": 3,
    "categories": [
      "Performance",
      "Config"
    ],
    "root_cause": "Single-threaded GC implementation with synchronous processing causing potential performance bottlenecks in large-scale deployments",
    "issue_number": 16101,
    "title": "Make TiKV GC support configurable thread count "
  },
  {
    "bug_location": "Metrics/Observability",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect metric generation and panel configuration in Grafana dashboard template",
    "issue_number": 16102,
    "title": "metrics: some minor bugs have side effects to observability"
  },
  {
    "bug_location": "raftstore/peer",
    "severity": 4,
    "categories": [
      "Transaction",
      "Network",
      "LoadBalance"
    ],
    "root_cause": "Key lookup failure during complex transaction and network partition scenarios",
    "issue_number": 16111,
    "title": "tikv panic for \"no entry found for key\""
  },
  {
    "bug_location": "raftstore/storage",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Imprecise error type tracking for data not ready scenarios in stale read processing",
    "issue_number": 16113,
    "title": "raftstore: record the data not ready error types more clearly"
  },
  {
    "bug_location": "Monitoring/Grafana Integration",
    "severity": 2,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Incompatibility between Grafana 9+ dashboard configuration and TiKV metrics visualization",
    "issue_number": 16144,
    "title": "Grafana heatmap does not display any data"
  },
  {
    "bug_location": "Grafana Monitoring Dashboard",
    "severity": 2,
    "categories": [
      "Performance",
      "Config"
    ],
    "root_cause": "Missing label filter for QPS metric type in Grafana dashboard configuration",
    "issue_number": 16148,
    "title": "grafana: QPS lacks the filter label of `type`."
  },
  {
    "bug_location": "TiKV Error Handling",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Inconsistent error type representation for deadline exceeded scenarios across different TiKV response types",
    "issue_number": 16154,
    "title": "Uniform deadline exceeded error types"
  },
  {
    "bug_location": "CDC (Change Data Capture)",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Resolved timestamp advancement mechanism failure during changefeed pause and resume operations, potentially caused by synchronization or state tracking issues in CDC component",
    "issue_number": 16156,
    "title": "ddl resolved ts not advance for 2 min "
  },
  {
    "bug_location": "CDC (Change Data Capture) and RocksDB apply layer",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Performance bottleneck during CDC incremental scan resumption, causing increased write memtable processing time and subsequent TiDB latency degradation",
    "issue_number": 16164,
    "title": "apply-write_memtable_time and TiDB latency increases during cdc initial scan"
  },
  {
    "bug_location": "profiler module",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Temporary file creation failure during heap profile dump, possibly due to insufficient permissions or temporary directory configuration issues",
    "issue_number": 16169,
    "title": "dump or get heap profile failed"
  },
  {
    "bug_location": "tikv-ctl cluster management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incomplete implementation of compact-cluster RPC handling when TiFlash nodes are present in the cluster",
    "issue_number": 16189,
    "title": "tikv-ctl compact-cluster doesn't work when TiFlash nodes exist"
  },
  {
    "bug_location": "storage/gc",
    "severity": 3,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential race condition or timeout issue in garbage collection test scenarios during concurrent key cleanup",
    "issue_number": 1620,
    "title": "storage GC test may fail too frequently in travis CI"
  },
  {
    "bug_location": "SQL Type Casting Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect implementation of duration to time type conversion logic causing incorrect result generation during data type casting",
    "issue_number": 16211,
    "title": "Wrong result of `cast_duration_as_time` "
  },
  {
    "bug_location": "Timezone Library",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config",
      "Upgrade"
    ],
    "root_cause": "Bundled tzdata in TiKV binary is not regularly updated, leading to potential timezone information inaccuracies",
    "issue_number": 16220,
    "title": "TiKV tzdata out of date and may cause correctness issues"
  },
  {
    "bug_location": "TiKV Import Module",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Memory management inefficiency during large-scale table creation and data import process",
    "issue_number": 16229,
    "title": "tikv oom crash during lightning import 80k tables"
  },
  {
    "bug_location": "Transaction Scheduler",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Storage"
    ],
    "root_cause": "Unbounded memory accumulation of `check_secondary_locks` RPCs blocked on transaction latches during slow raftstore operations",
    "issue_number": 16234,
    "title": "TiKV txn scheduler may cause OOM when raftstore runs slow"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) and Transaction Subsystem",
    "severity": 5,
    "categories": [
      "Network",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Network partition between TiKV and MinIO backup storage causing transaction resolution and GC processes to stall, potentially due to insufficient timeout handling or synchronization mechanisms",
    "issue_number": 16235,
    "title": "cluster can not provide service and tikv resolved ts can not go on after inject minio(pitr log backup path) network partition lasts for 10mins "
  },
  {
    "bug_location": "thread_group component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Group properties not properly initialized during thread group operations",
    "issue_number": 16236,
    "title": "tikv v7.4.0 crash"
  },
  {
    "bug_location": "Region Management / Load Balancing",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance",
      "Storage"
    ],
    "root_cause": "Uneven region distribution causing excessive memory pressure on specific TiKV nodes during large-scale table import",
    "issue_number": 16243,
    "title": "TiKV OOM when importing 130k tables via lightning"
  },
  {
    "bug_location": "Metrics/Monitoring",
    "severity": 2,
    "categories": [
      "Performance",
      "Config"
    ],
    "root_cause": "Missing instance-level filtering in Raft IO and Raft Propose metrics, preventing granular performance monitoring",
    "issue_number": 16251,
    "title": "metrics: Raft IO and Raft Propose sections missing filters `by instance`."
  },
  {
    "bug_location": "TitanDB Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Race condition during blob file manifest management during fallback from Titan to RocksDB, causing orphaned blob files",
    "issue_number": 16256,
    "title": "titan blob file can't be cleared when fallback"
  },
  {
    "bug_location": "Executor/Decimal Multiplication",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Inconsistent decimal multiplication truncation logic between TiKV and TiDB computational layers",
    "issue_number": 16268,
    "title": "executor:  different result between tikv and tidb when handling decimal multiplication truncating"
  },
  {
    "bug_location": "TiKV Client Interface",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Key decoding failure when mixing raw and transactional client operations, likely due to incompatible key encoding strategies between different client types",
    "issue_number": 16284,
    "title": "TiKV can't  read and write due to use rawclient and txnclient at the same time"
  },
  {
    "bug_location": "Transaction Timestamp Management",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Timestamp advancement mechanism failure during wide table read/write operations",
    "issue_number": 16290,
    "title": "Resolve ts can't advance for long time"
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 5,
    "categories": [
      "Storage",
      "Upgrade",
      "Config"
    ],
    "root_cause": "Incompatible configuration when enabling Titan storage engine during cluster restart, causing initialization failure",
    "issue_number": 16295,
    "title": "TiKV fails to restart after making Titan default on"
  },
  {
    "bug_location": "config/mod.rs",
    "severity": 4,
    "categories": [
      "Config",
      "CodeBug"
    ],
    "root_cause": "Invalid parsing of periodic-full-compact-start-times configuration with multiple timezone-annotated timestamps, causing configuration file validation failure during restart",
    "issue_number": 16299,
    "title": "tikv cannot restart twice after enable raftstore.periodic-full-compact-start-times"
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 5,
    "categories": [
      "Storage",
      "Network",
      "Transaction"
    ],
    "root_cause": "Potential race condition or stability issue during network partition with Titan storage engine enabled, causing unexpected TiKV node restarts and region unavailability",
    "issue_number": 16301,
    "title": "others tikv abnormally restart and workload report \u201cRegion is unavailable\u201d after inject one of tikv network partition or failure or rolling restart with titan enable"
  },
  {
    "bug_location": "raftstore/peer.rs",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Snapshot recovery process fails to handle uncommitted logs across leader term changes, causing apply process to block when waiting for logs that cannot be committed",
    "issue_number": 16307,
    "title": "Snapshot recovery wait apply may be blocked by logs from a pervious term"
  },
  {
    "bug_location": "engine_rocks_helper/sst_recovery",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incomplete file deletion mechanism for multi-region SST files with corruption, causing unexpected file retention and subsequent panic during recovery process",
    "issue_number": 16308,
    "title": "TiKV panic because it detects damaged SST file was not removed"
  },
  {
    "bug_location": "Transaction/CDC/Flashback",
    "severity": 5,
    "categories": [
      "Transaction",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Inconsistent transaction state after flashback operation, likely causing key assertion failures during data update",
    "issue_number": 16313,
    "title": "enable titan, update report error after flashback "
  },
  {
    "bug_location": "Transaction",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential monotonic consistency violation in distributed transaction processing, causing read anomalies during concurrent operations",
    "issue_number": 16314,
    "title": "Jepsen monotonic test failed with anomalies"
  },
  {
    "bug_location": "Transaction",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential race condition or inconsistent transaction ordering in global transaction management causing timeline contradiction during concurrent incremental operations",
    "issue_number": 16315,
    "title": "Jepsen monotonic test failed with timeline contradiction"
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect timestamp property tracking when Titan storage engine is enabled on write column family",
    "issue_number": 16319,
    "title": "Some table properties are incorrect if titan is enabled"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 3,
    "categories": [
      "Transaction",
      "Network"
    ],
    "root_cause": "Incorrect handling of read-only requests during leadership transition",
    "issue_number": 1632,
    "title": "Fix read only request handling for new leader"
  },
  {
    "bug_location": "raft_log_engine",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Performance optimization needed in Raft log engine implementation to reduce EBS (Elastic Block Store) jitter",
    "issue_number": 16324,
    "title": "Raft-Engine: reduce EBS jitter impact by the Raft-Engine rewrite"
  },
  {
    "bug_location": "Titan storage engine",
    "severity": 3,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Potential issue with Titan storage engine configuration or test case implementation",
    "issue_number": 16336,
    "title": "Test test_turnoff_titan failed"
  },
  {
    "bug_location": "Transaction Processing",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Potential deadlock or resource exhaustion in write transaction handling during high-load scenarios with large dataset",
    "issue_number": 16337,
    "title": "tidb qps drop to 0 for write command can't end "
  },
  {
    "bug_location": "raftstore/scheduler transaction locks",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Concurrent lock acquisition between raftstore and scheduler workers causing mutual blocking on PeerPessimisticLocks RwLock",
    "issue_number": 16340,
    "title": "Deadlock on PeerPessimisticLocks between raftstore and shceduler workers"
  },
  {
    "bug_location": "TiKV IO Handling",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "Storage"
    ],
    "root_cause": "Slow recovery mechanism during IO hang scenarios, causing extended performance degradation beyond expected recovery time",
    "issue_number": 16368,
    "title": "qps recover after 13mins when inject one of tikv io hang"
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Performance regression when Titan storage engine is enabled during data dumpling and point-in-time recovery operations",
    "issue_number": 16370,
    "title": "titan =true dumpling and pitr has performance regression,which is 24.26% and 11.35%"
  },
  {
    "bug_location": "tidb_query_datatype/codec/row/v2/row_slice",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Unexpected None value when processing row checksum bytes during data insertion, causing a panic in row slice parsing",
    "issue_number": 16371,
    "title": "tikv panic when enable row level checksum and insert some data"
  },
  {
    "bug_location": "resolved-ts tracking/metrics component",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Incorrect timestamp tracking and metric calculation for resolved transaction timestamps, potentially causing inaccurate monitoring of transaction progress",
    "issue_number": 16390,
    "title": "tikv max gap of resolved ts metrics may be incorrect"
  },
  {
    "bug_location": "backup/endpoint",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Unhandled Option::unwrap() on a None value during backup process, likely due to missing null check in region progress tracking",
    "issue_number": 16394,
    "title": "TiKV panic when running br backup due to \"called `Option::unwrap()` on a `None` value\""
  },
  {
    "bug_location": "Profiling/Diagnostic Component",
    "severity": 3,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Potential configuration or compatibility issue with continuous profiling (conprof) feature in TiKV deployment",
    "issue_number": 16396,
    "title": "tikv conprof failed"
  },
  {
    "bug_location": "logging/thread_id",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect thread ID generation or logging mechanism that always returns fixed value 0x5 instead of actual thread identifier",
    "issue_number": 16398,
    "title": "\"thread_id\" field in log is wrong and always 0x5"
  },
  {
    "bug_location": "Scheduler/LoadBalancer",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Scheduler unable to resume leader balancing after IO delay and fault recovery, potentially due to state synchronization or scheduling algorithm failure",
    "issue_number": 16403,
    "title": "leader can not balance again after fault recover from one of tikv io delay 100ms which lasts for 10mins with evict-slow-trend-schedule"
  },
  {
    "bug_location": "Expression Evaluation Component",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Inconsistent error message formatting during power function overflow handling between TiDB and TiKV",
    "issue_number": 16407,
    "title": "Error message is different from TiDB when `pow` is out of bound"
  },
  {
    "bug_location": "raftstore/raft-rs logging",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Logging configuration change removed identifier context from log messages",
    "issue_number": 16410,
    "title": "raftstore: Peer does not print `raft-id` and `peer-id` in `raft-rs` lib"
  },
  {
    "bug_location": "Profiling/Diagnostics Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "Unknown"
    ],
    "root_cause": "Potential scaling issue with continuous profiling in large TiKV clusters causing repeated failures",
    "issue_number": 16421,
    "title": "In a cluster of 100 tikv nodes, conprofiling continues to fail"
  },
  {
    "bug_location": "raftstore/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Election mechanism fails to handle leader selection during network partitions when hibernate regions are enabled, causing persistent leadership unavailability due to improper handling of missing ticks",
    "issue_number": 16429,
    "title": "hibernate region may not able to reelection a new leader for a long when leader is partitioned"
  },
  {
    "bug_location": "log_backup component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Unsigned 64-bit integer (u64) overflow during initial scanning process, causing potential infinite wait state",
    "issue_number": 16430,
    "title": "log_backup: u64 overflow may make initial scanning stuck "
  },
  {
    "bug_location": "raftstore/snapshot_sender",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Lack of timeout mechanism during snapshot transmission in slow network conditions causing potential leader transfer blocking",
    "issue_number": 16435,
    "title": "raftstore: Add a timeout mechanism on snapshot send"
  },
  {
    "bug_location": "Backup and Restore (BR) component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Storage"
    ],
    "root_cause": "AWS SDK does not support retrieving instance metadata via IMDSv2 (Instance Metadata Service version 2) authentication mechanism",
    "issue_number": 16443,
    "title": "TiKV aws storage sdk don't support retrieve instance metadata via IMDSv2"
  },
  {
    "bug_location": "CDC (Change Data Capture) endpoint",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unhandled Option::unwrap() on a None value in CDC endpoint registration process",
    "issue_number": 16444,
    "title": "cdc endpoint thread panics"
  },
  {
    "bug_location": "Build System",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Dependency conflict with multiple sources of the same package version",
    "issue_number": 16445,
    "title": "Cargo vendor fails with duplicate version of package \"encoding_rs\""
  },
  {
    "bug_location": "DR-AutoSync Replication Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "LoadBalance",
      "Upgrade"
    ],
    "root_cause": "Replication mode transition and node scale-in interaction causing operation deadlock during async mode switchover",
    "issue_number": 16465,
    "title": "(dr-autosync) v6.5.8 scale in one node from primary datacenter failed during async mode"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) checkpoint mechanism",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Network partition disrupting synchronization between TiKV nodes and PD leader, causing significant checkpoint timestamp lag during backup/recovery process",
    "issue_number": 16469,
    "title": "pitr checkpoint ts lag reached more than 8h after inject network partition between one of tikv and pd leader"
  },
  {
    "bug_location": "documentation",
    "severity": 1,
    "categories": [
      "Human"
    ],
    "root_cause": "Broken external documentation link",
    "issue_number": 16478,
    "title": "Fix style_doc link"
  },
  {
    "bug_location": "Placement Rules",
    "severity": 4,
    "categories": [
      "Config",
      "LoadBalance",
      "Storage"
    ],
    "root_cause": "Placement rule configuration prevents automatic recovery of down peer regions when node is scaled in, causing region replication and leader election issues",
    "issue_number": 16480,
    "title": "Down-peer-region can't recover when enable placement-rule policy"
  },
  {
    "bug_location": "Region Load Balancer",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Uneven distribution of leader and region replicas during data restoration process, causing potential performance and consistency issues",
    "issue_number": 16491,
    "title": "leader and region are not balanced during br restore data and losts of miss-peer region after restore finished"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Precision loss during datetime type casting with time zone information",
    "issue_number": 16492,
    "title": "cast string as datetime with time zone results in loss of precision"
  },
  {
    "bug_location": "JSON Function Processing",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Insufficient input validation for JSON path expressions during function processing",
    "issue_number": 16495,
    "title": "An exception should be thrown for `json_length(col, '$.*')`"
  },
  {
    "bug_location": "raftstore/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Incorrect peer comparison logic in GC message handling causes stale peer to ignore garbage collection message when peer role changes, leading to resolve timestamp blocking",
    "issue_number": 16504,
    "title": "Stale peer may cause resolve ts doesn't push"
  },
  {
    "bug_location": "Coprocessor JSON Comparison Module",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect integer comparison logic in JSON comparison function, causing inconsistent boolean result between TiKV and TiDB",
    "issue_number": 16512,
    "title": "Incorrect result for json compare and `json_contains`"
  },
  {
    "bug_location": "backup-restore component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Inconsistent snapshot timing causing restoration performance degradation when using EBS snapshot backup method",
    "issue_number": 16518,
    "title": "disk_snapshot_backup: The lag between taking snapshot will slow down the restoration"
  },
  {
    "bug_location": "TiKV Storage Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Upgrade",
      "Config"
    ],
    "root_cause": "Incompatible API version migration with existing data keys not written by TiDB",
    "issue_number": 16524,
    "title": "Failure with booting up TiKV with API v2 causes Fatal crash, which is probably a bit too severe."
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Unhandled error condition during TiCDC changefeed operation when a TiKV node failure is injected, causing unexpected panic in other TiKV nodes",
    "issue_number": 16526,
    "title": "other tikv panic when inject one of tikv failure with ticdc changfeed running"
  },
  {
    "bug_location": "TiCDC",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Scan speed limiter implementation fails to properly handle Titan storage engine configuration",
    "issue_number": 16539,
    "title": "TiCDC scan speed limiter seems doesn't work if Titan is enabled"
  },
  {
    "bug_location": "CDC (Change Data Capture)",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Network"
    ],
    "root_cause": "Potential region mapping or scanning instability during large-scale incremental data synchronization, causing resolved timestamp tracking failures",
    "issue_number": 16543,
    "title": "CDC resolved ts stucks, CDC incremental scan encountered lots of RegionNotFound error"
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential race condition or memory management issue during EBS volume recovery process causing TiKV instance instability",
    "issue_number": 16549,
    "title": "TiKV crash loop during EBS recovery"
  },
  {
    "bug_location": "backup-stream/event_loader",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Race condition during region merge operations preventing consistent snapshot retrieval, causing log backup task interruption",
    "issue_number": 16554,
    "title": "Log Backup Task Randomly Paused due to Failed to get Initial Snapshot"
  },
  {
    "bug_location": "Transaction Timestamp Management",
    "severity": 5,
    "categories": [
      "Transaction",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Timestamp resolution mechanism failure preventing transaction timestamp advancement",
    "issue_number": 16561,
    "title": "Resolve ts can't not advance"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Snapshot checksum scanning performed synchronously in raftstore thread, causing high message processing duration during store offline operations",
    "issue_number": 16564,
    "title": "High peer msg duration, should avoid snapshot IO in raftstore"
  },
  {
    "bug_location": "Grafana Dashboard Configuration",
    "severity": 2,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Incorrect metric type selection for tracking memory deallocation in TiKV monitoring dashboard",
    "issue_number": 16572,
    "title": "v7.5 tikv grafana [TiKV-Details] [Recently Released Bytes by Thread] incorrect configuration"
  },
  {
    "bug_location": "memory allocator/metrics",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Memory allocation tracking inconsistency between allocation and deallocation metrics for TiKV threads",
    "issue_number": 16573,
    "title": "v7.5 web(status_port) metrics tikv_allocator_thread_allocation values issue"
  },
  {
    "bug_location": "Backup and Restore (BR) SST Importer",
    "severity": 3,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Lack of metrics tracking concurrent SST import tasks, potentially causing task queue blocking and performance bottlenecks",
    "issue_number": 16583,
    "title": "need a metrics to show the number of inflight sst importer task"
  },
  {
    "bug_location": "Projection Executor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect handling of RpnStackNodeVectorValue::Generated value type in projection executor's expression evaluation logic",
    "issue_number": 16589,
    "title": "Projection executor crashes when expression returns RpnStackNodeVectorValue::Generated value"
  },
  {
    "bug_location": "Compaction Filter",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Inconsistent compaction behavior across TiKV peers, potentially due to race conditions or synchronization issues in the compaction filter implementation",
    "issue_number": 16591,
    "title": "compaction-filter sometimes stably fails to work"
  },
  {
    "bug_location": "RocksDB build system",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Compatibility issue with GCC 13 in RocksDB's performance flag implementation",
    "issue_number": 16593,
    "title": "TiKV built failed with GCC13"
  },
  {
    "bug_location": "DR Auto-Sync Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "LoadBalance"
    ],
    "root_cause": "Data inconsistency during unplanned datacenter switchover, likely caused by synchronization failure or incomplete state transfer during recovery process",
    "issue_number": 16595,
    "title": "(dr-autosync)after switch to backup datacenter in sync mode, tpcc report data inconsistency in table"
  },
  {
    "bug_location": "Store Message Processing",
    "severity": 4,
    "categories": [
      "Performance",
      "Network"
    ],
    "root_cause": "Lack of fine-grained metrics for tracking store message processing duration, preventing detailed performance investigation",
    "issue_number": 16600,
    "title": "High store message duration"
  },
  {
    "bug_location": "TiCDC Incremental Scan Worker",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction",
      "Unknown"
    ],
    "root_cause": "Inefficient incremental scan processing causing excessive CPU consumption during data replication",
    "issue_number": 16601,
    "title": "TiCDC incremental scan can usage too much CPU under some unkown conditions"
  },
  {
    "bug_location": "JSON/Type Casting Module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Improper handling of temporary binary type conversion to JSON during query execution, causing runtime panic",
    "issue_number": 16616,
    "title": "TiKV will panic if cast a temporary binary to json"
  },
  {
    "bug_location": "storage/txn/actions/check_txn_status.rs",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Unprotected rollback record handling in async commit/1PC transactions allowing potential transaction atomicity violation",
    "issue_number": 16620,
    "title": "`CheckTxnStatus` on primary key of optimistic transaction writing non-protectecd rollback can break transaction atomicity"
  },
  {
    "bug_location": "Timer Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Monotonic time inconsistency causing timer thread panic in tokio timer wheel processing",
    "issue_number": 16623,
    "title": "tikv crash or panic"
  },
  {
    "bug_location": "ReadPool Configuration",
    "severity": 3,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Metric initialization failure in unified read pool thread configuration, preventing accurate thread count reporting until explicit configuration modification",
    "issue_number": 16629,
    "title": "`tikv_unified_read_pool_thread_count` is always zero until online modifying `readpool.unified.max-thread-count`."
  },
  {
    "bug_location": "resolved_ts module",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect handling of rewritten transaction commit records with gc_fence, causing loss of rollback information and potential resolved_ts blocking",
    "issue_number": 16634,
    "title": "Writing protected rollback directly to rollback flag of another transaction's commit record may cause resolved_ts being blocked"
  },
  {
    "bug_location": "raftstore/store/fsm/store",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Index out of bounds access in StoreFsmDelegate during message handling, causing a runtime panic",
    "issue_number": 16639,
    "title": "TiKV panic index out of bounds"
  },
  {
    "bug_location": "TiKV Coprocessor",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Excessive concurrent coprocessor requests causing memory exhaustion during high-thread TPCC check operation",
    "issue_number": 16653,
    "title": "TPCC check with -T 1000 causes TiKV OOM "
  },
  {
    "bug_location": "Backup and Restore (BR) Monitoring Component",
    "severity": 2,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Missing default metric configuration in Grafana dashboard for log backup initial scan pending metric",
    "issue_number": 16656,
    "title": "br: add metrics tikv_log_backup_pending_initial_scan to grafana panel"
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Memory allocation and management inefficiency causing uncontrolled heap growth during extended cluster operation",
    "issue_number": 16658,
    "title": "Tikv OOM "
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Storage"
    ],
    "root_cause": "Potential I/O rate limiting and load balancing inefficiency during cluster scale-out, causing excessive disk throughput and CDC replication lag",
    "issue_number": 16661,
    "title": "slow store during TiKV scale out, and CDC lag up to 15s"
  },
  {
    "bug_location": "scheduler",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Timeout and retry mechanism in Java client causing excessive memory consumption due to long default maxExecutionDurationMs setting",
    "issue_number": 16676,
    "title": "8.0.0-alpha OOM"
  },
  {
    "bug_location": "RocksDB/TitanDB Testing Component",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Potential performance bottleneck or synchronization issue in proptest comparison between RocksDB and TitanDB implementations causing test timeout",
    "issue_number": 16681,
    "title": "test: test_rocks_titan_basic_ops timeouts"
  },
  {
    "bug_location": "CDC (Change Data Capture) / Resolved Timestamp Tracking",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Timestamp synchronization disruption during TiKV node rolling restart causing extended resolved timestamp lag",
    "issue_number": 16698,
    "title": "TiKV min resolved ts lag up to 30s during tikv rolling restart, resulting in cdc lag increase"
  },
  {
    "bug_location": "storage.io-rate-limit",
    "severity": 3,
    "categories": [
      "Config",
      "Performance",
      "Storage"
    ],
    "root_cause": "Configuration parameter for disk I/O rate limiting not functioning as expected, allowing higher than configured throughput during TiKV scaling operations",
    "issue_number": 16701,
    "title": "storage.io-rate-limit max-bytes-per-sec doesn't work perfectly"
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 5,
    "categories": [
      "Storage",
      "Upgrade",
      "CodeBug"
    ],
    "root_cause": "Potential data inconsistency during snapshot restore causing WAL (Write-Ahead Log) corruption and preventing proper database recovery",
    "issue_number": 16705,
    "title": "tikv failed at bootstrap complaining wal corruption "
  },
  {
    "bug_location": "TiKV Store",
    "severity": 5,
    "categories": [
      "Upgrade",
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential memory management or resource allocation issue during large-scale workload after version upgrade",
    "issue_number": 16716,
    "title": "TiKV store down when running workload"
  },
  {
    "bug_location": "backup-stream/utils",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Race condition in CallbackWaitGroup implementation causing intermittent test failures",
    "issue_number": 16739,
    "title": "log_backup: the type `CallbackWaitGroup` is error-prone"
  },
  {
    "bug_location": "raftstore/server initialization",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Upgrade"
    ],
    "root_cause": "Invalid argument during file/storage initialization, likely related to filesystem permissions or storage configuration during cluster restart",
    "issue_number": 1674,
    "title": "tikv cluster restart failed with opaque error"
  },
  {
    "bug_location": "CDC (Change Data Capture)",
    "severity": 4,
    "categories": [
      "Transaction",
      "Network",
      "Performance"
    ],
    "root_cause": "Resolved timestamp synchronization failure during TiKV node recovery, causing extended lag in change data capture process",
    "issue_number": 16741,
    "title": "resolved ts stuck for 10m when TiKV failure restored. "
  },
  {
    "bug_location": "IO Subsystem",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Async IO implementation causing unexpected performance degradation in workload processing",
    "issue_number": 16742,
    "title": "enable async io, hxxxk poc workload has 3.6%~6.0%+ regression in performance"
  },
  {
    "bug_location": "backup-stream/subscription_manager",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "File or directory not found during test execution, likely related to test setup or file path configuration",
    "issue_number": 16747,
    "title": "PiTR: unstable tests"
  },
  {
    "bug_location": "h2 networking component",
    "severity": 4,
    "categories": [
      "Network",
      "Security",
      "Performance"
    ],
    "root_cause": "Vulnerability in h2 library allowing CONTINUATION frame flood causing excessive CPU processing",
    "issue_number": 16766,
    "title": "h2 vulnerability issue"
  },
  {
    "bug_location": "Memory Profiling Component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Inconsistent jemalloc profiling sample rate configuration between runtime options and actual profiling settings",
    "issue_number": 16774,
    "title": "Wrong jemalloc profiling sample rate"
  },
  {
    "bug_location": "cdc/delegate.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Panic in Change Data Capture (CDC) delegate sink processing, likely due to timestamp or data synchronization issue",
    "issue_number": 16776,
    "title": "TiKV Server Crashed And Restart With Panic Info"
  },
  {
    "bug_location": "RaftStore/RawKV",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Potential race condition during leader transfer in RawKV test scenario",
    "issue_number": 16789,
    "title": "[flay test]  test_rawkv::test_leader_transfer "
  },
  {
    "bug_location": "TiKV Flashback/Stale Read Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "Performance"
    ],
    "root_cause": "Range boundary mismatch during historical data retrieval, causing request range to exceed physical data bounds",
    "issue_number": 16790,
    "title": "sql report \"Error 1105 (HY000): other error: Request range exceeds bound\" after flashback"
  },
  {
    "bug_location": "Raft Log Persistence Module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Inconsistent log index tracking during raft log application when persistence feature is enabled, causing potential data integrity and recovery issues",
    "issue_number": 16796,
    "title": "Apply raft log before persistence is not compatible with online unsafe recovery"
  },
  {
    "bug_location": "Configuration Management",
    "severity": 1,
    "categories": [
      "Config",
      "Human"
    ],
    "root_cause": "Typographical error in configuration file preventing correct HMAC validation",
    "issue_number": 16797,
    "title": "Typo in deny.toml"
  },
  {
    "bug_location": "Backup and Restore (BR) Component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Data synchronization inconsistency during complex fault scenarios involving PITR (Point-in-Time Recovery) and chaos testing",
    "issue_number": 16809,
    "title": "upstream and downstream data are inconsistent after running some fault test and then pitr restore and checksum"
  },
  {
    "bug_location": "Transaction Resolution Mechanism",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect lock resolution strategy during pipelined DML operations, using a limited lock resolution method instead of comprehensive region-wide lock scanning",
    "issue_number": 16810,
    "title": "Pipelined-DML uses resolve_lock_lite instead of resolve_lock when there is conflict"
  },
  {
    "bug_location": "resolved_ts/cmd.rs",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Unexpected row lock state during resolved timestamp processing, likely caused by race condition or inconsistent transaction state management",
    "issue_number": 16818,
    "title": "tikv panic for \"assertion failed: row.lock.is_none()\""
  },
  {
    "bug_location": "Monitoring/Grafana",
    "severity": 2,
    "categories": [
      "Performance",
      "Config"
    ],
    "root_cause": "Missing monitoring dashboard configuration for specific Raft log application metrics",
    "issue_number": 16820,
    "title": "missing grafana panels for \"apply unpersisted raft logs\""
  },
  {
    "bug_location": "RawKV",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unwrapping a None value during raw key guard operation, indicating a potential nil/null handling error in the RawKV test case",
    "issue_number": 16825,
    "title": "[flay test] test_rawkv::test_raw_put_key_guard"
  },
  {
    "bug_location": "Integration Test Framework",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Human"
    ],
    "root_cause": "Unstable test environment causing non-deterministic test failures",
    "issue_number": 16871,
    "title": "The integration tests are not stable"
  },
  {
    "bug_location": "Transaction Processing",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Pipelined DML locks incorrectly blocking read requests by not properly pushing min_commit_ts and resolving lock conflicts",
    "issue_number": 16880,
    "title": "Pipelined DML locks blocks read"
  },
  {
    "bug_location": "RaftStore/BufferBatchGet",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Inconsistent key handling in buffer batch get operation when keys are not sorted or have duplicates",
    "issue_number": 16882,
    "title": "some results missing when keys of buffer batch get are unsorted"
  },
  {
    "bug_location": "PD (Placement Driver) Election Process",
    "severity": 3,
    "categories": [
      "Config",
      "Network"
    ],
    "root_cause": "Single node PD deployment with incorrect peer URL configuration causing election failure",
    "issue_number": 16884,
    "title": "README.md About Deploy a playground with binary part Incorrect"
  },
  {
    "bug_location": "raftstore/store/fsm/store.rs",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Index out of bounds error during table split operation under high concurrency with sysbench",
    "issue_number": 16885,
    "title": "tikv panic when run sysbench and split table"
  },
  {
    "bug_location": "ReadIndex/RaftMessage handling in transaction commit path",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Race condition between read index message processing and transaction lock write, causing incorrect min_commit_ts computation during concurrent updates",
    "issue_number": 16892,
    "title": "ReadIndex raft message may fail to advance max_ts in the middle of write_lock"
  },
  {
    "bug_location": "Expression Evaluation Engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Inconsistent type casting behavior between TiDB and TiKV when converting expressions to decimal type",
    "issue_number": 16913,
    "title": "expr: cast as decimal function is inconsistent with TiDB"
  },
  {
    "bug_location": "TiKV JSON pushdown handler",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Incorrect implementation of JSON array append operation during query pushdown, causing runtime panic",
    "issue_number": 16930,
    "title": "json_array_append pushdown to tikv led tikv crash"
  },
  {
    "bug_location": "Request Processing Pipeline",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "Network"
    ],
    "root_cause": "Lack of detailed tracing and logging for request lifecycle bottlenecks, preventing precise identification of timeout origins",
    "issue_number": 16941,
    "title": "Add observability on why tikv requests are timing out"
  },
  {
    "bug_location": "GRPC Server Component",
    "severity": 3,
    "categories": [
      "Performance",
      "Network"
    ],
    "root_cause": "Lack of comprehensive observability metrics for GRPC server connections",
    "issue_number": 16942,
    "title": "Add observability on GRPC server connections"
  },
  {
    "bug_location": "backup-stream/endpoint.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Upgrade"
    ],
    "root_cause": "Potential state persistence issue during cluster restart causing PITR (Point-in-Time Recovery) status to become paused unexpectedly",
    "issue_number": 16946,
    "title": "pitr status became paused after restart all pd\u3001tikv\u3001tidb"
  },
  {
    "bug_location": "in-memory-engine",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Performance regression in OLTP insert operations when in-memory engine is enabled, causing more than 10% throughput reduction compared to standard configuration",
    "issue_number": 16947,
    "title": "in-memory-engine: OLTP-insert has more than 10% regression when in-memory-engine is enabled in a test environment"
  },
  {
    "bug_location": "backup-stream/endpoint.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect safe point handling during log backup task pause operation, causing misleading error message about GC safe point",
    "issue_number": 16956,
    "title": "log_backup: misleading error message reported when pausing a task"
  },
  {
    "bug_location": "raftstore/peer_storage",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Snapshot application failure during TiKV node recovery after extended downtime, causing repeated panic in region synchronization process",
    "issue_number": 16958,
    "title": "tikv panic repeatedly with \u201c\\\"[region 16697056] 19604003 applying snapshot failed\\\"\u201d after down this tikv for 20mins and recover"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Inconsistent type casting implementation between TiDB and TiKV for decimal string conversion",
    "issue_number": 16962,
    "title": "Different behavior of `cast_string_to_decimal` between TiDB and TiKV"
  },
  {
    "bug_location": "In-memory engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect implementation of manual table load mechanism using region labels, causing single-load limitation and range overlap issues",
    "issue_number": 16966,
    "title": "In-memory engine: manual load table does work properly"
  },
  {
    "bug_location": "Conversion Function",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Integer conversion operation causing potential numeric overflow without proper bounds checking",
    "issue_number": 16969,
    "title": "Overflow panic from `conv` function"
  },
  {
    "bug_location": "Type Casting Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Precision loss during float to string type conversion, likely due to floating-point representation inaccuracy",
    "issue_number": 16974,
    "title": "unexpected result: casting real to string "
  },
  {
    "bug_location": "Raft Snapshot Management",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incomplete snapshot file cleanup logic when leader snapshot transmission fails",
    "issue_number": 16976,
    "title": "Raft snapshot file is not deleted if the leader fails to send it out"
  },
  {
    "bug_location": "region_cache_memory_engine/range_manager",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Memory management failure during range eviction with concurrent write operations, causing an assertion error in range manager logic",
    "issue_number": 16994,
    "title": "in-memory-engine: tikv crash due to an assert failure"
  },
  {
    "bug_location": "Metrics/Monitoring",
    "severity": 2,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Missing per-thread memory usage metrics preventing detailed performance diagnostics",
    "issue_number": 17006,
    "title": "Lack of per thread metric for release-8.1 "
  },
  {
    "bug_location": "In-memory engine",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Inconsistent transaction state tracking during concurrent read operations, causing incorrect total count calculation in Snapshot Isolation (SI) mode",
    "issue_number": 17018,
    "title": "In-memory engine: wrong-total in jepsen test"
  },
  {
    "bug_location": "backup-stream/endpoint",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Attempting to spawn a Tokio task without an active Tokio runtime context",
    "issue_number": 17020,
    "title": "tikv panic due to: there is no reactor running, must be called from the context of a Tokio 1.x runtime"
  },
  {
    "bug_location": "SST Importer",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Block checksum mismatch during SST file import, indicating potential data corruption or transmission error during backup/restore process",
    "issue_number": 17022,
    "title": "BR restore failed: `Corruption: block checksum mismatch`"
  },
  {
    "bug_location": "Build System / Dependency Linking",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Missing or incompatible Snappy compression library linkage for ARM64 architecture",
    "issue_number": 17031,
    "title": "Build failure: Undefined symbols for architecture arm64: \"snappy::RawCompress\""
  },
  {
    "bug_location": "table_scan_executor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Out-of-bounds slice indexing in table scan executor causing runtime panic",
    "issue_number": 17033,
    "title": "TiKV panic: range end index 182 out of range for slice of length 180 in table_scan_executor"
  },
  {
    "bug_location": "Performance Optimization Layer",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Architecture-specific performance regression in AArch64 platform optimization",
    "issue_number": 17036,
    "title": "Compared with v7.5.1, v8.1.0 has a 7%-10% performance regression in oltp_read_write on openEuler 22.03 (LTS-SP1)"
  },
  {
    "bug_location": "tikv-ctl command line interface",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incomplete command output for Raft region state information",
    "issue_number": 17037,
    "title": "tikv-ctl raft region command output should include the region state"
  },
  {
    "bug_location": "raftstore/apply_fsm",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Unhandled 'EntriesUnavailable' error during Raft log application, causing panic when processing sysbench workload",
    "issue_number": 17040,
    "title": "tikv panic when run sysbench prepare"
  },
  {
    "bug_location": "raft_server",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Leadership election failure causing gRPC unavailability during cluster initialization",
    "issue_number": 17053,
    "title": "In-memory engine: failed to start raft_server in jepsen test"
  },
  {
    "bug_location": "pd-client",
    "severity": 3,
    "categories": [
      "Network",
      "Performance"
    ],
    "root_cause": "High CPU utilization during PD connection disruption, likely due to repeated reconnection attempts or inefficient error handling in worker threads",
    "issue_number": 17054,
    "title": "the pd-work cpu is nearly 100% when tikv disconnected from pd"
  },
  {
    "bug_location": "Memory Allocation (jemalloc)",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Deadlock in jemalloc profiling configuration when enabling specific memory profiling options",
    "issue_number": 17057,
    "title": "Deadlock when enable jemalloc prof."
  },
  {
    "bug_location": "In-memory Engine",
    "severity": 5,
    "categories": [
      "Storage",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Potential data corruption or inconsistent state in key-value storage during long-running transaction workload, causing default key/value not being found",
    "issue_number": 17060,
    "title": "In-memory engine: default not found in long tpcc run"
  },
  {
    "bug_location": "config/mod.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Unhandled configuration parsing error in periodic full compaction time configuration update",
    "issue_number": 17066,
    "title": "tikv panic when update config online for \u201craftstore.periodic-full-compact-start-times\u201d"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Inefficient memory management of region metadata during high-volume table creation, causing excessive memory consumption per region",
    "issue_number": 17072,
    "title": "TiKV may OOM if too many regions on a TiKV server "
  },
  {
    "bug_location": "MVCC Transaction Layer",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Inconsistent MVCC state during prewrite operation, likely caused by race condition or incomplete transaction state management during region split",
    "issue_number": 17074,
    "title": " In-memory engine: assertion failed during prewrite"
  },
  {
    "bug_location": "Docker build process",
    "severity": 2,
    "categories": [
      "Config",
      "Upgrade"
    ],
    "root_cause": "Dockerfile references outdated rust-toolchain file instead of rust-toolchain.toml after project configuration change",
    "issue_number": 17075,
    "title": "make docker fails in v8.1.0"
  },
  {
    "bug_location": "Region Worker/Snapshot Apply",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "High L0 file count blocking snapshot ingestion, causing significant delay in region snapshot application",
    "issue_number": 17078,
    "title": "Region is applying snapshot for a long time"
  },
  {
    "bug_location": "region_cache_memory_engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Incorrect iterator direction handling in range cache memory engine during backward scanning",
    "issue_number": 17079,
    "title": "In-memory Engine: assertion failed: self.direction == Direction::Backward"
  },
  {
    "bug_location": "region_cache_memory_engine/keys.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect endian encoding of sequence number and value type in key encoding, causing misalignment with RocksDB's internal key encoding standard",
    "issue_number": 17082,
    "title": "In-memory Engine: u64 endian in key encoding is wrong"
  },
  {
    "bug_location": "raftstore/merge_region",
    "severity": 4,
    "categories": [
      "Transaction",
      "LoadBalance",
      "Network"
    ],
    "root_cause": "Log synchronization gap during region merge process caused by rapid topology changes and replication mode switches",
    "issue_number": 17084,
    "title": "(dr-autosync)merge region hang for 28min after replication mode change from sync_recover to sync"
  },
  {
    "bug_location": "src/coprocessor_v2/plugin_registry.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Missing function definition for `is_library_file` during FreeBSD build process",
    "issue_number": 17090,
    "title": "Building on FreeBSD fails"
  },
  {
    "bug_location": "tikv_alloc component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Unstable Rust feature used in stable release channel",
    "issue_number": 17098,
    "title": "Compilation and packaging issues on NixOS"
  },
  {
    "bug_location": "RocksDB Mutex",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Multiple threads simultaneously attempting to acquire a RocksDB mutex, causing a system-wide lock contention deadlock",
    "issue_number": 17100,
    "title": "In-memory Engine: Deadlock during TPCC prepare "
  },
  {
    "bug_location": "Region Hibernate Mechanism",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Performance degradation during region awakening from hibernation state, causing extended commit log processing duration",
    "issue_number": 17101,
    "title": "Commit log duration is high when hibernate regions are awake"
  },
  {
    "bug_location": "In-memory Engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Write batch not properly cleared after load failure, potentially causing data inconsistency in lock column family",
    "issue_number": 17103,
    "title": "In-memory Engine: cached_write_batch is not cleared after load failure"
  },
  {
    "bug_location": "In-memory Engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Missing concurrency control mechanism between delete range and write operations, causing potential race condition and data inconsistency",
    "issue_number": 17104,
    "title": "In-memory Engine: race condition among delete range and write to memory engine"
  },
  {
    "bug_location": "In-memory Engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Memory tracking logic incorrectly calculates memory usage, potentially causing negative memory allocation tracking",
    "issue_number": 17106,
    "title": "In-memory Engine: memory usage goes to negative number"
  },
  {
    "bug_location": "CDC (Change Data Capture) and log-backup components",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "Config"
    ],
    "root_cause": "Default 5-second timeout for leader check in unreachable TiKV nodes causing significant resolved_ts latency",
    "issue_number": 17107,
    "title": "check_leader of resolved_ts used in CDC and log-backup always uses default timeout which is 5s"
  },
  {
    "bug_location": "region_cache_memory_engine/write_batch.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Incorrect sequence number management in cache engine causing potential write operation conflicts",
    "issue_number": 17114,
    "title": "In-memory Engine: cache engine's sequence number may differ from rocksdb "
  },
  {
    "bug_location": "region_cache_memory_engine",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Local clock used for safe_point calculation instead of centralized PD timestamp, potentially causing cache unavailability due to clock drift",
    "issue_number": 17123,
    "title": "In-memory Engine: `safe_point` in `gc_range` should be obtained from PD"
  },
  {
    "bug_location": "CDC (Change Data Capture)",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "Resolved timestamp synchronization instability causing significant latency jitter in change data replication process",
    "issue_number": 17124,
    "title": "resolved ts lag jiter to 40s"
  },
  {
    "bug_location": "In-memory Engine Lock Management",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Direct deletion of locks without writing tombstone, violating write atomicity guarantees in RocksDB and in-memory engine write sequence",
    "issue_number": 17127,
    "title": "In-memory Engine: lock should not be deleted directly"
  },
  {
    "bug_location": "In-memory Engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect assumption about range overlap during range eviction, potentially causing incorrect cache management when multiple ranges are involved",
    "issue_number": 17131,
    "title": "In-memory Engine: `range` to evict may contain multiple cached range"
  },
  {
    "bug_location": "Grafana Metrics Component",
    "severity": 2,
    "categories": [
      "Performance",
      "Config"
    ],
    "root_cause": "Inaccurate gRPC request source duration metric calculation in monitoring dashboard",
    "issue_number": 17133,
    "title": "TiDB Grafana gRPC request source duration is inaccurate"
  },
  {
    "bug_location": "In-memory Engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Race condition during range eviction where concurrent data loading is not properly handled",
    "issue_number": 17139,
    "title": "In-memory Engine: eviction should also consider loading ranges"
  },
  {
    "bug_location": "In-memory Engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect key encoding logic in delete range operation for lock column family, potentially causing incorrect key management in MVCC (Multi-Version Concurrency Control) implementation",
    "issue_number": 17140,
    "title": "In-memory Engine: encoding keys for lock cf should not contain mvcc in delete range"
  },
  {
    "bug_location": "region_cache_memory_engine/background.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Missing boundary specification during garbage collection process in memory engine, potentially causing incomplete or incorrect data cleanup",
    "issue_number": 17143,
    "title": "In-memory Engine: boundaries are not specified when GC"
  },
  {
    "bug_location": "in-memory engine",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect tag tracking during region split, causing potential debugging inaccuracies",
    "issue_number": 17147,
    "title": "In-memory Engine: the tag in `CacheRange` may be inaccurate "
  },
  {
    "bug_location": "region_cache_memory_engine/background.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Unexpected code path in background task processing, leading to unreachable code execution during memory engine operations",
    "issue_number": 17148,
    "title": "In-memory Engine: entered unreachable code in `next_to_match`"
  },
  {
    "bug_location": "log_backup_component",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Network"
    ],
    "root_cause": "Slow log backup flush mechanism with network storage interval causing significant backup lag",
    "issue_number": 17150,
    "title": "Log backup flush speed only ~20MB/s and log backup lag up to 13m"
  },
  {
    "bug_location": "In-memory Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Incomplete range consideration during memory eviction process, potentially causing data inconsistency or performance degradation",
    "issue_number": 17153,
    "title": "In-memory Engine: loading range should be considered in eviction"
  },
  {
    "bug_location": "PD (Placement Driver) Region Check Component",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Performance regression in region down-peer checking mechanism between TiKV versions 6.5.9 and 6.5.10, causing significantly increased check duration",
    "issue_number": 17162,
    "title": "Compared to version 6.5.9, the duration for completing the region check down-peer in version 6.5.10 has significantly increased."
  },
  {
    "bug_location": "In-Memory Engine (IME)",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Potential cache corruption or inconsistent system table loading in In-Memory Engine causing user authentication records to be improperly cached or discarded",
    "issue_number": 17163,
    "title": "In-memory Engine: SELECT command denied to user 'root'@'%"
  },
  {
    "bug_location": "backup-restore component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Premature termination of backup task when initial batch of regions do not intersect with specified subranges",
    "issue_number": 17168,
    "title": "backup: requests for subranges potentially being terminated prematurely."
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unhandled edge case in lock tracking logic causing an unreachable code path to be entered during CDC owner I/O hang scenario",
    "issue_number": 17172,
    "title": "tikv panic with \"[lib.rs:478] [\\\"internal error: entered unreachable code\\\"] \u201c when injection ticdc owner io hang"
  },
  {
    "bug_location": "Channel Communication Layer",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Race condition in channel message handling causing memory retention after receiver disconnection",
    "issue_number": 17178,
    "title": "Occasional memory leak in std mpsc channels and crossbeam channels "
  },
  {
    "bug_location": "In-memory Engine",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Delayed SST file loading after ingestion causing checksum verification failures",
    "issue_number": 17182,
    "title": "In-memory Engine: range should be evicted after sst ingestion "
  },
  {
    "bug_location": "Region Transfer/Scheduling",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Region transfer blocking during TiKV cluster scale-in operation, preventing complete node removal",
    "issue_number": 17191,
    "title": "tikv scaling in blocked due to one region can not transfer"
  },
  {
    "bug_location": "table creation/metadata management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Inefficient memory management during massive table creation, causing excessive memory allocation without proper resource control",
    "issue_number": 17199,
    "title": "Creating tons of tables make TiKV OOM"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Transaction rollback mechanism interfering with CDC change tracking process",
    "issue_number": 17211,
    "title": "rollback a txn_source specified transaction may blocks cdc"
  },
  {
    "bug_location": "raftstore/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Network"
    ],
    "root_cause": "Snapshot generation process does not handle peer removal gracefully, causing blocking of snapshot and subsequent configuration changes",
    "issue_number": 17213,
    "title": "Raft snapshot blocked when peer is removed right after requesting snapshot"
  },
  {
    "bug_location": "in-memory engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Inconsistent RocksDB snapshot state during cross-check task, causing potential data loss during delete operation",
    "issue_number": 17216,
    "title": "In-memory engine: panic in cross check task"
  },
  {
    "bug_location": "raftstore/peer_storage",
    "severity": 5,
    "categories": [
      "Network",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Failed to retrieve snapshot during network partition, causing Raft leader to panic when attempting to broadcast append entries",
    "issue_number": 17226,
    "title": "tikv panic with \"unexpected error: Store(Other(\\\\\\\"[components/raftstore/src/store/peer_storage.rs:545]: failed to get snapshot after 5 times\\\\\\\"  when injection network partition"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Negative lock count modification in TiKV's CDC delegate, indicating a synchronization or state tracking error during data replication",
    "issue_number": 17229,
    "title": "TiKV Panic: lock_count_modify should never be negative, start_ts:xxx"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unexpected nil/None value during lock count modification, causing an unwrap() panic in the CDC delegate sink processing",
    "issue_number": 17230,
    "title": "tikv panic: called `Option::unwrap()` on a `None` value"
  },
  {
    "bug_location": "CDC (Change Data Capture)",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Improper event handling after encountering an EventError, allowing unfinished incremental scans to continue emitting events to downstream",
    "issue_number": 17233,
    "title": "cdc: no more any events should be emit to a downstream after it retrieves an event error"
  },
  {
    "bug_location": "Profiling Component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect state management in CPU profiling mechanism causing persistent reporting state",
    "issue_number": 17234,
    "title": "Always keep reporting `Already in CPU Profiling`"
  },
  {
    "bug_location": "SST Ingestion Metrics",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Inconsistent documentation and implementation of SST file ingestion blocking behavior, leading to misleading metrics reporting",
    "issue_number": 17239,
    "title": "metrics: contradictory statistics on `non-block` and `block` flushing when `INGEST`."
  },
  {
    "bug_location": "RocksDB engine / Range Cache",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Race condition in delete_range operation that violates snapshot isolation guarantees, allowing potential data read after deletion",
    "issue_number": 17243,
    "title": "In-memory engine: evict in delete_range is too late"
  },
  {
    "bug_location": "PD Client/Cluster Discovery",
    "severity": 3,
    "categories": [
      "Network",
      "Config"
    ],
    "root_cause": "gRPC connection timeout during cluster member discovery, likely caused by insufficient network timeout configuration or network instability between PD servers",
    "issue_number": 17244,
    "title": "[client-bug] java-client test with tikv 6.9.3 failed. "
  },
  {
    "bug_location": "In-memory Engine Range Cache",
    "severity": 3,
    "categories": [
      "Transaction",
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential race condition or inconsistent handling of concurrent upsert operations in range cache during multi-threaded insert scenarios",
    "issue_number": 17245,
    "title": "In-memory Engine: Duplicate error when range cache enabled"
  },
  {
    "bug_location": "log backup task / raftstore",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Read index not ready during region merge operation during TiKV upgrade, preventing log backup snapshot retrieval",
    "issue_number": 17249,
    "title": "log backup task gets stuck and reports error when tikv upgrade"
  },
  {
    "bug_location": "Transaction Engine",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Concurrent transaction conflict with inconsistent timestamp management during high-concurrency read-write workload",
    "issue_number": 17252,
    "title": "In-memory Engine: sysbench oltp_read_write fails with \"assertion failed\""
  },
  {
    "bug_location": "Raft/Storage Layer",
    "severity": 5,
    "categories": [
      "Storage",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Data inconsistency during node recovery and load balancing, likely caused by incomplete record synchronization or index corruption during fault tolerance scenario",
    "issue_number": 17258,
    "title": "tpcc report \"failed Error 8133: data inconsistency in table: customer, index: PRIMARY, index-count:1 != record-count:0\\r\\n\" during tikv balance"
  },
  {
    "bug_location": "In-memory Engine Range Management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Incomplete range deletion tracking mechanism that fails to properly handle canceled or overlapping range loads during snapshot management",
    "issue_number": 17259,
    "title": "In-memory Engine: delete range task has been scheduled multiple times"
  },
  {
    "bug_location": "RocksDB Mutex/Instrumented Mutex",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Multiple threads simultaneously attempting to acquire the same RocksDB mutex, causing a potential deadlock scenario during concurrent database operations",
    "issue_number": 17260,
    "title": "In-memory Engine: Deadlock during TPCC run"
  },
  {
    "bug_location": "CONTRIBUTING.md documentation",
    "severity": 2,
    "categories": [
      "Human",
      "Config"
    ],
    "root_cause": "Inaccurate documentation about build prerequisites for MacOS development environment",
    "issue_number": 17265,
    "title": "Make CONTRIBUTING.md prerequisites more accurate"
  },
  {
    "bug_location": "backup-stream/subscription_manager",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Human"
    ],
    "root_cause": "Unhandled file creation error in test case, likely due to race condition or improper file handling in test setup",
    "issue_number": 17268,
    "title": "test_unretryable_failure failed when \"make test\""
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "High CPU load during TiFlash recovery causing significant QPS degradation in distributed transaction processing",
    "issue_number": 17270,
    "title": "tikv stability: qps drop 67% when one of tiflash recovers from failure due to high cpu of raftstore"
  },
  {
    "bug_location": "RocksDB Storage Component",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Unexpected change in RocksDB bloom filter naming convention during version upgrade, causing filter invalidation",
    "issue_number": 17272,
    "title": "Bloom filter is invalidated after upgrade from <=7.1 to >=7.2"
  },
  {
    "bug_location": "components/raftstore/src/store/fsm/apply.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Incorrect priority handling and premature commit of SST files during delegate processing, causing suboptimal scheduling and unnecessary disk writes",
    "issue_number": 17278,
    "title": "Reduce `apply_ctx.commit(self)` if there are ingest SST commands applying"
  },
  {
    "bug_location": "In-memory engine safe point management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Incorrect safe point inheritance and tracking during range eviction, causing potential snapshot read inconsistency",
    "issue_number": 17282,
    "title": "In-memory engine: safe point may be larger than snapshot read ts"
  },
  {
    "bug_location": "openssl dependency",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Security"
    ],
    "root_cause": "Undefined behavior in openssl library's MemBio buffer handling when buffer is empty",
    "issue_number": 17291,
    "title": "cargo deny warns RUSTSEC-2024-0357"
  },
  {
    "bug_location": "Raft logging component",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Excessive debug logging of large Raft message entries causing performance overhead and potential log bloat",
    "issue_number": 17294,
    "title": "Slow log is too large"
  },
  {
    "bug_location": "Flow Control Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Unexpected flow control mechanism activation during range destruction operation, causing performance degradation",
    "issue_number": 17304,
    "title": "Flow control is triggered when performing destroy range"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Backup Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incomplete cleanup of store-level safepoints when PITR log backup task is stopped or fails",
    "issue_number": 17316,
    "title": "pitr: store level safepoint not cleaned"
  },
  {
    "bug_location": "raft-engine",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect decoding of log headers during disk flush, causing unexpected byte parsing and potential offset errors",
    "issue_number": 17325,
    "title": "raft-engine: panic on decoding unexpected bytes when starting."
  },
  {
    "bug_location": "Log Backup/Restore Import Module",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Memory management inefficiency during log file download and apply process, causing excessive memory consumption and OOM conditions",
    "issue_number": 17327,
    "title": "PiTR log restore is very slow, lots of \"download and apply file failed\" due to \"memory is limited\" and OOM seen."
  },
  {
    "bug_location": "Coprocessor",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect string comparison handling for Unicode whitespace characters in LIKE operator implementation",
    "issue_number": 17332,
    "title": "copr: the behavior of `like` is not correct for string with characters equal with space"
  },
  {
    "bug_location": "IO/Storage Layer",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Potential IO hang causing persistent performance degradation during fault injection scenarios, preventing QPS recovery",
    "issue_number": 17363,
    "title": "qps continued to drop to zero during one of tikv io hang"
  },
  {
    "bug_location": "backup-restore (br) component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incomplete metrics configuration and dashboard organization for backup and restore metrics",
    "issue_number": 17369,
    "title": "br: recently added metrics are not configured in the grafana dashboard, audit and sync all br metrics "
  },
  {
    "bug_location": "build system",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Potential cross-platform compilation incompatibility with MacOS build environment",
    "issue_number": 17370,
    "title": "TiKV build fails in MacOS"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Storage"
    ],
    "root_cause": "Performance scaling inefficiency with massive number of empty regions during database restoration and workload distribution",
    "issue_number": 17375,
    "title": "Although performance degradation under massive regions is a known issue, in addition to merging regions, are there other methods available?"
  },
  {
    "bug_location": "Region Merge Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Inefficient region merging algorithm causing excessive memory consumption during large-scale database restoration with high merge concurrency",
    "issue_number": 17376,
    "title": "During the process of merging regions, TiKV's execution speed is very slow, and it occurs multipl OOM events."
  },
  {
    "bug_location": "raft_log_engine/components/raft_log_engine/src/engine.rs",
    "severity": 5,
    "categories": [
      "Storage",
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "File sequence number corruption during sysbench prepare operation, causing unexpected error in Raft log engine",
    "issue_number": 17383,
    "title": "tikv panic with \u201c[\\\"unexpected error: Store(Other(Other(\\\\\\\"[components/raft_log_engine/src/engine.rs:807]: Corruption: file seqno out of range\\\\\\\")))\u201d when run sysbench prepare"
  },
  {
    "bug_location": "Metrics/Monitoring",
    "severity": 2,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Incorrect metric calculation using 'Replicate Raft Log' instead of commit log for duration tracking",
    "issue_number": 17390,
    "title": "Metric: should use commit log instead of \"Replicate Raft Log\" in Duration Panel"
  },
  {
    "bug_location": "lock_manager/waiter_manager.rs",
    "severity": 5,
    "categories": [
      "Transaction",
      "Performance",
      "Network"
    ],
    "root_cause": "Unbatched RPC messages during lock contention scenarios causing excessive message overhead and potential channel message accumulation",
    "issue_number": 17394,
    "title": "Memory usage may grow unexpectedly and causes OOM in some high-concurrency lock contention scenarios"
  },
  {
    "bug_location": "range_cache_memory_engine/background.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Race condition during region split causing incorrect region state tracking before snapshot loading",
    "issue_number": 17403,
    "title": "In-memory engine: pending region split may cause tikv panic"
  },
  {
    "bug_location": "range_cache_memory_engine/range_manager.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "State transition mismatch between Active and Pending region states during region split operation",
    "issue_number": 17405,
    "title": "In-memory Engine: assert failed \"left: Active != right: Pending\" on RegionManager::split_region"
  },
  {
    "bug_location": "encryption/key_rotation",
    "severity": 5,
    "categories": [
      "Security",
      "Config",
      "CodeBug"
    ],
    "root_cause": "Failed decryption during master key rotation with Google Cloud KMS, likely due to authentication or key access issues",
    "issue_number": 17410,
    "title": "encryption: error occurs when rotating master key"
  },
  {
    "bug_location": "Flashback/Transaction",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Flashback mechanism can interrupt transaction atomicity by rolling back locks at an intermediate timestamp, causing partial transaction visibility",
    "issue_number": 17415,
    "title": "Flashback may break transaction atomicity"
  },
  {
    "bug_location": "Leader Election/Load Balancing Component",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Incorrect node performance detection during scaling, causing unintended leader eviction",
    "issue_number": 17444,
    "title": "the tikv that is being scaled is incorrectly identified as a slow node"
  },
  {
    "bug_location": "RocksDB In-Memory Engine",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential data consistency issue during range deletion and scanning operations in RocksDB skiplist implementation",
    "issue_number": 17456,
    "title": "In-memory engine: prop test failed"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "Storage"
    ],
    "root_cause": "Single-threaded log file upload mechanism becomes bottlenecked under network latency, preventing backup checkpoint advancement",
    "issue_number": 17464,
    "title": "Log flush is very slow when injecting S3 network latency 50ms for 10m"
  },
  {
    "bug_location": "raftstore/peer_fsm",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Race condition during snapshot processing where peer metadata is unexpectedly destroyed before snapshot handling completes",
    "issue_number": 17469,
    "title": "raftstore: TiKV panics due to self was destroyed from meta during receiving snapshot"
  },
  {
    "bug_location": "range_cache_memory_engine/range_manager.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Unexpected missing key during region iteration in in-memory engine, causing a panic when attempting to access a non-existent entry",
    "issue_number": 17483,
    "title": "In-memory Engine: `iter_overlapped_regions` panics no entry found for key"
  },
  {
    "bug_location": "range_cache_memory_engine/background",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Assertion failure in region range containment check during snapshot loading, indicating a potential mismatch between expected and actual region metadata",
    "issue_number": 17484,
    "title": "In-memory Engine: `on_snapshot_load_finished` assertion failed: region.contains_range()"
  },
  {
    "bug_location": "range_cache_memory_engine/background",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Assertion failure in background task state transition, where expected state 'Active' does not match actual state 'LoadingCanceled'",
    "issue_number": 17487,
    "title": "In-memory Engine: `BackgroundTask::LoadRegion` assert fails"
  },
  {
    "bug_location": "range_cache_memory_engine/background.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Assertion failure during snapshot load processing in the in-memory engine, likely due to inconsistent region metadata state during background worker operations",
    "issue_number": 17493,
    "title": "In-memory Engine: `on_snapshot_load_finished` assert fails \"region meta: CacheRegionMeta ...\""
  },
  {
    "bug_location": "raftstore/stale_read",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Race condition in stale read implementation causing async callback drop and test failure",
    "issue_number": 17497,
    "title": "Flaky test tests::failpoints cases::test_replica_stale_read::test_update_apply_index_before_sync_read_state"
  },
  {
    "bug_location": "Transaction/Prewrite",
    "severity": 3,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Missing generation field propagation in prewrite command for TiCDC synchronization",
    "issue_number": 17502,
    "title": "prewrite generation should be sent to TiCDC"
  },
  {
    "bug_location": "Backup and Restore (PiTR) Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Insufficient disk space during log backup restore, causing restore process to fail when writing log backup data",
    "issue_number": 17508,
    "title": "TiKV disk full during PiTR log restore"
  },
  {
    "bug_location": "RaftStore/Region Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "Upgrade"
    ],
    "root_cause": "Potential region metadata inconsistency during Point-in-Time Recovery (PiTR) restore process, causing region unavailability and extended recovery time",
    "issue_number": 17511,
    "title": " \"[tikv:9005]Region is unavailable\" seen for ~1.5h after PiTR restore"
  },
  {
    "bug_location": "range_cache_memory_engine/range_manager.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Assertion failure in new_region_meta method during region metadata management, likely due to unexpected state or invalid region metadata during background processing",
    "issue_number": 17548,
    "title": "In-memory Engine: `new_region_meta` assert fails"
  },
  {
    "bug_location": "range_cache_engine",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential race condition or timeout issue in range cache engine test causing intermittent failures",
    "issue_number": 17556,
    "title": "flaky test: test_load_with_split"
  },
  {
    "bug_location": "In-Memory Engine (IME)",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Region loading mechanism only triggered by write operations, causing read-heavy regions to remain uncached",
    "issue_number": 17561,
    "title": "In-memory engine: pending region may never load"
  },
  {
    "bug_location": "In-memory engine hotspot region identification",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Inefficient automatic hotspot region detection algorithm causing significant delay in performance optimization",
    "issue_number": 17562,
    "title": "In-memory engine: auto load is slow to identify hotspot regions"
  },
  {
    "bug_location": "TiKV Transaction Layer",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "Performance regression in transaction processing logic introduced by PR#17555, causing 7.4% QPS reduction in benchmarksql workload",
    "issue_number": 17568,
    "title": "PR#17555 may cause 7.4% performance regression in benchmarksql"
  },
  {
    "bug_location": "RangeCacheMemoryEngine",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config",
      "Performance"
    ],
    "root_cause": "Configuration changes are not dynamically tracked due to using raw config value instead of tracking mechanism",
    "issue_number": 17578,
    "title": "In-memory engine: online configuration change does not take effect for some configs"
  },
  {
    "bug_location": "server/raftkv/mod.rs",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect duration calculation including task wait time in scheduler worker pool metrics",
    "issue_number": 17579,
    "title": "Storage async write duration is inaccurate"
  },
  {
    "bug_location": "metrics/grafana/common.py",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect metric calculation using rate() instead of increase() for cumulative metrics, causing inaccurate data representation",
    "issue_number": 17586,
    "title": "heatmap panel should use increase instead of rate"
  },
  {
    "bug_location": "async-speed-limit component",
    "severity": 4,
    "categories": [
      "Performance",
      "Config"
    ],
    "root_cause": "Excessive default wait time (0.1 seconds) in flow control priority limiter causing high tail latency",
    "issue_number": 17589,
    "title": "tail lantency is too high when task is limited by flow control priority limiter"
  },
  {
    "bug_location": "Coprocessor Stats Engine",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incomplete handling of edge case where process value is zero during region auto-loading mechanism",
    "issue_number": 17592,
    "title": "In-memory Engine: auto load does not handle the case that process=0"
  },
  {
    "bug_location": "Raft/Region Split Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Race condition during region split and leader transfer when Raft cluster is in joint consensus state, causing synchronization and state transition issues",
    "issue_number": 17602,
    "title": "BatchSplit hang at joint state when execute `BatchSplit` and `move-hot-read-leader` at same time"
  },
  {
    "bug_location": "resource_control",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Duplicate resource consumption and potential unnecessary sleep in transaction task handling, causing inefficient resource management",
    "issue_number": 17603,
    "title": "resource_control: txn task may consume and sleep(if reached quota) twice"
  },
  {
    "bug_location": "CDC Initial Scan Task Management",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Initial scan tasks not properly cancelled or terminated during changefeed state transitions, causing prolonged pending tasks and potential resource blocking",
    "issue_number": 17606,
    "title": "TiKV initial scan task not cancel/termicated when changefeed paused or failed"
  },
  {
    "bug_location": "status_server",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential memory profiling configuration or implementation issue preventing heap dump generation via HTTP status server",
    "issue_number": 17607,
    "title": "[status_server] no heap dump result via http"
  },
  {
    "bug_location": "TiDB SQL Expression Engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of datetime arithmetic overflow conditions, potentially leading to incorrect calculation results or unexpected behavior during date/time manipulation",
    "issue_number": 17608,
    "title": "expression: date_add/date_sub does not handle overflow calculation properly"
  },
  {
    "bug_location": "resource_control/future.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Timing assertion in test case is too strict, causing intermittent test failures due to slight variations in execution duration",
    "issue_number": 17612,
    "title": "flaky test test_limited_future"
  },
  {
    "bug_location": "engine_rocks/RocksDB configuration loader",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Config"
    ],
    "root_cause": "Invalid RocksDB configuration parsing, causing failure to load database options due to malformed configuration file",
    "issue_number": 17614,
    "title": "tikv panic repeatedly after injection one of tikv failure and recovery"
  },
  {
    "bug_location": "Encoding/Character Set Processing",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incomplete or incorrect handling of GBK/GB18030 character encoding during complex query operations, specifically in grouping and aggregation functions",
    "issue_number": 17618,
    "title": "`encoding failed` when processing queries with GBK/GB18030"
  },
  {
    "bug_location": "CDC (Change Data Capture) Incremental Scan Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Inefficient handling of RocksDB tombstones during frequent table/partition drops, causing expensive iterator scanning across entire regions",
    "issue_number": 17620,
    "title": "truncate or drop table frequently may increase cdc incremental scan CPU usage obviously"
  },
  {
    "bug_location": "in_memory_engine/write_batch",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Unexpected non-empty save points during write batch eviction in in-memory engine, causing an assertion failure",
    "issue_number": 17630,
    "title": "In-memory engine: save point empty assertion failed"
  },
  {
    "bug_location": "raftstore/store/fsm/peer",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Unwrapping a None value during region bucket refresh, likely caused by an unexpected state during region size update",
    "issue_number": 17639,
    "title": "tikv panic repeatedly when tikv rolling restart after update region size"
  },
  {
    "bug_location": "TiKV Memory Management",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Potential memory allocation inefficiency or leak during TPC-C benchmark workload on Rocky Linux 8.9 with cgroup v2",
    "issue_number": 17641,
    "title": "tikv oom when using tiup on `rocky-8.9` for deployment"
  },
  {
    "bug_location": "GC Worker / In-memory Engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Direct RocksDB delete_range call bypasses WriteBatchWrapper, preventing proper region eviction tracking",
    "issue_number": 17644,
    "title": "In-memory Engine: GC worker delete range does not evict relevant regions"
  },
  {
    "bug_location": "MVCC Transaction Layer",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Inconsistent index and record state during concurrent delete and read operations, likely due to race condition in MVCC transaction management",
    "issue_number": 17645,
    "title": "In-memory Engine: data inconsistency in table: xx, index: yy, index-count:50 != record-count:49"
  },
  {
    "bug_location": "raftstore/stale_read",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Race condition in stale read implementation causing inconsistent state synchronization between Raft replicas",
    "issue_number": 17654,
    "title": "Flaky test `tests::failpoints cases::test_replica_stale_read::test_update_apply_index_before_sync_read_state`"
  },
  {
    "bug_location": "in_memory_engine/region_manager",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Synchronization error in garbage collection state tracking, likely a race condition in atomic flag management during region GC process",
    "issue_number": 17667,
    "title": "In-memory engine: in gc assertion failed"
  },
  {
    "bug_location": "CI/Build System",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Dependency management issue causing plugin library not being built during test compilation",
    "issue_number": 17671,
    "title": "build: CI tasks are failed on testing loading coprocessor plugins."
  },
  {
    "bug_location": "Transaction/Index",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Unique index constraint validation failure in distributed transaction processing, likely due to race condition or inconsistent constraint enforcement across TiKV nodes",
    "issue_number": 17673,
    "title": "tidb 5.7.25-TiDB-v5.4.3\u4e2d\u552f\u4e00\u7d22\u5f15\u9650\u5236\u5931\u6548"
  },
  {
    "bug_location": "Monitoring/Grafana Integration",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Histogram parsing failure in metrics visualization component",
    "issue_number": 17676,
    "title": "grafana histogram parse error "
  },
  {
    "bug_location": "pd_worker",
    "severity": 3,
    "categories": [
      "Performance",
      "LoadBalance",
      "Storage"
    ],
    "root_cause": "Slow score calculation mechanism fails to accurately reflect IO hang conditions, preventing proper node health detection",
    "issue_number": 17679,
    "title": "Slow score fails to update during IO hang"
  },
  {
    "bug_location": "Dependency Management",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Yanked crate futures-util 0.3.15 in Cargo.lock preventing successful build",
    "issue_number": 17689,
    "title": "yanked crate futures-util 0.3.15"
  },
  {
    "bug_location": "RocksDB Index Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Upgrade",
      "Storage"
    ],
    "root_cause": "Performance regression in index creation after RocksDB upgrade, potentially related to changes in storage engine optimization or indexing algorithm",
    "issue_number": 17691,
    "title": "add index is slowly compared to last version after upgrade rocksdb"
  },
  {
    "bug_location": "TiKV Storage Engine (Titan)",
    "severity": 5,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Memory management issue in Titan storage engine when handling large regions and concurrent backup/replication workloads",
    "issue_number": 17696,
    "title": "TiDB latency not stable and TiKV OOM if running cdc changefeed when titan is on and average region size is large"
  },
  {
    "bug_location": "in_memory_engine/write_batch.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Inconsistent region state during write preparation, causing an assertion failure when attempting to prepare a write for a region that is not the current active region",
    "issue_number": 17700,
    "title": "In-memory Engine: assert fails region 26972 is prepared for write before, but it is not the current region"
  },
  {
    "bug_location": "Raft Entry Processing",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Inefficient write batch size calculation when large Raft entry max size is configured, causing excessive memory allocation and latency",
    "issue_number": 17701,
    "title": "When raft entry max size is large, write batch size would be too large"
  },
  {
    "bug_location": "test infrastructure / compilation system",
    "severity": 2,
    "categories": [
      "CodeBug"
    ],
    "root_cause": "Compilation incompatibility in test modules, likely related to conditional compilation or import dependencies",
    "issue_number": 17707,
    "title": "*: Some tests fail to compile"
  },
  {
    "bug_location": "Raft/Peer Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "LoadBalance"
    ],
    "root_cause": "Resolved timestamp not advancing, causing leadership and replication issues in Raft consensus mechanism",
    "issue_number": 17708,
    "title": "resolved_ts doesn't push forward, it's stuck, which will cause ebs backup failed [restore for 15520]"
  },
  {
    "bug_location": "Backup/Restore Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Upgrade"
    ],
    "root_cause": "Lack of proper configuration validation during backup and restore process when Titan storage engine configuration differs between backup and restore clusters",
    "issue_number": 17709,
    "title": "If titan is enabled for the backup cluster, tikv will crash if titan is not enabled for restore. [restore for 15945]"
  },
  {
    "bug_location": "Restore/Backup Component",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Potential memory corruption or invalid state during volume restore operation",
    "issue_number": 17710,
    "title": "tikv crash during volumerestore [restore for 16673]"
  },
  {
    "bug_location": "Transaction Resolver",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Incorrect timestamp comparison logic causing resolved-ts to potentially equal commit timestamp, violating transaction resolution semantics",
    "issue_number": 17728,
    "title": "Resolved-ts must be strictly smaller than locks' min_commit_ts when calculating it"
  },
  {
    "bug_location": "TiFlash/Proxy",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Config",
      "Performance"
    ],
    "root_cause": "Configuration incompatibility during version upgrade from v6.1.7 to nightly, with unrecognized configuration options and gRPC service disruption",
    "issue_number": 17731,
    "title": "tiflash is always in `Disconnected` state when upgrading from v6.1.7 to nightly"
  },
  {
    "bug_location": "raftstore/raft_log_engine",
    "severity": 5,
    "categories": [
      "Storage",
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential data corruption or inconsistent state in Raft log engine after TiFlash node recovery",
    "issue_number": 17760,
    "title": "tikv panic after down one of tiflash and recover"
  },
  {
    "bug_location": "hybrid_engine/load_eviction",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Assertion failure during peer destruction when region peers list becomes unexpectedly empty during in-memory engine load eviction process",
    "issue_number": 17767,
    "title": "[In-memory-engine] tikv panic \"assertion failed: !region.get_peers().is_empty()\" when run workload"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 5,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Performance bottleneck in processing old_value_cb during large region size scenarios with Titan storage enabled, causing significant replication lag",
    "issue_number": 17781,
    "title": "CDC lag up to 2.5h when titan is on and region size is large (280MB+)"
  },
  {
    "bug_location": "in-memory-engine",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage",
      "Config"
    ],
    "root_cause": "Performance instability when in-memory engine configuration parameters are enabled, causing QPS jitter with increased amplitude",
    "issue_number": 17782,
    "title": "[In-memory-engine] qps jitter amplitude is larger when enable IME"
  },
  {
    "bug_location": "in_memory_engine/background",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Assertion failure during garbage collection in in-memory engine background task, likely due to unexpected state or race condition during long-running TPCC workload",
    "issue_number": 17788,
    "title": "[In-memory-engine] tikv panic \"[\\\"assertion failed: success\\\"] [backtrace=\\\"   0: tikv_util::set_panic_hook::{{closure}}\\\\n             at workspace/source/tikv/components/tikv_util/src/lib.rs:479:18\" after run tpcc a long time"
  },
  {
    "bug_location": "RaftStore Metrics",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Histogram metrics only capture region size during split planning, leading to incomplete region size tracking and potentially misleading metrics representation",
    "issue_number": 17791,
    "title": "Region Approximate Size metrics panel has misleading semantics"
  },
  {
    "bug_location": "in_memory_engine/region_manager.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Assertion failure in region metadata management during write batch clearing, indicating potential race condition or inconsistent state tracking in region metadata",
    "issue_number": 17797,
    "title": "flaky test: cases::test_in_memory_engine::test_load_with_eviction"
  },
  {
    "bug_location": "Backup and Restore (BR) component",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "ResizableRuntime implementation causing excessive test runtime overhead",
    "issue_number": 17807,
    "title": "br: The ResizableRuntime make some unit test taking too long to finish"
  },
  {
    "bug_location": "server/storage_stats_task",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Unhandled connection abort error during storage statistics initialization, causing an unwrap() panic when encountering an unexpected network/IO error",
    "issue_number": 17818,
    "title": "tikv panic [lib.rs:480] [\\\"called `Result::unwrap()` on an `Err` value: Other(Os { code: 103, kind: ConnectionAborted, message: \\\\\\\"Software caused connection abort\\\\\\\" })\\\""
  },
  {
    "bug_location": "import/sst_service",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Network"
    ],
    "root_cause": "Unhandled future polling error during PD failure injection while adding multi-index",
    "issue_number": 17830,
    "title": "tikv panics when injecting pd failure during add multi index"
  },
  {
    "bug_location": "In-memory Engine (IME)",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Performance regression in in-memory engine implementation during bank workload testing",
    "issue_number": 17838,
    "title": "In-memory Engine: bank workload has more than 12% regression when IME is enabled"
  },
  {
    "bug_location": "raftstore/store/entry_storage.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Null/None value unwrap during Raft log garbage collection after region merge operation, indicating a potential race condition or incomplete state management in merge process",
    "issue_number": 17840,
    "title": "tikv panic in on_raft_gc_log_tick after region is merged"
  },
  {
    "bug_location": "SQL Function Evaluation Engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incomplete implementation of mathematical function handling in TiKV's SQL processing layer",
    "issue_number": 17852,
    "title": "TiKV panic when using `radians` or `degrees`"
  },
  {
    "bug_location": "RocksDB metrics component",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Incomplete implementation of RocksDB bloom filter efficiency tracking metrics",
    "issue_number": 17866,
    "title": "RocksDB bloom filter efficiency metrics need to be fixed"
  },
  {
    "bug_location": "raftstore/entry_storage",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Assertion failure in entry cache append logic, indicating an index tracking inconsistency in Raft log entry management",
    "issue_number": 17868,
    "title": "TiKV panic with \"assertion failed: cached_last < trunc_to_idx\""
  },
  {
    "bug_location": "backup-restore component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Config"
    ],
    "root_cause": "Potential thread configuration issue causing restore process to stall during cluster recovery",
    "issue_number": 17874,
    "title": "br: restore failed on TiDB Cloud"
  },
  {
    "bug_location": "src/server/resolve.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Transaction"
    ],
    "root_cause": "Improper handling of tombstone store resolution in PD client, causing infinite error logging and reconnection attempts when encountering removed store IDs",
    "issue_number": 17875,
    "title": "resolving tikv store address return an error \"invalid store ID xxxx, not found\" leading to raft client infinite loop"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Network",
      "Transaction"
    ],
    "root_cause": "Unhandled Option::unwrap() on a None value during network partition, causing panic in sink_delete method of CDC delegate",
    "issue_number": 17876,
    "title": "multiple tikv panic when injection network partition between one of tikv and one of tidb"
  },
  {
    "bug_location": "signal/trap handling",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Network"
    ],
    "root_cause": "Invalid signal handling on MacOSX for TOGGLE_PROF_SIG, causing unexpected panic during signal registration",
    "issue_number": 1860,
    "title": "panic with TOGGLE_PROF_SIG in MacOSX"
  },
  {
    "bug_location": "server/grpc_service.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Incorrect snapshot task scheduling during leader crash, causing incomplete snapshot transfer and potential follower node panic",
    "issue_number": 1907,
    "title": "Snapshot truncated if leader crashes during snapshot transferring"
  },
  {
    "bug_location": "Raft snapshot transfer/application module",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incomplete validation of snapshot key range overlap during snapshot application, potentially causing data inconsistency or incorrect state transfer",
    "issue_number": 201,
    "title": "Check if we can apply snapshot when there is overlapped key ranges"
  },
  {
    "bug_location": "raftstore/region_heartbeat",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unexpected peer state tracking during server down scenario in Raft consensus mechanism, causing test failure in peer status detection logic",
    "issue_number": 2080,
    "title": "test: test_server_down_peers failed"
  },
  {
    "bug_location": "RocksDB build configuration",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incomplete SSE4.2 compilation flag configuration preventing hardware acceleration of CRC32 computation",
    "issue_number": 2303,
    "title": "static_unportable_release doesn't enable SSE in RocksDB"
  },
  {
    "bug_location": "Monitoring/Metrics",
    "severity": 2,
    "categories": [
      "Config"
    ],
    "root_cause": "Missing metric configuration or dashboard visualization for PD component in Grafana monitoring setup",
    "issue_number": 2494,
    "title": "metric: missing PD metric on Grafana"
  },
  {
    "bug_location": "region_cache.go",
    "severity": 4,
    "categories": [
      "Network",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "RPC request timeout causing store cache invalidation and potential routing/addressing inconsistency between TiKV stores and PD",
    "issue_number": 2496,
    "title": "bug: StoreNotMatch"
  },
  {
    "bug_location": "RocksDB Configuration Management",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config",
      "Storage"
    ],
    "root_cause": "Inconsistent RocksDB configuration between tikv-server and tikv-ctl, causing user properties to be unintentionally cleaned during database access",
    "issue_number": 2537,
    "title": "tikv-ctl should use the same config for opening rocksdb as tikv-servers"
  },
  {
    "bug_location": "storage::raftkv",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "LoadBalance"
    ],
    "root_cause": "Leadership transition causing read request failure during lease period",
    "issue_number": 2643,
    "title": "storage::test_raftkv::test_read_leader_in_lease randomly failed"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Stale command handling during leader change with uncommitted log entries",
    "issue_number": 2649,
    "title": "test_server_leader_change_with_uncommitted_log fails with stale command"
  },
  {
    "bug_location": "RaftStore/Compaction",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Unexpected zero bytes during lock column family compaction in test scenario",
    "issue_number": 2659,
    "title": "test_server_compact_lock_cf fail"
  },
  {
    "bug_location": "raftstore/transport",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unable to get leader of region during partition write test, indicating potential leadership election or consensus protocol instability",
    "issue_number": 2863,
    "title": "test_server_partition_write  failed on master"
  },
  {
    "bug_location": "storage",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Potential race condition or intermittent failure in garbage collection test scenario",
    "issue_number": 2909,
    "title": "test: test_storage_1gc sometimes failed in CI"
  },
  {
    "bug_location": "RaftStore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unable to elect a leader during configuration change, potentially causing split-brain scenario in Raft consensus",
    "issue_number": 2917,
    "title": "test: raftstore_cases::test_conf_change::test_server_split_brain failed"
  },
  {
    "bug_location": "RaftStore Lease Read Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Timeout during node callback handling when node is being destroyed, indicating potential race condition or improper resource management in lease read mechanism",
    "issue_number": 2948,
    "title": "test: raftstore_cases::test_lease_read::test_node_callback_when_destroyed panic"
  },
  {
    "bug_location": "Snapshot Generation Test",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Test directory not being properly cleaned or reset before snapshot generation test",
    "issue_number": 2957,
    "title": "test: test failpoints_cases::test_snap::test_generate_snapshot failed "
  },
  {
    "bug_location": "Unknown",
    "severity": 1,
    "categories": [
      "Unknown"
    ],
    "root_cause": "Insufficient issue details provided",
    "issue_number": 2966,
    "title": "for test sync "
  },
  {
    "bug_location": "Unknown",
    "severity": 1,
    "categories": [
      "Unknown"
    ],
    "root_cause": "Insufficient information provided in issue description",
    "issue_number": 2968,
    "title": "for test sync"
  },
  {
    "bug_location": "Unknown",
    "severity": 1,
    "categories": [
      "Unknown"
    ],
    "root_cause": "Insufficient information provided in issue description",
    "issue_number": 2969,
    "title": "for test sync"
  },
  {
    "bug_location": "Raft Configuration",
    "severity": 2,
    "categories": [
      "Config",
      "CodeBug"
    ],
    "root_cause": "Incorrect configuration placement in TOML template for WAL sync bytes parameter",
    "issue_number": 2977,
    "title": "conf: Raft wal-bytes-per-sync is in the wrong place "
  },
  {
    "bug_location": "Unknown",
    "severity": 1,
    "categories": [
      "Unknown"
    ],
    "root_cause": "Insufficient information provided in issue description",
    "issue_number": 2983,
    "title": "for test sync"
  },
  {
    "bug_location": "raftstore/store/util.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Lease state validation failure in Raft consensus mechanism, causing unexpected state transition between Valid and Expired states",
    "issue_number": 3044,
    "title": "thread 'raftstore::store::util::tests::test_lease' panicked at 'assertion failed"
  },
  {
    "bug_location": "Documentation System",
    "severity": 2,
    "categories": [
      "Human",
      "Config"
    ],
    "root_cause": "Broken documentation link or page removal without proper redirection",
    "issue_number": 3065,
    "title": "'How to use TiKV' is now unavailable"
  },
  {
    "bug_location": "Docker Build System",
    "severity": 2,
    "categories": [
      "Config",
      "Upgrade"
    ],
    "root_cause": "Outdated Docker image build configuration preventing automatic updates of Rust compiler version",
    "issue_number": 3077,
    "title": "Docker images aren't updating"
  },
  {
    "bug_location": "RaftStore/Region Leadership",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unable to retrieve leader for region during snapshot application test, likely due to leadership election or synchronization issue",
    "issue_number": 3079,
    "title": "raftstore_cases::test_split_region::test_server_apply_new_version_snapshot' panicked"
  },
  {
    "bug_location": "RaftStore/Region Leadership",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unable to retrieve leader for region during split operation test, indicating potential leadership election or state synchronization issue",
    "issue_number": 3080,
    "title": "'raftstore_cases::test_split_region::test_node_base_split_region_left_derive' panicked"
  },
  {
    "bug_location": "Region Recovery Mechanism",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Overly aggressive node deletion during region recovery that can potentially lose committed transaction logs",
    "issue_number": 3092,
    "title": "Unsafe recover should only drop some nodes instead of all down nodes"
  },
  {
    "bug_location": "Snapshot Generation Module",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Directory cleanup/preparation logic failure during snapshot test generation",
    "issue_number": 3129,
    "title": "test failpoints_cases::test_snap::test_generate_snapshot failed "
  },
  {
    "bug_location": "documentation",
    "severity": 1,
    "categories": [
      "Human"
    ],
    "root_cause": "Typographical error in documentation text",
    "issue_number": 3134,
    "title": "small readme typo"
  },
  {
    "bug_location": "Raftstore/Configuration Change",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Failure to retrieve expected key-value during learner node configuration change test, indicating potential synchronization or state transfer issue in Raft consensus implementation",
    "issue_number": 3147,
    "title": "Test test_node_learner_conf_change failed"
  },
  {
    "bug_location": "RaftStore/Replica Configuration",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Replica count adjustment mechanism failed to converge within expected timeout, preventing correct replica distribution",
    "issue_number": 3162,
    "title": "Test test_server_auto_adjust_replica failed"
  },
  {
    "bug_location": "Scheduler",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Synchronous slow log printing in scheduler thread blocking task processing",
    "issue_number": 3202,
    "title": "Avoid slow log blocking the scheduler"
  },
  {
    "bug_location": "Snapshot Generation Test",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Temporary files not being properly cleaned up during snapshot generation test, causing unexpected directory contents",
    "issue_number": 3215,
    "title": "failpoints_cases::test_snap::test_generate_snapshot failed"
  },
  {
    "bug_location": "CI/Build System",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Potential build configuration or compilation error in master branch causing continuous integration test failure",
    "issue_number": 3231,
    "title": "master CI failed"
  },
  {
    "bug_location": "RaftStore/Region Merge",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Region merge operation failed to complete, causing test failure during node restart scenario",
    "issue_number": 3255,
    "title": "test failpoints_cases::test_merge::test_node_merge_restart  failed"
  },
  {
    "bug_location": "logging/tracing",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Logging configuration not preserving source file and line number metadata during log generation",
    "issue_number": 3261,
    "title": "log not outputting source"
  },
  {
    "bug_location": "Raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Failure to retrieve leader during test scenario involving uncommitted log entries, potentially indicating a race condition or leadership election instability in Raft consensus protocol",
    "issue_number": 3262,
    "title": "test test_server_remove_leader_with_uncommitted_log failed"
  },
  {
    "bug_location": "server/readpool",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Test timeout mechanism not correctly handling thread synchronization, causing unexpected blocking or race condition in read pool thread management",
    "issue_number": 3266,
    "title": "Adjust read pool test timeouts"
  },
  {
    "bug_location": "raftstore/transport",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Network"
    ],
    "root_cause": "Leadership election or communication failure during node partition scenario, causing write operation to fail with 'peer is not leader' error",
    "issue_number": 3267,
    "title": "'raftstore_cases::test_transport::test_node_partition_write' panicked"
  },
  {
    "bug_location": "Raftstore/Region Merge",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Region merge operation failed to complete, causing a potential split-brain scenario where regions are not successfully merged as expected",
    "issue_number": 3268,
    "title": "test raftstore_cases::test_merge::test_node_merge_brain_split failed"
  },
  {
    "bug_location": "raftstore/cluster",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network"
    ],
    "root_cause": "Insufficient qualified stores during multi-node test setup, preventing test cluster initialization",
    "issue_number": 3272,
    "title": "Test test_multi_node_random_latency failed"
  },
  {
    "bug_location": "util/mpsc.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Timing-related assertion failure in MacOS environment, likely due to platform-specific timer precision or scheduling differences",
    "issue_number": 3298,
    "title": "test_unbounded fails on MacOS constantly"
  },
  {
    "bug_location": "src/util/mpsc.rs",
    "severity": 2,
    "categories": [
      "CodeBug"
    ],
    "root_cause": "Potential silent integer type conversion from u32 to i64 with possible data loss",
    "issue_number": 3299,
    "title": "clippy warnings in master"
  },
  {
    "bug_location": "CI/Test Infrastructure",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Clippy linting checks not properly enforced in continuous integration pipeline",
    "issue_number": 3300,
    "title": "CI is not failed when there is clippy issues"
  },
  {
    "bug_location": "storage/iterator",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Iterator state management inconsistency between seek() return value and valid() method, causing unexpected cursor behavior when seeking beyond data range",
    "issue_number": 3378,
    "title": "Cursor remains valid when seek to nothing again"
  },
  {
    "bug_location": "Region Leader Election",
    "severity": 3,
    "categories": [
      "CodeBug",
      "LoadBalance"
    ],
    "root_cause": "Incorrect leader staleness detection logic that fails to validate local peer's leadership status before triggering alert",
    "issue_number": 3384,
    "title": "False stale peer alert"
  },
  {
    "bug_location": "Raft consensus module test bench",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Transaction"
    ],
    "root_cause": "Race condition in message notification mechanism during network partition recovery tests",
    "issue_number": 3385,
    "title": "Some Prevote tests cannot pass reliably."
  },
  {
    "bug_location": "tikv-server startup process",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Lack of proper error handling and logging during startup when system configuration requirements are not met",
    "issue_number": 3387,
    "title": "tikv should provide error message if it can't start"
  },
  {
    "bug_location": "store.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Lack of heartbeat message processing in single-node TiKV cluster causing leader election failures across multiple regions",
    "issue_number": 3393,
    "title": "Single-node TiKV cluster logs abnormal leader missing "
  },
  {
    "bug_location": "RaftKV Storage Component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Unexpected nil snapshot during batch snapshot test, indicating potential snapshot retrieval or generation failure",
    "issue_number": 3434,
    "title": "test: test_batch_snapshot failed "
  },
  {
    "bug_location": "tikv-ctl binary",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Human"
    ],
    "root_cause": "Missing argument handling in main function causing panic when no arguments are provided",
    "issue_number": 3453,
    "title": "Starting `tikv-ctl` with no arguments results in unreachable!()"
  },
  {
    "bug_location": "RawKVApi Go client",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Client-side RPC connection failure during high-volume large payload insertions, causing silent key insertion failures without proper error propagation",
    "issue_number": 3474,
    "title": "Go client does not correctly insert, and gives no error"
  },
  {
    "bug_location": "logging/tracing",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Trace log evaluation occurs even when trace logging is disabled, causing unnecessary function execution and performance overhead",
    "issue_number": 3537,
    "title": "trace! is executed when log level > trace"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Hard state not saved for uninitialized peer, allowing term regression and potential concurrent leader election",
    "issue_number": 3572,
    "title": "Potential two different leaders on a same term"
  },
  {
    "bug_location": "tikv-ctl CLI tool",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Improper URL parsing and handling of schema prefixes in compact command execution",
    "issue_number": 3655,
    "title": "tikv-ctl compact run infinitely"
  },
  {
    "bug_location": "Test Infrastructure",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Potential issue with test runner configuration or panic handling mechanism in CI environment that prevents proper test result interpretation",
    "issue_number": 3664,
    "title": "#[should_panic] unit tests cannot pass CI"
  },
  {
    "bug_location": "test-bench",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential race condition or timing issue in region split test scenario",
    "issue_number": 3669,
    "title": "test_delay_split_region failed"
  },
  {
    "bug_location": "Coprocessor Selection Executor",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect implementation of start_scan and stop_scan interfaces in selection executor, causing potential scan operation interruption when combining selection with table or index scan",
    "issue_number": 3677,
    "title": "Selection executor does not handle start_scan or stop_scan correctly"
  },
  {
    "bug_location": "logging/async-logger",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Async logger configured to drop messages instead of blocking when channel is full, potentially losing critical diagnostic information",
    "issue_number": 3695,
    "title": "Consider choosing block strategy for slog-async"
  },
  {
    "bug_location": "RocksDB Background Thread",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential memory corruption or invalid memory access in RocksDB background thread during large dataset import",
    "issue_number": 3709,
    "title": "TiKV Server crashed in rocksdb.bg2"
  },
  {
    "bug_location": "coprocessor/codec/mysql/time",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of ambiguous datetime values during timezone conversions, particularly around daylight saving time transitions",
    "issue_number": 3721,
    "title": "tikv does not handle ambiguous time zones correctly"
  },
  {
    "bug_location": "gRPC server component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Performance"
    ],
    "root_cause": "Potential memory access violation or segmentation fault in TiKV server, likely related to memory management or gRPC server implementation in version 2.1.0-rc.4",
    "issue_number": 3744,
    "title": "three tikv crashes with version 2.1.0-rc.4"
  },
  {
    "bug_location": "coprocessor/codec/mysql/decimal",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Memory access violation during decimal division and modulo operation test, likely caused by buffer overflow or uninitialized memory access in vector manipulation",
    "issue_number": 3764,
    "title": "test_div_mod cannot pass Valgrind memcheck"
  },
  {
    "bug_location": "raftstore::store::snap::Snap",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Invalid memory access during snapshot generation, likely due to memory management or pointer handling error in snapshot building process",
    "issue_number": 3767,
    "title": "test_storage_apply_snapshot possible invalid memory write"
  },
  {
    "bug_location": "test-bench worker management",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Human"
    ],
    "root_cause": "Improper worker lifecycle management with missing explicit worker stop/release mechanism",
    "issue_number": 3768,
    "title": "Worker not correctly released in a lot of tests"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition or timing issue during leader-follower split operation that causes test failure in distributed consensus protocol",
    "issue_number": 3793,
    "title": "test_follower_slow_split failed"
  },
  {
    "bug_location": "test-bench",
    "severity": 2,
    "categories": [
      "CodeBug"
    ],
    "root_cause": "Potential test infrastructure or timing-related failure in test_handle_time method",
    "issue_number": 3804,
    "title": "test_handle_time failed"
  },
  {
    "bug_location": "Go Client Dependency Management",
    "severity": 3,
    "categories": [
      "Config",
      "Upgrade"
    ],
    "root_cause": "Incompatible dependency versions between etcd, gRPC, and TiKV client libraries causing import conflicts",
    "issue_number": 3828,
    "title": "tikv go-client dependency install failed"
  },
  {
    "bug_location": "test_bench",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Unknown"
    ],
    "root_cause": "Potential race condition or synchronization issue in region change observer test suite",
    "issue_number": 3829,
    "title": "test_region_change_observer failed"
  },
  {
    "bug_location": "test_kv_service",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential test infrastructure or service interaction failure in KV service testing framework",
    "issue_number": 3830,
    "title": "test_kv_service failed"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network"
    ],
    "root_cause": "Potential network partition handling issue causing node split brain scenario during cluster state synchronization",
    "issue_number": 3831,
    "title": "test_node_split_brain failed"
  },
  {
    "bug_location": "RaftStore/LeaseRead",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition or improper node destruction handling during lease read operation",
    "issue_number": 3833,
    "title": "test_node_callback_when_destroyed failed"
  },
  {
    "bug_location": "test-bench logging system",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Log prefix matching broke after test utility code refactoring, causing log collection failure",
    "issue_number": 3858,
    "title": "Tests logs disappeared"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition or incorrect replica adjustment logic in automatic replica management",
    "issue_number": 3860,
    "title": "test_server_auto_adjust_replica fails"
  },
  {
    "bug_location": "block_cache_metrics",
    "severity": 2,
    "categories": [
      "Performance",
      "Config"
    ],
    "root_cause": "Incorrect metric aggregation for block cache size across different TiKV server configurations",
    "issue_number": 3862,
    "title": "max block cache metrics shows uncorrectly"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Stale region metadata synchronization between PD and TiKV after region merge, causing inconsistent region state tracking",
    "issue_number": 3868,
    "title": "PD receives stale region info from TiKV"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of modulo operation with absolute value on signed integers during query filtering",
    "issue_number": 3893,
    "title": "Either mod or abs has bug"
  },
  {
    "bug_location": "HTTP Response Handler",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Network"
    ],
    "root_cause": "Duplicate header generation in HTTP response middleware, likely due to incorrect header configuration or middleware chain implementation",
    "issue_number": 3894,
    "title": "Bug: duplicate HTTP response headers"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Inconsistent peer configuration during region management, where an expected peer ID is missing from the region configuration",
    "issue_number": 3902,
    "title": " test: test_server_stale_peer_out_of_region failed"
  },
  {
    "bug_location": "copr/codec/mysql/time",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect handling of zero timestamp conversion, causing unexpected datetime adjustment and incorrect zero time validation in the `invalid_zero` method",
    "issue_number": 3953,
    "title": "copr/codec/mysql/time: invalid_zero function isn't working as expected."
  },
  {
    "bug_location": "raftstore::store::fsm::router",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unhandled SendError during router shutdown, causing unexpected panic in test benchmark",
    "issue_number": 3959,
    "title": "test raftstore::store::fsm::router::tests::bench_send failed"
  },
  {
    "bug_location": "split-check component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential memory corruption during LZ4 decompression in split-check operation, likely caused by unsafe memory access in LZ4_copy8 function",
    "issue_number": 3964,
    "title": "Tikv segfault at split-check"
  },
  {
    "bug_location": "coprocessor::endpoint",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Test assertion failure in handle time processing, indicating potential performance or timing inconsistency in coprocessor execution details",
    "issue_number": 3965,
    "title": "test coprocessor::endpoint::tests::test_handle_time failed"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition or synchronization issue during server configuration change in Raft consensus protocol implementation",
    "issue_number": 3966,
    "title": "test_server_safe_conf_change failed"
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Inconsistent peer configuration during region split operation, causing a missing peer ID in region configuration",
    "issue_number": 3969,
    "title": "raftstore::test_stale_peer::test_server_stale_peer_without_data_left_derive_when_split failed"
  },
  {
    "bug_location": "tikv::coprocessor::dag::expr::builtin_compare::ScalarFunc::in_string",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Inefficient string comparison algorithm in IN clause implementation, causing quadratic time complexity when comparing a value against multiple strings",
    "issue_number": 3977,
    "title": "in_string can be very slow"
  },
  {
    "bug_location": "Transaction Layer",
    "severity": 3,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Inconsistent transaction state tracking with orphaned records and timestamp misalignment during concurrent test scenarios",
    "issue_number": 4005,
    "title": "Gabage outputs in tests"
  },
  {
    "bug_location": "Raft snapshot merge mechanism",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition or incorrect synchronization during multiple snapshot merge operations in Raft consensus protocol implementation",
    "issue_number": 4026,
    "title": "test_node_merge_multiple_snapshots_together failed"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition or incorrect state management in learner replica handling during Raft consensus protocol",
    "issue_number": 4027,
    "title": "test_stale_learner failed"
  },
  {
    "bug_location": "raftstore/test_prevote",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unexpected PreVote or PreVoteResponse behavior during node failure scenario in Raft consensus protocol implementation",
    "issue_number": 4029,
    "title": "test_prevote_reboot_minority_followers failed"
  },
  {
    "bug_location": "RocksDB compression layer",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Memory corruption during LZ4 decompression in RocksDB block processing",
    "issue_number": 4048,
    "title": "tikv-server segfault with rocksdb:low2"
  },
  {
    "bug_location": "rust-rocksdb compilation process",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Missing Google gflags namespace declaration during compilation of RocksDB dependency",
    "issue_number": 4049,
    "title": "Build TiKV failed because 'google' has not been declared when compile rust-rocksdb"
  },
  {
    "bug_location": "raftstore/region_split",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unexpected region split behavior causing test assertion failure in region splitting logic",
    "issue_number": 4053,
    "title": "Test test_server_half_split_region fails"
  },
  {
    "bug_location": "Raft library message handling",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Overwriting message response in loop prevents multiple slave nodes from receiving read index response",
    "issue_number": 4071,
    "title": "raft lib bug: In handle_heartbeat_response, the leader will lose the reply request of readindex"
  },
  {
    "bug_location": "coprocessor/codec/mysql/time",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incomplete implementation of SQL mode ALLOW_INVALID_DATES in TiKV coprocessor time parsing logic",
    "issue_number": 4100,
    "title": "coprocessor: allow invalid dates SQL mode"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition or incorrect leader removal logic during configuration change in Raft consensus protocol",
    "issue_number": 4101,
    "title": "test_conf_change_remove_leader failed"
  },
  {
    "bug_location": "raftstore/store/fsm/batch",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition or inconsistent state in batch processing test scenario",
    "issue_number": 4102,
    "title": "raftstore::store::fsm::batch::tests::test_batch fails unexpectedly"
  },
  {
    "bug_location": "server::load_statistics::linux",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential race condition or unstable test implementation in Linux thread load statistic measurement",
    "issue_number": 4103,
    "title": "server::load_statistics::linux::tests::test_thread_load_statistic fails unexpectedly"
  },
  {
    "bug_location": "util::mpsc::batch test module",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Unknown"
    ],
    "root_cause": "Potential race condition or synchronization issue in multi-producer single-consumer batch channel test implementation",
    "issue_number": 4104,
    "title": "util::mpsc::batch::tests::test_batch_receiver fails unexpectedly"
  },
  {
    "bug_location": "raftstore/store/peer_storage.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Channel closure during snapshot application in Raft configuration change, causing unexpected panic during leader removal test",
    "issue_number": 4113,
    "title": "test: test_conf_change_remove_leader failed"
  },
  {
    "bug_location": "test-bench",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Code coverage tool not properly handling test failure scenarios",
    "issue_number": 4122,
    "title": "cargo tarpaulin  will report code coverage even if run tests failed"
  },
  {
    "bug_location": "raftstore snapshot garbage collection",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Snapshot garbage collection mechanism failed to clean up temporary snapshot files during test execution",
    "issue_number": 4127,
    "title": "raftstore::test_snap::test_server_snap_gc fails unexpectedly"
  },
  {
    "bug_location": "gRPC networking component",
    "severity": 3,
    "categories": [
      "Network",
      "CodeBug"
    ],
    "root_cause": "Incorrect IPv6 address parsing and handling in gRPC address resolution mechanism",
    "issue_number": 4136,
    "title": "TiKV fails to start with IPV6 address"
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Failure during region split operation with an empty key, causing an unexpected error when attempting to find a region",
    "issue_number": 4139,
    "title": "test raftstore::test_split_region::test_node_half_split_region failed"
  },
  {
    "bug_location": "Raft component",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Network"
    ],
    "root_cause": "Incorrect metric labeling using store ID instead of network address",
    "issue_number": 4172,
    "title": "should use address for the unreachable metric "
  },
  {
    "bug_location": "Titan storage component",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "File handle leak in Titan blob storage, preventing proper file descriptor closure and causing unnecessary disk space consumption",
    "issue_number": 4182,
    "title": "Titan may forget to close the file handle. "
  },
  {
    "bug_location": "Raft consensus module test bench",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition or synchronization issue in test configuration change scenario during Raft consensus state transition",
    "issue_number": 4193,
    "title": "cases::test_conf_change::test_write_after_destroy fails on CI"
  },
  {
    "bug_location": "raftstore",
    "severity": 2,
    "categories": [
      "Performance",
      "Human"
    ],
    "root_cause": "Excessive thread creation during concurrent test execution causing resource exhaustion",
    "issue_number": 4219,
    "title": "test failed with \"Resource temporarily unavailable\""
  },
  {
    "bug_location": "test logging infrastructure",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Test binary not compiled with jemalloc memory allocation library as expected",
    "issue_number": 4220,
    "title": "test log wrapper does not contain jemalloc"
  },
  {
    "bug_location": "TiKV/Importer Store Discovery",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Network"
    ],
    "root_cause": "Failure to filter out tombstoned (decommissioned) stores during store discovery, leading to potential connection attempts to invalid store addresses",
    "issue_number": 4221,
    "title": "TiKV/Importer should ignore tombstoned stores"
  },
  {
    "bug_location": "raftstore/store/fsm/store.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unhandled Option::unwrap() on a None value during store heartbeat processing with PD (Placement Driver)",
    "issue_number": 4224,
    "title": "raftstore panic when staring TiKV"
  },
  {
    "bug_location": "raftstore/conf_change",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Replica count adjustment mechanism failing to converge to expected state within timeout period",
    "issue_number": 4225,
    "title": "test raftstore::test_conf_change::test_node_auto_adjust_replica failed"
  },
  {
    "bug_location": "snap-send process",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "CodeBug"
    ],
    "root_cause": "Resource leak in file descriptor management, causing excessive pipe and eventpoll file descriptors to remain open without proper cleanup",
    "issue_number": 4233,
    "title": "too many pipe and eventpoll files "
  },
  {
    "bug_location": "raftstore/store/fsm/router",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential race condition or synchronization issue in Raftstore router test suite",
    "issue_number": 4244,
    "title": "raftstore::store::fsm::router::tests::test_basic failed"
  },
  {
    "bug_location": "test_stale_read",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition during region merging and stale read operations causing test failure",
    "issue_number": 4246,
    "title": "cases::test_stale_read::test_stale_read_during_merging failed"
  },
  {
    "bug_location": "coprocessor::endpoint",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Test assertion failure in handle time processing, indicating potential performance measurement or timing inconsistency in coprocessor endpoint test",
    "issue_number": 4247,
    "title": "test coprocessor::endpoint::tests::test_handle_time failed"
  },
  {
    "bug_location": "pd/client.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network"
    ],
    "root_cause": "Duplicate PD endpoint validation failure causing connection issues during leader restart test",
    "issue_number": 4258,
    "title": "tests: test_restart_leader_insecure failed"
  },
  {
    "bug_location": "Titan storage engine",
    "severity": 3,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Incompatibility between Titan's file management and DeleteFilesInRanges operation causing potential read inconsistency",
    "issue_number": 4288,
    "title": "Titan is not compatible with DeleteFilesInRanges"
  },
  {
    "bug_location": "coprocessor/endpoint.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Error wrapping mechanism incorrectly handles parsing errors, causing incorrect error reporting and potential snapshot retrieval failures",
    "issue_number": 4293,
    "title": "coprocessor: parsing error is never returned"
  },
  {
    "bug_location": "rocksdb/import/engine",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Assertion failure in encrypted file size handling during SST file ingestion, likely due to incorrect file prefix length calculation",
    "issue_number": 4295,
    "title": "test: test_sst_writer(I guess) failed with assertion failed in rocksdb"
  },
  {
    "bug_location": "util/MustConsumeVec",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Destructor panics when vector is not empty, causing potential double-panic and process abort",
    "issue_number": 4309,
    "title": "MustConsumeVec double-panics in dtor"
  },
  {
    "bug_location": "PendingCmd destructor",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Panic in destructor when callback is not handled, which violates Rust's destructor guidelines and can cause unexpected program termination",
    "issue_number": 4310,
    "title": "PendingCmd panics on drop"
  },
  {
    "bug_location": "gRPC credentials builder",
    "severity": 4,
    "categories": [
      "Security",
      "CodeBug"
    ],
    "root_cause": "Improper handling of sensitive credential data during object destruction, potentially leaving secrets exposed in memory",
    "issue_number": 4311,
    "title": "ChannelCredentialsBuilder leaks secrets on drop"
  },
  {
    "bug_location": "logging/rotating_file_logger",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Panic in destructor during file flush operation, violating Rust API guidelines for safe resource cleanup",
    "issue_number": 4312,
    "title": "RotatingFileLoggerPanics on drop"
  },
  {
    "bug_location": "RocksEngineCore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Potential panic in destructor during thread join handle resolution, which violates Rust safety guidelines for destructors",
    "issue_number": 4313,
    "title": "RocksEngineCore can probably panic on drop"
  },
  {
    "bug_location": "codec/buffer",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Memory reallocation failure in buffer vector implementation during test case",
    "issue_number": 4318,
    "title": "test buffer::tests::test_vec_reallocate failed"
  },
  {
    "bug_location": "pd-client",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Config"
    ],
    "root_cause": "Stale PD member list cache prevents TiKV from dynamically updating cluster membership information when all original PD members are replaced",
    "issue_number": 4319,
    "title": "TiKV may fail to find PD cluster if all PD members are changed "
  },
  {
    "bug_location": "Network Port Binding",
    "severity": 3,
    "categories": [
      "Network",
      "Config",
      "CodeBug"
    ],
    "root_cause": "Improper socket port binding configuration using SO_REUSEPORT without proper conflict detection",
    "issue_number": 4323,
    "title": "TiKV may not report errors if port is in conflict"
  },
  {
    "bug_location": "coprocessor/codec/convert.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Inconsistent string-to-integer conversion handling between TiDB and TiKV, causing unexpected error when processing scientific notation with large exponents",
    "issue_number": 4327,
    "title": "float_str_to_int_string should not return error"
  },
  {
    "bug_location": "Log Management",
    "severity": 3,
    "categories": [
      "Storage",
      "Config",
      "Network"
    ],
    "root_cause": "Incomplete log truncation mechanism in containerized/Kubernetes environment causing potential log loss during TiKV server restart",
    "issue_number": 4328,
    "title": "TiKV lost logs in Kubernetes/Docker environment"
  },
  {
    "bug_location": "server/node.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Upgrade"
    ],
    "root_cause": "Incomplete initialization state handling during TiKV restart, causing store identification and bootstrapping failure",
    "issue_number": 4333,
    "title": "TiKV may fail to restart if it fails after writing store ident and before bootstrapping cluster"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Potential race condition or data consistency issue during configuration change in Raft consensus protocol, causing key-value retrieval failure",
    "issue_number": 4356,
    "title": "test: raftstore::test_conf_change::test_server_split_brain failed"
  },
  {
    "bug_location": "logging/fatal! macro",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Log buffer not being flushed immediately during critical error scenarios, causing incomplete log output",
    "issue_number": 4358,
    "title": "fatal! macro doesn't flush logs"
  },
  {
    "bug_location": "CaseTraceLogger",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incomplete logging mechanism in trace logging implementation that fails to capture key-value store metadata during cluster initialization",
    "issue_number": 4361,
    "title": "slog kvs are missing in CaseTraceLogger "
  },
  {
    "bug_location": "test-bench",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential issue with thread load statistic measurement mechanism causing incorrect CPU usage reporting",
    "issue_number": 4364,
    "title": "test: test_thread_load_statistic fails on CI"
  },
  {
    "bug_location": "Build System/Compilation",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Compilation configuration not properly detecting or enabling SSE4.2 instruction set support on compatible Intel CPU",
    "issue_number": 4367,
    "title": "error: ./target/debug/tikv-server does not enable sse4.2"
  },
  {
    "bug_location": "tikv-server startup configuration",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Improper handling of default data directory path during server initialization, causing path canonicalization failure",
    "issue_number": 4392,
    "title": "tikv-server doesn't do data-dir check on startup if using the default data-dir value"
  },
  {
    "bug_location": "tikv_alloc",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Problematic feature configuration in cargo that makes conditional compilation of memory allocator difficult to manage",
    "issue_number": 4409,
    "title": "Reorganize tikv_alloc's cargo features"
  },
  {
    "bug_location": "Build System",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Compilation configuration incompatibility with system memory allocator",
    "issue_number": 4410,
    "title": "TiKV doesn't build with the system allocator"
  },
  {
    "bug_location": "Build System",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Missing protoc compiler dependency in Dockerfile build environment",
    "issue_number": 4422,
    "title": "protoc required by build but not included in Dockerfile"
  },
  {
    "bug_location": "test-bench",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "External dependency (Tarpaulin) has a known issue preventing code coverage measurement",
    "issue_number": 4491,
    "title": "Coverage is not working"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Peer configuration inconsistency during leader transfer in Raft consensus protocol test scenario",
    "issue_number": 4506,
    "title": "test_server_transfer_leader_safe fails"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Incorrect handling of unapplied log entries during leader read operations, causing unexpected read responses",
    "issue_number": 4507,
    "title": "test_server_read_leader_with_unapplied_log fails"
  },
  {
    "bug_location": "tcmalloc crate",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incompatible and uncontrolled libunwind linking during build process",
    "issue_number": 4520,
    "title": "Link libunwind into tcmalloc crate"
  },
  {
    "bug_location": "logging/endpoint",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Overly verbose protobuf logging causing readability issues",
    "issue_number": 4523,
    "title": "Make logs of protobuf structures more friendly"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incomplete progress tracking of learner nodes during log replication, causing potential log discontinuity and system instability",
    "issue_number": 4551,
    "title": "Progress of learner should also be considered before merging"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Incorrect log synchronization during region merge causing inconsistent applied and commit indices",
    "issue_number": 4560,
    "title": "Raft logs in CommitMerge should forward to raftstore first"
  },
  {
    "bug_location": "Raft leader election component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Inconsistent index tracking during log compaction and peer merge operations, causing potential state mismatch between truncated_index and applied_index",
    "issue_number": 4581,
    "title": "Panic on leader election after `catch_up_log_for_merge`"
  },
  {
    "bug_location": "TiKV Client gRPC Connection",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "Transaction"
    ],
    "root_cause": "gRPC streaming connection repeatedly failing during high-load sysbench workload, causing repeated client-side connection recreation attempts",
    "issue_number": 4584,
    "title": "infinite \"batchRecvLoop re-create streaming fail\" after sysbench"
  },
  {
    "bug_location": "tikv::coprocessor::codec::mysql::Duration",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Integer overflow during negation operation in Duration method, likely caused by handling extreme or edge case nanosecond values",
    "issue_number": 4609,
    "title": "Fuzz test failed for tikv::coprocessor::codec::mysql::Duration"
  },
  {
    "bug_location": "tikv::coprocessor::codec::mysql::Duration::parse",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Integer multiplication overflow during duration parsing, likely due to insufficient input validation or boundary checking in the parsing logic",
    "issue_number": 4610,
    "title": "Fuzz test failed for tikv::coprocessor::codec::mysql::Duration::parse"
  },
  {
    "bug_location": "coprocessor",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Large memory size of Result type causing inefficient memory copying and performance overhead",
    "issue_number": 4676,
    "title": "coprocessor::Result<T> is at least 176 bytes.."
  },
  {
    "bug_location": "raftstore/store/peer.rs",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Incorrect lease management during leader transfer causing potential stale read inconsistencies",
    "issue_number": 4680,
    "title": "ReadIndex when transferring leader may not be correct"
  },
  {
    "bug_location": "logging/panic_hook",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Resource deadlock during thread synchronization in panic hook logging mechanism, likely caused by circular dependency or improper thread drop handling",
    "issue_number": 4686,
    "title": "Resource deadlock avoided (os error 35) on panic hook"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incomplete datetime validation and processing logic that does not fully match MySQL's behavior for edge case datetime scenarios",
    "issue_number": 4692,
    "title": "A series of date time related bugs"
  },
  {
    "bug_location": "Dependency Management",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Security",
      "Upgrade"
    ],
    "root_cause": "Vulnerable protobuf crate version (2.0.4) with potential out-of-memory issue",
    "issue_number": 4703,
    "title": "cargo audit error"
  },
  {
    "bug_location": "raftstore/store/peer.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Network",
      "Transaction"
    ],
    "root_cause": "Premature error handling in message sending logic causing potential message loss and communication disruption in Raft consensus protocol",
    "issue_number": 4717,
    "title": "Sending messages should be more tolerant"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition or inconsistent state during peer re-addition in Raft consensus protocol test scenario",
    "issue_number": 4736,
    "title": "raftstore::test_tombstone::test_server_readd_peer failed"
  },
  {
    "bug_location": "storage/mvcc/reader",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Incorrect garbage collection (GC) decision logic based on row version without considering delete versions and compaction state",
    "issue_number": 4740,
    "title": "MvccProperty should consider delete version"
  },
  {
    "bug_location": "storage::lock_manager::waiter_manager",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition or synchronization issue in waiter manager test suite",
    "issue_number": 4742,
    "title": "storage::lock_manager::waiter_manager::tests::test_waiter_manager failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 1,
    "categories": [
      "CodeBug"
    ],
    "root_cause": "Minor spelling inconsistencies in code comments or documentation",
    "issue_number": 4758,
    "title": "*: Many possible spelling mistakes."
  },
  {
    "bug_location": "ReadIndex Service",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Network"
    ],
    "root_cause": "Duplicate context generation in ReadIndex requests causing potential request conflicts and potential read loss between leader and follower nodes",
    "issue_number": 4764,
    "title": "ReadIndex service may got duplicate `ctx` "
  },
  {
    "bug_location": "RaftStore/LeaseRead",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Region peer configuration failure during local read cache test, causing disconnection and region not found errors",
    "issue_number": 4776,
    "title": "test: test_local_read_cache failed"
  },
  {
    "bug_location": "Build System",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Inconsistent SSE configuration handling during build process, where SSE check script does not respect explicit SSE disable configuration",
    "issue_number": 4900,
    "title": "should not check SSE if we disable it"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Leader transfer failure during network partition scenario, likely due to incorrect minority/majority detection logic in Raft consensus protocol implementation",
    "issue_number": 4902,
    "title": "test_prevote_partition_leader_in_minority_detect_in_majority failed"
  },
  {
    "bug_location": "raftstore/store/fsm/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unhandled race condition during region merge shutdown where message transmission may fail before result unwrapping",
    "issue_number": 5032,
    "title": "Panic on force send commit merge during shutdown "
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5034,
    "title": "libtitan_sys built failed on gcc 9.1"
  },
  {
    "bug_location": "Test Profiling/Debug Configuration",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Debug information configuration prevents line number display for libtikv components during test backtraces",
    "issue_number": 5049,
    "title": "Line number isn't displayed in backtrace for tests"
  },
  {
    "bug_location": "raftstore/store/peer",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unhandled error conditions during snapshot and merge operations causing unexpected panics in Raft message sending and transport layer",
    "issue_number": 5054,
    "title": "test: test cases::test_merge::test_node_request_snapshot_reject_merge failed"
  },
  {
    "bug_location": "Documentation",
    "severity": 2,
    "categories": [
      "Human",
      "CodeBug"
    ],
    "root_cause": "Outdated or incorrect documentation links in README.md and wiki pages",
    "issue_number": 5064,
    "title": "Broken link in Wiki (again)"
  },
  {
    "bug_location": "Build System",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Makefile rule inconsistency with Dockerfile build target",
    "issue_number": 5069,
    "title": "build_release is not appear in makefile rules"
  },
  {
    "bug_location": "server::raft_client",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Network",
      "Transaction"
    ],
    "root_cause": "Unexpected behavior in Raft client reconnection test, where the test expects an error condition that is not being triggered",
    "issue_number": 5073,
    "title": "test: test server::raft_client::test_raft_client_reconnect failed"
  },
  {
    "bug_location": "Build System / Cargo Integration",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config",
      "Upgrade"
    ],
    "root_cause": "Potential cargo build pipelining race condition during library compilation, causing intermittent library format resolution failures",
    "issue_number": 5130,
    "title": "error: crate `librocksdb_sys` required to be available in rlib format, but was not found in this form"
  },
  {
    "bug_location": "Raft Consensus Module",
    "severity": 3,
    "categories": [
      "Transaction",
      "CodeBug",
      "LoadBalance"
    ],
    "root_cause": "Potential misunderstanding of Raft replica distribution and fault tolerance mechanism",
    "issue_number": 5159,
    "title": "How does 5 tikv nodes handle two nodes fault?"
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Network"
    ],
    "root_cause": "Potential race condition or network isolation handling issue in Raft consensus protocol leader election mechanism",
    "issue_number": 5186,
    "title": "Unstable test: raftstore::test_prevote::test_server_isolated_follower_leader_does_not_change"
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Potential race condition or inconsistent state during random server restart test scenario, causing key retrieval failure",
    "issue_number": 5188,
    "title": "`raftstore::test_multi::test_multi_server_random_restart` spuriously failed"
  },
  {
    "bug_location": "Raft Log Replication",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Network"
    ],
    "root_cause": "Potential race condition or synchronization issue during node merge and log catch-up process, causing test failures in log replication restart scenarios",
    "issue_number": 5202,
    "title": "test_node_merge_catch_up_logs_restart failed"
  },
  {
    "bug_location": "storage/raft_storage",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Unexpected non-None value during garbage collection test, causing assertion failure in test_auto_gc function",
    "issue_number": 5206,
    "title": "test storage::test_raft_storage::test_auto_gc panic "
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Inconsistent peer configuration during region merge operation, specifically a missing learner peer with ID 2 in the region configuration",
    "issue_number": 5207,
    "title": "test raftstore::test_merge::test_node_merge_with_slow_learner failed"
  },
  {
    "bug_location": "raftstore::store::fsm::router",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Channel or router queue became full during benchmark, causing an unwrap() panic when sending messages",
    "issue_number": 5209,
    "title": "bench raftstore::store::fsm::router::tests::bench_send failed"
  },
  {
    "bug_location": "engine/util.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Inconsistent file deletion behavior in range operation, causing test assertion failure between expected and actual file count",
    "issue_number": 5239,
    "title": "util::tests::test_delete_all_files_in_range can fail"
  },
  {
    "bug_location": "Build System/Cargo Configuration",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Missing 'profiler' component directory during Docker build process, causing Cargo build failure",
    "issue_number": 5245,
    "title": "tag 3.0.2 build container error"
  },
  {
    "bug_location": "server/config.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Overly strict validation of advertise address prevents valid zero-based hostnames from being accepted, causing configuration rejection",
    "issue_number": 5273,
    "title": "Cannot create TiKV with advertise addr starting with `0`"
  },
  {
    "bug_location": "Test Framework",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Unknown"
    ],
    "root_cause": "Insufficient test log details to definitively determine specific technical cause",
    "issue_number": 528,
    "title": "investigate failed test "
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition during leader transfer and read index operation in Raft consensus protocol implementation",
    "issue_number": 5284,
    "title": "CI failed in raftstore::test_lease_read::test_read_index_when_transfer_leader_1"
  },
  {
    "bug_location": "Configuration Parser",
    "severity": 3,
    "categories": [
      "Config",
      "Upgrade"
    ],
    "root_cause": "Configuration schema mismatch during version upgrade, indicating changes in configuration key names or structure between TiKV versions",
    "issue_number": 5286,
    "title": "unknown field `import-dir`, expected `num-threads` or `stream-channel-window` for key `import`"
  },
  {
    "bug_location": "Cargo Build System",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Removal of binary target configuration in Cargo.toml breaking standard Rust project build workflow",
    "issue_number": 5289,
    "title": "Cargo Run workflow no longer usable"
  },
  {
    "bug_location": "raftstore/coprocessor/split_check/keys.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Function only considers transactional KV regions, ignoring raw KV regions when calculating approximate keys",
    "issue_number": 5319,
    "title": "approximate_keys for raw kv region always be 0"
  },
  {
    "bug_location": "raftstore/region_heartbeat",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Timestamp synchronization issue in region heartbeat test causing test failure",
    "issue_number": 5329,
    "title": "Test test_region_heartbeat_timestamp failed"
  },
  {
    "bug_location": "TiKV metrics server",
    "severity": 3,
    "categories": [
      "Security",
      "Config"
    ],
    "root_cause": "TLS configuration not consistently applied across all TiKV service endpoints during cluster setup",
    "issue_number": 5340,
    "title": "Metrics page not using HTTPS after TLS enabled"
  },
  {
    "bug_location": "TiKV SQL Type Conversion",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incomplete or incorrect data type handling for NULL and complex type conversions during SQL query processing",
    "issue_number": 535,
    "title": "mysql-test InvalidDataType"
  },
  {
    "bug_location": "raftstore/store/fsm/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Multiple concurrent mailbox shutdown attempts causing meta corruption during peer destruction process",
    "issue_number": 5365,
    "title": "TiKV panic with multiple shutdown mailbox "
  },
  {
    "bug_location": "raftstore/peer.rs",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Lease expiration mechanism causing unnecessary remote lease renewals when thread count increases, leading to performance overhead",
    "issue_number": 5388,
    "title": "Lease expire rate increases as thread count increases"
  },
  {
    "bug_location": "TLS/SSL Connection Handling",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Network",
      "Config"
    ],
    "root_cause": "Incorrect SSL certificate file path resolution in LibreSSL implementation",
    "issue_number": 5389,
    "title": "LibreSSL can not find a valid cert file"
  },
  {
    "bug_location": "TiDB Query Codec",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Insufficient type conversion validation when converting Decimal to Double in TPC-H Q1 query processing",
    "issue_number": 5404,
    "title": "TPC-H Q1 lead to TiKV Panic"
  },
  {
    "bug_location": "RaftStore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Stale command during leader transfer in Raft consensus protocol test scenario",
    "issue_number": 5411,
    "title": "Test test_server_pd_transfer_leader failed"
  },
  {
    "bug_location": "RaftStore/PD Client",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Inconsistent region peer configuration during replica read test, where expected peer configuration does not match actual region state",
    "issue_number": 5467,
    "title": "unstable test: cases::test_replica_read::test_wait_for_apply_index'"
  },
  {
    "bug_location": "storage/raft_storage",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Stale command processing error during transaction commit, likely caused by race condition or outdated state in Raft consensus mechanism",
    "issue_number": 5499,
    "title": "storage::test_raft_storage::test_auto_gc, panicked at 'called `Result::unwrap()` on an `Err` value: Txn(Engine(Request(message: \"stale command\")))', src/libcore/result.rs:1165:5"
  },
  {
    "bug_location": "components/tidb_query",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Multiple symbol definition conflict during linking, likely caused by duplicate symbol definitions in OpenSSL and gRPC libraries",
    "issue_number": 5509,
    "title": "run `cargo test` in master's components/tidb_query failed"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Leadership election and peer state synchronization issue in Raft consensus protocol implementation",
    "issue_number": 5516,
    "title": "raftstore::test_tombstone::test_server_readd_peer response header { error { message: \"peer is not leader for region 1, leader may None\" not_leader { region_id: 1 } } } has error"
  },
  {
    "bug_location": "pd::worker",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Assertion failure in CPU usage calculation during PD client statistics collection, indicating potential statistical tracking or calculation error",
    "issue_number": 5579,
    "title": "test pd::tests::test_collect_stats failed "
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition or inconsistent state handling during leader read with unapplied log entries",
    "issue_number": 5611,
    "title": "test_server_read_leader_with_unapplied_log failed"
  },
  {
    "bug_location": "test-bench/region-split",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential performance bottleneck or deadlock in region splitting mechanism causing test timeout",
    "issue_number": 5628,
    "title": "test cases::test_split_region::test_follower_slow_split has been running for over 60 seconds"
  },
  {
    "bug_location": "Region Replication Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Memory exhaustion during large-scale region replication, likely due to inefficient memory management or insufficient memory allocation for high-volume data transfer",
    "issue_number": 575,
    "title": "tikv-server failed to replicate lots of regions with OOM error."
  },
  {
    "bug_location": "backup component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Leadership election or region state synchronization issue causing leader lookup failure during backup test",
    "issue_number": 5786,
    "title": "test_backup::test_backup_meta failed"
  },
  {
    "bug_location": "RocksDB/Storage Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Config"
    ],
    "root_cause": "Memory management issue with large number of regions causing memory exhaustion and potential instruction corruption",
    "issue_number": 5851,
    "title": "Tikv 2.1.18 crashes with Illegal instruction and the cluster is very unstable "
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Incomplete peer state management during region split and learner peer initialization, causing stale and orphaned peer information",
    "issue_number": 5852,
    "title": "Split may leave stale region info"
  },
  {
    "bug_location": "raftstore/store/fsm/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unhandled edge case where a new peer with empty region info receives a read request before applying snapshot, causing panic",
    "issue_number": 5875,
    "title": "Replica read may cause tikv panic"
  },
  {
    "bug_location": "RaftStore Transport",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Network"
    ],
    "root_cause": "Unable to get leader of region during secure connection test, likely due to transport layer configuration or initialization issue",
    "issue_number": 5893,
    "title": "test: test_secure_connect failed"
  },
  {
    "bug_location": "raftstore/store/fsm/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Race condition during region merge with leader change, causing unexpected None value unwrapping",
    "issue_number": 5981,
    "title": "May meet panic when processing merge with leader changed"
  },
  {
    "bug_location": "tidb_query_codegen",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Inconsistent function metadata generation in code generation for RPN functions, specifically in init_metadata return type and constructor generation",
    "issue_number": 6033,
    "title": "tidb_query_codegen unit test fails"
  },
  {
    "bug_location": "tidb_query/codec/mysql/decimal.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Insufficient input validation in read_decimal causing integer overflow during parsing of corrupted decimal data",
    "issue_number": 6044,
    "title": "read_decimal may panic when data is corrupted"
  },
  {
    "bug_location": "build/test configuration",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Feature configuration mismatch preventing failpoint tests from being executed during standard build process",
    "issue_number": 6097,
    "title": "Failpoint tests will not run when make dev"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Inconsistent replica list management during unsafe recovery with partial replica corruption",
    "issue_number": 6107,
    "title": "tikv-ctl unsafe recover can lead to inconsistent replica list"
  },
  {
    "bug_location": "tikv_util component",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Incompatible method call in protobuf-build library, likely due to version mismatch or API change",
    "issue_number": 6145,
    "title": "Fail to execute `cargo check` in `components/tikv_util` "
  },
  {
    "bug_location": "lock_manager",
    "severity": 3,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Potential race condition or synchronization issue during region shuffling that causes stale command errors in deadlock detection test",
    "issue_number": 6153,
    "title": "test_detect_deadlock_when_shuffle_region failed"
  },
  {
    "bug_location": "GC (Garbage Collection) module",
    "severity": 2,
    "categories": [
      "Transaction",
      "Performance"
    ],
    "root_cause": "Unnecessary GC triggering on idle cluster causing excessive seek operations without active writes",
    "issue_number": 6165,
    "title": "GC is always triggered on idle cluster"
  },
  {
    "bug_location": "lock_manager",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unable to get leader of region during test, causing test failure when attempting to transfer leader in a deadlock detection scenario",
    "issue_number": 6226,
    "title": "test server::lock_manager::test_detect_deadlock_when_shuffle_region failed"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Leader transfer failure in Raft consensus mechanism, likely due to inconsistent state during region hibernation",
    "issue_number": 6247,
    "title": "Test test_server_down_peers_with_hibernate_regions fails"
  },
  {
    "bug_location": "Raft consensus/network partition handling",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Inadequate network partition resilience in distributed consensus protocol, causing service unavailability under split-brain scenarios",
    "issue_number": 6254,
    "title": "TiKV fails to serve after network partition"
  },
  {
    "bug_location": "Metrics/Monitoring Component",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential race condition or inconsistent thread I/O statistic collection during concurrent test execution",
    "issue_number": 6277,
    "title": "test_thread_io_statistics fails on CI"
  },
  {
    "bug_location": "raftstore/store/fsm/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Race condition in handling configuration change results during snapshot restoration, causing unexpected membership state conflicts",
    "issue_number": 6344,
    "title": "Panic on late arriving confchange result"
  },
  {
    "bug_location": "RaftStore",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Incorrect snapshot region tracking allowing overlapping snapshot applications that can cause data inconsistency",
    "issue_number": 6366,
    "title": "Two overlapped snapshot may be applied together which may cause panic or loss of data"
  },
  {
    "bug_location": "Raft configuration change component",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Inefficient snapshot generation and transmission process with multiple heartbeat delays causing configuration change latency",
    "issue_number": 6420,
    "title": "Conf change is too slow"
  },
  {
    "bug_location": "RawKV storage component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect key encoding in batch get request processing for raw key-value operations",
    "issue_number": 6437,
    "title": "RawGet may be incorrect when get request batch is enabled"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect hibernation logic that fails to properly handle pending read requests, potentially causing request blocking",
    "issue_number": 6439,
    "title": "peer shouldn't hibernate if it has pending reads"
  },
  {
    "bug_location": "components/tidb_query/src/rpn_expr/impl_math.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Strict floating point comparison without considering numerical precision",
    "issue_number": 6448,
    "title": "clippy fails because of strict comparison of f64"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Race condition during region merge process where log synchronization fails to ensure complete data transfer between source and target peers",
    "issue_number": 6460,
    "title": "A failed merge may cause a later success merge lost data"
  },
  {
    "bug_location": "Configuration Validation Component",
    "severity": 3,
    "categories": [
      "Config",
      "Human"
    ],
    "root_cause": "Insufficient configuration option name validation allowing misspelled or incorrect configuration keys to be silently accepted without error",
    "issue_number": 6498,
    "title": "Configuration validation doesn't find misspelled option names in config file"
  },
  {
    "bug_location": "RocksDB Write Column Family",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Keys are not being scanned and added in lexicographically sorted order during snapshot generation and backup processes",
    "issue_number": 6519,
    "title": "RocksDB scanning Write CF returns keys out of order"
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 3,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential data inconsistency between RaftDB and KVDB during power failure when sync-log is disabled",
    "issue_number": 6540,
    "title": "When sync-log=false, fsync raftdb before kvdb persists data"
  },
  {
    "bug_location": "Fuzzing Infrastructure",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Protobuf build method `generate_files()` not found, causing compilation failure during fuzzer setup",
    "issue_number": 6591,
    "title": "Can not run fuzzer"
  },
  {
    "bug_location": "backup-restore component",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Potential file writing/upload failure causing SST files to be zeroed during S3 backup",
    "issue_number": 6594,
    "title": "backup: upload to S3 may have been corrupted"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Transaction"
    ],
    "root_cause": "Leader hibernation interrupts heartbeat mechanism, causing leadership discovery failure and blocking read index requests during replica restart",
    "issue_number": 6596,
    "title": "raftstore: read index may fail after node restart"
  },
  {
    "bug_location": "raftstore/store/read_queue.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unwrapping a None value during replica read processing, indicating a potential race condition or incorrect state management in the read index queue",
    "issue_number": 6607,
    "title": "tikv panic about replica reads"
  },
  {
    "bug_location": "RocksDB Memory Allocator",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Assertion failure in JemallocNodumpAllocator destructor during memory deallocation",
    "issue_number": 6651,
    "title": "Coredump when running test case"
  },
  {
    "bug_location": "Raft Region Management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Hibernated regions not properly synchronized after node restart, causing potential replica read performance degradation",
    "issue_number": 6667,
    "title": "Broadcast WakeUp message for awakening hibernated regions after node restarts"
  },
  {
    "bug_location": "Raft Log Replication",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "Inefficient log synchronization mechanism during idle region periods, causing delayed log propagation due to infrequent heartbeat intervals",
    "issue_number": 6670,
    "title": "Catch up logs is too slow when region is idle"
  },
  {
    "bug_location": "TiKV Query Execution Engine",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Non-deterministic query execution causing inconsistent result sets during TPC-H benchmark queries, likely related to concurrent transaction processing or index join implementation",
    "issue_number": 6676,
    "title": "TPC-H q8 & q21 results are wrong on master"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Unsafe caching mechanism that can return stale or incorrect data when using different timestamps for the same request payload",
    "issue_number": 6690,
    "title": "Coprocessor Cache is unsound"
  },
  {
    "bug_location": "lib.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Logging order of startup messages has been altered, with configuration checks and PD connection messages now preceding the welcome message",
    "issue_number": 6702,
    "title": "Welcome message are not the first message when starting anymore"
  },
  {
    "bug_location": "gRPC client configuration in raft_client.rs",
    "severity": 4,
    "categories": [
      "Network",
      "Config",
      "Performance"
    ],
    "root_cause": "Default gRPC message size limit (10MiB) is insufficient for batch gRPC operations where multiple messages are merged, causing connection resets when total message size exceeds limit",
    "issue_number": 6727,
    "title": "The default size limit of grpc is too small after using batch grpc"
  },
  {
    "bug_location": "deadlock_detector",
    "severity": 2,
    "categories": [
      "Transaction",
      "Network"
    ],
    "root_cause": "Incorrect leader role change detection causing unnecessary reconnection attempts when leader remains unchanged",
    "issue_number": 6985,
    "title": "Deadlock detector sometimes reconnects to the same leader"
  },
  {
    "bug_location": "test configuration management",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Test configuration files not being properly cleaned up or generated in temporary directories during test execution",
    "issue_number": 6994,
    "title": "tests resource leak"
  },
  {
    "bug_location": "transaction/pessimistic_lock",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Inefficient lock acquisition algorithm when multiple Write::Lock entries exist, causing significant performance degradation during pessimistic locking operations",
    "issue_number": 7024,
    "title": "`acquire_pessimistic_lock` has very poor performance if there are many `Write::Lock`s"
  },
  {
    "bug_location": "storage/mvcc/txn.rs",
    "severity": 3,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Incorrect handling of pessimistic lock conflicts in pipelined transaction processing, causing unintended transaction rollback",
    "issue_number": 7070,
    "title": "Pipelined pessimistic lock may roll back a normal transaction"
  },
  {
    "bug_location": "raftstore/store/fsm/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "UUID comparison mismatch during Raft consensus operation, indicating potential data inconsistency or synchronization issue",
    "issue_number": 7104,
    "title": "tikv panic"
  },
  {
    "bug_location": "Read Pool",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Unintended performance regression in read pool implementation, likely introduced by commit d2af249",
    "issue_number": 7122,
    "title": "Performance regression on Sysbench between Feb 10 and Feb 28"
  },
  {
    "bug_location": "pd-client",
    "severity": 5,
    "categories": [
      "Network",
      "CodeBug"
    ],
    "root_cause": "gRPC connection retry mechanism fails to properly re-establish communication with PD after restart, preventing region heartbeats",
    "issue_number": 7124,
    "title": "TiKV cannot send messages to PD after reconnect"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incomplete column metadata handling during complex multi-table join with left and right join operations",
    "issue_number": 713,
    "title": "Coprocessor Bug"
  },
  {
    "bug_location": "Coprocessor/Expression Evaluation",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Incorrect type conversion of Bytes column during boolean evaluation, causing inconsistent query behavior when converting non-numeric string to integer",
    "issue_number": 7141,
    "title": "unexpected behavior for bytes column in where clause"
  },
  {
    "bug_location": "Backup and Restore (BR) component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect range handling during RawKV data restoration, where SST metadata range filtering does not properly exclude out-of-range keys",
    "issue_number": 7163,
    "title": "Restoring a subrange of backed up rawkv data may import keys that are out of range"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition or inconsistent state management in stale peer handling during Raft consensus protocol",
    "issue_number": 7183,
    "title": "raftstore::test_stale_peer::test_server_stale_peer_out_of_region failed"
  },
  {
    "bug_location": "gRPC server",
    "severity": 4,
    "categories": [
      "Network",
      "CodeBug",
      "Security"
    ],
    "root_cause": "TLS configuration handling error causing memory corruption during gRPC server initialization",
    "issue_number": 7209,
    "title": "TiKV gRPC server coredump when TLS is enabled"
  },
  {
    "bug_location": "Raft Replication Module",
    "severity": 4,
    "categories": [
      "Transaction",
      "Network",
      "Performance"
    ],
    "root_cause": "Synchronization failure during snapshot application causing read operation suspension when network partition occurs",
    "issue_number": 7229,
    "title": "Replication read should be resumed after applying snapshot"
  },
  {
    "bug_location": "backup/external_storage/s3_storage",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential memory or connection handling issue during large S3 backup operation, causing hyper client connection failure",
    "issue_number": 7236,
    "title": "Backup to S3 panics for a table ~30 GiB"
  },
  {
    "bug_location": "tidb_query/codec/mysql/time/tz.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Timezone parsing failure with 'posixrules' due to chrono_tz implementation limitations",
    "issue_number": 7243,
    "title": "Unknown time zone 'posixrules'"
  },
  {
    "bug_location": "Transaction Layer",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Incorrect lock resolution logic preventing reads when optimistic and pessimistic locks coexist",
    "issue_number": 7289,
    "title": "Optimistic lock blocks reads"
  },
  {
    "bug_location": "gRPC client connection",
    "severity": 3,
    "categories": [
      "Network",
      "Config"
    ],
    "root_cause": "TiKV attempting to establish HTTP/1.x connection instead of gRPC over TLS when configured with secure endpoints",
    "issue_number": 7312,
    "title": "TiKV try to connect HTTP/1.x server"
  },
  {
    "bug_location": "Raft/Follower Read Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Network",
      "LoadBalance"
    ],
    "root_cause": "Unexpected response handling in batch receive loop during follower read under complex distributed testing scenarios",
    "issue_number": 7324,
    "title": "batchRecvLoop receives a unknown response when testing follower read"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Lock resolver tracking inconsistency or duplicate commit command processing in apply worker",
    "issue_number": 7325,
    "title": "TiKV panic while running cdc"
  },
  {
    "bug_location": "raftstore/raft_log",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Commit index out of range during Raft log processing, indicating potential synchronization or state tracking issue in Raft consensus protocol implementation",
    "issue_number": 7353,
    "title": "raft::raft_log::RaftLog<T>::commit_to paniked"
  },
  {
    "bug_location": "storage/mvcc/txn.rs",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Race condition in pessimistic transaction rollback handling that allows inconsistent state by collapsing rollback records across different transaction attempts",
    "issue_number": 7364,
    "title": "Txn: collapsing the rollback record of pessimistic lock could cause inconsistency"
  },
  {
    "bug_location": "tikv-ctl",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Pure virtual method called during error handling, causing unexpected termination and core dump when attempting to recover from non-existent database path",
    "issue_number": 7373,
    "title": "tikv-ctl coredump"
  },
  {
    "bug_location": "external_storage/s3_storage",
    "severity": 3,
    "categories": [
      "Storage",
      "Network",
      "Performance"
    ],
    "root_cause": "AWS S3 signature timeout limitation prevents single-part uploads exceeding 15 minutes",
    "issue_number": 7375,
    "title": "external_storage: if S3 upload cannot finish within 15 minutes it will fail"
  },
  {
    "bug_location": "readpool configuration",
    "severity": 2,
    "categories": [
      "Config",
      "CodeBug"
    ],
    "root_cause": "Incomplete configuration validation for unified read pool settings, allowing inconsistent or invalid stack size configurations",
    "issue_number": 7384,
    "title": "Configuration is not fully checked if the unified read pool is partially enabled"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Peer state handling logic fails to process wake-up messages for tombstone/destroyed peers during region merge operations",
    "issue_number": 7386,
    "title": "Wake up message should be considered even when target peer is tombstone"
  },
  {
    "bug_location": "TiKV Lock File Management",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config",
      "Human"
    ],
    "root_cause": "Improper lock file permission handling preventing multi-user TiKV startup",
    "issue_number": 7462,
    "title": "After a user starts TiKV, other users will not be able to start TiKV"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Network",
      "Transaction"
    ],
    "root_cause": "Incomplete peer state tracking during network partition and peer removal process",
    "issue_number": 7475,
    "title": "An uninitialized peer can be recreated"
  },
  {
    "bug_location": "unified read pool",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "Using transaction start timestamp as task ID, which incorrectly prioritizes query execution and blocks smaller queries behind larger ones",
    "issue_number": 7484,
    "title": "The unified read pool should not use start_ts as task id"
  },
  {
    "bug_location": "raftstore/gRPC thread",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Potential deadlock in batch Raft RPC call handling, causing thread blocking and preventing message transmission",
    "issue_number": 7493,
    "title": "batch raft call deadlock"
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Network"
    ],
    "root_cause": "Potential race condition or network isolation handling issue in Raft consensus protocol leader election mechanism",
    "issue_number": 7500,
    "title": "raftstore::test_prevote::test_server_isolated_follower_leader_does_not_change failed"
  },
  {
    "bug_location": "tidb_query_datatype/codec/row/v2/row_slice.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Unsafe pointer casting with potential misalignment causing undefined behavior in memory access",
    "issue_number": 7613,
    "title": "Undefined behavior in tidb_query_datatype's RowSlice"
  },
  {
    "bug_location": "Raft Command Handler",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Deprecated Raft command type being reused after intended deprecation, causing potential compatibility issues between TiKV and TiFlash components",
    "issue_number": 7653,
    "title": "Deprecated raft cmd AdminCmdType::Split is reused"
  },
  {
    "bug_location": "util test suite",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Potential file deletion range handling error in utility test function",
    "issue_number": 7694,
    "title": "util::tests::test_delete_all_files_in_range failed"
  },
  {
    "bug_location": "RaftDB Configuration Checker",
    "severity": 2,
    "categories": [
      "Config",
      "Storage"
    ],
    "root_cause": "Insufficient error handling for non-existent RaftDB configuration directory during pre-startup validation",
    "issue_number": 7720,
    "title": "config-check fails if raftdb dictionary doesn't exist"
  },
  {
    "bug_location": "Storage Management",
    "severity": 3,
    "categories": [
      "Storage",
      "Config"
    ],
    "root_cause": "Inconsistent storage size tracking between TiKV and PD components",
    "issue_number": 7721,
    "title": "The TiKV and PD storage size is not equal"
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Memory management inefficiency during large-scale data preparation, likely related to memory allocation and garbage collection during high-concurrency workload",
    "issue_number": 7741,
    "title": "TiKV OOM"
  },
  {
    "bug_location": "Performance/Load Management",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Potential resource exhaustion or performance degradation during long-running TPCC workload causing gradual QPS reduction",
    "issue_number": 7745,
    "title": "QPS continue reduce when running TPCC"
  },
  {
    "bug_location": "Raft Scaling Component",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Performance instability during dynamic cluster reconfiguration, likely due to data redistribution and Raft consensus protocol overhead during node addition",
    "issue_number": 7746,
    "title": "TiKV scale out has high jitter on Latency"
  },
  {
    "bug_location": "Region Leader Election",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Upgrade",
      "LoadBalance"
    ],
    "root_cause": "Premature leader hibernation during cluster rolling update causing leadership transfer instability",
    "issue_number": 7747,
    "title": "Fail to evict leader when rolling update a new hibernated cluster"
  },
  {
    "bug_location": "Backup & Restore (BR) component",
    "severity": 4,
    "categories": [
      "Network",
      "Storage",
      "Performance"
    ],
    "root_cause": "Incomplete error handling and retry mechanism for GCS/S3 file downloads during restore process, causing connection reset and context cancellation errors",
    "issue_number": 7846,
    "title": "BR ignore some GCS and S3 errors which should retry when restore"
  },
  {
    "bug_location": "Backup/Restore Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "Performance"
    ],
    "root_cause": "Channel communication failure between TiKV peers during restore process, causing progress to stall at 76%",
    "issue_number": 7849,
    "title": "Progress of Backup / Restore occasionally stuck"
  },
  {
    "bug_location": "Backup/Restore Component",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "Config"
    ],
    "root_cause": "Network connectivity issues during AWS S3 backup with collation enabled, causing connection timeouts and dispatch errors",
    "issue_number": 7850,
    "title": "AWS br backup failed when collation is open "
  },
  {
    "bug_location": "Raft Region Management",
    "severity": 4,
    "categories": [
      "Transaction",
      "LoadBalance",
      "Network"
    ],
    "root_cause": "Potential race condition or leadership transition issue during follower read workflow causing region unavailability",
    "issue_number": 7853,
    "title": "Encounter region unavailable when selecting data"
  },
  {
    "bug_location": "coprocessor/endpoint.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Unexpected cancellation of oneshot channel during follower read workflow, likely due to race condition or premature channel closure",
    "issue_number": 7854,
    "title": "Got oneshot canceled error when selecting data"
  },
  {
    "bug_location": "raftstore/store/fsm/store.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Transaction"
    ],
    "root_cause": "Incorrect error handling for channel disconnection scenarios, potentially causing incorrect state management during region removal or shutdown",
    "issue_number": 7855,
    "title": "Channel disconnected error is not handled correctly"
  },
  {
    "bug_location": "Backup and Restore (BR) Component",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Lack of resilience in handling TiKV node failures during backup process, causing premature backup termination instead of leader re-election and retry",
    "issue_number": 7857,
    "title": "Kill a tikv process when backup, br will exit quickly"
  },
  {
    "bug_location": "gRPC TLS connection handling",
    "severity": 4,
    "categories": [
      "Security",
      "Network",
      "Config"
    ],
    "root_cause": "Incorrect TLS version negotiation or certificate validation in TiKV's gRPC client/server connection",
    "issue_number": 7891,
    "title": "tikv-ctl failed to connect to tikv when TLS is enabled"
  },
  {
    "bug_location": "config.rs",
    "severity": 4,
    "categories": [
      "Config",
      "CodeBug"
    ],
    "root_cause": "Missing required 'type' field in security.encryption.master-key configuration section",
    "issue_number": 7901,
    "title": "TiKV fails to start"
  },
  {
    "bug_location": "Region Merge Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Potential race condition or incorrect synchronization during region merge operation that allows write operations to an already merged source region",
    "issue_number": 7909,
    "title": "cases::test_merge::test_node_merge_write_data_to_source_region_after_merging failed"
  },
  {
    "bug_location": "GC (Garbage Collection) Metrics Component",
    "severity": 2,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "Metrics sampling interval too long compared to GC operation speed, causing metrics values to be missed before they can be captured",
    "issue_number": 7924,
    "title": "Some metrics about GC doesn't shows properly when GC runs too quickly."
  },
  {
    "bug_location": "storage/mvcc/point_getter",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential memory corruption or invalid iterator state during RocksDB iterator traversal in point getter implementation",
    "issue_number": 7939,
    "title": "Point getter aborts in RocksDB iterator Next()"
  },
  {
    "bug_location": "Performance/Upgrade Path",
    "severity": 3,
    "categories": [
      "Performance",
      "Upgrade",
      "CodeBug"
    ],
    "root_cause": "Performance regression or unexpected optimization during version upgrade from 4.0.0-rc.2 to nightly build",
    "issue_number": 7940,
    "title": "from 4.0 upgrade to nightly. the QPS improve much"
  },
  {
    "bug_location": "Raft Recovery Mechanism",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Slow leader election and region recovery process during node failure, causing extended QPS restoration time",
    "issue_number": 7941,
    "title": "It takes 10 mins to recover QPS"
  },
  {
    "bug_location": "raftstore/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect batching of read index requests causing stale commit index responses",
    "issue_number": 7971,
    "title": "raftstore: incorrectly batch read index request"
  },
  {
    "bug_location": "Titan GC (Garbage Collection)",
    "severity": 3,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Incorrect file deletion logic in garbage collection process that could attempt to delete an already deleted blob file",
    "issue_number": 7977,
    "title": "Titan: Fix GC may delete a already deleted blob file"
  },
  {
    "bug_location": "raftstore/peer.rs",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Race condition during region merge allowing stale local read before leader lease suspension",
    "issue_number": 8034,
    "title": "Local read may get stale data during merging"
  },
  {
    "bug_location": "logging/log-integration",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incorrect log routing configuration after logger switch from RocksDB native logger to TiKV logger",
    "issue_number": 8117,
    "title": "raftdb log is printed into tikv log file"
  },
  {
    "bug_location": "components/raftstore/src/store/worker/pd.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect metrics update logic causing inaccurate leader activity tracking",
    "issue_number": 8155,
    "title": "Active written leaders is wrong"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Memory accumulation from unprocessed change events during changefeed pause, causing excessive memory consumption when resuming",
    "issue_number": 8168,
    "title": "TiKV keeps OOM if there are too many change events need to be sent to CDC"
  },
  {
    "bug_location": "Placement Driver (PD)",
    "severity": 4,
    "categories": [
      "CodeBug",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Insufficient zone-aware replica management logic during partial datacenter failure",
    "issue_number": 8169,
    "title": "Change max replicas may cause region unavailable"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Column reference error during query processing, indicating potential schema mismatch or data inconsistency in select statement",
    "issue_number": 818,
    "title": "select statement get an error."
  },
  {
    "bug_location": "Raft consensus configuration management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unsafe leader transition during configuration change, allowing potential data loss during leadership transfer",
    "issue_number": 8184,
    "title": "Don't allow conf remove until leader has applied to current term"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Lock observer notification occurs before log application, causing potential lock observation race condition during distributed garbage collection",
    "issue_number": 8211,
    "title": "Should notify lock observer after applying logs"
  },
  {
    "bug_location": "storage/mvcc/reader",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Inefficient key version retrieval method using `seek` instead of potentially more performant `next` for accessing older record versions",
    "issue_number": 8229,
    "title": "MvccTxn can't make use of near_seek"
  },
  {
    "bug_location": "RocksDB Engine Component",
    "severity": 5,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Data corruption during RocksDB compaction process, causing overlapping key ranges in Level 6 SST files",
    "issue_number": 8243,
    "title": "TiKV panic 'error: Corruption: L6 has overlapping ranges'"
  },
  {
    "bug_location": "CDC (Change Data Capture) Failpoint Module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Failpoint registration or execution mechanism not functioning correctly during merge operation, preventing expected code path interruption",
    "issue_number": 8244,
    "title": "failpoints::test_register::test_merge not work"
  },
  {
    "bug_location": "raft/failpoint",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Failpoint configuration mechanism not correctly triggering panic in local reader configuration change test scenario",
    "issue_number": 8248,
    "title": "cases::test_conf_change::test_destroy_local_reader not work"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential issue with failpoint configuration or test instrumentation in coprocessor test case",
    "issue_number": 8254,
    "title": "cases::test_coprocessor::test_deadline_2 not work"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential issue with failpoint configuration or test instrumentation in coprocessor module",
    "issue_number": 8255,
    "title": "cases::test_coprocessor::test_parse_request_failed_2 not work"
  },
  {
    "bug_location": "Monitoring/Grafana Integration",
    "severity": 3,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Dashboard configuration incompatibility between TiKV version 3.0.16 and Grafana dashboard code designed for version 4.0",
    "issue_number": 8260,
    "title": "Grafana dashboard cannot display the contents of perf context normally in v3.0.16"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition or inconsistent state management in Raft learner peer handling during test scenario",
    "issue_number": 8272,
    "title": "raftstore::test_stale_peer::test_stale_learner failed"
  },
  {
    "bug_location": "Raft component",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition or synchronization issue in region worker destruction during snapshot processing",
    "issue_number": 8278,
    "title": "cases::test_merge::test_node_merge_crash_when_snapshot not work"
  },
  {
    "bug_location": "replica_read_component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential issue with duplicate read index context handling in replica read mechanism",
    "issue_number": 8285,
    "title": "cases::test_replica_read::test_duplicate_read_index_ctx not work"
  },
  {
    "bug_location": "Region Merge Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Potential memory leak during random region merge operations, causing gradual memory consumption without proper resource cleanup",
    "issue_number": 8315,
    "title": "Memory  usage gradually grows up and OOM when running random merge"
  },
  {
    "bug_location": "raftstore/lease_read",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Potential race condition or synchronization issue in snapshot management during lease read operations",
    "issue_number": 8329,
    "title": "Flaky test: raftstore::test_lease_read::test_node_batch_id_in_lease"
  },
  {
    "bug_location": "Raft Snapshot Management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Snapshot handling race condition where uninitialized regions can leave stale entries in pending snapshot tracking, preventing future snapshot processing",
    "issue_number": 8373,
    "title": "pending_snapshot_regions may not cleaned as expected"
  },
  {
    "bug_location": "sys/cgroup.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "CPU quota calculation can return 0 when cgroup limit is set to a fractional value less than 1, causing potential thread pool starvation",
    "issue_number": 8423,
    "title": "cpu_cores_quota can return 0 which makes unified thread pool hang for long running query"
  },
  {
    "bug_location": "raftstore/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Race condition in peer and apply FSM destruction process causing potential panic when registering a new peer with an outdated apply FSM",
    "issue_number": 8433,
    "title": "Optimization in peer/apply fsm destroy process may lead to a panic in apply fsm's register"
  },
  {
    "bug_location": "Configuration Management",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Config"
    ],
    "root_cause": "Incomplete configuration write when disk storage is exhausted, leading to potential configuration corruption during startup sequence",
    "issue_number": 8438,
    "title": "Last configuration can be corrupted when disk is full"
  },
  {
    "bug_location": "gRPC status port binding",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Network",
      "Config"
    ],
    "root_cause": "TLS configuration prevents specific IP status port binding when not using 0.0.0.0",
    "issue_number": 8449,
    "title": "Status port not binded on the first start when TLS is enabled"
  },
  {
    "bug_location": "raftstore/store/fsm/store.rs",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Request vote messages potentially dropped during Raft split operation, causing potential leadership election instability",
    "issue_number": 8456,
    "title": "Request vote messages are lost during split"
  },
  {
    "bug_location": "gRPC connection management",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Lack of connection retry backoff mechanism and inefficient DNS resolution strategy in TiKV's network communication layer",
    "issue_number": 8476,
    "title": "Too many dns queries when one node is down"
  },
  {
    "bug_location": "HTTP server component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Network"
    ],
    "root_cause": "Invalid HTTP header name parsing in Hyper library causing unexpected panic during HTTP request processing",
    "issue_number": 8478,
    "title": "TiKV nodes output FATAL log \"[FATAL] [lib.rs:499] [\"header name validated by httparse: InvalidHeaderName { _priv: () }\"]\""
  },
  {
    "bug_location": "coprocessor/aggregation_executor",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Logical row processing error during batch aggregation function execution, causing panic when constructing identical logical rows larger than batch size",
    "issue_number": 8481,
    "title": "copr: tikv panic when executes aggr func "
  },
  {
    "bug_location": "util",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Unknown"
    ],
    "root_cause": "Potential undefined behavior in crossbeam-channel library dependency",
    "issue_number": 8492,
    "title": "crossbeam-channel 0.4 has undefined behavior"
  },
  {
    "bug_location": "test-bench/merge module",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition or inconsistent state during multiple merge rollback operations in TiKV transaction processing",
    "issue_number": 8506,
    "title": "cases::test_merge::test_node_mutiple_rollback_merge failed"
  },
  {
    "bug_location": "GC Worker / RaftStore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Inconsistent peer configuration during region lifecycle management, causing unexpected region state validation failure",
    "issue_number": 8537,
    "title": "cases::test_gc_worker::test_notify_observer_after_apply will panic"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition or inconsistent state during replication mode switching in Raft consensus protocol implementation",
    "issue_number": 8544,
    "title": "raftstore::test_replication_mode::test_switching_replication_mode failed"
  },
  {
    "bug_location": "BatchTableScanExecutor",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Assertion failure in column length comparison during batch table scan execution, indicating a potential data consistency or synchronization issue in the vector executor",
    "issue_number": 8555,
    "title": "cop: panic by assertion failure in BatchTableScanExecutor"
  },
  {
    "bug_location": "coprocessor",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Assertion failure in table scan executor when processing cluster index with specific query pattern",
    "issue_number": 8557,
    "title": "copr: table scan's assertion fail while enable cluster index by default in TiDB"
  },
  {
    "bug_location": "TiKV Performance Layer",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Performance regression in wide table query processing, causing significant drops in transactions per second and increased latency",
    "issue_number": 8574,
    "title": "09-01 Performance regression on wide table"
  },
  {
    "bug_location": "Transaction Lock Table",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Race condition and synchronization issue in async commit transaction handling causing lock table deadlock",
    "issue_number": 8576,
    "title": "Deadlock in the lock table"
  },
  {
    "bug_location": "Transaction Commit Protocol",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Async commit mechanism fails to ensure linearizability and causal consistency across distributed transactions",
    "issue_number": 8589,
    "title": "Async commit does not ensure linearizability"
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Unexpected system time change causing timestamp comparison failure in raftstore thread",
    "issue_number": 859,
    "title": "Add a detecting module to monit system time"
  },
  {
    "bug_location": "pd_client",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Network"
    ],
    "root_cause": "Synchronization issue between PD client thread and threads performing sync requests, causing a deadlock due to conflicting read and write lock acquisition",
    "issue_number": 8610,
    "title": "Deadlock between the PD client thread and other threads calling PD sync requests."
  },
  {
    "bug_location": "Status Port/TLS Configuration",
    "severity": 4,
    "categories": [
      "Network",
      "Config",
      "CodeBug"
    ],
    "root_cause": "TLS connection handling inconsistency in status port, causing connection state corruption after mixed protocol (HTTPS/HTTP) requests",
    "issue_number": 8641,
    "title": "Status port refused connection when tls enabled"
  },
  {
    "bug_location": "Snapshot Handling Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Race condition in snapshot processing where overlapping region snapshots can cause inconsistent state and potential panic during concurrent snapshot application",
    "issue_number": 866,
    "title": "receiving two overlapped snapshot at same time causes panic"
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Race condition during concurrent snapshot generation and blob file deletion in Titan storage engine, causing potential panic when encountering missing blob files during range iteration",
    "issue_number": 8661,
    "title": "UnsafeDestroyRange may cause panic for a concurrent generating snap when Titan enabled"
  },
  {
    "bug_location": "server/raft_client",
    "severity": 3,
    "categories": [
      "Network",
      "CodeBug"
    ],
    "root_cause": "Potential gRPC connection handling failure during client reconnection test scenario",
    "issue_number": 8666,
    "title": "server::raft_client::test_raft_client_reconnect failed"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Unstable Raft leader election mechanism triggered by cluster reload, causing multiple regions to simultaneously start new election processes",
    "issue_number": 8678,
    "title": "TiKV always starting a new election"
  },
  {
    "bug_location": "pd_client/lock_manager/deadlock_detector",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Nested `block_on` executor calls causing runtime executor conflict",
    "issue_number": 8692,
    "title": "PD client panics on Deadlock's get_leader_info due to nested `block_on`"
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Unexpected compression behavior in RocksDB level 6 compaction, causing inefficient data compression compared to lower levels",
    "issue_number": 8721,
    "title": "compression ratio of L6 is even smaller than L5/L4"
  },
  {
    "bug_location": "status_server",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Security",
      "Network"
    ],
    "root_cause": "TLS handshake error causing unhandled exception in status server HTTP handler",
    "issue_number": 8738,
    "title": "TiKV status down when accept http requests with TLS enabled"
  },
  {
    "bug_location": "encryption/manager",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Security",
      "Performance"
    ],
    "root_cause": "Mutex contention in encryption key management during concurrent file operations, causing thread blocking and potential system slowdown",
    "issue_number": 8761,
    "title": " Mutex conflict of encryption makes pd-worker deal with heartbeat slow."
  },
  {
    "bug_location": "backup/restore component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Config"
    ],
    "root_cause": "Transient AWS credential resolution failure during STS token-based authentication, likely due to token refresh or parsing issue in credential provider chain",
    "issue_number": 8768,
    "title": "backup/restore via S3 using STS transiently fail with \"Couldn't find AWS credentials in default sources or k8s environment\""
  },
  {
    "bug_location": "raftstore/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Assertion failure in epoch state check during Raft command proposal, indicating a potential conflict in version or configuration tracking",
    "issue_number": 8781,
    "title": "Assert fails on epoch_state post_propose"
  },
  {
    "bug_location": "raftstore/store/fsm/store",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "HashMap lookup failure during peer creation in random merge scheduler, indicating potential race condition or state synchronization issue",
    "issue_number": 8783,
    "title": "maybe_create_peer_internal fails due to hashmap entry not found"
  },
  {
    "bug_location": "Metrics/Monitoring",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect metric type handling in monitoring query, using rate() on a Gauge metric which is not designed for rate calculation",
    "issue_number": 8834,
    "title": "Wrong compaction pending bytes metrics"
  },
  {
    "bug_location": "Region Split Mechanism",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential race condition or synchronization issue during region split operation, specifically affecting follower replica's split performance",
    "issue_number": 8846,
    "title": "cases::test_split_region::test_follower_slow_split failed"
  },
  {
    "bug_location": "gc_worker/compaction_filter.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential error handling issue causing infinite loop when Key::split_on_ts_for() fails to parse key",
    "issue_number": 8861,
    "title": "potential endless loop in compaction_filter"
  },
  {
    "bug_location": "backup_service",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Insufficient input validation for external storage path during backup operation, causing unhandled error condition that triggers panic",
    "issue_number": 8901,
    "title": "Invalid external storage path will cause TiKV panic during backup"
  },
  {
    "bug_location": "Backup and Restore (BR) Component",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage",
      "Upgrade"
    ],
    "root_cause": "Inefficient file/key dictionary rewriting process when enabling encryption at rest during large data restore operations",
    "issue_number": 8916,
    "title": "Restore slow down due to rewriting file/key.dict after enabling encryption at rest"
  },
  {
    "bug_location": "coprocessor/dag/executor/table_scan",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Slice index out of range during table scan operation, likely caused by incorrect index calculation or memory access",
    "issue_number": 8941,
    "title": "Index out of range for slice in TableScan"
  },
  {
    "bug_location": "Transaction Layer",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incomplete transaction commit handling causing stale lock state preventing valid insert after delete",
    "issue_number": 8964,
    "title": "Insertion after deletion meets duplicate entry error in optimistic transactions"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 5,
    "categories": [
      "Upgrade",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Version compatibility issue during region split operation causing inconsistent region epoch tracking between different TiKV versions",
    "issue_number": 8967,
    "title": "Rolling upgrade from 2.1 or older version to newer version may break region replicas consistency"
  },
  {
    "bug_location": "TiKV server core runtime",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Integer underflow when decrementing a zero-valued counter, causing unexpected wraparound to maximum value",
    "issue_number": 8970,
    "title": "TiKV server panic after restarted"
  },
  {
    "bug_location": "coprocessor/string_functions",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect UTF8 string comparison logic that does not properly handle collation rules",
    "issue_number": 8986,
    "title": "copr: UTF8 functions should consider collation"
  },
  {
    "bug_location": "RaftStore/Transport",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Transport channel becoming full, preventing new network connections and message propagation between TiKV peers",
    "issue_number": 8987,
    "title": "reigon is unavailable: one tikv server does not accept new connections"
  },
  {
    "bug_location": "logging/async_logger",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Race condition or premature thread termination in async logging mechanism causing panic log loss before output",
    "issue_number": 8998,
    "title": "Panic log may be lost when using async logger"
  },
  {
    "bug_location": "gRPC service layer",
    "severity": 4,
    "categories": [
      "Network",
      "Performance"
    ],
    "root_cause": "Inability to handle large gRPC response payloads exceeding 4GB, causing memory allocation and slice buffer handling failure",
    "issue_number": 9012,
    "title": "TiKV panics when the size of response exceeds 4GB"
  },
  {
    "bug_location": "Raft Learner Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Race condition during region split when learner is isolated during leader hibernation, preventing proper replica creation and potentially causing region unavailability",
    "issue_number": 9022,
    "title": "Learner may not be created after a long time when enabling hibernate region"
  },
  {
    "bug_location": "PD (Placement Driver) worker",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Inefficient region size calculation blocking worker thread during scheduling for large regions",
    "issue_number": 9024,
    "title": "PD worker is blocked by `get_region_approximate_size`"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 3,
    "categories": [
      "Transaction",
      "LoadBalance",
      "Config"
    ],
    "root_cause": "Incomplete leader transfer handling during interrupted configuration change, causing peers to remain in a pending conf change state and blocking leader transfer",
    "issue_number": 9041,
    "title": "Peer reject to transfer leader because of pending conf change"
  },
  {
    "bug_location": "RocksDB Compaction/Region Info Accessor",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Synchronous blocking call in single background thread causing deadlock during compaction partitioner creation",
    "issue_number": 9044,
    "title": "compaction guard: deadlock when creating compaction partitioner"
  },
  {
    "bug_location": "storage/mod.rs",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Premature lock handling causing incomplete batch retrieval when memory locks are encountered",
    "issue_number": 9069,
    "title": "batch_get doesn't return all results when encountering a memory lock"
  },
  {
    "bug_location": "Raft consensus/apply system",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Network"
    ],
    "root_cause": "Proposals are sent to apply system regardless of commit status, causing incorrect callback invocation during network partitions",
    "issue_number": 9070,
    "title": "committed_cb is always invoked which affects the correctness of async apply"
  },
  {
    "bug_location": "Importer/Key Manager",
    "severity": 4,
    "categories": [
      "Storage",
      "Security",
      "CodeBug"
    ],
    "root_cause": "Incorrect file management during encrypted data import, leading to potential key manager synchronization failure",
    "issue_number": 9099,
    "title": "TiKV panic because of ingest failed by importer."
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "Transaction",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Unpredictable callback timing in Raft leadership transitions causing request stalls and potential server overload",
    "issue_number": 9113,
    "title": "More fine-grained control on callback for raftstore"
  },
  {
    "bug_location": "encryption/DataKeyManager",
    "severity": 4,
    "categories": [
      "Security",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Non-atomic file operations with encryption metadata updates causing potential inconsistency between file system and encryption key management",
    "issue_number": 9115,
    "title": "encryption: handle non-atomic file operations"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect handling of NewRowFormat during column retrieval, causing default column values to be incorrectly processed",
    "issue_number": 9119,
    "title": "Coprocessor incorrectly handles NewRowFormat"
  },
  {
    "bug_location": "encryption/master_key/kms",
    "severity": 3,
    "categories": [
      "Security",
      "Upgrade",
      "Config"
    ],
    "root_cause": "SSL certificate verification failure during KMS key retrieval, preventing TiKV server initialization",
    "issue_number": 9126,
    "title": "TiKV panic during upgrading from v4.0.8 to nightly"
  },
  {
    "bug_location": "backup-restore component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "In-memory region data buffering without size limit causing excessive memory consumption during large region backup",
    "issue_number": 9144,
    "title": "TiKV OOM because BR read a large region in memory"
  },
  {
    "bug_location": "server::kv_service",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Potential region splitting failure during test scenario, indicating an issue with region management or test infrastructure",
    "issue_number": 9151,
    "title": "server::kv_service::test_split_region failed"
  },
  {
    "bug_location": "storage/txn/latch.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Bitwise AND (&) operator causing slot index collision in latch management, potentially leading to incorrect transaction synchronization",
    "issue_number": 9162,
    "title": "Slot collision on latch with and (&) operator"
  },
  {
    "bug_location": "raftstore/split_controller.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect region split logic when sample boundaries are zero but contained keys are non-zero",
    "issue_number": 9170,
    "title": "Load split cannot work when the request covers the region"
  },
  {
    "bug_location": "cdc/resolved_ts",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Inconsistent tracking of transaction locks and commit timestamps during TiCDC replication, causing assertion failures and unexpected panics in the resolved timestamp resolver",
    "issue_number": 9171,
    "title": "TiKV server panic when running with TiCDC"
  },
  {
    "bug_location": "unified thread pool scheduler",
    "severity": 3,
    "categories": [
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Ineffective request scheduling and prioritization in unified thread pool causing resource contention between transactional and scan workloads",
    "issue_number": 9188,
    "title": "The unified pool cannot schedule kv_scan, raw_scan and analyze requests well"
  },
  {
    "bug_location": "Transaction Layer",
    "severity": 4,
    "categories": [
      "Transaction",
      "LoadBalance",
      "CodeBug"
    ],
    "root_cause": "Race condition during follower read with async commit and leader shuffling, causing potential transaction inconsistency",
    "issue_number": 9208,
    "title": "tipocket list_append fails on follower read and async commit"
  },
  {
    "bug_location": "Transaction Component",
    "severity": 3,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Potential race condition or incorrect transaction state management in multi-transfer implementation",
    "issue_number": 921,
    "title": "Investigate the 'multiTransfer' CI failure."
  },
  {
    "bug_location": "GarbageCollection",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Transaction"
    ],
    "root_cause": "SST file with multiple key versions prevents GC optimization skipping for large region ranges during low write flow",
    "issue_number": 9216,
    "title": "TiKV's distributed GC may keep scanning and consuming CPU when there's very low write flow"
  },
  {
    "bug_location": "profiler",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Potential memory or resource management issue during CPU profiling that causes TiKV process termination",
    "issue_number": 9217,
    "title": "TiKV is killed during profiling via TiDB-dashboard"
  },
  {
    "bug_location": "TiKV Performance Tracking",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Inaccurate slow query time tracking mechanism that fails to account for all time components during query processing",
    "issue_number": 9223,
    "title": "slow-query information is not accuracy"
  },
  {
    "bug_location": "security/cert_verification",
    "severity": 4,
    "categories": [
      "Security",
      "CodeBug"
    ],
    "root_cause": "Insufficient error handling in CN certificate verification, leading to ambiguous error messages that do not clearly indicate the specific verification failure",
    "issue_number": 9234,
    "title": "security: explicit error message for CN verification error"
  },
  {
    "bug_location": "logging/security module",
    "severity": 4,
    "categories": [
      "Security",
      "CodeBug"
    ],
    "root_cause": "Incomplete implementation of log redaction mechanism, allowing sensitive information to be exposed in logs when redaction is supposedly enabled",
    "issue_number": 9249,
    "title": "security: not all logs being redact when log redaction is enabled"
  },
  {
    "bug_location": "DDL (Data Definition Language) Component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Potential index creation or modification failure during concurrent DDL operations",
    "issue_number": 9253,
    "title": "FAIL: /home/jenkins/agent/workspace/tikv_ghpr_integration_ddl_test/go/src/github.com/pingcap/tidb-test/_vendor/pkg/mod/github.com/pingcap/tidb@v0.0.0-20190520045437-2d70e4da27d7/cmd/ddltest/index_test.go:127: TestDDLSuite.TestIndex failed"
  },
  {
    "bug_location": "config.rs",
    "severity": 3,
    "categories": [
      "Config",
      "CodeBug"
    ],
    "root_cause": "Configuration parameter not properly implemented for runtime configuration update, causing config file changes to be ignored",
    "issue_number": 9327,
    "title": "Can not change region-split-check-diff through config file"
  },
  {
    "bug_location": "Raft read index request handling",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect leader transfer handling causing stale read index request processing",
    "issue_number": 9351,
    "title": "Stale read when using read_index request"
  },
  {
    "bug_location": "tidb_query_expr component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Type comparison and conversion incompatibility when building with PROST=1, specifically involving FieldTypeTp and type conversion traits",
    "issue_number": 9352,
    "title": "Failed to build TiKV with PROST=1"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Race condition during peer splitting where uninitialized and split peers with same region ID can cause readiness mapping errors",
    "issue_number": 9388,
    "title": "ready can be advanced by a wrong peer"
  },
  {
    "bug_location": "Metrics/Statistics Collection",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Incomplete key tracking in read flow statistics, causing metrics inconsistency between TiKV and PD during high-volume point read operations",
    "issue_number": 9391,
    "title": "TiKV's \"read flow\" statistics are inconsistent with \"store read rate\" statistics of PD"
  },
  {
    "bug_location": "system time handling component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Config",
      "Performance"
    ],
    "root_cause": "Insufficient handling of system time changes, causing unexpected panic when system time is modified by NTP service",
    "issue_number": 9392,
    "title": "TiKV  panicked when system time back"
  },
  {
    "bug_location": "Lightning Import Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Memory exhaustion during large data import with insufficient memory allocation",
    "issue_number": 9407,
    "title": "tikv w/ 8GB mem OOM while lightning importing"
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Inefficient file purging mechanism during region migration causing slow disk space reclamation",
    "issue_number": 9429,
    "title": "Files did not purged after `DeleteFilesInRanges`"
  },
  {
    "bug_location": "build system",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Dependency resolution failure during documentation generation, likely due to incompatible or missing package configuration in Cargo.toml",
    "issue_number": 9444,
    "title": "make doc is broken"
  },
  {
    "bug_location": "pd_client/util.rs",
    "severity": 4,
    "categories": [
      "Network",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Blocking gRPC threads during PD leader update, causing keepalive watchdog timeout and connection closure",
    "issue_number": 9463,
    "title": "Blocking PD leader update leads to gRPC keepalive watchdog fire and disrupts online service"
  },
  {
    "bug_location": "MVCC/GarbageCollection",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Garbage collection mechanism not functioning, causing version accumulation and performance degradation during high-concurrency TPCC workload",
    "issue_number": 9466,
    "title": "Performance decline in TPCC bench due to the absence of GC"
  },
  {
    "bug_location": "raftstore.LocalReader",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Invalid read delegates are not being properly garbage collected when no requests are processed, leading to memory leak",
    "issue_number": 9469,
    "title": "raftstore: the invalid read delegates in local reader may not be removed forever"
  },
  {
    "bug_location": "Encryption/Key Management Component",
    "severity": 4,
    "categories": [
      "Security",
      "Config",
      "Storage"
    ],
    "root_cause": "Improper error handling during AWS KMS key initialization, causing file corruption and unrecoverable state",
    "issue_number": 9488,
    "title": "TiKV crashes continuously if access AWS KMS failed at the first time"
  },
  {
    "bug_location": "Lightning Import/Ingest Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Race condition during SST file ingestion where duplicate ingest requests are not properly handled, causing TiKV to panic when attempting to re-ingest already processed files",
    "issue_number": 9496,
    "title": "TiKV panic when Lightning send twice ingest request."
  },
  {
    "bug_location": "pd-client",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Config"
    ],
    "root_cause": "Lack of validation for leader field in PD member response during endpoint initialization, causing potential connection hang",
    "issue_number": 9513,
    "title": "`validate_endpoints` does not check `leader` field of `GetMembersResponse`"
  },
  {
    "bug_location": "raftstore/store/util.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Local reader delegate not updating peer ID correctly after node restart and region reconfiguration",
    "issue_number": 9517,
    "title": "Local reader delegate keeps stale ID"
  },
  {
    "bug_location": "Backup and Restore (BR) Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "LoadBalance"
    ],
    "root_cause": "Inefficient region management during backup process causing significant region consolidation and memory pressure",
    "issue_number": 9520,
    "title": "BR will lead to the emergence of big regions with version v4.0.9"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 3,
    "categories": [
      "Transaction",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Resolved timestamp not being correctly updated during async commit transactions, potentially causing synchronization issues in change data capture",
    "issue_number": 9523,
    "title": "Resolved TS is not updated with async commit transactions"
  },
  {
    "bug_location": "Transaction GC Component",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Timestamp synchronization failure during garbage collection of async commit transactions, causing potential transaction consistency violations",
    "issue_number": 9526,
    "title": "incompatibility between gc and async commit"
  },
  {
    "bug_location": "TiKV Dashboard Memory Reporting",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect memory quota calculation in memory usage reporting, using container memory limit instead of actual system memory capacity",
    "issue_number": 9532,
    "title": "TiKV reported memory is wrong"
  },
  {
    "bug_location": "profiling/debug module",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Jemalloc heap profiling interface failure during pprof dump operation",
    "issue_number": 9538,
    "title": "heap pprof failed"
  },
  {
    "bug_location": "Security/Encryption Module",
    "severity": 4,
    "categories": [
      "Security",
      "Config",
      "Network"
    ],
    "root_cause": "KMS request timeout mechanism or configuration parsing error when establishing connection to KMS endpoint",
    "issue_number": 9541,
    "title": "TiKV cannot send KMS request"
  },
  {
    "bug_location": "Diagnostic Service",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Recursive stack overflow during hardware information collection, likely caused by infinite recursion in disk type detection logic",
    "issue_number": 9555,
    "title": "TiKV aborts when accesses hardware info"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Uninitialized voter peer during region splitting can create inconsistent leader election state",
    "issue_number": 9579,
    "title": "The uninitialized voter which comes from splitting may lead to two same-term raft leaders"
  },
  {
    "bug_location": "pd_client and raft client",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Network"
    ],
    "root_cause": "Inconsistent handling of tombstone store between PD client and raft client, causing store recognition failure",
    "issue_number": 9590,
    "title": "raft client doesn't recognize tombstone store"
  },
  {
    "bug_location": "AutoSplitController in raftstore/split_controller.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Incorrect key encoding assumption for raw KV mode, causing regions to remain unsplit beyond optimal size",
    "issue_number": 9613,
    "title": "load based split can't handle raw kv well"
  },
  {
    "bug_location": "raftstore/peer",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Race condition between commit merge and post_apply operations causing read delegate removal before update",
    "issue_number": 9615,
    "title": "commit merge and post_apply can make tikv panic"
  },
  {
    "bug_location": "Compaction Filter",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential deadlock during write operations in RocksDB compaction filter when write stall occurs, causing all compaction threads to block indefinitely",
    "issue_number": 9627,
    "title": "Potential deadlock for GC in Compaction Filter"
  },
  {
    "bug_location": "raft/lease_read",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Potential time drift and unreliable lease read mechanism due to lack of precise monotonic clock validation",
    "issue_number": 964,
    "title": "raft: use monotonic clock check for leader lease read "
  },
  {
    "bug_location": "storage/mvcc/reader",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Unexpected transaction record state during async commit resolution, likely caused by race condition or inconsistent transaction state tracking",
    "issue_number": 9654,
    "title": "txn record found but not expected"
  },
  {
    "bug_location": "Region Scheduling",
    "severity": 4,
    "categories": [
      "Performance",
      "Config",
      "LoadBalance"
    ],
    "root_cause": "Hibernate region configuration interfering with learner peer addition performance, causing slow operator execution during region scheduling",
    "issue_number": 9681,
    "title": "Add learner operator runs very slow when hibernate region is on"
  },
  {
    "bug_location": "pd-client",
    "severity": 4,
    "categories": [
      "Network",
      "Config",
      "Performance"
    ],
    "root_cause": "Unconditional periodic PD leader reconnection causing unnecessary connection drops and potential region heartbeat interruption",
    "issue_number": 9690,
    "title": "pd connection is reconnecting every 10 minutes"
  },
  {
    "bug_location": "Label Configuration",
    "severity": 3,
    "categories": [
      "Config",
      "Upgrade"
    ],
    "root_cause": "Inconsistent label format between PD and TiKV during cluster deployment",
    "issue_number": 9699,
    "title": "label format is inconsistent with PD"
  },
  {
    "bug_location": "encryption/storage_engine",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Storage",
      "Security"
    ],
    "root_cause": "Potential key dictionary incompatibility during version upgrade causing storage engine corruption, specifically with CURRENT file format during encryption key rotation",
    "issue_number": 9701,
    "title": "Corruption when upgrade from v4.0.9 to nightly"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect type casting and comparison handling for empty varchar columns during delete operations",
    "issue_number": 9704,
    "title": "Inconsisitent with Mysql When a varchar column only have empty string"
  },
  {
    "bug_location": "raft_client.rs",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Excessive message batching exceeding gRPC message size limits, causing connection aborts and potential communication failures",
    "issue_number": 9714,
    "title": "batch messages can be too large in new raft client implementation"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Imprecise lease duration calculation leading to potential premature election initiation",
    "issue_number": 9728,
    "title": "Lease check should consider tick count"
  },
  {
    "bug_location": "raftstore/split_controller",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Invalid random sampling range in auto-split controller causing range sampling error",
    "issue_number": 9733,
    "title": "Random crashes with \"Uniform::sample_single called with low >= high\" after update to 4.0.11"
  },
  {
    "bug_location": "raftstore/peer.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect index calculation when searching in a ring buffer VecDeque, potentially returning a smaller propose time due to not adjusting index for wraparound",
    "issue_number": 9748,
    "title": "The propose time may be smaller than the real one"
  },
  {
    "bug_location": "raftstore/store",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "Network"
    ],
    "root_cause": "Snapshot recovery failure during node restart after extended downtime, causing missing snapshot file and leadership transition conflicts",
    "issue_number": 975,
    "title": "server panics when restarting after stopped for a few hours"
  },
  {
    "bug_location": "Profiling/Debug Component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Memory access violation during concurrent profiling operation, likely due to race condition or unsafe memory manipulation in the profiling handler",
    "issue_number": 9765,
    "title": "Built-in profiling can cause a segmentation fault"
  },
  {
    "bug_location": "Raftstore GC snapshot processing",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Unnecessary CRC32 calculation during snapshot garbage collection, causing excessive and unproductive I/O overhead",
    "issue_number": 9786,
    "title": "Raftstore GC snapshot calculates crc32 which is useless and causes unnecessary IO"
  },
  {
    "bug_location": "Region Statistics Tracking",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Potential metrics calculation error in region write bytes tracking mechanism causing abnormal statistical reporting",
    "issue_number": 9796,
    "title": "Region written stats is abnormal"
  },
  {
    "bug_location": "PD Client Connection Module",
    "severity": 5,
    "categories": [
      "Network",
      "Performance",
      "Config"
    ],
    "root_cause": "Lack of reconnection rate limiting mechanism causing repeated connection attempts during PD leader absence",
    "issue_number": 9799,
    "title": "There is no limit to reconnect with PD"
  },
  {
    "bug_location": "TiKV Configuration Validation",
    "severity": 4,
    "categories": [
      "Config",
      "Upgrade"
    ],
    "root_cause": "Unrecognized configuration option 'split.size-threshold' during config validation in upgrade process",
    "issue_number": 9816,
    "title": "Tiup upgrade failed because of config-check failure"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of maximum unsigned 64-bit integer value in IN expression comparison logic",
    "issue_number": 9821,
    "title": "coprocessor IN expr not handle max_u64 properly"
  },
  {
    "bug_location": "raftstore/peer.rs",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Snapshot generation logic fails to trigger under specific conditions involving region Ready state and committed entries",
    "issue_number": 9859,
    "title": "raftstore: leader may not send snapshot to the follower"
  },
  {
    "bug_location": "encryption/file_dict_file",
    "severity": 4,
    "categories": [
      "Storage",
      "Security",
      "CodeBug"
    ],
    "root_cause": "Disk full condition causing file corruption and CRC32 checksum mismatch in encryption file dictionary",
    "issue_number": 9886,
    "title": "tikv can't startup after the disk is full"
  },
  {
    "bug_location": "Raft Apply Task Statistics",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect metric calculation for task wait duration in Raft apply task handling, potentially leading to inaccurate performance measurements",
    "issue_number": 9893,
    "title": "There seems to be a problem with the statistics of the Raft propose/Apply wait duration metric"
  },
  {
    "bug_location": "Store metrics calculation",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Incorrect snapshot total size calculation causing inconsistent store used size metrics during TiKV restart",
    "issue_number": 9896,
    "title": "Store used size changes a lot after restarting"
  },
  {
    "bug_location": "GC (Garbage Collection) Worker",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug",
      "Config"
    ],
    "root_cause": "Overly simplistic alert rule that checks GC task count without considering actual data modification activity",
    "issue_number": 9910,
    "title": "the rule for TiKV_GC_can_not_work alert should be enhanced to check with GC speed instead of GC task"
  },
  {
    "bug_location": "components/engine_rocks/src/engine_iterator.rs",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Invalid iterator state check causing unexpected panic during database iteration",
    "issue_number": 9913,
    "title": "tikv keep panicking at components/engine_rocks/src/engine_iterator.rs:39 "
  },
  {
    "bug_location": "PD Store Status Reporting",
    "severity": 3,
    "categories": [
      "Storage",
      "Config"
    ],
    "root_cause": "Potential disk space calculation or reporting mechanism error in TiKV's storage metadata tracking",
    "issue_number": 9920,
    "title": "Incorrect available and used disk size"
  },
  {
    "bug_location": "raft_client",
    "severity": 3,
    "categories": [
      "Network",
      "CodeBug"
    ],
    "root_cause": "Incorrect connection handling when store IDs mismatch during Raft message transmission",
    "issue_number": 9929,
    "title": "raft_client: drop connect when receive raft message with mismatch store id"
  },
  {
    "bug_location": "RocksDB Compaction Layer",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Inefficient compaction handling during large table truncation, causing excessive pending bytes and potential write stalls",
    "issue_number": 9936,
    "title": "Too many compaction pending bytes when truncating a huge table in TiDB"
  },
  {
    "bug_location": "Profiler/Debug Module",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incompatible libunwind behavior on macOS 11 causing segmentation fault during CPU profiling when attempting to access memory at null pointer",
    "issue_number": 9957,
    "title": "Built-in CPU profiling causes a segmentation fault on macOS 11"
  },
  {
    "bug_location": "TiCDC Incremental Scan Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "RocksDB snapshot retention mechanism causing prolonged snapshot holding during large incremental data scans",
    "issue_number": 9961,
    "title": "TiCDC old value and incremental scan may hold RocksDB snapshot too long"
  },
  {
    "bug_location": "GC Worker",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Inefficient handling of tombstone keys during garbage collection process",
    "issue_number": 9965,
    "title": "gc keys can't handle tombstones effectively"
  },
  {
    "bug_location": "Coprocessor/Expression Evaluation",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Index out of bounds error during string parsing and time conversion in expression evaluation, likely caused by unsafe array access or incorrect boundary checking",
    "issue_number": 9970,
    "title": "index out of bounds: the len is 4 but the index is 4"
  },
  {
    "bug_location": "Snapshot Recovery and Encryption Components",
    "severity": 3,
    "categories": [
      "Storage",
      "Security",
      "CodeBug"
    ],
    "root_cause": "Incomplete checksum verification during snapshot recovery and encryption header handling, potentially allowing undetected data corruption and loss",
    "issue_number": 9971,
    "title": "Avoid unnoticeable data loss due to inappropriate handling of checksum"
  },
  {
    "bug_location": "Monitoring/Grafana",
    "severity": 2,
    "categories": [
      "Config"
    ],
    "root_cause": "Incorrect metric unit configuration in Grafana dashboard visualization",
    "issue_number": 9976,
    "title": "The unit of some panels are wrong in Grafana"
  },
  {
    "bug_location": "raftstore/peer",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Assertion failure in merge operation related to committed transaction tracking, likely due to inconsistent state during concurrent merge and transaction processing",
    "issue_number": 9980,
    "title": "raftstore panics during merge"
  },
  {
    "bug_location": "CDC Endpoint",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Inefficient thread processing for high-volume change data capture operations, causing excessive CPU consumption during insert workloads",
    "issue_number": 9981,
    "title": "CDC endpoint thread CPU usage 100% with insert 800ops"
  },
  {
    "bug_location": "Configuration Parsing",
    "severity": 2,
    "categories": [
      "Config",
      "CodeBug"
    ],
    "root_cause": "Inconsistent comment handling and validation in configuration template generation",
    "issue_number": 9990,
    "title": "Provided config-template.toml is not valid"
  },
  {
    "bug_location": "CDC Endpoint",
    "severity": 5,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Memory buffer overflow from excessive scan event accumulation during high-volume data insertion",
    "issue_number": 9996,
    "title": "TiKV OOM due to CDC endpoint CPU 100% under 20k insert op/s"
  }
]