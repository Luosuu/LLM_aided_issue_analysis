[
  {
    "bug_location": "Monitoring/Metrics",
    "severity": 2,
    "categories": [
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Incomplete CPU graph representation for backup, restore, and CDC threads in monitoring dashboard",
    "issue_number": 10003,
    "title": "backup, restore and cdc CPU graphs should also be included in \"Thread CPU\""
  },
  {
    "bug_location": "RaftStore",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Replication",
      "Performance"
    ],
    "root_cause": "Incorrect handling of peer state during region hibernation, causing potential false reporting of down peers to PD during leader heartbeat",
    "issue_number": 10017,
    "title": "raftstore: down peer collection did not consider hibernate region"
  },
  {
    "bug_location": "RocksDB Snapshot Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Memory"
    ],
    "root_cause": "TiKV blocked during SST file upload to S3, preventing snapshot release and causing resource leakage",
    "issue_number": 10027,
    "title": "rocksdb snapshot doesn't released and make space not reclaimed."
  },
  {
    "bug_location": "CDC (Change Data Capture)",
    "severity": 3,
    "categories": [
      "Transaction",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Old value cache ineffective for pessimistic transaction writes due to lock acquisition mechanism reading previous writes, causing cache miss",
    "issue_number": 10032,
    "title": "CDC old value cache does not work effectively for caching pessimistic txn write"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Cache implementation does not handle Put commands without previous writes by not adding Oldvalue::None to cache, causing unnecessary cache misses",
    "issue_number": 10036,
    "title": "CDC old value cache does not work effectively for caching Put commands"
  },
  {
    "bug_location": "Router/Channel Memory Management",
    "severity": 5,
    "categories": [
      "Memory",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Memory leak in crossbeam channel due to sender references not being properly released when receivers are dropped, causing gradual memory accumulation in long-running clusters",
    "issue_number": 10037,
    "title": "Memory keeps growing in long running large scale cluster"
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 2,
    "categories": [
      "Storage",
      "Monitoring"
    ],
    "root_cause": "Incorrect metrics calculation for Titan blob file sizes, causing discrepancy between reported and actual storage usage",
    "issue_number": 10052,
    "title": "Incorrect Titan blob files size metrics"
  },
  {
    "bug_location": "Region Heartbeat / Leader Transfer Scheduler",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Region heartbeat requires disk access to load approximate size and key count, which becomes a bottleneck when disk utilization is near 100%, causing leadership transfer timeouts",
    "issue_number": 10065,
    "title": "transfer leader scheduler will timeout if disk load is near 100%"
  },
  {
    "bug_location": "CDC (Change Data Capture)",
    "severity": 2,
    "categories": [
      "Performance",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "CDC sending changes for un-captured key ranges, causing unnecessary old value caching in TiKV regions",
    "issue_number": 10073,
    "title": "CDC should not send changes of un-captured key ranges"
  },
  {
    "bug_location": "raftstore/raft_log",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Storage"
    ],
    "root_cause": "Inconsistent Raft log state causing commit index to exceed last known index during heartbeat processing",
    "issue_number": 10078,
    "title": "Panic because the index to committed goes past the last index"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Memory leak in old value cache management after region deregistration",
    "issue_number": 10091,
    "title": "CDC old value cache keeps grow after all regions are deregistered"
  },
  {
    "bug_location": "Snapshot/Replication Component",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Replication",
      "Performance"
    ],
    "root_cause": "Missing assignment of snapshot applying count, causing metric to remain perpetually zero",
    "issue_number": 10094,
    "title": "Applying snapshot count is always 0"
  },
  {
    "bug_location": "components/raftstore/src/store/fsm/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Incorrect assumption in split check logic that `approximate_size` indicates a previous split check, preventing region splitting when size thresholds are not met through write commands",
    "issue_number": 10111,
    "title": "Region may not trigger split check no matter how large it is"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Unlimited scan task concurrency causing unbounded memory buffering during large data scans, leading to potential Out of Memory (OOM) conditions",
    "issue_number": 10114,
    "title": "CDC unlimited scan task concurrency may cause OOM"
  },
  {
    "bug_location": "CDC (Change Data Capture)",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Scan tasks for different changefeeds interfere with each other, causing blocking and checkpoint lag when processing large historical data scans",
    "issue_number": 10117,
    "title": "CDC scan tasks interferes between changefeeds"
  },
  {
    "bug_location": "CDC (Change Data Capture) endpoint",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Unexpected handling of downstream deregistration with a 'Noop' transaction extra operation, causing an unwrap() panic in the CDC endpoint's deregistration logic",
    "issue_number": 10118,
    "title": "CDC panic on deregister downstream"
  },
  {
    "bug_location": "jemalloc memory allocation",
    "severity": 2,
    "categories": [
      "Memory",
      "Config"
    ],
    "root_cause": "Malformed configuration string in jemalloc memory allocation library causing unnecessary log messages",
    "issue_number": 10130,
    "title": "<jemalloc>: Malformed conf string in tikv_stderr.log"
  },
  {
    "bug_location": "TiKV Profiling/Monitoring",
    "severity": 4,
    "categories": [
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Potential race condition or resource contention during concurrent profiling under high load",
    "issue_number": 10131,
    "title": "TiKV disconnected after profiled on TiDB Dashboard under load"
  },
  {
    "bug_location": "raftstore/raft_log",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Storage"
    ],
    "root_cause": "Race condition during snapshot application causing incorrect index calculation when multiple snapshots are processed concurrently",
    "issue_number": 1014,
    "title": "panic at 'slice out of range'"
  },
  {
    "bug_location": "jemalloc memory profiling",
    "severity": 3,
    "categories": [
      "Performance",
      "Memory",
      "Config"
    ],
    "root_cause": "Default jemalloc profiling configuration in TiUP causing unnecessary backtrace generation during memory allocation",
    "issue_number": 10150,
    "title": "jemalloc will fetch backtrace at every malloc"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect comparison logic between signed and unsigned 64-bit integers during aggregate function (MAX/MIN) computation",
    "issue_number": 10158,
    "title": "copr bug: Agg func Max/Min got bug when compare signed and unsigned int64"
  },
  {
    "bug_location": "resolved_ts/region_registration_endpoint",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Repeated region registration attempts due to prolonged region split operation, causing excessive logging and potential system performance degradation",
    "issue_number": 10163,
    "title": "resolved_ts: register region busy loop prints too many logs"
  },
  {
    "bug_location": "Region Split/Scheduler",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Delayed region split execution under high write pressure causing inconsistent region metadata and access attempts before split completion",
    "issue_number": 10168,
    "title": "Region not found because of access before splitting"
  },
  {
    "bug_location": "TiKV Coprocessor",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Lack of concurrency control for analyze/checksum requests in TiKV's coprocessor, causing excessive task scheduling and potential query timeouts",
    "issue_number": 10172,
    "title": "Limit the concurrency of analyze/checksum like DAG does"
  },
  {
    "bug_location": "CDC (Change Data Capture) Service",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Missing cluster ID validation in CDC request handling, causing incorrect error reporting when no CDC nodes are present",
    "issue_number": 10182,
    "title": "CDC should validate request's cluster ID"
  },
  {
    "bug_location": "Monitoring/Dashboard",
    "severity": 2,
    "categories": [
      "Monitoring"
    ],
    "root_cause": "Incorrect panel rendering or data mapping in region-related dashboard view",
    "issue_number": 10195,
    "title": "Wrong region related panel"
  },
  {
    "bug_location": "Titan storage engine",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Incorrect handling of blob file deletion during range deletion in Titan storage engine, causing unexpected panic",
    "issue_number": 10208,
    "title": "TiKV panic on split check due to Titan missing blob error"
  },
  {
    "bug_location": "RaftEngine/PeerStorage",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Slow seek operation when finding minimum log index, potentially due to excessive tombstone entries during log cleanup",
    "issue_number": 10210,
    "title": "Cleaning raft log costs too much time during destroying a peer"
  },
  {
    "bug_location": "raftstore/apply_worker",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Apply term not correctly updated during snapshot generation, causing potential metadata inconsistency in Raft log replication",
    "issue_number": 10225,
    "title": "apply term should be assigned when applying snapshot"
  },
  {
    "bug_location": "CDC (Change Data Capture)",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Overly verbose logging during changefeed pause operation, generating unnecessary INFO level logs about region observation status",
    "issue_number": 10255,
    "title": "CDC some info level logs are very verbose"
  },
  {
    "bug_location": "Backup and Restore (BR) component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Network"
    ],
    "root_cause": "Incorrect S3 endpoint signature generation due to trailing slash in endpoint URL",
    "issue_number": 10265,
    "title": "BR backup random fails \"SignatureDoesNotMatch\" for HCP S3 storage"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "CDC incremental scanner default behavior fills RocksDB block cache, causing unnecessary cache pollution and potential performance degradation for online read services",
    "issue_number": 10267,
    "title": "CDC incremental scan pollutes rocksdb block cache"
  },
  {
    "bug_location": "raftstore/snapshot_worker",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Replication"
    ],
    "root_cause": "Message channel congestion preventing snapshot apply result notification, potentially caused by message queue overflow during node recovery",
    "issue_number": 1028,
    "title": "failed to send msg SnapApplyRes [region_id: 912, is_success: true] after 5 tries"
  },
  {
    "bug_location": "backup-restore component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Unnecessary thread pool recycling and potential thread leakage in backup thread management, causing inefficient thread handling",
    "issue_number": 10287,
    "title": "Backup should not recycle threads since we set thread number via config"
  },
  {
    "bug_location": "Metrics/Monitoring",
    "severity": 3,
    "categories": [
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Metrics tracking mechanism not correctly updating RocksDB duration metrics, potentially using a max aggregation instead of current/latest value",
    "issue_number": 10294,
    "title": "RocksDB duration metrics aren't refreshed"
  },
  {
    "bug_location": "file_system/rate_limiter.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Test assertion failure in rate limiter implementation, indicating potential inaccuracy in bytes written compared to expected rate limit calculation",
    "issue_number": 10303,
    "title": "Test test_rate_limited_hybrid_flow fails"
  },
  {
    "bug_location": "file_system/rate_limiter.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Rate limiter test failure due to unexpected I/O throughput measurement, possibly indicating an inaccurate rate limiting implementation or test condition",
    "issue_number": 10306,
    "title": "Test test_rate_limited_light_flow fails"
  },
  {
    "bug_location": "unified-read-pool",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Memory",
      "Performance"
    ],
    "root_cause": "Potential memory corruption or invalid memory access during profiling operation, likely in libgcc runtime library",
    "issue_number": 10308,
    "title": "TiKV segfaults during profiling"
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Leadership transition issue during peer hibernation, causing leader election instability",
    "issue_number": 10309,
    "title": "test_leader_demoted_when_hibernated failed"
  },
  {
    "bug_location": "Metrics/Monitoring",
    "severity": 3,
    "categories": [
      "Monitoring",
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incorrect metric calculation for hibernate region statistics, potentially due to improper tracking or synchronization of region awakening events",
    "issue_number": 10330,
    "title": "TiKV Hiberate Peers metric has negative numbers"
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Monitoring"
    ],
    "root_cause": "Incorrect calculation or tracking of hibernate peer metrics, potentially causing inconsistent region state tracking",
    "issue_number": 10331,
    "title": "raftstore: incorrect hibernate peers metrics"
  },
  {
    "bug_location": "raftstore/peer",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Race condition in read index handling where assertion fails when processing multiple read index requests under leadership changes and lease expiration",
    "issue_number": 10347,
    "title": "tikv panicked during tipocket-bank2 test"
  },
  {
    "bug_location": "Raftstore",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Non-deterministic response handling during node shutdown, causing potential transaction state inconsistency due to race conditions in callback processing and leader transfer",
    "issue_number": 10353,
    "title": "response from raftstore is not deterministic"
  },
  {
    "bug_location": "TiKV Configuration",
    "severity": 3,
    "categories": [
      "Config",
      "Upgrade"
    ],
    "root_cause": "Attempting to enable TTL (Time-To-Live) on an existing non-TTL TiKV instance during upgrade, which is not supported",
    "issue_number": 10367,
    "title": "upgrade from v4.0.3 to v5.1.0 fail for tikv timeout,report critical config check failed: can't enable ttl on a non-ttl instance"
  },
  {
    "bug_location": "Monitoring",
    "severity": 2,
    "categories": [
      "Monitoring",
      "CodeBug"
    ],
    "root_cause": "Metric `tikv_channel_full` was intentionally removed in a previous code change (PR #4232), leaving a monitoring gap for channel full events",
    "issue_number": 10377,
    "title": "Channel full metric needs to be updated"
  },
  {
    "bug_location": "raftstore/peer_storage",
    "severity": 5,
    "categories": [
      "Storage",
      "Replication",
      "Performance"
    ],
    "root_cause": "Failed to get snapshot after multiple retry attempts during high-concurrency workload preparation",
    "issue_number": 10407,
    "title": "TiKV panic during go-tpc prepare"
  },
  {
    "bug_location": "TiKV Cluster Availability",
    "severity": 4,
    "categories": [
      "Performance",
      "Replication",
      "LoadBalance"
    ],
    "root_cause": "Unexpected performance degradation during node failure and recovery, likely related to data rebalancing and leader transfer mechanisms",
    "issue_number": 10408,
    "title": "QPS drop a lot when a tikv is killed"
  },
  {
    "bug_location": "CDC (Change Data Capture) sink channel",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Memory"
    ],
    "root_cause": "Quota not properly freed when channel message sending fails, causing resource leak in CDC sink channel",
    "issue_number": 10411,
    "title": "CDC sink quota leaks after channel is dropped"
  },
  {
    "bug_location": "TiKV Configuration Management",
    "severity": 3,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Configuration update mechanism does not immediately apply new interval settings, instead waiting for the previous interval to timeout before taking effect",
    "issue_number": 10426,
    "title": "Online changing `resolved-ts.advance-ts-interval` can't take effect immediately"
  },
  {
    "bug_location": "Backup/Restore Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Race condition between SST file cleanup and region split/ingest operations, causing file deletion before region replica can apply ingest request",
    "issue_number": 10438,
    "title": "TiKV panic because of ingest files not exist."
  },
  {
    "bug_location": "Store Initialization",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage",
      "Replication"
    ],
    "root_cause": "Inefficient handling of tombstone regions during store restart, causing extensive time spent fetching and deleting raft entries",
    "issue_number": 10444,
    "title": "TiKV spend more than 20 minutes to (re)start"
  },
  {
    "bug_location": "Memory Management",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Memory leak or inefficient memory allocation under high write concurrency, causing Out of Memory (OOM) condition during stress testing",
    "issue_number": 10445,
    "title": "TiKV OOM under high write pressure"
  },
  {
    "bug_location": "Transaction",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Prewrite operation lacks idempotency guarantee, causing inconsistent transaction state when RPC failures occur during transaction commit",
    "issue_number": 10468,
    "title": "Anomaly found by jepsen append workload"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 10492,
    "title": "TiKV OOM under high pressure"
  },
  {
    "bug_location": "Raft Consensus Module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Uninitialized peer can be recreated due to empty region state, allowing multiple leaders in the same term which breaks Raft consensus protocol invariants",
    "issue_number": 10533,
    "title": "destroy uninitialized peer can make it possible to recreate old peer"
  },
  {
    "bug_location": "storage/mod.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Incorrect key encoding during load-based region splitting, causing raw keys instead of memcomparable encoded keys to be used for region split decision",
    "issue_number": 10542,
    "title": "load-base-split may split regions with unencoded keys"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 10549,
    "title": "Unstable test `import::test_sst_service::test_duplicate_and_close`"
  },
  {
    "bug_location": "Transaction",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Potential lost update issue during append operations detected by Jepsen testing, indicating a transaction consistency problem in concurrent append scenarios",
    "issue_number": 10552,
    "title": "Lost update was detected by jepsen append"
  },
  {
    "bug_location": "Transaction Layer",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Potential race condition in transaction commit process causing stale data reads during concurrent transactions with different isolation modes",
    "issue_number": 10570,
    "title": "Transaction may read staled data"
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 5,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "Incorrect block cache configuration when deploying multiple TiKV instances on a single machine, leading to memory exhaustion during data migration workloads",
    "issue_number": 10580,
    "title": "Tikv node oom happened while dm job working"
  },
  {
    "bug_location": "storage.io-rate-limiter",
    "severity": 3,
    "categories": [
      "Performance",
      "Config"
    ],
    "root_cause": "IO rate limiter configuration does not effectively control SST importer write operations during data import",
    "issue_number": 10581,
    "title": "ratelimiter: sst_importer local write IO is not limited"
  },
  {
    "bug_location": "Raft Consensus Module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Unsafe configuration change handling in 2-replica Raft groups that can lead to leader destruction and region unavailability during voter removal",
    "issue_number": 10595,
    "title": "Remove voter from a 2 replica raft group may lead to region unavailable"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 10658,
    "title": "Fail to profile TiKV in aarch64"
  },
  {
    "bug_location": "RocksDB/Storage Engine",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Incorrect L0 file ingestion and deletion handling logic causing excessive L0 file accumulation beyond configured limits",
    "issue_number": 10680,
    "title": "L0 files are not fully respected when using ingest deletion"
  },
  {
    "bug_location": "Configuration Management",
    "severity": 2,
    "categories": [
      "Config",
      "CodeBug"
    ],
    "root_cause": "Configuration parameter 'raftstore.raft-max-inflight-msgs' is not properly registered in the configuration query system",
    "issue_number": 10683,
    "title": "raftstore.raft-max-inflight-msgs can not support query through show config"
  },
  {
    "bug_location": "server/storage",
    "severity": 4,
    "categories": [
      "Storage",
      "Recovery",
      "CodeBug"
    ],
    "root_cause": "Improper handling of disk space exhaustion scenario, causing panic instead of graceful error handling",
    "issue_number": 10688,
    "title": "tikv panic with \"no such file or directory\" when disk space is not enough"
  },
  {
    "bug_location": "TiKV Storage",
    "severity": 4,
    "categories": [
      "Storage",
      "Memory",
      "Upgrade"
    ],
    "root_cause": "Potential memory configuration issue during version upgrade, likely related to incorrect block-cache-size configuration causing repeated TiKV crashes under high-stress workload",
    "issue_number": 10694,
    "title": "After upgrading from 5.1.0 to the 5.2.0 master, run sysbench insert and query\uff0csome tikv crash repeatedly"
  },
  {
    "bug_location": "TiKV Analyze Component",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Auto analyze feature consuming excessive memory during full sampling, causing memory consumption of approximately 22MB per region, potentially leading to OOM on memory-constrained instances",
    "issue_number": 10701,
    "title": "v5.2.0: run sysbench insert abort 40min, one tikv oom"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 10702,
    "title": "after two tikv restart\uff0c The \u201cMax Resolved TS gap\u201d display is unreasonable"
  },
  {
    "bug_location": "TiKV Storage",
    "severity": 5,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Improper handling of disk full scenarios causing DDL operations to become stuck without proper error propagation",
    "issue_number": 10708,
    "title": "After the disk is full, create table\u3001drop table\u3001truncate table operators are all stuck\uff0cand no any prompt message"
  },
  {
    "bug_location": "Storage/Memory Management",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "Potential memory pressure caused by bandwidth throttling leading to out-of-memory condition during high-load sysbench workload",
    "issue_number": 10728,
    "title": "TiKV oom when inject bandwidth limit"
  },
  {
    "bug_location": "DDL Worker",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "Recovery"
    ],
    "root_cause": "Improper handling of disk full conditions during DDL operations, causing task blocking and retry deadlock",
    "issue_number": 10731,
    "title": "*: Another drop table blocked problem when tikv is disk full"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 10737,
    "title": "After a slow node appears, the alarm that prompts the user to open the slow node processing is not generated"
  },
  {
    "bug_location": "Load Balancer / Slow Node Detection",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Overly sensitive slow node detection mechanism that incorrectly identifies a node as slow during high-concurrency workloads with hotspot traffic",
    "issue_number": 10744,
    "title": "When the business has a hot spot, one node is mistakenly detected as a slow node"
  },
  {
    "bug_location": "storage/txn/flow_controller",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Unexpected None value unwrapping in flow controller's tick_l0 method during heavy write workload, causing a panic",
    "issue_number": 10752,
    "title": "Panic in flow controller"
  },
  {
    "bug_location": "PD Scheduler",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Slow leader eviction mechanism when a node experiences bandwidth throttling, causing extended QPS recovery time",
    "issue_number": 10754,
    "title": "run sysbench prepare\uff0cAfter a slow node appears, QPS recover is too slow due to PD scheduler slow"
  },
  {
    "bug_location": "Store Detection/Load Balancing",
    "severity": 3,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Inefficient store detection algorithm that fails to quickly identify and respond to slow nodes under CPU-constrained environments",
    "issue_number": 10756,
    "title": "run sysbench update\uff0cAfter a slow node appears, QPS recover is too slow due to detection algorithm"
  },
  {
    "bug_location": "Region Split/Merge Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Inaccurate calculation of approximate keys and region size during region split/merge operations, leading to incorrect region management",
    "issue_number": 10762,
    "title": "Insert data into single table\uff0c after running for a period of time, large region are generated"
  },
  {
    "bug_location": "Replication/DR-AutoSync",
    "severity": 4,
    "categories": [
      "Replication",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Regions unable to transition from SIMPLE_MAJORITY to INTEGRITY_OVER_LABEL during disaster recovery auto-sync mode, causing synchronization blocking",
    "issue_number": 10772,
    "title": "region cannot be INTEGRITY_OVER_LABEL in dr-auto-sync"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 10803,
    "title": "truncate table during running sysbench insert\uff0cone tikv panic four times"
  },
  {
    "bug_location": "Monitoring",
    "severity": 2,
    "categories": [
      "Monitoring",
      "CodeBug"
    ],
    "root_cause": "Monitoring dashboard not capturing aborted connection events correctly",
    "issue_number": 10808,
    "title": "aborted connection doesn't show error in error dashboard"
  },
  {
    "bug_location": "raftstore/store/fsm/store.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Early return in snapshot garbage collection logic prevents cleanup of corrupted snapshot files, causing storage accumulation and potential performance issues",
    "issue_number": 10813,
    "title": "corrupted snapshot can prevent snapshot files gc"
  },
  {
    "bug_location": "RocksDB Storage Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Memory",
      "Performance"
    ],
    "root_cause": "Potential memory management issue with jemalloc, where memory is not being properly released back to the system during RocksDB compaction",
    "issue_number": 10832,
    "title": "rocksdb background error. db: kv, reason: compaction, error: IO error: While appending to file: /var/lib/tikv/data/db/438734.sst: Cannot allocate memory\""
  },
  {
    "bug_location": "raftstore/store/peer.rs",
    "severity": 4,
    "categories": [
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Incorrect peer destruction logic during configuration changes, causing unexpected peer removal when processing snapshots and configuration change entries",
    "issue_number": 1084,
    "title": "TiKV panic on `commit_to()`"
  },
  {
    "bug_location": "Logger/Slogger Thread",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug",
      "Monitoring"
    ],
    "root_cause": "Logger thread becomes a bottleneck when processing excessive slow logs, causing log message queue to accumulate and block system performance, particularly under high coprocessor load",
    "issue_number": 10841,
    "title": "*:Logger thread can be bottleneck and slow down the whole TiKV instance"
  },
  {
    "bug_location": "coprocessor_plugin",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Unsafe dynamic plugin loading mechanism that allows unintended plugin removal during runtime",
    "issue_number": 10854,
    "title": "copr_plugin: should disallow unregistering plugins"
  },
  {
    "bug_location": "Flow Control Component",
    "severity": 5,
    "categories": [
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Uneven flow control mechanism causing significant QPS degradation under high concurrency and load",
    "issue_number": 10879,
    "title": "Under high pressure and hot business, QPS dropped bottom due to uneven flow control"
  },
  {
    "bug_location": "CDC",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "Flow control mechanism preventing snapshot release when CDC process is paused or slow",
    "issue_number": 10900,
    "title": "CDC may hold snapshots for a long time when eventfeed is flowcontrolled"
  },
  {
    "bug_location": "Build/Compilation",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "DWARF debug information corruption during full Link Time Optimization (LTO) compilation, causing invalid debug symbol references",
    "issue_number": 10906,
    "title": "DWARF info of dist_release binary is incorrect"
  },
  {
    "bug_location": "Raftstore",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Deadlock caused by incorrect locking sequence of store_meta and global_replication_state during region migration",
    "issue_number": 10909,
    "title": "Raftstore deadlock during migrating regions"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Increased sample data size in TiDB 5.1+ causing analyze table requests to exceed 60-second timeout limit for TiKV coprocessor requests",
    "issue_number": 10924,
    "title": "Analyze table failed due to tikv server timeout error during lighting import"
  },
  {
    "bug_location": "TiKV Storage",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "High memory pressure during sysbench workload causing out-of-memory (OOM) conditions for some TiKV instances",
    "issue_number": 10927,
    "title": "the status of two store are abnormal and some tikv oom while run sysbench"
  },
  {
    "bug_location": "RegionCollector",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Performance"
    ],
    "root_cause": "Potential issue in leader tracking metrics calculation that prevents accurate leader transfer tracking during rolling restart",
    "issue_number": 10942,
    "title": "evict leader timeout during rolling restart"
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 5,
    "categories": [
      "Storage",
      "CodeBug",
      "Recovery"
    ],
    "root_cause": "Potential data corruption during import process causing RocksDB internal state inconsistency",
    "issue_number": 10947,
    "title": "TiKV panic during import data because of RocksDB Corruption."
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 10949,
    "title": "Correct test coverage information in README"
  },
  {
    "bug_location": "Raft Log Garbage Collection (Log GC)",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication",
      "Performance"
    ],
    "root_cause": "Periodic raft log purge tick incorrectly triggering unnecessary log compaction and snapshot generation during cluster rolling update, potentially due to an overflow calculation in estimated region log",
    "issue_number": 10954,
    "title": "Periodically raft purge tick can cause unnecessary snapshots between rolling update"
  },
  {
    "bug_location": "resolved_ts component",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Unbounded coroutine accumulation when using futures::join_all without proper timeout or cancellation mechanism, causing potential memory exhaustion when one TiKV node becomes unresponsive",
    "issue_number": 10965,
    "title": "Coroutine leaking after one TiKV got stuck"
  },
  {
    "bug_location": "TiKV Configuration Parser",
    "severity": 2,
    "categories": [
      "Config",
      "CodeBug"
    ],
    "root_cause": "Configuration parser only supports numeric values for compaction-style instead of string literals, causing configuration parsing failure",
    "issue_number": 11028,
    "title": "`compaction-style` setting is broken"
  },
  {
    "bug_location": "Startup/Initialization",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Recovery",
      "Storage"
    ],
    "root_cause": "Potential filesystem or mount configuration issue causing panic during TiKV startup process",
    "issue_number": 11037,
    "title": "TiKV panic when startup"
  },
  {
    "bug_location": "components/raftstore/src/store/fsm/apply.rs",
    "severity": 2,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Performance context is reset too early, causing incorrect latency measurements when multiple writes occur in a single round of the apply batch system",
    "issue_number": 11044,
    "title": "Should reset perf context after writing to kvdb"
  },
  {
    "bug_location": "server/service/kv.rs",
    "severity": 4,
    "categories": [
      "Network",
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Improper error handling when Raft channel becomes full, causing RPC stream disruption and potential cluster instability",
    "issue_number": 11047,
    "title": "Channel full could break the raft connection"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Incorrect event sending order causing potential data inconsistency during delta scan snapshot process",
    "issue_number": 11055,
    "title": "cdc: Init event should be sent after all observed events have been sent"
  },
  {
    "bug_location": "Raft Leader Election",
    "severity": 4,
    "categories": [
      "Network",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Large gRPC message causing connection abort between TiKV nodes, leading to network partition and disrupted leader election process",
    "issue_number": 11075,
    "title": "After a tikv failure is recovered, the leader of this tikv is always zero"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Overly aggressive message batching in CDC causing excessive memory consumption beyond default quota of 512MB",
    "issue_number": 11082,
    "title": "CDC message batching is too aggressive and may exceed the default memory quota   "
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 11117,
    "title": "auto backup error"
  },
  {
    "bug_location": "CDC (Change Data Capture) component, specifically in delegate.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Race condition during region epoch handling and downstream connection management, causing a panic when a region loses its downstream connection unexpectedly",
    "issue_number": 11123,
    "title": "CDC panic \"region 298252 miss downstream\""
  },
  {
    "bug_location": "raftstore",
    "severity": 5,
    "categories": [
      "Replication",
      "Recovery"
    ],
    "root_cause": "Assertion failure during peer destruction while handling snapshot, likely indicating a race condition or improper state management during TiKV scaling operation",
    "issue_number": 11130,
    "title": "one tikv panic while scaling in tikv"
  },
  {
    "bug_location": "server/status_server/profile",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Improper state management in heap profiling activation, where `on_end` was not called, causing an assertion failure when attempting to reactivate profiling",
    "issue_number": 11156,
    "title": "Panic when activating heap profiling"
  },
  {
    "bug_location": "server/snap.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Replication"
    ],
    "root_cause": "Snapshot state tracking fails to deregister 'SnapEntry::Receiving' when network errors occur during snapshot transfer, causing incorrect snapshot state count",
    "issue_number": 11157,
    "title": "Incorrect snapshot state count if an error occurs on receiving snapshot"
  },
  {
    "bug_location": "status_server/mod.rs",
    "severity": 5,
    "categories": [
      "Security"
    ],
    "root_cause": "Unrestricted file access through heap profile debug endpoint, allowing potential unauthorized file read access",
    "issue_number": 11161,
    "title": "security issue of dumping heap profile through http api"
  },
  {
    "bug_location": "documentation",
    "severity": 2,
    "categories": [
      "CodeBug"
    ],
    "root_cause": "Lack of maintenance and update of release notes in CHANGELOG.md",
    "issue_number": 11167,
    "title": "Update release note in the TiKV repo"
  },
  {
    "bug_location": "Raft Consensus Module",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug",
      "Network"
    ],
    "root_cause": "Async IO changes break existing slow node detection mechanism by disrupting latency measurement of store loop operations",
    "issue_number": 11178,
    "title": "Slow node detection is not compatible with raft async io"
  },
  {
    "bug_location": "CDC/Resolved_TS Module",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Stale non-pessimistic locks can be prewritten without constraint checks, potentially breaking transaction consistency assumptions in the DeltaScanner and CDC processing",
    "issue_number": 11187,
    "title": "Stale non-pessimistic lock may be incompatible with CDC and possibly affect data correctness"
  },
  {
    "bug_location": "coprocessor/statistics/analyze",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Unhandled edge case in row sampling when samples are empty and max_sample_size is 0, causing an unwrap() panic",
    "issue_number": 11192,
    "title": "Panic in analyze sampling"
  },
  {
    "bug_location": "metrics/threads_linux.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance",
      "Memory"
    ],
    "root_cause": "Thread metrics labels are not properly cleaned up after thread exit, causing memory leak and potential resource accumulation",
    "issue_number": 11195,
    "title": "Label leaking of thread metrics"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 11197,
    "title": "Unstable test about slow store detection"
  },
  {
    "bug_location": "External Storage Thread Management",
    "severity": 2,
    "categories": [
      "Memory",
      "Performance"
    ],
    "root_cause": "Ineffective thread list cache mechanism when using block_on_external_io, leading to potential memory fragmentation during thread creation",
    "issue_number": 11204,
    "title": "Possible memory fragmentation risk when using external storage"
  },
  {
    "bug_location": "tikv-ctl logging system",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Async logging drain not properly flushed when process exits, causing incomplete log output due to std::process::exit preventing log drain flushing",
    "issue_number": 11210,
    "title": "tikv-ctl's output is incomplete, maybe slog-async is not flushed when tikv-ctl process exits"
  },
  {
    "bug_location": "gc_worker/gc_worker.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Missing data key prefix when retrieving regions for garbage collection, causing incorrect key range processing",
    "issue_number": 11217,
    "title": "TiKV GcKeys task doesn't work when called with multiple keys (at least in 5.1 but I think for everything)"
  },
  {
    "bug_location": "RocksDB Configuration Parsing",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Config",
      "Storage"
    ],
    "root_cause": "RocksDB configuration option introduced in newer version cannot be parsed by older TiKV versions during rollback, causing option parsing failure",
    "issue_number": 11226,
    "title": "TiKV rollback encounters panic \"failed to load_latest_options\""
  },
  {
    "bug_location": "Configuration/Resource Metering",
    "severity": 2,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Default configuration for resource-metering.enabled was incorrectly set to true instead of false, potentially causing unnecessary performance overhead when TopSQL is not enabled",
    "issue_number": 11235,
    "title": "v5.2.2 resource-metering.enabled default value is true, should be false"
  },
  {
    "bug_location": "Apply Thread / SST Ingestion",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Verify-checksum operation is performed synchronously in a single apply-thread, causing significant performance bottleneck when multiple SST files need to be ingested",
    "issue_number": 11239,
    "title": "import: TiKV verify-checksum in apply-thread and it cost a lot of time"
  },
  {
    "bug_location": "rocksdb/compaction",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Invalid data key processing during RocksDB compaction job, causing unexpected panic",
    "issue_number": 11290,
    "title": "Panic in rocksdb compaction job"
  },
  {
    "bug_location": "coprocessor/statistics/analyze",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential race condition or unhandled edge case in row sampling statistics collection, causing an unwrap() on a None value during benchmark dataset cleanup",
    "issue_number": 11295,
    "title": "panic when preparing tpcc/ch benchmark dataset"
  },
  {
    "bug_location": "components/tikv_util/src/sys/cgroup.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Assertion failure in cgroup parsing logic when attempting to insert key-value pairs, likely due to an unexpected state in container/cgroup configuration",
    "issue_number": 11296,
    "title": "Can't run tikv inside container"
  },
  {
    "bug_location": "Monitoring/Metrics",
    "severity": 3,
    "categories": [
      "Monitoring",
      "CodeBug"
    ],
    "root_cause": "Incorrect metric calculation where instance selection filter is missing from denominator in average gRPC duration expression",
    "issue_number": 11299,
    "title": "Incorrect by-instance average gRPC duration"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 11313,
    "title": "hint_min_ts can cause downstream data incorrupt"
  },
  {
    "bug_location": "PD (Placement Driver) Load Balancing",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance",
      "Replication"
    ],
    "root_cause": "Slow leader transfer and balance operators with frequent timeouts, possibly related to resource constraints or network issues",
    "issue_number": 11319,
    "title": "one tikv failure was restored after one minute, but the leader start the balance after 3 hours"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "Performance",
      "Replication",
      "Network"
    ],
    "root_cause": "Synchronous log entry fetching blocks RaftStore processing, causing high latency during node recovery and network fault scenarios",
    "issue_number": 11320,
    "title": "Raft should fetch log entries in async way"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Intermittent test failure in joint configuration change handling, likely due to race condition or synchronization issue in test_resolve module",
    "issue_number": 11325,
    "title": "test_resolve::test_joint_confchange failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 11340,
    "title": "GC scan detail has no data"
  },
  {
    "bug_location": "components/tikv_util/src/cgroupp.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Integer parsing error when handling cgroup limits larger than 2^63-1, causing parsing overflow",
    "issue_number": 11348,
    "title": "tidb-cluster can not run pod of basic-tikv-0"
  },
  {
    "bug_location": "Logging/Time Configuration",
    "severity": 2,
    "categories": [
      "Config",
      "Human"
    ],
    "root_cause": "Timezone configuration incompatibility in Alpine Linux Docker container with musl libc, causing incorrect log timestamp rendering",
    "issue_number": 1135,
    "title": "log time is incorrect in docker"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "Replication"
    ],
    "root_cause": "Blocking wait for leader check requests when a TiKV node is disconnected, causing resolved timestamp (resolved ts) to lag",
    "issue_number": 11351,
    "title": "resolved ts lag increased after stoping a tikv"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 11353,
    "title": "cgroup code parses memory.limit_in_bytes fail"
  },
  {
    "bug_location": "RaftStore/Peer",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of local Raft message (MsgUnreachable) causing message routing errors during high-load sysbench workload",
    "issue_number": 11371,
    "title": "tikv error in log: Raft raft: cannot step raft local message"
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "Potential memory management issue during high-stress workload with concurrent sysbench prepare and query operations, leading to out-of-memory (OOM) conditions",
    "issue_number": 11379,
    "title": "tikv oom when run sysbench prepare while query under stress"
  },
  {
    "bug_location": "TiKV Control (tikv-ctl) Raft Database Interface",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Recovery"
    ],
    "root_cause": "Potential issue with Raft database state retrieval mechanism in tikv-ctl when accessing region states after cluster shutdown",
    "issue_number": 11393,
    "title": "tikv-ctl can't open raft db correctly"
  },
  {
    "bug_location": "raftstore/apply",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Inconsistent commit state causing region state jump backward, potentially due to race condition or incorrect state synchronization in Raft apply process",
    "issue_number": 11396,
    "title": "two tikv crash repeatly after run some stress workload"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Inefficient log truncation mechanism causing excessive RaftDB write operations and accumulation of small SST files during garbage collection tasks",
    "issue_number": 11404,
    "title": "raftstore: TiKV can not deleted the truncated raft log as soon as possible"
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 4,
    "categories": [
      "Performance",
      "Memory",
      "Storage"
    ],
    "root_cause": "Potential memory leak or inefficient resource management during long-running write-intensive workloads, causing performance degradation and out-of-memory (OOM) conditions",
    "issue_number": 11410,
    "title": "performance is getting worse and two tikv oom after running sysbech write-only for a long time"
  },
  {
    "bug_location": "TiKV Scaling Component",
    "severity": 5,
    "categories": [
      "Performance",
      "LoadBalance",
      "Replication"
    ],
    "root_cause": "Performance instability during horizontal scaling of TiKV nodes, causing significant QPS drops when expanding cluster from 3 to 6 nodes",
    "issue_number": 11424,
    "title": "QPS dropped severely many times during scale out in DBaaS"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 11431,
    "title": "QPS drop 1% ~4% for Point Get workload in v5.3.0, compared to v5.2.2"
  },
  {
    "bug_location": "Monitoring/AlertRules",
    "severity": 3,
    "categories": [
      "Monitoring",
      "Config"
    ],
    "root_cause": "Stale alert rule condition using deprecated apply snapshot counter after its removal, causing ineffective low disk space alerting",
    "issue_number": 11434,
    "title": "ineffective alert rule for low space"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Monitoring"
    ],
    "root_cause": "Incorrect error type renaming in coprocessor alert rules, causing potential monitoring misalignment",
    "issue_number": 11437,
    "title": "wrong coprocessor alert rules"
  },
  {
    "bug_location": "server/coprocessor/endpoint",
    "severity": 4,
    "categories": [
      "Memory",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Memory management issue in coprocessor endpoint task processing, likely related to memory allocation/deallocation race conditions or invalid pointer handling",
    "issue_number": 1144,
    "title": "tikv-server: Segmentation fault"
  },
  {
    "bug_location": "Storage",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Incorrect handling of start_key and end_key during reverse scan, causing memory lock detection failure and potential stale data reads",
    "issue_number": 11440,
    "title": "Reverse scan can't detect memory locks"
  },
  {
    "bug_location": "Integration Test Framework",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "Unknown"
    ],
    "root_cause": "Potential timeout or synchronization issue in integration test environment causing test hang",
    "issue_number": 11461,
    "title": "TiKV hangs with latest commit in integration tests"
  },
  {
    "bug_location": "raftstore/apply.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Incorrect peer role validation during learner addition, causing duplicate learner peer rejection when attempting to demote a voter to learner",
    "issue_number": 11463,
    "title": "tikv error when demote follower to learner"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 11466,
    "title": "copr: got bug when cast invalid utf8 to real"
  },
  {
    "bug_location": "Region Management",
    "severity": 2,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "During Lightning import, TiKV's region split mechanism creates unnecessary empty regions when checking split conditions after import job completion, potentially causing temporary region fragmentation",
    "issue_number": 11470,
    "title": "Big/Fast increase of empty regions after import of tpcc 5000 warehouses"
  },
  {
    "bug_location": "raftstore/store/fsm/peer.rs",
    "severity": 5,
    "categories": [
      "Upgrade",
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Inconsistent peer state during region merge after version upgrade, causing unexpected merge result and panic",
    "issue_number": 11475,
    "title": "One Tikv panic after upgrade from 5.0.1 to 5.3.0"
  },
  {
    "bug_location": "Upgrade/Recovery",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Recovery",
      "Transaction"
    ],
    "root_cause": "Potential synchronization or state recovery issue during version upgrade from 5.0.1 to 5.3.0, causing extended hanging after TiKV node recovery",
    "issue_number": 11484,
    "title": "Upgrade hung more than 1 hours when upgrade from 5.0.1 to 5.3.0"
  },
  {
    "bug_location": "Configuration Parser",
    "severity": 3,
    "categories": [
      "Config",
      "Upgrade"
    ],
    "root_cause": "Configuration enum variant mismatch during configuration parsing after configuration schema change in nightly version",
    "issue_number": 11489,
    "title": "nightly upgrade panic due to unrecognized configuration for raft-engine.recovery-mode"
  },
  {
    "bug_location": "tidb_query_expr",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Substring function not fully supported in batch processing mode, causing an error when used in index merge query scenarios",
    "issue_number": 11495,
    "title": "Got error when substring is used in filter"
  },
  {
    "bug_location": "Monitoring/AlertRules",
    "severity": 3,
    "categories": [
      "Monitoring",
      "CodeBug",
      "Config"
    ],
    "root_cause": "Alert rule regex pattern for apply worker thread name became outdated, causing monitoring alert to fail when thread naming convention changed",
    "issue_number": 11517,
    "title": "Alert rule is wrong as the name of apply worker has changed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 11524,
    "title": "br restore encrypted backup failed \"Bad table magic number\""
  },
  {
    "bug_location": "RaftStore",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Incorrect handling of Raft log terms during region merge, causing inconsistent state machine and potential data corruption",
    "issue_number": 11526,
    "title": "After inserted 1 billions data in to tidb, the one of TiKV never able to startup, neither restart."
  },
  {
    "bug_location": "storage/mvcc/reader/scanner",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Unexpected panic during forward scanning of MVCC data, likely due to an edge case in timestamp cursor movement logic",
    "issue_number": 11541,
    "title": "Unexpected panic when scan_next"
  },
  {
    "bug_location": "storage/mvcc/reader/scanner/forward.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Invalid write cursor state during MVCC scanner operation, likely caused by incorrect cursor management or timestamp tracking in the forward scanner",
    "issue_number": 11543,
    "title": "Assertion failed: `self.cursors.write.valid()?`"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 11547,
    "title": "One TiKV OOM when it exhausts memory in one numa"
  },
  {
    "bug_location": "PD Worker Thread",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Potential mutex deadlock in PD worker thread synchronization mechanism, likely related to timer thread blocking for over 500ms",
    "issue_number": 11549,
    "title": "TiKV PD worker thread deadlocks"
  },
  {
    "bug_location": "storage/mod.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Monitoring",
      "Storage"
    ],
    "root_cause": "Raw scan metrics incorrectly tracking keys read across column families, potentially skipping write column family during key scanning",
    "issue_number": 11571,
    "title": "keys read metrics in raw_scan is wrong"
  },
  {
    "bug_location": "components/raftstore/src/store/worker/compact.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Manual compaction mechanism only checks tombstone for write column family, which does not apply to RawKV using default column family, preventing storage space reclamation",
    "issue_number": 11575,
    "title": "Manual compaction is not enabled for raw kv usage"
  },
  {
    "bug_location": "TiKV Storage",
    "severity": 5,
    "categories": [
      "Memory",
      "Storage",
      "Performance"
    ],
    "root_cause": "Memory exhaustion during high-concurrency bulk insert workload, causing TiKV instances to crash and fail to restart",
    "issue_number": 11581,
    "title": "v5.2.2: 2 tikvs oom and can't start after 1 hours "
  },
  {
    "bug_location": "tikv-ctl",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect handling of region properties for RawKV mode, causing default/zero values to be returned instead of actual region metadata",
    "issue_number": 11583,
    "title": "tikv-ctl region-properties doesn't work on rawkv"
  },
  {
    "bug_location": "Storage",
    "severity": 5,
    "categories": [
      "Storage",
      "Config",
      "CodeBug"
    ],
    "root_cause": "Incompatible data key version when attempting to enable TTL storage with existing data keys not written by TiDB",
    "issue_number": 11588,
    "title": "TiKV panic on restart when `storage.enable_ttl=true`"
  },
  {
    "bug_location": "tikv-ctl logging system",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Monitoring"
    ],
    "root_cause": "Potential logging configuration issue in offline/recovery mode preventing debug log output",
    "issue_number": 11589,
    "title": "Some logs of tikv-ctl are not printed on offline mode"
  },
  {
    "bug_location": "RaftStore",
    "severity": 3,
    "categories": [
      "Performance",
      "Replication"
    ],
    "root_cause": "Potential inefficiency in async snapshot transfer mechanism causing unexpected high tail latency during read-only workloads",
    "issue_number": 11596,
    "title": "Unexpected high tail latency of async snapshot"
  },
  {
    "bug_location": "components/tikv_util/src/sys/cgroup.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incorrect parsing of cgroup file with two lines, likely due to an assumption of single-line cgroup file format in systemd container environment",
    "issue_number": 11609,
    "title": "tikv panicked in parse_proc_cgroup_v2"
  },
  {
    "bug_location": "Transaction/Pessimistic Locking",
    "severity": 3,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Potential race condition in pessimistic transaction locking mechanism where write conflicts can occur during index key prewriting, especially in scenarios with concurrent transactions and partial key ",
    "issue_number": 11612,
    "title": "Sysbench read and write workload hit 9007 Write conflict error"
  },
  {
    "bug_location": "raftstore/peer",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of aborted snapshots causing potential panic during Raft log reconciliation when log entries between applied_index and last_index are missing",
    "issue_number": 11618,
    "title": "Panic after snapshot is aborted"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 1162,
    "title": "rust: add debug message in time panic "
  },
  {
    "bug_location": "Transaction",
    "severity": 3,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential race condition or synchronization issue in pessimistic lock handling introduced by async-io changes in callback mechanism",
    "issue_number": 11649,
    "title": "More frequent 'pessimistic lock not found' since v5.3.0"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 11669,
    "title": "table_storage_stats.TABLE_KEYS is much higher than actual rows causing analyze wrong sample ratio "
  },
  {
    "bug_location": "TiKV Storage/SST Importer",
    "severity": 4,
    "categories": [
      "Memory",
      "Storage",
      "Performance"
    ],
    "root_cause": "Excessive memory consumption during backup/restore operation on ARM64 architecture, likely exacerbated by Transparent Huge Pages (THP) being enabled",
    "issue_number": 11670,
    "title": "tikv keep oom on arm64 k8s deployments during br restore"
  },
  {
    "bug_location": "RocksDB/Storage Layer",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Out-of-order key insertion during range cleanup or compaction, potentially causing data inconsistency or corruption",
    "issue_number": 11673,
    "title": "Can't cleanup overlapped ranges due to out-or-order keys"
  },
  {
    "bug_location": "System Information/Metrics Collection",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Upgrade",
      "Performance"
    ],
    "root_cause": "Limitation in procfs library parsing kernel patch version, causing panic when patch version exceeds 255",
    "issue_number": 11697,
    "title": "panic on 'Failed to parse patch version'"
  },
  {
    "bug_location": "Build System",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incompatible library linking on macOS due to compiler differences between gcc and clang",
    "issue_number": 1172,
    "title": "failed to static link rocksdb on latest OS X"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Race condition between snapshot file garbage collection and peer state persistence, causing potential data loss and inconsistency during snapshot application and recovery",
    "issue_number": 11746,
    "title": "raftstore: snapshot files may deleted while the peer state is still `PeerState::Applying`"
  },
  {
    "bug_location": "Memory Management",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Insufficient memory control and cgroup memory limit recognition during high-write workload in sysbench test",
    "issue_number": 11747,
    "title": "sysbench write only run makes tikv oom"
  },
  {
    "bug_location": "GC Worker",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "GcKeys tasks blocking UnsafeDestroyRange requests, causing excessive GC processing time and potential system stall",
    "issue_number": 11752,
    "title": "GcKeys tasks in GC worker block UnsafeDestroyRange requests and make GC stuck"
  },
  {
    "bug_location": "resource_metering",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Assertion failure in test case where expected read keys count (4) does not match actual read keys count (8)",
    "issue_number": 11765,
    "title": "Unstable test: resource_metering::test_read_keys::test_read_keys_coprocessor"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 11767,
    "title": "Resolved ts exceed 1 hours for more than 30min without any fault inject"
  },
  {
    "bug_location": "Raft Message Handling",
    "severity": 4,
    "categories": [
      "Performance",
      "Config",
      "Transaction"
    ],
    "root_cause": "Suboptimal configuration of server.raft-msg-flush-interval parameter causing performance degradation in message processing",
    "issue_number": 11769,
    "title": "21% tps degradation on ycsb workloada from commit 74cd8a"
  },
  {
    "bug_location": "external_storage",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incomplete or incorrect dependency and import configurations for cloud storage features, specifically for dylib and gRPC backends",
    "issue_number": 11773,
    "title": "cloud: build failed when `grpc` or `dylib` backend enabled."
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 11774,
    "title": "one tikv\uff08recovered pod\uff09oom repeatedly when network fault recover while run tpcc"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 11775,
    "title": "Log level for IO snooper failure is too high"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 11789,
    "title": "tikv have not logs saved in k8s "
  },
  {
    "bug_location": "RocksDB Rate Limiter",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage",
      "Config"
    ],
    "root_cause": "Rate limiter mechanism fails to control compaction when pending bytes exceed a threshold, potentially causing uncontrolled compaction despite configured rate limits",
    "issue_number": 11810,
    "title": "RocksDB rate limit takes no effect when pending bytes is high"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 11835,
    "title": "attach resource metering tag expects to be a extremely lightweight job but not now"
  },
  {
    "bug_location": "Configuration Management",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Config"
    ],
    "root_cause": "Deprecated configuration parameter 'resource-metering.enabled' was not properly handled during version upgrade from pre-5.4.0 to 5.4.0, causing configuration validation failure",
    "issue_number": 11836,
    "title": "param resource-metering.enabled  is deleted in v5.4.0, upgrade to 5.4.0 will fail if pre-upgrade version include this config"
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "Potential memory management issue under high concurrency and IO delay conditions, causing simultaneous out-of-memory (OOM) events in multiple TiKV nodes",
    "issue_number": 11838,
    "title": "two tikv oom at the same time while run sysbench_oltp_read_write and inject IO delay"
  },
  {
    "bug_location": "Raftstore",
    "severity": 2,
    "categories": [
      "Replication",
      "Network",
      "Performance"
    ],
    "root_cause": "Stale peer handling during network chaos, where a removed peer remains unaware of its status and waits for a long timeout before self-destruction",
    "issue_number": 11847,
    "title": "resolved_ts: max resolved ts gap exceed 1 hours  while inject network chaos during run stale read"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Race condition during region merge where ReadDelegate is unexpectedly removed, causing panic when attempting to access a non-existent delegate",
    "issue_number": 11852,
    "title": "Got unexpected panic during tipocket ledger test"
  },
  {
    "bug_location": "Configuration Management",
    "severity": 3,
    "categories": [
      "Upgrade",
      "Config",
      "CodeBug"
    ],
    "root_cause": "Incompatible configuration parameter `heavy_load_wait_duration` with different default values across TiKV versions, causing downgrade failure",
    "issue_number": 11861,
    "title": "downgrade from nightly to v5.2.x will fail"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 11893,
    "title": "The predicate on the bit column cannot be pushed down"
  },
  {
    "bug_location": "GC (Garbage Collection) Module",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Inefficient iterator scanning of tombstones during garbage collection, causing unnecessary scans of deleted keys outside the intended range",
    "issue_number": 11902,
    "title": "GC Keys tasks executes slowly due to millions of seek_tombstone"
  },
  {
    "bug_location": "gc_worker/gc_worker.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Task scheduling error causing incorrect maintenance of scheduled_tasks counter, leading to false GcWorkerTooBusy state when task scheduling fails",
    "issue_number": 11903,
    "title": "False GcWorkerTooBusy caused by incorrect scheduled_tasks"
  },
  {
    "bug_location": "GC Worker",
    "severity": 2,
    "categories": [
      "Performance",
      "Monitoring"
    ],
    "root_cause": "Incorrect tracking of pending garbage collection tasks, where the counter represents increments rather than the actual current total number of pending tasks",
    "issue_number": 11915,
    "title": "gc-worker pending tasks can not represent the current total number of real pending but the number of increments "
  },
  {
    "bug_location": "tikv_util::sys::cgroup",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Unwrapping an Option that is None when attempting to retrieve CPU cores from cgroup configuration",
    "issue_number": 11920,
    "title": "tikv-server --config-check failed caused by  \u2018called Option::unwrap() on a None value\u2019 when exec tiup-cluster deploy"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 11923,
    "title": "tikv abnormally reset in 5.1.3 hotfix version"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "Performance",
      "Memory",
      "Transaction"
    ],
    "root_cause": "Potential memory allocation stall causing slow Raft message handling, possibly related to page swapping",
    "issue_number": 11927,
    "title": "Slow to handle Raft messages but no obvious latency source"
  },
  {
    "bug_location": "TiKV Stale Read Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Replication",
      "Performance"
    ],
    "root_cause": "Resolved timestamp advancement blocked by unresolved locks during time offset injection in PD leader, causing region unavailability",
    "issue_number": 11937,
    "title": "Stale read hit data is not ready after inject some errors to pd"
  },
  {
    "bug_location": "Timer/Tokio Timer Component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Performance",
      "Memory"
    ],
    "root_cause": "Integer overflow and incorrect time wheel calculation in tokio-timer's level calculation function, causing index out of bounds when elapsed time approaches 2^36",
    "issue_number": 11940,
    "title": "TiKV running over 2 years may panic"
  },
  {
    "bug_location": "Metrics/Monitoring",
    "severity": 3,
    "categories": [
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Inconsistent sampling periods for different load metrics causing potential monitoring and performance tracking inaccuracies",
    "issue_number": 11941,
    "title": "metrics: load base split event with different period"
  },
  {
    "bug_location": "raftstore",
    "severity": 5,
    "categories": [
      "Network",
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Unwrap on None value during network loss scenario in Raft message handling, likely caused by network interruption disrupting Raft communication state",
    "issue_number": 11951,
    "title": "one tikv panic after inject network-loss"
  },
  {
    "bug_location": "RaftStore/Region Leadership",
    "severity": 4,
    "categories": [
      "Network",
      "Replication",
      "Transaction"
    ],
    "root_cause": "During network loss or TiKV node failure, region leadership election process becomes blocked when a peer is applying snapshot, preventing new leader election and causing region unavailability",
    "issue_number": 11960,
    "title": "report \u201c9005: Region is unavailable\u201d while inject network loss fault for one tikv or down one of tikv or tikv rolling restart"
  },
  {
    "bug_location": "Profiler/Unwind Mechanism",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance",
      "Memory"
    ],
    "root_cause": "Potential issue with libgcc_s unwind implementation during CPU profiling, likely related to incorrect debug information generation",
    "issue_number": 11964,
    "title": "5.1.4 tikv restart in many testcases after start cpu profiling"
  },
  {
    "bug_location": "raftstore/peer_storage",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Unexpected None value when accessing term during Raft log processing, likely during cluster scale-in operation",
    "issue_number": 11973,
    "title": "One tikv panic at tikv_util/src/lib.rs:465 when do scale in"
  },
  {
    "bug_location": "Memory Management",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Transparent Huge Pages (THP) on ARM architecture causing excessive memory allocation, with default page size of 64kb leading to 512MB per THP, causing unexpected memory consumption during TiKV startup",
    "issue_number": 11979,
    "title": "TiKV memory usage problem when THP enabled"
  },
  {
    "bug_location": "Raft Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Replication",
      "Performance"
    ],
    "root_cause": "Potential log synchronization issue during long-term node recovery, causing persistent log lag when Raft Engine is enabled",
    "issue_number": 11989,
    "title": "After a tikv fault (more than 30 minutes) is restored, the raft log lag has been chasing"
  },
  {
    "bug_location": "coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Arrow chunk codec reserves space for fixed-length fields even when values are null, causing response size amplification beyond 4GB limit",
    "issue_number": 11994,
    "title": "TiKV copr resp size may exceed 4GB"
  },
  {
    "bug_location": "TiKV Storage",
    "severity": 5,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "Potential memory management issue causing out-of-memory (OOM) during high-load OLTP workload tests",
    "issue_number": 12002,
    "title": "Master: Tikv oom in daily run test plan"
  },
  {
    "bug_location": "Grafana Dashboard",
    "severity": 2,
    "categories": [
      "Monitoring",
      "CodeBug"
    ],
    "root_cause": "Incorrect metric type and visualization configuration in Grafana dashboard for TiKV metrics",
    "issue_number": 12007,
    "title": "Some grafana expression are wrong or suboptimal"
  },
  {
    "bug_location": "Raft Engine Apply Process",
    "severity": 4,
    "categories": [
      "Performance",
      "Replication",
      "Storage"
    ],
    "root_cause": "Follower apply process cannot keep up with leader append rate, causing entry cache eviction and log synchronization challenges",
    "issue_number": 12009,
    "title": "Apply can't catch up when raft engine is enabled"
  },
  {
    "bug_location": "Raft Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Replication"
    ],
    "root_cause": "Inefficient log compaction and purging mechanism for follower Raft logs, causing extremely slow recovery when large numbers of log files accumulate",
    "issue_number": 12011,
    "title": "follower Raft logs cannot be force compacted and purged by Raft Engine"
  },
  {
    "bug_location": "split_controller.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config",
      "Performance"
    ],
    "root_cause": "ReadStats is created using default() without considering user-configured sample_num, which can lead to insufficient sampling for Load Base Split",
    "issue_number": 12014,
    "title": "The creation of ReadStats should take into account the user configuration of sample_num"
  },
  {
    "bug_location": "backup/writer.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Unhandled empty key scenario during raw backup operation, causing a panic in the backup writer when processing an entry with an empty key",
    "issue_number": 12015,
    "title": "backup: TiKV panic when execute \"br backup raw\" with an empty key entry"
  },
  {
    "bug_location": "RaftStore/StoreFsmDelegate",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Assertion failure during handling of stale Raft messages after region merge, where a peer's ID does not match expected constraints during message processing",
    "issue_number": 12023,
    "title": "RaftStore panicked during jepsen bank test"
  },
  {
    "bug_location": "components/file_system/src/rate_limiter.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Rate limiter test cases failing due to potential timing or calculation inconsistencies in rate limiting assertions",
    "issue_number": 12024,
    "title": " The two testcase test_rate_limited_light_flow,test_rate_limited_hybrid_flow failed by running `make test`"
  },
  {
    "bug_location": "Monitoring/Metrics",
    "severity": 3,
    "categories": [
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Potential issue with region size metric calculation or reporting mechanism in Grafana dashboard",
    "issue_number": 12025,
    "title": "Approximate Region size in grafana didn't correspond to the actual value"
  },
  {
    "bug_location": "raftstore/peer_storage",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Race condition in async log fetching where a peer attempts to generate a snapshot while in an applying snapshot state, caused by insufficient leadership and term change validation during asynchronous ",
    "issue_number": 12026,
    "title": "tikv panic:918 unexpected state: Applying(0)"
  },
  {
    "bug_location": "RocksDB/Storage",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "Memory not being properly released after large query execution, potentially due to inefficient memory management in RocksDB or connection buffer handling",
    "issue_number": 1203,
    "title": "TiKV memory usage"
  },
  {
    "bug_location": "Load Balancer / Region Scheduler",
    "severity": 3,
    "categories": [
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Region split and scheduling mechanism not converging quickly enough when strict-picking-store is set to false, potentially due to QPS threshold not being met in test environment",
    "issue_number": 12032,
    "title": "Store read query is not converging after 30m when set strict-picking-store to false"
  },
  {
    "bug_location": "RaftEngine/Storage",
    "severity": 4,
    "categories": [
      "Storage",
      "Upgrade",
      "CodeBug"
    ],
    "root_cause": "Incompatible data migration logic between RocksDB and Raft Engine during version transitions, causing data inconsistency and potential data loss",
    "issue_number": 12045,
    "title": "RocksDB and Raft Engine data exist simultaneously due to operational errors"
  },
  {
    "bug_location": "RaftStore/read_queue",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication",
      "Network"
    ],
    "root_cause": "Race condition during leader transfer causing out-of-bounds access in read index queue when network packets are delayed and leadership changes",
    "issue_number": 12046,
    "title": "RaftStore panicked with out of bounds access error"
  },
  {
    "bug_location": "raftstore/store/fsm/peer.rs",
    "severity": 4,
    "categories": [
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Incorrect region merge validation logic causing panic when comparing peer epochs during region merge",
    "issue_number": 12048,
    "title": "RaftStore panicked when merging region"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 12064,
    "title": "truncating infinit cause panic"
  },
  {
    "bug_location": "raftstore/apply",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Unexpected state during region merge operation in the Raft apply process, causing a panic in the region merge execution logic",
    "issue_number": 12073,
    "title": "raft_store: unexpected state of merging region region "
  },
  {
    "bug_location": "coprocessor/tracker.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance",
      "Monitoring"
    ],
    "root_cause": "Logging mechanism for slow coprocessor tasks does not capture requests exceeding 1-minute deadline",
    "issue_number": 12088,
    "title": "tracker.rs doesn't log the slow request if it exceeds the deadline "
  },
  {
    "bug_location": "Advance/Leader Check Component",
    "severity": 2,
    "categories": [
      "Replication",
      "Network"
    ],
    "root_cause": "TiFlash does not implement the check_leader RPC, causing repeated error logs during leader verification",
    "issue_number": 12092,
    "title": "v5.3.1 version, run tpch workload, print lots of [advance.rs:273] [\"check leader failed\"] "
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "Potential memory leak or inefficient memory management during long-running OLTP insert workloads, causing out-of-memory (OOM) condition after approximately 4 hours of continuous operation",
    "issue_number": 12107,
    "title": "Tikv oom after continue run oltp insert for about  4 hours"
  },
  {
    "bug_location": "Backup and Restore (BR) Component",
    "severity": 3,
    "categories": [
      "Upgrade",
      "Config"
    ],
    "root_cause": "Version compatibility check preventing restore operation when TiKV and BR versions have major version mismatches",
    "issue_number": 12112,
    "title": "running BR in incompatible version of cluster"
  },
  {
    "bug_location": "Transaction",
    "severity": 3,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Assertion check incorrectly triggers false-positive error when a pessimistic transaction is rolled back and another transaction writes a new version of the index key",
    "issue_number": 12113,
    "title": "Assertion false-positive on non-unique index key in pessimistic transaction that's rolled back by another transaction"
  },
  {
    "bug_location": "raft-engine",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential issue with entry tracking mechanism causing continuous entry count increase during workload",
    "issue_number": 12158,
    "title": "Entry count of raft-engine seems not right"
  },
  {
    "bug_location": "Memory Management / Replication Component",
    "severity": 5,
    "categories": [
      "Memory",
      "Replication",
      "Performance"
    ],
    "root_cause": "Potential memory leak or inefficient memory management during extended node failure scenario with high replication factor",
    "issue_number": 12159,
    "title": "TiKV OOM under TPCC workload when two tikv down for 10 minutes "
  },
  {
    "bug_location": "raftstore metrics",
    "severity": 4,
    "categories": [
      "Performance",
      "Memory",
      "CodeBug"
    ],
    "root_cause": "Integer overflow during high-load sysbench insert operations causing metrics tracking to fail",
    "issue_number": 12160,
    "title": "raftstore: Integer overflow problems on memory metrics"
  },
  {
    "bug_location": "Raft Log Replication",
    "severity": 4,
    "categories": [
      "Replication",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Potential issue with log synchronization and recovery mechanism after a TiKV node is temporarily down for an extended period",
    "issue_number": 12161,
    "title": "raft log lag is more and more when down one tikv for 38min"
  },
  {
    "bug_location": "Encryption/RaftEngine",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Security",
      "Storage"
    ],
    "root_cause": "Invalid nonce/counter length for encryption initialization, causing failure to create raft engine when encryption is enabled",
    "issue_number": 12162,
    "title": "Failed to create raft engine when encryption enabled."
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Upgrade"
    ],
    "root_cause": "Potential race condition during leader election in hibernating regions during cluster rolling upgrade, causing unexpected leader banishment",
    "issue_number": 12166,
    "title": "region leaders can be banished in an election timeout when rolling upgrade a cluster"
  },
  {
    "bug_location": "Profiling/Monitoring Component",
    "severity": 3,
    "categories": [
      "Monitoring",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect handling of heap profiling configuration when prof_active is initially set to false, preventing proper heap profile generation",
    "issue_number": 12180,
    "title": "Heap profiling not works when prof_active is set to false before starting"
  },
  {
    "bug_location": "gRPC",
    "severity": 5,
    "categories": [
      "Network",
      "CodeBug",
      "Security"
    ],
    "root_cause": "SSL handshake failure due to bad elliptic curve point parsing, leading to segmentation fault",
    "issue_number": 12198,
    "title": "Handshake failed with fatal error SSL_ERROR_SSL"
  },
  {
    "bug_location": "Timer/Future Handling",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Memory"
    ],
    "root_cause": "Potential race condition or illegal memory access causing a future to be polled after it has already returned Poll::Ready",
    "issue_number": 12202,
    "title": "Map must not be polled after it returned `Poll::Ready`"
  },
  {
    "bug_location": "Quota Limiter",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect implementation of CPU time limiting method that unnecessarily reduces configured foreground CPU time",
    "issue_number": 12218,
    "title": "Quota limiter: correct the mehod to limit cpu"
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 5,
    "categories": [
      "Memory",
      "Storage",
      "Performance"
    ],
    "root_cause": "Memory leak or excessive memory consumption during long-running sysbench workload, potentially related to Raft engine configuration changes",
    "issue_number": 12229,
    "title": "after running sysbench read_write for a long time\uff083 days\uff09, one tikv oom"
  },
  {
    "bug_location": "raftstore/read_queue.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Assertion failure in advance_leader_reads method due to mismatched UUID, likely caused by race condition or inconsistent state tracking during leader reads in Raft consensus protocol",
    "issue_number": 12230,
    "title": "raftstore: unexpected panic when advance_leader_reads"
  },
  {
    "bug_location": "resolved_ts/endpoint.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Recovery"
    ],
    "root_cause": "Unwrapping a closed channel during server shutdown, causing unexpected panic in resolved timestamp worker",
    "issue_number": 12231,
    "title": "resolved_ts: unexpected panic when build_scan_task"
  },
  {
    "bug_location": "raftstore/store/fsm/store.rs",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of region merge state during rollback, causing peer ID inconsistency and potential unintended peer destruction",
    "issue_number": 12232,
    "title": "tikv panic:peer id increased after region is merged, message peer id 2228, local peer id 1140, region id: 649"
  },
  {
    "bug_location": "resource_metering",
    "severity": 5,
    "categories": [
      "Upgrade",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential issue in resource metering records handling during upload process, causing assertion failure when records collection is unexpectedly empty",
    "issue_number": 12234,
    "title": "after upgrade from v4.0.0  to v6.0.0, tikv report \" [FATAL] [lib.rs:468] [\"assertion failed: self.others.is_empty()\"] [backtrace=\"   0: tikv_util::set_panic_hook::{{closure}}\""
  },
  {
    "bug_location": "TiKV Configuration Validation",
    "severity": 3,
    "categories": [
      "Upgrade",
      "Config",
      "CodeBug"
    ],
    "root_cause": "Configuration validation in v6.0.0 fails when upgrading from v3.0.11 due to scheduler-worker-pool-size exceeding cgroup CPU quota limitations",
    "issue_number": 12238,
    "title": "v3.0.11 upgrade to v6.0.0 fail with \"critical config check failed: raft engine dir have been changed, former is ''"
  },
  {
    "bug_location": "raftstore/peer",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Inconsistent hash computation during region consistency check when using compaction filter for garbage collection, likely due to file deletion affecting snapshot integrity",
    "issue_number": 12253,
    "title": "tikv panic\uff1a[region 12786953] 12786955 hash at 226 not correct, want \\\"\\\\200\\\\207\\\\326{\\\", got \\\"5\\\\305'R\\\"!!!\""
  },
  {
    "bug_location": "RaftStore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect mathematical logic in slow score calculation, causing potential inaccurate performance scoring due to two specific code issues: incorrect comparison and potential duration double-counting",
    "issue_number": 12254,
    "title": "slow score calculation is not accurate"
  },
  {
    "bug_location": "Raft Engine",
    "severity": 4,
    "categories": [
      "Memory",
      "Network",
      "Performance"
    ],
    "root_cause": "During network disconnection, log entries cannot be garbage collected by the leader, causing indefinite accumulation of in-memory Raft Engine index and leading to memory growth",
    "issue_number": 12255,
    "title": "two tikv oom after inject tikv network-loss and recovery for some time"
  },
  {
    "bug_location": "Raftstore",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Replication"
    ],
    "root_cause": "Excessive resource consumption during node recovery, causing log application bottleneck and OOM guard triggering write request rejections",
    "issue_number": 12259,
    "title": "tikv stability: QPS fell to zero for a few minutes after fault recover from minority tikv network-loss due to full of \"Async Apply CPU\""
  },
  {
    "bug_location": "engine_rocks/raw_util.rs",
    "severity": 5,
    "categories": [
      "Upgrade",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "TiKV cannot find the original encryption key after directory renaming during version upgrade from 5.3.1/5.4.0 to 6.0.0",
    "issue_number": 12269,
    "title": "when upgrade cluster to v6.0.0, tikv throws an error."
  },
  {
    "bug_location": "Documentation",
    "severity": 2,
    "categories": [
      "Config",
      "Human"
    ],
    "root_cause": "Incomplete documentation for dynamic configuration settings, missing important notes about configuration limitations in specific deployment scenarios",
    "issue_number": 12302,
    "title": "6.0.0 document issue for config online modify"
  },
  {
    "bug_location": "TiKV Storage/Coprocessor",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Performance regression in table full scan operations, likely related to changes in caching mechanism or query execution strategy between v5.4.0 and v6.0.0",
    "issue_number": 12306,
    "title": "Compared with v5.4.0, the performance of TPCH drop 4.5% in v6.0.0 with the duration of TableFullScan increases and cache_hit_count decreases"
  },
  {
    "bug_location": "Region Scheduling/Recovery",
    "severity": 4,
    "categories": [
      "Replication",
      "Recovery",
      "Transaction"
    ],
    "root_cause": "During disaster recovery, a region scheduling operation was interrupted, potentially leaving a region unable to elect a leader after switchover",
    "issue_number": 12307,
    "title": "dr-autosync: After switch to backup cluster in sync mode, scan table hit 9005: Region is unavailable "
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Disabling flow control with aggressive RocksDB write buffer configurations causes write stalls and transaction performance degradation",
    "issue_number": 12316,
    "title": "set storage.flow-control.enable: false\uff0cthe transaction OPS drops to 0. after the load is stopped, connect to tidb cluster executes SQL statements without response"
  },
  {
    "bug_location": "PD Client",
    "severity": 4,
    "categories": [
      "Replication",
      "Transaction",
      "Recovery"
    ],
    "root_cause": "Inconsistent store ID mapping during disaster recovery switchover, causing PD client connection failures",
    "issue_number": 12320,
    "title": "dr-autosync: dr tikvs hung  if primary tikvs startup again after do disaster recovery"
  },
  {
    "bug_location": "external_storage/s3_backend",
    "severity": 2,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Incorrect handling of small file parts during S3 multi-part upload, causing upload failures for files read in small chunks",
    "issue_number": 12325,
    "title": "external_storage: the `s3` backend cannot update data from some not-in-memory `Read` types"
  },
  {
    "bug_location": "tikv-ctl tool",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect string matching or parsing logic for SST file names in bad-ssts verification tool",
    "issue_number": 12329,
    "title": "tikv-ctl: error string match in `bad-ssts` tool"
  },
  {
    "bug_location": "Transaction",
    "severity": 3,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential transaction lock conflict during full table scan with follower read, causing key locking error",
    "issue_number": 12341,
    "title": "full scan report other error when open follower read"
  },
  {
    "bug_location": "PD Client",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Transaction"
    ],
    "root_cause": "PD client incorrectly handles and retries invalid gRPC requests, causing potential region heartbeat failures and stale region information",
    "issue_number": 12345,
    "title": "PD client should handle errors properly"
  },
  {
    "bug_location": "raftstore/store/fsm/store.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Potential race condition or data inconsistency during peer creation in Raft store, causing inability to find expected entry for a key",
    "issue_number": 12348,
    "title": "raftstore: panicked with `no entry found for key`"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 12353,
    "title": "Many failed query OPM after tikv fault recover and cause QPS falls zero"
  },
  {
    "bug_location": "Region Management",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Statistical error in region size calculation for large number of partitioned tables, potentially related to region splitting and sizing algorithms",
    "issue_number": 12365,
    "title": "Single table with 7000 partitioned tables\uff0cApproximate Region size exception"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Replication"
    ],
    "root_cause": "Hash verification failure during network fault injection and table splitting, potentially caused by race conditions or inconsistent state synchronization",
    "issue_number": 12366,
    "title": "tikv panic when inject network fault repeatly and  split tables"
  },
  {
    "bug_location": "RocksDB Engine/Compaction Layer",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Inefficient SST ingestion strategy causing excessive L0 compactions and potential disk space exhaustion when cleaning up destroyed ranges",
    "issue_number": 12367,
    "title": "too many compactions and even write stalls occur by cleaning range by ingesting SSTs"
  },
  {
    "bug_location": "raftstore/apply.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Race condition during region split and peer replacement, causing unexpected tombstone region state handling",
    "issue_number": 12368,
    "title": "raftstore: panicked due to trying to apply a tombstone region already existed in kv engine"
  },
  {
    "bug_location": "server.rs",
    "severity": 2,
    "categories": [
      "Config",
      "Security"
    ],
    "root_cause": "Permission conflict when creating lock files across different user contexts, preventing TiKV server startup due to insufficient file access rights",
    "issue_number": 12399,
    "title": "File locks for working around port conflict are sometimes inaccessible "
  },
  {
    "bug_location": "TiKV Server",
    "severity": 2,
    "categories": [
      "Network",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Connection interruption during TiKV reload process causing temporary unreachable state",
    "issue_number": 12408,
    "title": "The tikv component triggers the alarm type = \"unreachable\" when reloading"
  },
  {
    "bug_location": "Memory Management",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance"
    ],
    "root_cause": "Potential memory leak or inefficient memory release mechanism causing continuous short-term memory usage increase",
    "issue_number": 12416,
    "title": "TiKV Short-term memory usage continues to rise"
  },
  {
    "bug_location": "RocksDB",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Performance bottleneck during region merge caused by excessive tombstone seeks in RocksDB's clean_stale_ranges operation",
    "issue_number": 12421,
    "title": "Take more than 5 hours to merge empty region after drop 6T data"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Rapid insertion causing flow control issues with dynamic region management, leading to potential region hotspot scheduling problems",
    "issue_number": 12423,
    "title": "\u3010Dynamic Regions\u3011QPS dropped to zero while sysbench load data"
  },
  {
    "bug_location": "Region Scheduler",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance",
      "Storage"
    ],
    "root_cause": "Ineffective region distribution mechanism causing unbalanced capacity between TiKV nodes during high-load insert workloads",
    "issue_number": 12425,
    "title": "\u3010Dynamic Regions\u3011unbalanced capacity between tikv, and a tikv generates an insufficient capacity alarm"
  },
  {
    "bug_location": "Raft Peer Management",
    "severity": 3,
    "categories": [
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Race condition in learner peer addition and pending peer state management, causing persistent pending peer state during region configuration changes",
    "issue_number": 12431,
    "title": "Pending peer may exist for a long time when adding a learner peer"
  },
  {
    "bug_location": "tikv-ctl",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incomplete manifest dump implementation in TiKV's ldb subcommand, causing truncated or partial output compared to original RocksDB ldb tool",
    "issue_number": 12438,
    "title": "tikv-ctl ldb subcommand can't print right info"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 12453,
    "title": "TiKV automatic restart, non-oom"
  },
  {
    "bug_location": "RaftLogEngine",
    "severity": 5,
    "categories": [
      "Storage",
      "CodeBug",
      "Recovery"
    ],
    "root_cause": "Potential data corruption or codec error during Raft log engine initialization, preventing TiKV server startup after configuration update",
    "issue_number": 12463,
    "title": "tikv can't start when running go-ycsb rawkv workload"
  },
  {
    "bug_location": "raftstore/split_check",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Assertion failure during region split check after dropping large tables, likely due to unexpected region state or key count mismatch",
    "issue_number": 12467,
    "title": "[Dynamic Regions] tikv panic after drop big table"
  },
  {
    "bug_location": "RaftEngine",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Potential synchronization or write failure during snapshot application, causing inconsistent Raft log state",
    "issue_number": 12470,
    "title": "Raft Engine is in inconsistent state after snapshot apply is interrupted"
  },
  {
    "bug_location": "RaftStore/PD Interaction",
    "severity": 5,
    "categories": [
      "Replication",
      "Network",
      "Transaction"
    ],
    "root_cause": "PD loses region leader information during cluster restart, causing store ID resolution failures when attempting to query leader information",
    "issue_number": 12478,
    "title": "v5.4.1: after restart all tikv/pd/tidb, one tikv continue report error \"invalid store ID 0, not found\""
  },
  {
    "bug_location": "tikv_util/sys/cgroup.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Memory"
    ],
    "root_cause": "Unexpected `None` value when attempting to retrieve CPU cores quota from cgroup, causing an unwrap() panic during TiKV server initialization",
    "issue_number": 12479,
    "title": "`cpu_cores_quota` cause TiKV Panic"
  },
  {
    "bug_location": "Transaction",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Lack of constraint validation before assertion, allowing potential transaction inconsistency in secondary index key handling",
    "issue_number": 12487,
    "title": "Do constraint check before returning an assertion error"
  },
  {
    "bug_location": "causal_ts/tso.rs",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Race condition in timestamp batch renewal causing potential timestamp fallback when multiple threads concurrently acquire and update TSO batches",
    "issue_number": 12489,
    "title": "Causal timestamp fall back"
  },
  {
    "bug_location": "RawKV API V2",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Timestamp causality violation in RawKV API V2 causing inconsistent key-value ordering and potential data integrity issues",
    "issue_number": 12498,
    "title": "RawKV API V2 timestamp causality violation"
  },
  {
    "bug_location": "PD Client",
    "severity": 3,
    "categories": [
      "Network",
      "Config",
      "Replication"
    ],
    "root_cause": "Unexpected reconnection behavior when handling StoreTombstone event during store removal via pd-ctl",
    "issue_number": 12506,
    "title": "PD client keeps reconnecting on error StoreTombstone"
  },
  {
    "bug_location": "backup-stream/endpoint",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Recovery"
    ],
    "root_cause": "Inconsistent state management during log backup task stop, causing region registration mismatch and triggering a panic when attempting to observe a region with no active task",
    "issue_number": 12507,
    "title": "TiKV panic after stop PiTR log backup task"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Checkpoint mechanism failure preventing log backup progress tracking",
    "issue_number": 12508,
    "title": "Checkpoint doesn't move forward during log backup"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 5,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "Memory exhaustion during log backup scanning when processing large amounts of historical log data",
    "issue_number": 12509,
    "title": "TiKV OOM after starting log backup when there are lots of log to be scanned"
  },
  {
    "bug_location": "Raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Region splitting mechanism fails to trigger after configuration change and restart, preventing proper data distribution and size management",
    "issue_number": 12510,
    "title": "Raftstore: tikv does not split regions whose  size is larger than the max region size after restart with config change"
  },
  {
    "bug_location": "log-backup",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Temporary log backup folders are not being automatically cleaned up after task completion or termination",
    "issue_number": 12512,
    "title": "Clean log-backup.temp-path after a log backup task is stopped"
  },
  {
    "bug_location": "backup-stream/event_loader",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Recovery"
    ],
    "root_cause": "Channel of paired_future_callback was unexpectedly canceled during log backup task, causing a panic in the event loader",
    "issue_number": 12513,
    "title": "TiKV panic when running PiTR log backup"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Replication",
      "Storage"
    ],
    "root_cause": "Checkpoint synchronization delay between TiKV stores causing inconsistent log backup progress",
    "issue_number": 12514,
    "title": "Log backup checkpoint gap > 5 mins  during log backup"
  },
  {
    "bug_location": "Log Backup Service",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Recovery"
    ],
    "root_cause": "TiKV OOM preventing periodic GC service point updates, causing checkpoint timestamp to be unexpectedly GCed",
    "issue_number": 12515,
    "title": "log backup checkpoint_ts get smaller than gc_safepoint"
  },
  {
    "bug_location": "PD (Placement Driver) Configuration",
    "severity": 3,
    "categories": [
      "Config",
      "CodeBug"
    ],
    "root_cause": "Inconsistent region size configuration between TiKV config file and PD config display, potentially causing configuration synchronization issues",
    "issue_number": 12518,
    "title": "[Dynamic Region] region size config is not inconsistent between tikv config file and pd config show "
  },
  {
    "bug_location": "RawKV Compaction Filter",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Likely an issue with metrics tracking or initialization of the total counter in the RawKV compaction filter reporting mechanism",
    "issue_number": 12522,
    "title": "RawKV Compaction filter report total is aways 0"
  },
  {
    "bug_location": "Performance Profiling Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Memory",
      "CodeBug"
    ],
    "root_cause": "Potential memory access issue in signal handler during performance profiling, likely related to unhandled edge case in perf_signal_handler",
    "issue_number": 12525,
    "title": "TiKV restarts due to segfault"
  },
  {
    "bug_location": "RawKV Compaction Filter",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Potential issue with total counter initialization or update mechanism in RawKV compaction filter reporting logic",
    "issue_number": 12535,
    "title": "RawKV Compaction filter report total is aways 0"
  },
  {
    "bug_location": "Backup and Restore (BR) Component",
    "severity": 5,
    "categories": [
      "Storage",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Potential data synchronization or replay issue during point-in-time recovery (PiTR) log restore process",
    "issue_number": 12538,
    "title": "[BR] Data inconsistency after br log restore"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 12541,
    "title": "unstable resouce_metering::test_dynamic_config::test_report_interval"
  },
  {
    "bug_location": "TiDB Query Expression Component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unhandled UTF-8 character conversion error during cast operation, causing panic when processing invalid UTF-8 encoded characters",
    "issue_number": 12542,
    "title": "TiKV panic when `cast` func pushdown with process invalid utf8 character"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 12543,
    "title": "space reserve fails on aufs"
  },
  {
    "bug_location": "Backup and Restore (BR) component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Incompatible key format handling during backup and restore between API v1 and API v2 clusters",
    "issue_number": 12548,
    "title": "[rawkv backup] error key format in response files when backup from v1 to v2 and range is not empty "
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Bucket keys incorrectly include timestamp information during dynamic region and region buckets configuration",
    "issue_number": 12549,
    "title": "raftstore: bucket keys should not contain TS information"
  },
  {
    "bug_location": "TLS Metrics",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Monitoring"
    ],
    "root_cause": "Thread-local metrics are not consistently flushed across different thread pools, causing potential data loss and unreliable metrics collection, especially in async/await contexts where thread executio",
    "issue_number": 12562,
    "title": "tls metrics is not reliable"
  },
  {
    "bug_location": "raftstore/store/fsm/peer",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Incompatibility between consistency check and compaction filter GC mechanism, causing hash verification failure in region 28137",
    "issue_number": 12564,
    "title": "6.1.0: Tikv panic and report [lib.rs:491] [\"[region 28137] 28138 hash at 2799 not correct"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 12571,
    "title": "Revert \"[backup apiv2] fix file range in backup response\" #12570"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Potential race condition or memory management issue during dynamic region splitting under high concurrency workload",
    "issue_number": 12574,
    "title": "[Dynamic Region] tikv panic repeatly"
  },
  {
    "bug_location": "Apply",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect handling of bucket_meta in try_batch, using the first bucket_meta instead of the latest one",
    "issue_number": 12578,
    "title": "In try_batch, we should use the latest apply's bucket_meta."
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Recovery"
    ],
    "root_cause": "Incorrect GC safepoint calculation during log backup fatal errors, potentially causing data loss by allowing premature log deletion",
    "issue_number": 12579,
    "title": "[BR] gc safepoint not set correctly when there is log backup fatal error "
  },
  {
    "bug_location": "Backup and Recovery (BR) Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Monitoring"
    ],
    "root_cause": "Excessive log generation during PD (Placement Driver) error scenario during log backup task",
    "issue_number": 12580,
    "title": "[BR] huge logs when PD error during log backup"
  },
  {
    "bug_location": "Backup and Recovery (BR) component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Log backup checkpoint fails to progress when encountering unresolved transaction locks, preventing proper backup checkpoint advancement",
    "issue_number": 12582,
    "title": "[BR] log backup checkpoint_ts doesn't move forward when there is residual tidb lock"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 12583,
    "title": "After br log pausing, the global checkpoint is still moving forward"
  },
  {
    "bug_location": "GC Worker / Storage API",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Incompatibility between TiDB's lock resolution mechanism and TiKV's API V2 storage mode, causing invalid key range handling during garbage collection",
    "issue_number": 12595,
    "title": "when enable api v2 , TiDB resolve lock meet error"
  },
  {
    "bug_location": "Region Split/Coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Region splitting configuration may not be triggering splits for large regions as expected, potentially due to interaction between batch-split-limit, region-max-size, and region-split-size parameters",
    "issue_number": 12597,
    "title": "[Dynamic Region] Some large regions did not split as expected"
  },
  {
    "bug_location": "Region Split/Bucket Management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Incorrect calculation of region_split_check_diff when region buckets are enabled, causing split ticks to be skipped for large regions",
    "issue_number": 12598,
    "title": "default size of region_split_check_diff is too large when big region is enabled"
  },
  {
    "bug_location": "Storage Configuration",
    "severity": 3,
    "categories": [
      "Config",
      "Storage",
      "Upgrade"
    ],
    "root_cause": "Configuration validation prevents changing storage API version and enabling TTL after initial setup",
    "issue_number": 12600,
    "title": "Fail to startup when modify storage.api-version to 2 from 1"
  },
  {
    "bug_location": "Transaction",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Lack of idempotency in pessimistic prewrite handling when commit timestamp exceeds maximum allowed timestamp, causing potential transaction inconsistency during retry scenarios",
    "issue_number": 12615,
    "title": "Pessimistic prewrite may be not idempotent on CommitTsTooLarge"
  },
  {
    "bug_location": "components/raftstore/src/store/peer_storage.rs",
    "severity": 4,
    "categories": [
      "Storage",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Inconsistent state between apply state and raft state during snapshot application, potentially causing incorrect log recovery when a node crashes during snapshot process",
    "issue_number": 12618,
    "title": "checks for snapshot last index is wrong"
  },
  {
    "bug_location": "lock_manager",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential deadlock detection logic error causing panic during concurrent transaction lock management",
    "issue_number": 12619,
    "title": "test_detect_deadlock_basic panics"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Delayed state transition of removed peer to Tombstone status, allowing potential replica recreation during network partition",
    "issue_number": 1262,
    "title": "Mark tombstone immediately when applying a confchagne"
  },
  {
    "bug_location": "QuotaLimiter",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Invalid configuration causing panic when converting float seconds to Duration, specifically when the value is too large or NaN",
    "issue_number": 12622,
    "title": "all tikv panic when update config online "
  },
  {
    "bug_location": "Region Merge Handling",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Unsafe handling of uncommitted merge regions during force leader state transitions, potentially causing data loss during region merges across distributed stores",
    "issue_number": 12627,
    "title": " Handle unapplied commit merges in unavailable regions during unsafe recovery."
  },
  {
    "bug_location": "TiKV Coprocessor",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Dynamic region bucket generation and reporting mechanism appears to have inconsistent behavior when recreating databases and tables",
    "issue_number": 12642,
    "title": "DynamicRegion: Buckets are not working as expected."
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication",
      "Performance"
    ],
    "root_cause": "Potential race condition or state synchronization issue during cluster restart that causes incorrect checkpoint timestamp tracking",
    "issue_number": 12643,
    "title": "[br] checkpoint_ts metrics abnormal, log backup start-ts is display sometimes."
  },
  {
    "bug_location": "Online Recovery Component",
    "severity": 4,
    "categories": [
      "Recovery",
      "Performance"
    ],
    "root_cause": "Timeout during unsafe recovery process when attempting to force leader election across multiple TiKV stores",
    "issue_number": 12644,
    "title": "[Online Recovery] Unsafe online recovery failed with info \u201cunsafe recovery enters exit force leader stage exceeds timeout\u201d"
  },
  {
    "bug_location": "raftstore/apply",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of minimum match index during region merge, causing a panic when first index is greater than minimum index",
    "issue_number": 12663,
    "title": "tikv panic: 12475 first index 56 > min_index 55, skip pre merge"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Inconsistent region metadata in StoreFsm, causing a panic when trying to create a peer with missing region entry",
    "issue_number": 12664,
    "title": "raftstore: tikv panicked with `no entry found for key`"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 12670,
    "title": "tikv returns duplicated metrics result."
  },
  {
    "bug_location": "TiKV SQL Execution Engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Potential error handling issue in CONV function implementation when processing empty or invalid string inputs",
    "issue_number": 12673,
    "title": "tikv crash when conv empty string"
  },
  {
    "bug_location": "RawKV API, Timestamp Management",
    "severity": 5,
    "categories": [
      "Transaction",
      "Replication",
      "Storage"
    ],
    "root_cause": "Timestamp causality violation during region merge, causing inconsistent key-value state with out-of-order timestamps",
    "issue_number": 12680,
    "title": "RawKV API V2 timestamp causality violation"
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Upgrade",
      "CodeBug"
    ],
    "root_cause": "Potential data file corruption or missing SST file during RocksDB background compaction process during version upgrade",
    "issue_number": 12698,
    "title": "v5.2.4 upgrade to v6.1.0, tikv report panic for \" [FATAL] [lib.rs:491] [\"rocksdb background error. db: raft, reason: compaction, error: IO ...\""
  },
  {
    "bug_location": "Build System/Cargo",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Incompatible tonic library import or version mismatch causing compilation failure in test for subcrate with security feature",
    "issue_number": 12711,
    "title": "*: `cargo test` cannot compile "
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 12717,
    "title": "cdc data lost when transaction conflicts during incremental scan"
  },
  {
    "bug_location": "MVCC/RocksDB Compaction Layer",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "MVCC versions are not being efficiently purged during read-heavy workloads, causing excessive iterator overhead and performance degradation",
    "issue_number": 12729,
    "title": "the invisible mvcc versions are not purged by gc"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 12739,
    "title": "Wrong check in time parsing"
  },
  {
    "bug_location": "BR (Backup & Restore)",
    "severity": 4,
    "categories": [
      "Storage",
      "Recovery"
    ],
    "root_cause": "Unable to download and apply log backup file from S3 storage, possibly due to file access permissions, network issues, or incorrect file path",
    "issue_number": 12750,
    "title": "No such file or directory when download and apply log backup file"
  },
  {
    "bug_location": "Backup & Restore (BR) component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Recovery"
    ],
    "root_cause": "Potential decryption mismatch between backup logs and KV files during restore point operation, causing data integrity issues leading to TiKV pod restart",
    "issue_number": 12751,
    "title": "TiKV panic when execute br restore point"
  },
  {
    "bug_location": "Coprocessor/Region Split Check",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config",
      "Performance"
    ],
    "root_cause": "Online config updating of region-split-size does not automatically trigger region split check",
    "issue_number": 12768,
    "title": "[Dynamic Regions] Some large regions did not split due to split check did not run after update region split config online"
  },
  {
    "bug_location": "log_backup_checkpoint",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Replication"
    ],
    "root_cause": "Log backup resolver timestamp is not advancing correctly, causing checkpoint to stall",
    "issue_number": 12782,
    "title": "br log backup checkpoint doesn't move forward"
  },
  {
    "bug_location": "Monitoring/Metrics",
    "severity": 3,
    "categories": [
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Potential inaccurate IO utilization metric collection mechanism for disk monitoring, specifically on EC2 instances with multiple NVMe drives",
    "issue_number": 12790,
    "title": "TiKV IOUTIL not correct on EC2(4c instance)"
  },
  {
    "bug_location": "TSO (Timestamp Oracle) Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Network"
    ],
    "root_cause": "PD Leader failure causing extended TSO service recovery time, leading to workload disruption and long request latencies",
    "issue_number": 12794,
    "title": "Workload Error when killing pd Leader in HA test"
  },
  {
    "bug_location": "log-backup",
    "severity": 5,
    "categories": [
      "Storage",
      "Recovery",
      "Performance"
    ],
    "root_cause": "Premature GC of checkpoint safe point when log-backup task encounters an error, causing potential data loss recovery window",
    "issue_number": 12802,
    "title": "log-backup: The checkpoint-ts do not keep 24h when the task status become error."
  },
  {
    "bug_location": "Memory Management",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Recovery"
    ],
    "root_cause": "Potential memory leak during chaos testing and node restart scenarios, possibly exacerbated by cloud configuration",
    "issue_number": 12807,
    "title": "TiKV oom due to memory leak"
  },
  {
    "bug_location": "raftstore/store/fsm/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Race condition during peer destruction and region split where an uninitialized peer's log GC task is processed incorrectly, causing a panic when attempting to recover a delayed destroy operation",
    "issue_number": 12825,
    "title": "raftstore: tikv panicked due to trying to recover a delayed destroy"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Potential issue with 'CaptureChange' callback being dropped during TiKV server restarts, preventing log backup checkpoint from moving forward",
    "issue_number": 12854,
    "title": "br log backup checkpoint lag > 1H after stop restarting tikv for 1H"
  },
  {
    "bug_location": "TiKV Status Server",
    "severity": 3,
    "categories": [
      "Config",
      "Network"
    ],
    "root_cause": "TLS certificate validation fails when using IP addresses instead of localhost for status address configuration",
    "issue_number": 12867,
    "title": "tikv should support use hostname instead of ip for status_address"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Network"
    ],
    "root_cause": "Excessive retry mechanism for S3 file flush causing high request volume and potential service disruption",
    "issue_number": 12869,
    "title": "Br Log backup flush file retry results in too many s3 request"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Log backup generates excessive small files in single directory, causing filesystem limitations and performance degradation",
    "issue_number": 12885,
    "title": "br log backup too many small files and too many files in single folder"
  },
  {
    "bug_location": "Compaction",
    "severity": 3,
    "categories": [
      "Performance",
      "Memory",
      "Storage"
    ],
    "root_cause": "High thread count during cluster compaction potentially causing excessive memory allocation and out-of-memory condition",
    "issue_number": 12887,
    "title": "compact-cluster using high threads  may cause oom"
  },
  {
    "bug_location": "Encryption Key Management in Raft Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Memory",
      "Security"
    ],
    "root_cause": "Lack of encryption key cleanup mechanism for WAL files when using Raft Engine, causing memory and disk space leaks",
    "issue_number": 12890,
    "title": "Encryption keys for WALs are not cleaned up when Raft Engine is enabled"
  },
  {
    "bug_location": "Backup & Restore (BR) Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "Performance degradation when running concurrent log backup and full backup operations, causing significant latency increase",
    "issue_number": 12896,
    "title": "[BR] Duration 99 triples  when log backup and full backup are in progress"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Recovery"
    ],
    "root_cause": "External storage upload failure during log backup process, likely related to AWS S3/Rusoto SDK interaction",
    "issue_number": 12902,
    "title": "PiTR log backup fails due to \"failed to put object rusoto error\""
  },
  {
    "bug_location": "Backup & Restore (BR) Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "Potential CPU resource contention during large region restoration, causing coprocessor task timeout when processing regions larger than 1GB",
    "issue_number": 12904,
    "title": "br restore timeout when region size more then 1GB\uff08tested 1GB and 10GB\uff09"
  },
  {
    "bug_location": "Distributed Recovery (DR) Auto-Sync Module",
    "severity": 4,
    "categories": [
      "Replication",
      "Network",
      "Transaction"
    ],
    "root_cause": "Timeout handling failure during cross-datacenter failover scenario when backup datacenter stores are down, preventing query execution in primary cluster",
    "issue_number": 12914,
    "title": "dr-auto-sync: After down tikv in backup cluster, query in primary cluster hit 9002: TiKV timeout"
  },
  {
    "bug_location": "Backup & Restore (BR) Component",
    "severity": 5,
    "categories": [
      "Storage",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Incorrect handling of partition table metadata during log backup and point-in-time recovery (PiTR) restore process",
    "issue_number": 12916,
    "title": "[BR] partition table created during log backup are not restored correctly"
  },
  {
    "bug_location": "Log Backup",
    "severity": 4,
    "categories": [
      "Storage",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Potential race condition or data synchronization issue during network partition and TiKV restart, causing log backup checkpoint inconsistency",
    "issue_number": 12918,
    "title": "BR log backup data lost if tikv restart followed by tikv network partition"
  },
  {
    "bug_location": "Encryption/Security Component",
    "severity": 4,
    "categories": [
      "Security",
      "Config",
      "Storage"
    ],
    "root_cause": "Incompatible encryption configuration during TiKV reload, specifically an invalid nonce/counter length for AES encryption",
    "issue_number": 12919,
    "title": "redhat8.4 OS , v6.1.0 add \"data-encryption-method\", reload tikv fail "
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Inconsistent type casting behavior between TiDB and TiKV for string to time conversion, causing potential query result mismatches during pushdown operations",
    "issue_number": 12932,
    "title": "cast string as time behaviour is incompatible with TiDB"
  },
  {
    "bug_location": "components/pd_client/src/client.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential deadlock due to nested read locks on the same RwLock, which can cause thread blocking if interleaved with write locks",
    "issue_number": 12933,
    "title": "components/pd_client: potential deadlocks caused by double-readlock in call_option"
  },
  {
    "bug_location": "pd_client/client.rs",
    "severity": 5,
    "categories": [
      "Network",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Heartbeat stream disconnection without proper reconnection mechanism when encountering gRPC errors, causing TiKV nodes to fail reporting region heartbeats",
    "issue_number": 12934,
    "title": "Master: two tikv don't report region heartbeat after inject fault to pd leader"
  },
  {
    "bug_location": "RawChecksum API",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Key encoding not properly handled for APIv2 in raw checksum operation",
    "issue_number": 12950,
    "title": "raw_checksum does not encode key for apiv2"
  },
  {
    "bug_location": "tikv_util/sys/thread.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Incompatible futures library version causing type resolution failure for ThreadPoolBuilder",
    "issue_number": 12963,
    "title": "Can't compile and run individual tests"
  },
  {
    "bug_location": "Network/Cluster",
    "severity": 4,
    "categories": [
      "Network",
      "Replication",
      "LoadBalance"
    ],
    "root_cause": "Insufficient network partition tolerance in multi-AZ deployment causing service unavailability when one availability zone is isolated",
    "issue_number": 12966,
    "title": "one az is isolated from other az networks, and the service is unavailable for 4 minutes"
  },
  {
    "bug_location": "Raft Log Replication",
    "severity": 3,
    "categories": [
      "Replication",
      "Performance",
      "Recovery"
    ],
    "root_cause": "Potential performance bottleneck in log catchup mechanism during node recovery after extended downtime",
    "issue_number": 12979,
    "title": "In version 6.1.0 , a tikv failure was restored after 13 minutes, and the raft log lag has not been able to catch up"
  },
  {
    "bug_location": "Memory Management",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Recovery"
    ],
    "root_cause": "Potential memory leak or inefficient memory management during network recovery and high-load TPCC workload",
    "issue_number": 12983,
    "title": "one tikv oom after 40 minutes of recovery from network loss"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Monitoring"
    ],
    "root_cause": "Potential synchronization or delay issue in checkpoint information retrieval for log backup status display",
    "issue_number": 12990,
    "title": "[PiTR] br log status doesn't show checkpoint info for few minutes after starting a log backup task "
  },
  {
    "bug_location": "TiKV Server",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Config"
    ],
    "root_cause": "Systemd memory management mechanism differences in RedHat 8.5 causing repeated out-of-memory (OOM) kills of TiKV processes",
    "issue_number": 12997,
    "title": "RedHat 8.5:  Tikv oom repeatedly after deploy cluster via tiup "
  },
  {
    "bug_location": "TiKV Server Configuration/Engine",
    "severity": 5,
    "categories": [
      "Upgrade",
      "Config",
      "CodeBug"
    ],
    "root_cause": "Configuration option 'enable_pipelined_commit' not found during TiKV startup, preventing engine initialization",
    "issue_number": 13007,
    "title": "tikv restart failed\uff08upgrade new image and CrashLoopBackOff\uff09"
  },
  {
    "bug_location": "server/engine_rocks",
    "severity": 5,
    "categories": [
      "Upgrade",
      "Config",
      "Storage"
    ],
    "root_cause": "Configuration option mismatch during TiKV upgrade, specifically missing 'enable_multi_thread_write' option which prevents successful restart",
    "issue_number": 13015,
    "title": "after upgrade tikv restart failed (option not match)"
  },
  {
    "bug_location": "RocksDB Block Cache",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Backup process incorrectly populating RocksDB block cache during raw key-value backup, potentially causing unnecessary cache evictions for active user read workloads",
    "issue_number": 13020,
    "title": "Should not fill cache when backup rawkv"
  },
  {
    "bug_location": "components/api_version/src/api_v2.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect range validation logic for reverse scan that does not account for key order swap in reverse scanning",
    "issue_number": 13021,
    "title": "API v2 range check will fail in reverse scan"
  },
  {
    "bug_location": "Transaction/Scheduler",
    "severity": 2,
    "categories": [
      "Transaction",
      "Performance",
      "Monitoring"
    ],
    "root_cause": "Metrics tracking for in-memory pessimistic lock key count is not properly implemented, causing zero values in scheduler key write metrics",
    "issue_number": 13026,
    "title": "in-mem pessimistic lock does not record the scheduled key count"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 3,
    "categories": [
      "Performance",
      "Replication",
      "Storage"
    ],
    "root_cause": "Potential performance bottleneck in log backup checkpoint mechanism causing delayed synchronization of backup checkpoints",
    "issue_number": 13030,
    "title": "BR log backup checkpoint lag > 10 min sometimes"
  },
  {
    "bug_location": "Backup and Restore (BR) component",
    "severity": 4,
    "categories": [
      "Network",
      "Storage",
      "Recovery"
    ],
    "root_cause": "Potential data integrity issue during network partition causing checksum mismatch in meta files during point-in-time recovery (PiTR)",
    "issue_number": 13034,
    "title": "BR restore point fails due to \"\u201cfailed to restore meta files: checksum mismatch\" after injecting network partition bw s3 and tikv for 1 hour"
  },
  {
    "bug_location": "raftstore/apply",
    "severity": 5,
    "categories": [
      "Storage",
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Titan storage engine integration failure during TiKV restart, causing 'Not implemented' error in write operation",
    "issue_number": 13038,
    "title": "tikv cannot start when open titan"
  },
  {
    "bug_location": "Store/Apply Memory Management",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "Linear memory growth with increasing number of regions, causing memory exhaustion when region count is extremely high (300k+ regions)",
    "issue_number": 13042,
    "title": "tikv oom when tikv instance has a large number of regions such as 300k+"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential checkpoint tracking or synchronization issue preventing log backup checkpoint from advancing",
    "issue_number": 13044,
    "title": "Br log backup checkpoint lag doesn't move foward"
  },
  {
    "bug_location": "SQL Execution Layer",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect type conversion and boolean evaluation in WHERE clause condition, causing unexpected row update when the condition should evaluate to false",
    "issue_number": 13045,
    "title": "Confusing behavior in the UPDATE statement"
  },
  {
    "bug_location": "raftstore/store/fsm/store.rs",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug",
      "Network"
    ],
    "root_cause": "Inefficient broadcast of StoreUnreachable messages across large number of regions, causing excessive CPU consumption and potential system stall",
    "issue_number": 13054,
    "title": "Raftstore may broadcast too many messages after one peer store become unreachable"
  },
  {
    "bug_location": "Transaction Scheduler",
    "severity": 2,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Unnecessary thread CPU time collection even when QuotaLimiter auto-tune is disabled, causing performance overhead in critical transaction processing paths",
    "issue_number": 13055,
    "title": "ThreadTime is collected accidentally even if QuotaLimiter auto-tune is disabled"
  },
  {
    "bug_location": "log-backup",
    "severity": 4,
    "categories": [
      "Storage",
      "Recovery",
      "Transaction"
    ],
    "root_cause": "Timestamp inconsistency during point-in-time recovery (PiTR) restore process, where restored timestamp exceeds log backup range",
    "issue_number": 13062,
    "title": "log-backup: PiTR restore failed due to restored_ts > log backup range"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Initial log backup scan generates high resource overhead during event generation, causing significant performance degradation in cluster operations",
    "issue_number": 13068,
    "title": "br log backup initial scan downgrade cluster performance "
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 13077,
    "title": "v6.1.0: After network isolation is recovered, QPS drops more than 50% due to high commit log duration"
  },
  {
    "bug_location": "encryption/manager",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Security"
    ],
    "root_cause": "Incorrect handling of encryption state transition when changing encryption method from AES to plaintext, causing assertion failures during file operations",
    "issue_number": 13081,
    "title": "v6.2.0-alpha: change \"data-encryption-method\" from \"aes192-ctr\" to \"plaintext\" got failed"
  },
  {
    "bug_location": "CPU Quota Management",
    "severity": 2,
    "categories": [
      "Performance",
      "Config"
    ],
    "root_cause": "Improper CPU time tracking mechanism causing overhead between foreground and background request CPU quota calculations",
    "issue_number": 13084,
    "title": "Avoid mutual contamination of cpu limitation of foreground and background cpu quota"
  },
  {
    "bug_location": "Monitoring",
    "severity": 2,
    "categories": [
      "Monitoring",
      "CodeBug"
    ],
    "root_cause": "Potential issue with metrics collection or rendering for unified read pool CPU usage chart in TiKV dashboard",
    "issue_number": 13086,
    "title": "metrics: the unified read pool cpu usage chart in tikv-details usage is always empty"
  },
  {
    "bug_location": "TiKV Memory Management",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Config"
    ],
    "root_cause": "Memory usage limit configuration not effectively controlling actual memory consumption, causing potential OOM and unexpected service restarts",
    "issue_number": 13090,
    "title": "Redhat 8.4: tikv restart by systemd after set memory-usage-limit less than vm message"
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Upgrade"
    ],
    "root_cause": "Incompatibility between RocksDB versions causing unexpected bottommost compactions that degrade performance in sysbench oltp_update_index workload",
    "issue_number": 13095,
    "title": "commit b558d0 causing 7%~14% performance degradation for sysbench oltp_update_index workload"
  },
  {
    "bug_location": "Metrics/Monitoring",
    "severity": 2,
    "categories": [
      "Monitoring",
      "Upgrade"
    ],
    "root_cause": "Potential metrics collection or display issue during version upgrade from v5.4.0 to v6.2.0, specifically affecting unified read pool CPU metrics visualization",
    "issue_number": 13096,
    "title": "upgrade to master\uff08v6.2.0\uff09\uff0cthe metrics of unified read pool cpu does not display properly"
  },
  {
    "bug_location": "Log Backup Storage Module",
    "severity": 4,
    "categories": [
      "Storage",
      "Config"
    ],
    "root_cause": "Potential incompatibility or configuration issue with Google Cloud Storage (GCS) external storage integration for log backup",
    "issue_number": 13106,
    "title": "log backup failed when use GCS as external storage"
  },
  {
    "bug_location": "Stale Read Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Replication",
      "Performance"
    ],
    "root_cause": "Safe timestamp (safe_ts) is consistently zero, preventing stale read from progressing in multi-zone configuration",
    "issue_number": 13110,
    "title": "Stale read meet `Data not ready` for a long time"
  },
  {
    "bug_location": "coprocessor/split_check/half.rs",
    "severity": 3,
    "categories": [
      "Performance",
      "LoadBalance",
      "CodeBug"
    ],
    "root_cause": "Split checker cannot sample enough keys in small tables, preventing region splitting and load balancing",
    "issue_number": 13111,
    "title": "Load base split don't work in scenario of pure little table scan"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 13116,
    "title": "proc file read failure flooding logs"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 13123,
    "title": "raft engine panic during recovery"
  },
  {
    "bug_location": "backup-restore",
    "severity": 4,
    "categories": [
      "Replication",
      "Transaction",
      "Recovery"
    ],
    "root_cause": "Region merge process causing read index not being ready, with a gap between PrepareMerge and CommitMerge commands exceeding 30 seconds, preventing log backup checkpoint from moving forward",
    "issue_number": 13126,
    "title": "br log backup checkpoint doesn't move forward due to\"read index not ready, reason can not read index due to merge, region 94812\""
  },
  {
    "bug_location": "TiKV CDC (Change Data Capture)",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Performance"
    ],
    "root_cause": "Resolved timestamp advancement mechanism appears to be stuck during RawKV replication, preventing proper change data capture progress",
    "issue_number": 13144,
    "title": "RawKV CDC does not advance in some scenario"
  },
  {
    "bug_location": "RocksDB Configuration Management",
    "severity": 3,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Current implementation requires TiKV restart to change max_subcompactions parameter, limiting runtime configuration flexibility",
    "issue_number": 13145,
    "title": "Make max_subcompactions dynamically changeable"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 13147,
    "title": "cdc endpoint CPU expression is not correct"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 13153,
    "title": "Apply-db_mutex_lock_nanos up to 5min for 10min during upgrade from 5.3 to 6.2"
  },
  {
    "bug_location": "Region Split/Load Balancing",
    "severity": 3,
    "categories": [
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Overly aggressive region splitting during low-intensity table scan operations, causing unnecessary region fragmentation",
    "issue_number": 13154,
    "title": "Load split: split too fast during do little table scan"
  },
  {
    "bug_location": "raftstore/store/fsm",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Storage"
    ],
    "root_cause": "Race condition in ApplyRes handling where mailbox full conditions can cause partial region split state inconsistency, leading to potential data integrity and region creation failures",
    "issue_number": 13160,
    "title": "raftstore: apply res may be dropped silently"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "Transaction",
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Race condition during leadership transfer that can cause inconsistent transaction execution across different Raft terms",
    "issue_number": 1317,
    "title": "Make sure different stages of a kv command execute raft command in a same term."
  },
  {
    "bug_location": "engine_test and server components",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Memory"
    ],
    "root_cause": "Potential deadlock due to nested mutex locks with potential race conditions during tablet/table operations",
    "issue_number": 13186,
    "title": "DoubleLock in components/engine_test and server"
  },
  {
    "bug_location": "pd_client",
    "severity": 4,
    "categories": [
      "Network",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Potential deadlock in RwLock synchronization during network isolation recovery, causing TiKV node to remain in an unrecoverable state",
    "issue_number": 13191,
    "title": "Inject a minority of tikv network isolation\uff0cafter network isolation was recovered, one tikv status still show down and can not transfer leader back"
  },
  {
    "bug_location": "TiKV Replication/Recovery Component",
    "severity": 4,
    "categories": [
      "Replication",
      "Recovery",
      "Transaction"
    ],
    "root_cause": "Potential issue with min resolved timestamp reset mechanism causing TiKV node disconnection during DR auto-sync mode transition",
    "issue_number": 13194,
    "title": "v6.2.0: run in dr-auto-sync mode, tikv will down after reset min resovled ts"
  },
  {
    "bug_location": "Transaction",
    "severity": 4,
    "categories": [
      "Transaction",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Inconsistent data state during disaster recovery switchover and version reset, potentially related to resolved timestamp synchronization in multi-datacenter configuration",
    "issue_number": 13203,
    "title": "v6.2.0 dr auto sync: data is not consistent after switch to backup dc and reset version to min resolved ts"
  },
  {
    "bug_location": "Transaction",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Potential performance bottleneck or deadlock in version reset process for disaster recovery auto-sync feature, causing unexpectedly long operation time",
    "issue_number": 13208,
    "title": "v6.2.0 dr auto sync: reset-to-version didn't finished after 20min with only 7G data in it"
  },
  {
    "bug_location": "Tablet Registry",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Recursive lock acquisition in tablet open/create process causing potential deadlock when attempting to access or create tablets",
    "issue_number": 13213,
    "title": "`open_tablet` may cause deadlock"
  },
  {
    "bug_location": "pd_client",
    "severity": 5,
    "categories": [
      "Network",
      "Recovery",
      "CodeBug"
    ],
    "root_cause": "Inability to handle prolonged PD (Placement Driver) connection interruption, causing TiKV nodes to panic when PD is down for an extended period",
    "issue_number": 13240,
    "title": "all tikv panic when pd down for some time"
  },
  {
    "bug_location": "Network Connection Management",
    "severity": 3,
    "categories": [
      "Network",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Ineffective handling of asynchronous socket connection states, leading to potential message buffer overflow during connection establishment or failure",
    "issue_number": 1325,
    "title": "server: drop message when socket is still connecting "
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 13251,
    "title": "PiTR log backup checkpoint doesn't move foward"
  },
  {
    "bug_location": "QuotaLimiter",
    "severity": 4,
    "categories": [
      "Performance",
      "Config"
    ],
    "root_cause": "Coprocessor read traffic not properly controlled by foreground quota limiter's read bytes configuration",
    "issue_number": 13256,
    "title": "QuotaLimiter: coprocessor read traffic should be controlled by foreground quota limiter's read bytes"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 13274,
    "title": "metric: description is incorrect in GC compaction filter panel"
  },
  {
    "bug_location": "Storage",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Expensive `Instant::now()` syscalls on AWS Xen virtualized EC2 instances due to default 'xen' clock source, causing significant performance overhead in time-sensitive operations",
    "issue_number": 13280,
    "title": "Storage pool regression on AWS EC2 instance type i3.4xlarge"
  },
  {
    "bug_location": "Point-in-Time Recovery (PiTR) Restore Component",
    "severity": 5,
    "categories": [
      "Storage",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Potential data synchronization or checkpoint tracking issue during log backup and point-in-time restore process",
    "issue_number": 13281,
    "title": "data inconsistency after br PiTR restore "
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Replication"
    ],
    "root_cause": "Checkpoint tracking mechanism fails to handle long-duration transactions efficiently, causing abnormal lag in log backup progress",
    "issue_number": 13304,
    "title": "Br log backup checkpoint lag abnormal when there is long duration transaction"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential inefficient log backup generation mechanism causing excessive log backup file size during large data preparation tasks",
    "issue_number": 13306,
    "title": "Lots of log backup generated during data preparation"
  },
  {
    "bug_location": "resolved_ts/advance.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Incorrect address selection when sending check leader message to TiFlash proxy, using store.address instead of store.peer_address",
    "issue_number": 13310,
    "title": "The address of the check leader message sent to the TiFlash proxy is wrong"
  },
  {
    "bug_location": "raftstore/store/fsm/store.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Race condition during peer replication and region split causing metadata inconsistency between region ranges and regions",
    "issue_number": 13311,
    "title": "TiKV panicked due to corrupted meta"
  },
  {
    "bug_location": "RocksDB Compaction",
    "severity": 2,
    "categories": [
      "Performance",
      "Storage",
      "Config"
    ],
    "root_cause": "Limited compaction threads (max-background-jobs) causing manual compaction to block auto-compaction and trigger write stalls",
    "issue_number": 13338,
    "title": "Manual compaction triggers flow control/write stall"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 13339,
    "title": "Manual compaction blocks tikv shutdown"
  },
  {
    "bug_location": "resolved_ts component",
    "severity": 3,
    "categories": [
      "Replication",
      "Performance"
    ],
    "root_cause": "tracked_index not being updated for IngestSST and 1PC scenarios, causing potential data synchronization inaccuracies between leader and followers/learners",
    "issue_number": 13353,
    "title": "resolved_ts:  IngestSST do not update the tracked_index"
  },
  {
    "bug_location": "TiKV/RocksDB",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Potential performance bottleneck related to RocksDB background compaction and TSO (Timestamp Oracle) retrieval under high IO load",
    "issue_number": 1336,
    "title": "test: QPS fails to 0 in block writer test "
  },
  {
    "bug_location": "Read Index Request Queue",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Network isolation causing read index requests to become stuck in queue, leading to significant QPS degradation after network recovery",
    "issue_number": 13379,
    "title": "inject one tikv network isolated for 50m\uff0cafter recover fault\uff0cqps drop 80%+ last for few minutes due to the read index request is stuck in the queue for a long time"
  },
  {
    "bug_location": "Performance",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Potential regression in performance optimization introduced in commit 68f99, causing measurable sysbench oltp_read_write performance degradation between 4.4% and 5.3%",
    "issue_number": 13394,
    "title": "commit 68f99 causing sysbench oltp_read_write performance degradation 4.4%- 5.3%"
  },
  {
    "bug_location": "tikv-server",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Invalid address configuration handling when advertise-addr is set to an empty string, causing program termination instead of falling back to default address",
    "issue_number": 1340,
    "title": "advertise-addr = \"\" causes program exit"
  },
  {
    "bug_location": "Transaction",
    "severity": 3,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Incorrect handling of rollback records during pessimistic transaction conflict resolution, where rollback records on non-unique index keys with timestamps greater than the transaction's for_update_ts ",
    "issue_number": 13425,
    "title": "False positive PessimisticLockNotFound when a non-pessimistic key has newer rollbacks"
  },
  {
    "bug_location": "tikv-ctl",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Unexpected wire type error when parsing Raft log entry, likely due to incorrect data deserialization or corrupted log entry",
    "issue_number": 1343,
    "title": "tikv-ctl: panic when print raft log"
  },
  {
    "bug_location": "CDC (Change Data Capture) Endpoint",
    "severity": 4,
    "categories": [
      "Transaction",
      "Network",
      "Performance"
    ],
    "root_cause": "Incorrect TSO (Timestamp Oracle) cache management during PD leader failure, causing periodic cache flushing to disrupt request processing",
    "issue_number": 13430,
    "title": "The QPS almost dropped to 0 when leader of PD is killed"
  },
  {
    "bug_location": "Snapshot Transfer/Replication Component",
    "severity": 4,
    "categories": [
      "Replication",
      "Network",
      "Performance"
    ],
    "root_cause": "Concurrent snapshot generation for multiple peers causing multiple SST file transfers to TiFlash, potentially leading to processing overhead and potential data consistency risks",
    "issue_number": 13445,
    "title": "TiKV send multi SST to TiFlash"
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 2,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Inaccurate size estimation for compressed blob data with repeated patterns when Titan is enabled, causing unnecessary region splits",
    "issue_number": 13446,
    "title": "titan: unexpected increasing empty regions when inserting large records"
  },
  {
    "bug_location": "Storage/MVCC/Compaction Filter",
    "severity": 5,
    "categories": [
      "Storage",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Compaction filter incorrectly garbage collecting key versions during flashback and GC interactions, causing data inconsistency when versions are prematurely deleted across column families",
    "issue_number": 13448,
    "title": "change gc_life_time from 12h to 10 minutes after flashback, report [errors.rs:409] [\"txn aborts\"] [err_code=KV:Storage:DefaultNotFound]"
  },
  {
    "bug_location": "TiKV Type Casting Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Inconsistent float/decimal to datetime type casting behavior between TiKV and TiDB implementations",
    "issue_number": 13458,
    "title": "Cast float/decimal as time behavior inconsistent with TiDB/MySQL"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 13463,
    "title": "deadlock on root_db lock by open_tablet() for tablet factory"
  },
  {
    "bug_location": "RocksDB Configuration",
    "severity": 3,
    "categories": [
      "Config",
      "Performance",
      "Storage"
    ],
    "root_cause": "Limited configuration flexibility for RocksDB write-stall and compaction settings",
    "issue_number": 13467,
    "title": "Support configuring RocksDB write-stall settings smaller than flow control"
  },
  {
    "bug_location": "PD Client/gRPC Interface",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Network",
      "CodeBug"
    ],
    "root_cause": "Incompatible gRPC method between PD v6.2.0 and TiKV v6.3.0 during startup, specifically an unimplemented 'IsSnapshotRecovering' method",
    "issue_number": 13497,
    "title": "br: tikv startup failure when tikv v6.3.0 connect to pd v6.2.0"
  },
  {
    "bug_location": "RawKV API V2 / Causal Timestamp Provider",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Timestamp causality violation during leader election, where raw_put requests can be processed with incorrect timestamps before proper leader establishment and causal timestamp flushing",
    "issue_number": 13502,
    "title": "RawKV API V2 timestamp causality violation"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 13513,
    "title": "regexp function using empty pattern should report error"
  },
  {
    "bug_location": "TiKV Store/IO Scheduler",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "LoadBalance"
    ],
    "root_cause": "Slow store recovery mechanism not efficiently handling IO hang scenarios, causing extended QPS recovery time beyond expected 2 minutes",
    "issue_number": 13524,
    "title": "qps recovery after 15min when one tikv io hang"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Log Backup Module",
    "severity": 4,
    "categories": [
      "Transaction",
      "Recovery"
    ],
    "root_cause": "GC safepoint timestamp conflict during log backup task pause operation, where the current transaction timestamp is lower than the GC safepoint timestamp",
    "issue_number": 13532,
    "title": "PITR: failed to pause a log-backup task with the error  BR:Backup:ErrBackupGCSafepointExceeded"
  },
  {
    "bug_location": "Region Deletion/Cleanup",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Recovery"
    ],
    "root_cause": "Inefficient stale region cleanup mechanism causing extended TiKV instance reboot time when handling large numbers of stale regions",
    "issue_number": 13534,
    "title": "Slow stale range deletion at TiKV instance reboots"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 13537,
    "title": "The expr for `GC in Compaction Filter` meets parse error"
  },
  {
    "bug_location": "RawKV Client",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Potential race condition in atomic compare-and-swap (CAS) operation causing inconsistent state updates during concurrent modifications",
    "issue_number": 13550,
    "title": "rawkv: causality violation for atomic commands"
  },
  {
    "bug_location": "tikv_util/lru.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Memory"
    ],
    "root_cause": "Unsafe memory manipulation in LruCache implementation causing undefined behavior during Rust borrow checking, likely due to incorrect pointer and reference management in the Trace struct",
    "issue_number": 13551,
    "title": "LruCache is unsound"
  },
  {
    "bug_location": "Snapshot/Region",
    "severity": 5,
    "categories": [
      "Replication",
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Race condition between snapshot taking and region configuration change, potentially causing incomplete data snapshot due to incorrect ordering of lease check and snapshot operations",
    "issue_number": 13553,
    "title": "Batch snapshot is unsound"
  },
  {
    "bug_location": "raftstore/log_unstable",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Out-of-bounds slice access during Raft log truncation and appending, likely due to incorrect handling of empty log entries during region merge operations",
    "issue_number": 13554,
    "title": "TiKV panic repeatedly: [FATAL] [lib.rs:465] [\"unstable.slice[22, 23] out of bound[22, 22], raft_id: 115835, region_id: 115832\"]"
  },
  {
    "bug_location": "PD Client/Version Compatibility",
    "severity": 3,
    "categories": [
      "Upgrade",
      "Config",
      "Replication"
    ],
    "root_cause": "Version mismatch between TiKV and PD client causing incompatibility during version upgrade from 6.3.0 to master",
    "issue_number": 13562,
    "title": "when `tikv:6.3.0` checkout to `tikv:master` in k8s, k8s pod cannot start and `crashloopbackoff`"
  },
  {
    "bug_location": "tidb_query_datatype/src/expr/ctx.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Inconsistent SQL mode constant definitions between TiKV and TiDB, causing potential runtime mismatches in SQL mode interpretation",
    "issue_number": 13566,
    "title": "Sql mode constants are not consistent with TiDB"
  },
  {
    "bug_location": "RawKV",
    "severity": 4,
    "categories": [
      "Transaction",
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Race condition in atomic compare-and-swap operations during network/node failures, causing potential inconsistent state when retrying operations",
    "issue_number": 13573,
    "title": "rawkv: correctness violation for raw atomic commands"
  },
  {
    "bug_location": "RawKV Test Suite",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Unstable region merge operation in test environment, causing test failure",
    "issue_number": 13582,
    "title": "test: cases::test_rawkv::test_region_merge is unstable"
  },
  {
    "bug_location": "Configuration Validation",
    "severity": 3,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Configuration validation does not handle low CPU resource scenarios, causing startup failure when CPU cores are limited to less than 1",
    "issue_number": 13586,
    "title": "TiKV start failed with less than 1 core CPU on Kubernetes"
  },
  {
    "bug_location": "RawKV Storage Test",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Potential race condition or synchronization issue in unit test causing unstable test behavior",
    "issue_number": 13599,
    "title": "rawkv: unit test \"test_storage::test_raw_put_key_guard\" is not correct and unstable"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Checkpoint Mechanism",
    "severity": 4,
    "categories": [
      "Performance",
      "Recovery",
      "Replication"
    ],
    "root_cause": "Potential synchronization or state recovery issue during TiKV pod restart that causes significant checkpoint lag in PITR process",
    "issue_number": 13616,
    "title": "PITR checkpoint lag increases to ~2H after restarting TiKVs"
  },
  {
    "bug_location": "Backup and Restore (PITR)",
    "severity": 4,
    "categories": [
      "Performance",
      "Recovery",
      "Transaction"
    ],
    "root_cause": "Potential synchronization or checkpoint tracking issue during TiDB pod failure and recovery, causing extended checkpoint lag in Point-in-Time Recovery (PITR) process",
    "issue_number": 13617,
    "title": "PITR lag > 5min after TiDB failure chaos for 10 minutes for 1 of the 3 TiDB pod"
  },
  {
    "bug_location": "backup-restore",
    "severity": 3,
    "categories": [
      "Performance",
      "Replication",
      "LoadBalance"
    ],
    "root_cause": "Move-to-better-location scheduler appears to interfere with Point-in-Time Recovery (PITR) log backup tasks, causing significant checkpoint lag during relocation process",
    "issue_number": 13619,
    "title": "PITR lag > 5min when move-to-better-location scheduler running"
  },
  {
    "bug_location": "Storage/GC",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Potential inconsistent state after flashback operation preventing proper storage cleanup",
    "issue_number": 13620,
    "title": "after flashback, gc work normal but storage available size is not freed"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Checkpoint Mechanism",
    "severity": 3,
    "categories": [
      "Performance",
      "Replication",
      "Recovery"
    ],
    "root_cause": "During single AZ failure, log backup checkpoint synchronization mechanism experiences significant lag, potentially due to network disruption, consensus protocol challenges, or recovery process ineffic",
    "issue_number": 13622,
    "title": "PITR checkpoint lag might >5min during/right after single AZ disaster happens"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Checkpoint Mechanism",
    "severity": 3,
    "categories": [
      "Network",
      "Performance",
      "Recovery"
    ],
    "root_cause": "Potential network failure recovery handling issue causing extended checkpoint lag in log backup tasks after S3 network partition",
    "issue_number": 13623,
    "title": "PITR checkpoint lag might be > 5 min for ~10 min after S3 network failure recovered"
  },
  {
    "bug_location": "TiKV Heartbeat Mechanism",
    "severity": 3,
    "categories": [
      "Performance",
      "Config",
      "Replication"
    ],
    "root_cause": "Fixed 10-second delay before sending first heartbeat to PD after TiKV service startup, causing scheduling delays during rolling upgrades",
    "issue_number": 13627,
    "title": "Send heartbeat to PD as soon as the service is ready"
  },
  {
    "bug_location": "storage/txn/scheduler.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Recovery"
    ],
    "root_cause": "Failed to send write finished notification to scheduler, likely due to channel closure during shutdown process",
    "issue_number": 1363,
    "title": "Panic when stop"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Checkpoint Component",
    "severity": 4,
    "categories": [
      "Network",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Network partition prevents checkpoint advancement, causing potential data recovery inconsistency during distributed system communication failure",
    "issue_number": 13632,
    "title": "PITR checkpoint not move forward during tikv and tidb advance owner network partition"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Checkpoint Component",
    "severity": 4,
    "categories": [
      "Network",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Checkpoint mechanism fails to progress during partial network partition between TiKV and PD leader, likely due to communication disruption affecting backup coordination",
    "issue_number": 13637,
    "title": "PITR checkpoint doesn't move forward during one TiKV and PD leader network partition"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Checkpoint",
    "severity": 4,
    "categories": [
      "Performance",
      "Replication",
      "Upgrade"
    ],
    "root_cause": "Potential synchronization or data transfer inefficiency during TiKV rolling restart causing significant checkpoint lag in log backup task",
    "issue_number": 13638,
    "title": "PITR checkpoint lag > 10min during tikv rolling restart"
  },
  {
    "bug_location": "RaftStore/Apply",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Incorrect region metadata handling during BatchSplit and PrepareFlashback commands, causing region version inconsistency when write batches are not properly flushed",
    "issue_number": 13643,
    "title": "flashback ddl will hang when restart all pd nodes during flashback command"
  },
  {
    "bug_location": "TiKV Codec/Serialization Layer",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Lack of support for FieldTypeTp::Geometry data type in TiKV's encoding/decoding mechanisms",
    "issue_number": 13651,
    "title": "\"Unsupported FieldType\" for FieldTypeTp::Geometry"
  },
  {
    "bug_location": "TiKV Cgroup Utility",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Assertion failure in cgroup information processing, likely due to incompatible cgroup configuration in the Gitpod environment",
    "issue_number": 13660,
    "title": "tiup playground fails with a tikv error during execution in the Gitpod environment"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 3,
    "categories": [
      "Storage",
      "Replication",
      "Memory"
    ],
    "root_cause": "Potential hardware-induced memory bit flip causing Raft message corruption",
    "issue_number": 13668,
    "title": "tikv panic due to corrupted raft message"
  },
  {
    "bug_location": "raftstore/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Bucket stats reset and split check interval are synchronized, preventing effective bucket re-splitting due to potential race condition between stats reset and split check",
    "issue_number": 13671,
    "title": "Bucket re-split hardly works"
  },
  {
    "bug_location": "Storage/Flashback",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Potential race condition during multi-batch flashback where region failures and retry mechanisms can cause incomplete data restoration, leading to data inconsistency",
    "issue_number": 13672,
    "title": "Data inconsistency after flashback "
  },
  {
    "bug_location": "Raftstore",
    "severity": 4,
    "categories": [
      "Performance",
      "Replication"
    ],
    "root_cause": "Performance degradation when a TiFlash node is down, causing increased process ready duration likely due to region scheduling or replica handling complexity",
    "issue_number": 13676,
    "title": "Process ready duration is high when one tiflash is down"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 13688,
    "title": "pitr backup lag continuously increase in dbaas gcp environment"
  },
  {
    "bug_location": "Performance Core",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Potential regression in performance optimization introduced by commit a4dc37b, causing 2-4% performance degradation across different workloads",
    "issue_number": 13692,
    "title": "commit a4dc37b causes 2%~4% performance regression in different workloads"
  },
  {
    "bug_location": "Flashback/Compaction Process",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Recovery"
    ],
    "root_cause": "Potential deadlock in flashback process when processing a single key during compaction, causing read phase to get stuck in an infinite loop",
    "issue_number": 13704,
    "title": "Flashback process is blocked for compaction pending too much data"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 2,
    "categories": [
      "CodeBug"
    ],
    "root_cause": "Incorrect library name reference in test configuration for coprocessor_v2 plugin",
    "issue_number": 13708,
    "title": "Incorrect library name in coprocessor_v2 plugin tests"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) component",
    "severity": 3,
    "categories": [
      "Performance",
      "Replication"
    ],
    "root_cause": "Potential resource contention or inefficient checkpoint management during backup restore process causing log backup lag to increase",
    "issue_number": 13714,
    "title": "PITR lag increase during br restore"
  },
  {
    "bug_location": "resolved_ts",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Multiple write records for the same key in a single Raft command batch during flashback operation, causing an assertion failure in row write handling",
    "issue_number": 13723,
    "title": "flashback during workload, tikv panic lead to tidb down"
  },
  {
    "bug_location": "Snapshot/Raft Replication Component",
    "severity": 4,
    "categories": [
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Improper state management during snapshot generation when channel capacity is exceeded, causing peer state to become stuck in 'Generating' status",
    "issue_number": 1373,
    "title": "Discard scan snapshot result may lead to invalid peer state"
  },
  {
    "bug_location": "Store Thread / gRPC Channel",
    "severity": 4,
    "categories": [
      "Memory",
      "Network",
      "Performance"
    ],
    "root_cause": "Message channel congestion during network partition/IO hang, causing unprocessed Raft messages to accumulate and consume excessive memory",
    "issue_number": 13731,
    "title": "tikv oom after this tikv io delay/hang or one of tikv network partition last for 50mins and recover"
  },
  {
    "bug_location": "DDL (Data Definition Language) Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Incorrect handling of keys with only delete MVCC records during flashback operation, causing processing logic to break and potentially leading to deadlock",
    "issue_number": 13743,
    "title": "flashback ddl is hang when some key's last mvcc record is delete"
  },
  {
    "bug_location": "config.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Validation logic for max_background_jobs does not handle CPU quota less than 1 correctly",
    "issue_number": 13752,
    "title": "validate max_background_jobs will fail when cpu quota is less than 1"
  },
  {
    "bug_location": "Raft/Network Layer",
    "severity": 4,
    "categories": [
      "Network",
      "Replication",
      "LoadBalance"
    ],
    "root_cause": "Network isolation causing TiKV node to become unavailable and unable to recover within expected timeframe, potentially due to insufficient network partition handling or slow leader election mechanism",
    "issue_number": 13758,
    "title": "one tikv is isolated from others tikv, and services are unavailable during a failure"
  },
  {
    "bug_location": "raftstore/read_worker",
    "severity": 5,
    "categories": [
      "Network",
      "CodeBug",
      "Recovery"
    ],
    "root_cause": "Unwrapping a None value during local read request validation when network fault is injected, causing a panic in the LocalReaderCore",
    "issue_number": 13764,
    "title": "tikv panic when inject network fault"
  },
  {
    "bug_location": "TiKV String Comparison/Like Function",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Lack of charset information propagation when new collation is disabled, causing incorrect string comparison in like operations",
    "issue_number": 13769,
    "title": "When the new collation is not enabled, `_` pattern in like misbehaved"
  },
  {
    "bug_location": "Configuration Management",
    "severity": 3,
    "categories": [
      "Upgrade",
      "Config"
    ],
    "root_cause": "Configuration option 'causal-ts.available-interval' was renamed to 'causal-ts.alloc-ahead-buffer' in v6.4.0, causing config validation failure during upgrade process",
    "issue_number": 13772,
    "title": "v6.3.0 upgrade to v6.4.0 will fail when has causal-ts.available-interval config in v6.3.0"
  },
  {
    "bug_location": "raftstore/store/worker/read.rs",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Upgrade",
      "Transaction"
    ],
    "root_cause": "Unwrapping a None value in LocalReaderCore::validate_request, likely caused by unexpected state during upgrade or transaction processing",
    "issue_number": 13812,
    "title": "tikv fatal error after upgrading when running ch and cc workload"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 13829,
    "title": "reset-to-version segfault"
  },
  {
    "bug_location": "Backup and Restore Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Max resolved timestamp mechanism fails to advance during data restoration process, potentially causing synchronization and tracking issues in distributed transactions and change data capture",
    "issue_number": 13838,
    "title": "TiKV Max Resolved TS stop to advance when restore / import data into tikv"
  },
  {
    "bug_location": "storage/mvcc/reader",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Assertion failure during MVCC read operation where expected operation type (Put) does not match actual operation type (Delete)",
    "issue_number": 13839,
    "title": "tikv restart for \"[\"assertion failed: `(left == right)`\\n  left: `Delete`,\\n right: `Put`\"] \""
  },
  {
    "bug_location": "storage/mvcc/reader",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of write record types during scan_writes, potentially returning incorrect write record for flashback operation",
    "issue_number": 13844,
    "title": "Flashback may overwrite wrong write record"
  },
  {
    "bug_location": "Network Configuration",
    "severity": 4,
    "categories": [
      "Network",
      "Config",
      "CodeBug"
    ],
    "root_cause": "TiKV server binding primarily to IPv4 addresses when IPv6 is the default network, causing connection failures during restore operations",
    "issue_number": 13847,
    "title": "IPv6 support: tikv should listen on IPv6 instead of IPv4\uff0c which cause br restore failed."
  },
  {
    "bug_location": "RaftEngine/LogBatch",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Buffer size limitation causing panic when log batch exceeds 2GB during write operations",
    "issue_number": 13848,
    "title": "TiKV panic because the buffer size in log_batch is more than 2G when append to RaftEngine."
  },
  {
    "bug_location": "Flashback Component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Improper handling of potentially invalid start keys during flashback operation, which can lead to prewriting and committing keys without proper validation",
    "issue_number": 13861,
    "title": "Flashback may prewrite and commit an illegal key"
  },
  {
    "bug_location": "import/sst_service.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Tokio runtime context not properly initialized, causing a panic when attempting to use time-related operations without an active runtime",
    "issue_number": 13862,
    "title": "TiKV boost failed."
  },
  {
    "bug_location": "import/sst_service.rs",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Tokio runtime context not properly initialized when calling time-related functions, causing a fatal error in the SST importer service",
    "issue_number": 13866,
    "title": "[FATAL] [lib.rs:495] [\"there is no reactor running, must be called from the context of a Tokio 1.x runtime\"]"
  },
  {
    "bug_location": "Flashback/Region Management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Recovery",
      "Transaction"
    ],
    "root_cause": "Incomplete region state management during flashback operation, causing some regions to remain in a transitional 'flashback progress' state",
    "issue_number": 13868,
    "title": "after flashback, admin check table report \" region 20009 is in flashback progress\""
  },
  {
    "bug_location": "PeerFSM",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Race condition between flashback state update and read request proposal, causing potential blocking of read requests during flashback preparation",
    "issue_number": 13870,
    "title": "Flashback state may not be visible immediately in PeerFSM"
  },
  {
    "bug_location": "sst_importer",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Recovery"
    ],
    "root_cause": "Attempting to start an asynchronous runtime within an existing runtime during PITR, causing a runtime conflict",
    "issue_number": 13874,
    "title": "sst_importer: TiKV panics when doing PITR"
  },
  {
    "bug_location": "storage/txn/commands/mod.rs",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Incorrect handling of empty end_key during flashback operation, causing incomplete key restoration when end_key is not properly specified",
    "issue_number": 13879,
    "title": "Flashback may miss the last region"
  },
  {
    "bug_location": "SstImporter",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Recovery",
      "Storage"
    ],
    "root_cause": "Attempting to start a runtime from within an existing runtime during PITR (Point-in-Time Recovery) restore process, causing a fatal panic in the tokio runtime",
    "issue_number": 13883,
    "title": "tikv panic during pitr restore "
  },
  {
    "bug_location": "Backup and Restore (PITR) Component",
    "severity": 5,
    "categories": [
      "Network",
      "Recovery",
      "Transaction"
    ],
    "root_cause": "TiKV fails to advance PITR checkpoint when unable to establish connection with PD during network disruption or AZ failure",
    "issue_number": 13898,
    "title": "PITR checkpoint not advance if tikv fails to connect to PD at start"
  },
  {
    "bug_location": "Configuration Management",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Config"
    ],
    "root_cause": "Configuration option 'cdc.raw-min-ts-outlier-threshold' was deleted in v6.4, causing configuration validation failure during upgrade from v6.4.0 to v6.5.0",
    "issue_number": 13906,
    "title": "v6.4.0 upgrade to v6.5.0 fail when has config \"cdc.raw-min-ts-outlier-threshold\" in old version"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 13918,
    "title": "coprocessor cpu alert is wrong"
  },
  {
    "bug_location": "server/raftkv",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Unhandled error when calling Result::unwrap() on a canceled async operation",
    "issue_number": 13926,
    "title": "called `Result::unwrap()` on an `Err` value: Canceled"
  },
  {
    "bug_location": "tikv-ctl",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Log directory creation occurs unconditionally during command initialization, even for commands that do not require logging",
    "issue_number": 13927,
    "title": "`tikv-ctl` creates log directory regardless of command"
  },
  {
    "bug_location": "coprocessor_plugin_api",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Unnecessary recompilation of coprocessor_plugin_api package during each 'make dev' execution, indicating potential build system or dependency caching issue",
    "issue_number": 13950,
    "title": "coprocessor_plugin_api get rebuild every time running `make dev`"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 13955,
    "title": "br: ebs restore could not success when network down more than 5 seconds"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) module",
    "severity": 5,
    "categories": [
      "Storage",
      "Transaction",
      "Recovery"
    ],
    "root_cause": "Potential data inconsistency during log replay when handling flashback transactions, causing incorrect record selection across column families",
    "issue_number": 13958,
    "title": "PITR: data inconsistency after PITR with log containing flashback data. "
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) backup service",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Security",
      "Network"
    ],
    "root_cause": "Certificate verification failure during TLS connection establishment, likely due to complex certificate chain validation issue",
    "issue_number": 13959,
    "title": "log-backup: log backup cannot work in some conditions"
  },
  {
    "bug_location": "raftstore-v2/router/response_channel.rs",
    "severity": 2,
    "categories": [
      "CodeBug"
    ],
    "root_cause": "Type definition mismatch or missing type definition for FlushChannel in the compilation scope",
    "issue_number": 13968,
    "title": "raftstore-v2: exists compilation errors"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 1397,
    "title": "server: can not pass etcd local test with send/receive corruption enabled"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 13977,
    "title": "pitr panic due to raft ready too large"
  },
  {
    "bug_location": "backup-stream",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Unnecessary warning logs being generated during region changes when no backup task is active, causing log noise in the backup stream observer",
    "issue_number": 14012,
    "title": "Logging from backup-stream is noisy"
  },
  {
    "bug_location": "TiKV Configuration/Resource Allocation",
    "severity": 3,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Configuration validation for max_background_jobs fails when CPU request is 1 or less, preventing volume snapshot restore",
    "issue_number": 14017,
    "title": "tidb-operator volume-snapshot restore failure when tikv request cpu less or equal to 1"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 14033,
    "title": "TiKV OOM itself"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 14038,
    "title": "txn: the left pessimistic lock may break the constraint check"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 14071,
    "title": "Log backup: failed to register task after online reload certification when TLS enable"
  },
  {
    "bug_location": "Network/CDC",
    "severity": 3,
    "categories": [
      "Network",
      "Performance"
    ],
    "root_cause": "Changes in network communication protocol or heartbeat mechanism introduced in v6.5.0 that increased idle network traffic, likely related to CDC (Change Data Capture) component",
    "issue_number": 14092,
    "title": "Inter-TiKV idle network traffic increases about 100% since v6.5.0"
  },
  {
    "bug_location": "backup-restore component",
    "severity": 3,
    "categories": [
      "Network",
      "Storage"
    ],
    "root_cause": "HTTP 500 error during Azure blob storage upload with timeout, indicating potential network or storage service connectivity issue",
    "issue_number": 14093,
    "title": "When azblob is used, retry for http code 500 error"
  },
  {
    "bug_location": "Backup and Restore (PiTR) Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Resolved timestamp advancement without proper leadership verification, potentially causing incorrect checkpoint and potential data loss during leadership changes",
    "issue_number": 14099,
    "title": "PiTR resolved ts is not reliable and may lost data in extreme cases"
  },
  {
    "bug_location": "coprocessor/endpoint.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect handling of scan details tracking during batch coprocessor query processing, causing partial key processing metrics to be incorrectly recorded or overwritten",
    "issue_number": 14109,
    "title": "cop: some scan details batch are mistakenly dropped"
  },
  {
    "bug_location": "RocksDB Flush Mechanism",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Race condition in flush job ordering and tracking, causing inconsistent state tracking during column family flush operations",
    "issue_number": 14113,
    "title": "[dynamic regions] TiKV panics with \"default not found\""
  },
  {
    "bug_location": "Storage/Memtable",
    "severity": 4,
    "categories": [
      "Memory",
      "Storage",
      "Performance"
    ],
    "root_cause": "Memory flush cannot keep up with write speed, causing memtable to grow beyond memory quota during high-concurrency data import",
    "issue_number": 14114,
    "title": "[Dynamic Regions] TiKV OOM in tpcc 25K import data "
  },
  {
    "bug_location": "Tablet GC Worker",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Potential deadlock in RocksDB compaction process during tablet garbage collection, causing task accumulation and blocking store size reporting",
    "issue_number": 14115,
    "title": "[Dynamic Regions] Tablet GC tasks are accumulated overtime and incorrect store size reported"
  },
  {
    "bug_location": "PD (Placement Driver)",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Replication",
      "Config"
    ],
    "root_cause": "Witness configuration and leader balancing logic appears to be incompatible, causing uneven leader distribution across TiKV stores when witness mode is enabled",
    "issue_number": 14118,
    "title": "\u3010witness\u3011after enable witness\uff0ctikv can not balance leader"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 14122,
    "title": "Stale read request timeout when resolved-ts is slow"
  },
  {
    "bug_location": "Tablet/CacheTablet",
    "severity": 4,
    "categories": [
      "Storage",
      "Replication",
      "Performance"
    ],
    "root_cause": "CacheTablet fails to properly remove old peer cache after peer destruction, causing snapshot application inconsistencies during TiKV scale-in operations",
    "issue_number": 14128,
    "title": "[Dynamic Regions] TiKV scale-in failed"
  },
  {
    "bug_location": "Restore/Region Management",
    "severity": 4,
    "categories": [
      "Performance",
      "Replication",
      "Storage"
    ],
    "root_cause": "Potential performance degradation and region unavailability when witness mode is enabled during backup and restore operations",
    "issue_number": 14130,
    "title": "\u3010witness\u3011after enable witness\uff0cbr restore very slowly and failed with error [tikv:9005]Region is unavailable"
  },
  {
    "bug_location": "Region Scheduler",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Hot regions are not being dynamically split when write load becomes concentrated, causing uneven write distribution across nodes",
    "issue_number": 14135,
    "title": "[dynamic regions] region is not split for hot write"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) / Log Backup Component",
    "severity": 5,
    "categories": [
      "Transaction",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Witness peer failure during log backup process when network latency is introduced, causing inability to recover snapshot and pause PITR task",
    "issue_number": 14137,
    "title": "\u3010witness\u3011enable witness and pitr\uff0cinject one tikv io delay 200ms last for 5m\uff0cpitr task became to paused"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 14141,
    "title": "rawkv: integration test case `test_raw_put_key_guard` is flaky"
  },
  {
    "bug_location": "raftstore/peer",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Replication",
      "Storage"
    ],
    "root_cause": "Hash verification failure during region peer synchronization, likely caused by concurrent region scheduling and witness node configuration",
    "issue_number": 14142,
    "title": "\u3010witness\u3011enable witness and add randomly scheduler\uff0cafter run workload some time\uff0ctikv panic"
  },
  {
    "bug_location": "Flashback Region Handler",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Incorrect error handling during flashback region operation, returning unexpected error type that disrupts client retry logic",
    "issue_number": 14143,
    "title": "Flashback should not return any unexpected error to the client"
  },
  {
    "bug_location": "Region Transfer/Scheduling",
    "severity": 4,
    "categories": [
      "Replication",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Potential issue with leader and region transfer mechanism when using witness nodes during scale-in operations",
    "issue_number": 14144,
    "title": "\u3010witness\u3011enable witness\uff0cscale in one tikv can not successfully due to the leaders and regions can not transfer successfully"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 14146,
    "title": "cdc: integration test case test_rawkv_sacn is unstable"
  },
  {
    "bug_location": "TiDB Query Expression Builder",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Unsupported handling of MySQL ENUM type in expression evaluation",
    "issue_number": 14156,
    "title": "query got tikv error \"other error: [components/tidb_query_expr/src/types/expr_builder.rs:74]: Unsupported expression type MysqlEnum\""
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Component",
    "severity": 4,
    "categories": [
      "Recovery",
      "Replication",
      "Config"
    ],
    "root_cause": "Potential state persistence or recovery mechanism failure during cluster-wide restart, causing PITR task to become disabled unexpectedly",
    "issue_number": 14159,
    "title": "after restart all tikv pods\u3001pd pods\u3001tidb pods\uff0cpitr task became to disabled"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) component",
    "severity": 4,
    "categories": [
      "Replication",
      "Upgrade",
      "Network"
    ],
    "root_cause": "Inability to dynamically detect and adapt to changes in the etcd cluster membership during cluster scaling operations",
    "issue_number": 14165,
    "title": "log-backup: PITR cannot detect new members in the etcd cluster"
  },
  {
    "bug_location": "Region Load Balancer",
    "severity": 3,
    "categories": [
      "LoadBalance",
      "Performance",
      "Replication"
    ],
    "root_cause": "Store's available space metric not correctly updated after region movement, causing uneven data distribution during scale-out",
    "issue_number": 14178,
    "title": "[Dynamic Regions] store is not balanced during the scale-out"
  },
  {
    "bug_location": "Store/Region Management",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance",
      "Replication"
    ],
    "root_cause": "Potential issue with store size tracking and tablet garbage collection during dynamic region redistribution",
    "issue_number": 14179,
    "title": "[Dynamic Regions] store size is not reduced during the scale-out when the region count is reduced"
  },
  {
    "bug_location": "log-backup",
    "severity": 4,
    "categories": [
      "Recovery",
      "Network",
      "Storage"
    ],
    "root_cause": "Unable to advance backup progress when PD (Placement Driver) is unavailable, indicating a potential resilience/failover issue in log backup mechanism",
    "issue_number": 14184,
    "title": "log-backup: cannot advance the progress when PD loss."
  },
  {
    "bug_location": "Resource Control Component",
    "severity": 3,
    "categories": [
      "Config",
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Resource group detection mechanism is not functioning correctly when resource control is enabled",
    "issue_number": 14191,
    "title": "resource_control: resource group is not detected "
  },
  {
    "bug_location": "read_pool",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Divide by zero error in time slice calculation during TPC workload, likely caused by an unexpected zero duration in read pool time slice update mechanism",
    "issue_number": 14200,
    "title": "tikv FATAL ERROR when running tpc "
  },
  {
    "bug_location": "read_pool",
    "severity": 5,
    "categories": [
      "Network",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential divide by zero error during time slice update in read pool worker thread during network partition scenario",
    "issue_number": 14205,
    "title": "tikv panic when inject network-partition "
  },
  {
    "bug_location": "log-backup",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Dashmap's segment lock mechanism limits concurrent log download operations, creating a performance bottleneck",
    "issue_number": 14206,
    "title": "log-backup: get rid of the dashmap locks"
  },
  {
    "bug_location": "raftstore-v2/ready module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Incorrect peer cache retrieval logic causing peer isolation when attempting to respond to out-of-configuration peers",
    "issue_number": 14211,
    "title": "[dynamic regions] wrong peer cache can make a peer isolated forever"
  },
  {
    "bug_location": "PD (Placement Driver) Region Scheduler",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Balance region scheduler fails to select regions for scheduling when most regions are considered hot regions during moderate read-write traffic",
    "issue_number": 14214,
    "title": "[Dynamic Regions] PD does not schedule region-balance when there's moderate read-write traffic "
  },
  {
    "bug_location": "Raft Consensus Module",
    "severity": 4,
    "categories": [
      "Replication",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Learner peer cannot be promoted to voter during dynamic region configuration change, causing region replication configuration to get stuck",
    "issue_number": 14215,
    "title": "[Dynamic Regions] Scale-in is blocked because one region peer cannot be converted from learner to voter"
  },
  {
    "bug_location": "Raft Consensus Module",
    "severity": 4,
    "categories": [
      "Replication",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Race condition during node configuration change and snapshot application, causing inconsistent node state and leadership election failure",
    "issue_number": 14217,
    "title": "\u3010witness\u3011restart one tikv every 20min\uff0ctpcc report 9005 at  certain fault"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Component",
    "severity": 4,
    "categories": [
      "Replication",
      "Recovery",
      "Transaction"
    ],
    "root_cause": "Potential failure handling weakness in PITR task management during TiKV node failure scenario",
    "issue_number": 14219,
    "title": "\u3010witness\u3011inject one tikv failure for 10m, pitr task became to paused"
  },
  {
    "bug_location": "Snapshot Transfer Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "LoadBalance"
    ],
    "root_cause": "Bandwidth limitation mechanism not properly implemented during snapshot transfer in dynamic region scaling operations",
    "issue_number": 14221,
    "title": "[Dynamic Regions] the limit can not work in transfer snapshot  "
  },
  {
    "bug_location": "pprof module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Memory",
      "Performance"
    ],
    "root_cause": "Pipe file descriptors are not properly closed in short-term threads during CPU profiling, causing continuous file descriptor leaks in TiKV's pprof module",
    "issue_number": 14224,
    "title": "pipe fd leaks in tikv-6.1.0 with cpu profiler running"
  },
  {
    "bug_location": "Raft Replication Component",
    "severity": 4,
    "categories": [
      "Network",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Failed leader transfer and notification mechanism during network loss scenarios, causing witness peer to remain in pending state and preventing log synchronization",
    "issue_number": 14228,
    "title": "\u3010witness\u3011inject one tikv network loss for in 10min repeatly\uff0cfor certain inject and after recover fault\uff0craft log lag is more and more"
  },
  {
    "bug_location": "BatchSystem",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Memory"
    ],
    "root_cause": "Unsafe unwrap operation on potentially None mailbox during concurrent batch system shutdown, which could lead to runtime panic",
    "issue_number": 14233,
    "title": "[dynamic regions] potential unwrap on None when getting mailbox"
  },
  {
    "bug_location": "Backup and Restore (PITR)",
    "severity": 4,
    "categories": [
      "Network",
      "Replication",
      "Recovery"
    ],
    "root_cause": "PD leader network partition causing disruption in Point-in-Time Recovery (PITR) recovery point objective (RPO) synchronization",
    "issue_number": 14241,
    "title": "pitr rpo is more than 5m last for 9min after inject pdleader network partition which trigger pd changed leader"
  },
  {
    "bug_location": "Coprocessor (Cop) Paging Executor",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Paging mechanism not correctly stopping or limiting requests when LIMIT is applied in a partitioned table query",
    "issue_number": 14254,
    "title": "copr: paging not stop with limit executor"
  },
  {
    "bug_location": "raftstore-v2/snapshot",
    "severity": 4,
    "categories": [
      "Replication",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Concurrent snapshot reception causing SST file size corruption during region replication, with race conditions in snapshot transfer and log truncation mechanisms",
    "issue_number": 14256,
    "title": "[dynamic regions] snapshot corruption"
  },
  {
    "bug_location": "Dependencies/OpenSSL",
    "severity": 4,
    "categories": [
      "Security",
      "Upgrade"
    ],
    "root_cause": "Outdated OpenSSL library version with known security vulnerability (CVE-2023-0286)",
    "issue_number": 14257,
    "title": "upgrade openssl version to fix CVE-2023-0286"
  },
  {
    "bug_location": "Tracker/Metrics",
    "severity": 2,
    "categories": [
      "Performance",
      "Monitoring"
    ],
    "root_cause": "Suspended time is incorrectly aggregated into process time, causing misleading performance metrics",
    "issue_number": 14262,
    "title": "suspended-time should be passed through to TiDB to avoid confusion"
  },
  {
    "bug_location": "Region",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Dynamic region with large size and improperly split buckets causing request timeout exceeding 60 seconds",
    "issue_number": 14284,
    "title": "[Dynamic Regions]: select count(*) returns \"context deadline exceeded \" error"
  },
  {
    "bug_location": "Backup and Restore (PITR) Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Recovery",
      "Storage"
    ],
    "root_cause": "Potential task dispatch race condition or runtime task management issue during large-scale log data restoration from S3 storage",
    "issue_number": 14285,
    "title": "PITR: dispatch task is gone: runtime dropped the dispatch task"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential issue with paging request handling in coprocessor, causing incomplete result set during grouped query",
    "issue_number": 14291,
    "title": "copr: paging request stop unexpectedly"
  },
  {
    "bug_location": "RocksDB Storage Layer",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential metrics calculation discrepancy in multi-RocksDB storage size measurement, causing inconsistent disk usage reporting between different measurement methods",
    "issue_number": 14295,
    "title": "[Dynamic Regions] Multi-RocksDB's store size is 18% more than single-rocksdb"
  },
  {
    "bug_location": "MultiRocks DB Storage Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential memory/storage allocation inefficiency in MultiRocks DB implementation causing unexpected store size growth during long-running workloads",
    "issue_number": 14301,
    "title": "[Dynamic Regions]After using MultiRocks DB, the store size of TiKV surged unexpectedly, which is not in line with expectations"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Hardcoded timeout of 20s is insufficient for large data regions, causing potential request failures during distributed query processing",
    "issue_number": 14303,
    "title": "[Dynamic Regions] coprocessor's hardcode timeout may not work when bucket is big"
  },
  {
    "bug_location": "RaftStore/ReadIndexQueue",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Memory",
      "Transaction"
    ],
    "root_cause": "Resource leak in index read callback, likely due to improper handling of drop/cleanup mechanisms in the read index request processing",
    "issue_number": 14306,
    "title": "[Dynamic Regions] Multi-RocksDB's tikv panic with resource leak detected: callback of index read"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 14308,
    "title": "[Dynamic Regions] go-tpc prepare faild with partitioned-raft-kv"
  },
  {
    "bug_location": "pd_client",
    "severity": 3,
    "categories": [
      "Network",
      "CodeBug"
    ],
    "root_cause": "gRPC connection failure during PD client test scenarios, causing transient channel failures",
    "issue_number": 14309,
    "title": "pd_client tests fail occassionary"
  },
  {
    "bug_location": "Transaction",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Race condition in pessimistic lock handling when using WakeUpModeForceLock, allowing stale lock requests to succeed without proper conflict detection",
    "issue_number": 14311,
    "title": "Replayed pessimistic lock requests using `WakeUpModeForceLock` may lead to correctness issue"
  },
  {
    "bug_location": "Backup and Restore (BR) component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Raft entry size exceeding maximum allowed size during point-in-time recovery (PITR) restore, likely due to incomplete size calculation of write requests",
    "issue_number": 14313,
    "title": "pitr restore data fail for \"failed: raft entry is too large, region 170, entry size 8616655\""
  },
  {
    "bug_location": "Raft Consensus Module",
    "severity": 5,
    "categories": [
      "Transaction",
      "Replication",
      "Performance"
    ],
    "root_cause": "Race condition between mailbox registration and scheduling raft ticks, causing peers to get stuck in an invalid lease state and preventing leader election",
    "issue_number": 14315,
    "title": "[Dynamic Regions] QPS drop form 28k to 1k with workload error code 9005 during one tikv down "
  },
  {
    "bug_location": "raftstore-v2/raft/apply",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Attempting to unwrap a None Option during snapshot application in the Raft apply process, indicating a potential nil/missing value handling issue",
    "issue_number": 14316,
    "title": "[Dynamic Regions] multiple rocksdb tikv panic with Option::unwrap() on a None value"
  },
  {
    "bug_location": "RocksDB Configuration Management",
    "severity": 3,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Dynamic configuration update mechanism for RocksDB parameters not fully implemented or propagated correctly",
    "issue_number": 14320,
    "title": "[Dynamic Regions]  dynamic change on max-compactions, write-buffer-limit  do not work "
  },
  {
    "bug_location": "Raftstore-v2",
    "severity": 2,
    "categories": [
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Incomplete metrics implementation in new Raftstore version",
    "issue_number": 14321,
    "title": "[Dynamic Regions]  Some metrics are missing in raftstore-v2"
  },
  {
    "bug_location": "raftstore-v2/operation/command",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Potential null or uninitialized apply_scheduler in dynamic region handling during Raft committed entries processing",
    "issue_number": 14324,
    "title": "[Dynamic Regions] mutiple rocksdb tikv panic for apply_scheduler should be something"
  },
  {
    "bug_location": "Region Split Scheduler",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug",
      "LoadBalance"
    ],
    "root_cause": "Bucket flow report triggering region splits too aggressively, causing excessive region splits and potential request failures",
    "issue_number": 14333,
    "title": "[Dynamic Regions] Bucket flow report triggers much faster region split than before. "
  },
  {
    "bug_location": "GC (Garbage Collection) module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "GC process malfunctioning in partitioned-raft-kv configuration, causing interruption of data cleanup and potential performance degradation",
    "issue_number": 14339,
    "title": "[Dynamic Regions] GC did not work properly with partitioned-raft-kv"
  },
  {
    "bug_location": "Memory Allocator",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance"
    ],
    "root_cause": "Potential memory fragmentation issue in TiKV's memory allocation mechanism, causing gradual memory growth and eventual Out of Memory (OOM) condition",
    "issue_number": 14346,
    "title": "TiKV OOM Killed"
  },
  {
    "bug_location": "raftstore-v2/operation/command/admin/split",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Unexpected unwrap() of a None value during region split operation, likely due to missing null check or incorrect state management in region splitting logic",
    "issue_number": 14347,
    "title": "[Dynamic Regions] multiple rocksdb tikv panic"
  },
  {
    "bug_location": "RocksDB Engine / Tablet Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Memory"
    ],
    "root_cause": "Too many open file descriptors during dynamic region management, causing RocksDB engine initialization failures",
    "issue_number": 14352,
    "title": "[Dynamic Regions] multiple rocksdb panic with too many open files"
  },
  {
    "bug_location": "Region Split Mechanism",
    "severity": 4,
    "categories": [
      "Performance",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Concurrent region split operations causing excessive retry and blocking read-index requests when multiple splits occur simultaneously",
    "issue_number": 14355,
    "title": "[Dynamic Regions] When there's about 100 split happening roughly at same time, some splits are very slow."
  },
  {
    "bug_location": "RocksDB Memtable",
    "severity": 4,
    "categories": [
      "Memory",
      "Storage",
      "Performance"
    ],
    "root_cause": "High memory consumption by RocksDB memtables during dynamic region operations, potentially causing out-of-memory (OOM) conditions",
    "issue_number": 14356,
    "title": "[Dynamic Regions] tikv oom due to memtable is highly used"
  },
  {
    "bug_location": "TiKV Server",
    "severity": 4,
    "categories": [
      "Recovery",
      "Network",
      "Replication"
    ],
    "root_cause": "Potential timeout or synchronization issue during cluster restart causing TiKV server initialization failure",
    "issue_number": 14362,
    "title": "[Dynamic Regions] all tidbs can not start after restart all tidbs\u3001tikvs\u3001pds"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Potential race condition in leader election process during node restart with nobarrier mount option, causing inconsistent view of leadership state across cluster nodes",
    "issue_number": 14368,
    "title": "[Dynamic Regions] a region fails to elect leader after restart with force (with nobarrier mount option)"
  },
  {
    "bug_location": "Resource Control Component",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Performance regression introduced by enabling resource control by default, causing 6-8% performance degradation in write-heavy workloads",
    "issue_number": 14375,
    "title": "Enabling resource control by default results in 6%- 8% performance regression"
  },
  {
    "bug_location": "TiKV Server",
    "severity": 4,
    "categories": [
      "Replication",
      "Recovery",
      "Network"
    ],
    "root_cause": "Potential failure handling issue during node shutdown or network interruption causing TPCC test failure with error code 9005",
    "issue_number": 14377,
    "title": "[witness] tpcc report 9005 when inject tikv down"
  },
  {
    "bug_location": "Region Leader Election",
    "severity": 5,
    "categories": [
      "Replication",
      "Performance",
      "Transaction"
    ],
    "root_cause": "When a TiKV node experiences an IO hang, the remaining peers cannot successfully elect a new leader, causing the entire region's QPS to drop to zero",
    "issue_number": 14378,
    "title": "[Dynamic Regions] qps drop to zero after inject one tikv io hang"
  },
  {
    "bug_location": "raftstore/entry_storage",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Attempted to unwrap a None value during Raft term retrieval, likely due to missing or uninitialized entry in the Raft log storage",
    "issue_number": 14386,
    "title": "[Dynamic Regions] multiple rocksdb tikv panic for called `Option::unwrap()` on a `None` value"
  },
  {
    "bug_location": "raftstore/read_queue",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Unexpected region UUID mismatch during read index queue processing, causing a panic in the read queue advancement logic",
    "issue_number": 14388,
    "title": "[Dynamic Regions] multiple rocksdb tikv panic for unexpected uuid detected"
  },
  {
    "bug_location": "raftstore-v2/split",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Failed to accept snapshot during region split operation, causing panic in TiKV's Raft store component",
    "issue_number": 14389,
    "title": "[Dynamic Regions] multiple rocksdb tikv panic for failed to accept snapshot"
  },
  {
    "bug_location": "storage/txn/actions/check_txn_status.rs",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Unexpected transaction record found during transaction status checking, likely due to race condition or inconsistent transaction state management in dynamic region scenarios",
    "issue_number": 14390,
    "title": "[Dynamic Regions] multiple rocksdb tikv panic for txn record found but not expected"
  },
  {
    "bug_location": "Flow Controller",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Flow control mechanism fails to properly evaluate and respond to high compaction pending bytes, preventing appropriate write throttling",
    "issue_number": 14392,
    "title": "Flow controller doesn't throttle when compaction pending bytes is high"
  },
  {
    "bug_location": "Raft Replication/Log Replication",
    "severity": 4,
    "categories": [
      "Network",
      "Replication",
      "Memory"
    ],
    "root_cause": "Lack of flow control during network partition recovery, causing excessive log replication and memory pressure leading to OOM",
    "issue_number": 14411,
    "title": "[Dynamic Regions] tikv oom after Inject one tikv network_partition last for 50m and recover due to lack of flow control for log replication"
  },
  {
    "bug_location": "Metrics/Monitoring",
    "severity": 3,
    "categories": [
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Metrics recording for YATP (Yet Another Thread Pool) task poll duration is not functioning correctly after resource control became default in nightly TiKV version",
    "issue_number": 14424,
    "title": "The histogram `tikv_yatp_task_poll_duration` is missing"
  },
  {
    "bug_location": "backup-stream",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Network",
      "Transaction"
    ],
    "root_cause": "Unhandled future polling scenario in backup stream checkpoint manager, causing a fatal runtime error when resolving a future that has already been resolved",
    "issue_number": 14426,
    "title": "tikv panic with fatal error \"Resolved future is not supposed to be polled again.\" when run ha-tidb-random-to-all-network-partition-last-for-10m"
  },
  {
    "bug_location": "Monitoring/Grafana",
    "severity": 2,
    "categories": [
      "Monitoring",
      "CodeBug"
    ],
    "root_cause": "Potential issue with Grafana dashboard query configuration for resource group QPS metrics",
    "issue_number": 14427,
    "title": "Grafana display anomaly"
  },
  {
    "bug_location": "Transaction",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Performance regression introduced in commit bec40346e641f480b35c6a83bfe6e4fd169ddc01, causing 2-4% performance drop in write-heavy workloads",
    "issue_number": 14429,
    "title": "commit bec403 causes 2%-4% in write-heavy workloads"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "Performance",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Region split operation blocks raftstore message processing due to memtable flush, causing write performance degradation",
    "issue_number": 14447,
    "title": "[Dynamic Regions] Split can significantly impacts write QPS"
  },
  {
    "bug_location": "log-backup component",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "Recovery"
    ],
    "root_cause": "Excessive retry mechanism when PD connection is lost, causing high CPU consumption and log spam",
    "issue_number": 14451,
    "title": "log-backup: log backup retry may be too frequent and consuming many CPU resource"
  },
  {
    "bug_location": "log-backup/subscription_manager.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Unnecessary logging in log backup module when no backup task is active, causing excessive log generation",
    "issue_number": 14453,
    "title": "log-backup: verbose log printed even there isn't a log bakcup task"
  },
  {
    "bug_location": "TiKV Configuration Management",
    "severity": 4,
    "categories": [
      "Config",
      "Upgrade"
    ],
    "root_cause": "Configuration option 'server.snap-max-write-bytes-per-sec' is not recognized in TiKV 7.0.0, causing configuration validation failure during upgrade",
    "issue_number": 14455,
    "title": "tikv will upgrade fail with config: server.snap-max-write-bytes-per-sec"
  },
  {
    "bug_location": "Raft/Network",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "High network traffic overhead when partitioned-raft-kv is enabled, likely due to excessive inter-node communication during region scheduling",
    "issue_number": 14458,
    "title": "[Dynamic Regions] improve the network traffic spikes when partitioned-raft-kv is enabled"
  },
  {
    "bug_location": "TiKV Restart/Recovery Process",
    "severity": 4,
    "categories": [
      "Recovery",
      "Performance",
      "Config"
    ],
    "root_cause": "Extended recovery time during heavy write workloads causing restart timeout, potentially due to log replay or region synchronization overhead",
    "issue_number": 14459,
    "title": "[Dynamic Regions] When tikv's under heavy write, the restart time needed exceeds tiup's timeout"
  },
  {
    "bug_location": "storage.partitioned-raft-kv",
    "severity": 5,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Performance degradation in partitioned-raft-kv storage engine during long-running TPCC workload, causing significant QPS drop",
    "issue_number": 14460,
    "title": "[Dynamic Regions] TPCC 1K 7d stability test: QPS drops dramatically when partitioned-raft-kv is enabled compared to baseline"
  },
  {
    "bug_location": "Raft-Engine",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Performance jitter caused by Raft-Engine's load rewrite mechanism under heavy read-write workloads",
    "issue_number": 14462,
    "title": "[Dynamic Regions] Raft-Engine rewrite load results in qps jitter. "
  },
  {
    "bug_location": "RaftEngine",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Config"
    ],
    "root_cause": "Mismatch between max_prefill_count (96) and large purge-threshold (200GB), causing ineffective log recycling and performance degradation",
    "issue_number": 14468,
    "title": "[Dynamic Regions] max_prefill_count conflicts with large purge-threshhold"
  },
  {
    "bug_location": "PD Scheduler",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Default PD scheduler configuration does not effectively distribute write load across TiKV nodes when using QPS and keys as balancing metrics",
    "issue_number": 14469,
    "title": "[Dynamic Regions] TiKV CPUs are not balanced under heavy write with default PD scheduler setting"
  },
  {
    "bug_location": "RocksDB/Compaction",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Compaction flow spikes causing IO latency disruptions in raft-engine, leading to unstable QPS and latency during insert operations",
    "issue_number": 14470,
    "title": "[Dynamic Regions] qps and latency is unstable for oltp_insert"
  },
  {
    "bug_location": "Raft Engine Recovery",
    "severity": 3,
    "categories": [
      "Upgrade",
      "Performance"
    ],
    "root_cause": "Slow log recovery process during cluster upgrade, potentially due to insufficient recovery threads for large threshold (200GB)",
    "issue_number": 14472,
    "title": "[Dynamic Regions] upgrade from v6.6. to v7.0 is slow due to Recovering raft logs"
  },
  {
    "bug_location": "Raft Engine Recovery",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Recovery"
    ],
    "root_cause": "Slow log recovery process with high write traffic, potential log file corruption during restart causing extended recovery time",
    "issue_number": 14478,
    "title": "[Dynamic Regions] Raft log recovery takes 450s after restart, even with 16 raft-engine recovery threads"
  },
  {
    "bug_location": "GC Worker",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Recovery"
    ],
    "root_cause": "Invalid hex encoding during database drop operation, likely caused by incorrect data transformation in garbage collection process",
    "issue_number": 14480,
    "title": "GC delete range report error\uff1aencoding/hex: invalid byte"
  },
  {
    "bug_location": "Raft Engine / Recovery Module",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Recovery"
    ],
    "root_cause": "Extremely slow Raft log replay mechanism causing prolonged TiKV recovery times, with low QPS and inefficient log entry reading",
    "issue_number": 14481,
    "title": "[Dynamic Regions] replay raft log after restart is too slow (> 5hour in some cases)"
  },
  {
    "bug_location": "raft-engine",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Inefficient log append and rewrite flow causing high disk I/O overhead, particularly on storage-constrained environments like GCP pd-ssd disks",
    "issue_number": 14487,
    "title": "[Dynamic Regions] raft-engine with v2 is slower than baseline due to append flow on GCP pd-ssd disk"
  },
  {
    "bug_location": "RocksDB Block Cache / Filter Management",
    "severity": 2,
    "categories": [
      "Performance",
      "Storage",
      "Memory"
    ],
    "root_cause": "Bloom filter block size grows excessively under large data volumes with partitioned-raft-kv, causing block cache overwhelming",
    "issue_number": 14496,
    "title": "[Dynamic Regions] When the TiKV has a large data size, filter and index block can be too big"
  },
  {
    "bug_location": "raftstore_v2::operation::bucket",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Attempted to unwrap a None value during region bucket refresh, indicating a potential nil/uninitialized reference handling issue",
    "issue_number": 14506,
    "title": "[Dynamic Regions] bucket refresh panic"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 14507,
    "title": "resource_control: virtual time overflow after update_min_virtual_time"
  },
  {
    "bug_location": "TiKV Region",
    "severity": 4,
    "categories": [
      "Replication",
      "Network",
      "Storage"
    ],
    "root_cause": "Region unavailability during datacenter switchover in DR auto-sync mode, potentially caused by replication or synchronization issues",
    "issue_number": 14508,
    "title": "[dr-autosync] query hit 9005 after switch to backup dc in sync mode"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 14514,
    "title": "[Dynamic Regions] Large block cache filter insert when Single TiKV has 6TB data"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Network",
      "Replication",
      "LoadBalance"
    ],
    "root_cause": "Network partition causing region unavailability during distributed communication between TiKV nodes",
    "issue_number": 14532,
    "title": "[Dynamic Regions] workload report 9005: Region is unavailable during one tikv network partition form other tikv"
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential disk space exhaustion due to uncontrolled compaction process and high pending compaction bytes limit",
    "issue_number": 14540,
    "title": "Should avoid disk explosion caused by compaction"
  },
  {
    "bug_location": "Raft Peer/Store Component",
    "severity": 4,
    "categories": [
      "Replication",
      "Network",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of store unreachability, allowing Raft messages from failed nodes to incorrectly transition region peer states from Chaos to Ordered",
    "issue_number": 14547,
    "title": "Seems region leaders can lose after one tikv instance fail"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Network"
    ],
    "root_cause": "Raft tick scheduling logic fails to handle scenarios where a peer applies a snapshot, potentially causing region unavailability during network partitions",
    "issue_number": 14548,
    "title": "[Dynamic Regions] raft tick may be missing"
  },
  {
    "bug_location": "storage/txn/actions/commit.rs",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect handling of stale pessimistic lock requests when committing locks, potentially overwriting committed transaction records and causing data inconsistency",
    "issue_number": 14551,
    "title": "Stale pessimistic lock requests in force-lock mode may lead to data corruption or in consistency after resolving lock"
  },
  {
    "bug_location": "Region Split/Batch Split Component",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Batch region split causing significant latency overhead when TTL (Time-To-Live) is enabled, potentially due to inefficient split synchronization or coordination mechanisms",
    "issue_number": 14566,
    "title": "[Dynamic Regions] batch split cause significant  latency jitter"
  },
  {
    "bug_location": "RaftstoreV2",
    "severity": 3,
    "categories": [
      "Replication",
      "Performance"
    ],
    "root_cause": "Peer status update mechanism is not synchronizing quickly enough after region split operation, causing prolonged pending peer state",
    "issue_number": 14572,
    "title": "raftstore-v2: peer status not update in time after region split."
  },
  {
    "bug_location": "Snapshot/Tablet Management",
    "severity": 5,
    "categories": [
      "Storage",
      "Performance",
      "Replication"
    ],
    "root_cause": "Failed snapshot transfers are not being properly cleaned up, causing accumulation of large snapshot files in tablet_snap directory",
    "issue_number": 14581,
    "title": "[Dynamic Regions] tablet_snap takes 1.4TB and TiKV running out of space"
  },
  {
    "bug_location": "Encryption Key Management",
    "severity": 4,
    "categories": [
      "Security",
      "Storage"
    ],
    "root_cause": "Potential key ID collision mechanism that can inadvertently overwrite existing encryption keys",
    "issue_number": 14585,
    "title": "encryption key id collision will erase old key"
  },
  {
    "bug_location": "Index Merge Query Processing",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Potential schema handling issue during index merge query with partitioned table, causing column value retrieval failure",
    "issue_number": 14619,
    "title": "TiKV reports error like \"3th column is missing value\" when add `ExtraPhysTblID` in schema"
  },
  {
    "bug_location": "Unified Read Pool",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Incorrect tracking of running tasks in unified read pool, causing tasks to remain queued and not being properly cleared after workload stops",
    "issue_number": 14633,
    "title": "[Dynamic Regions] the value of metric tikv_unified_read_pool_running_tasks is wrong causing server is busy error"
  },
  {
    "bug_location": "Transaction",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Incomplete transaction status verification mechanism when handling check_txn_status across different client versions",
    "issue_number": 14636,
    "title": "Support verifying whether it's primary when handling check_txn_status"
  },
  {
    "bug_location": "Backup & Restore (BR) Region Splitting",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance",
      "Replication"
    ],
    "root_cause": "Excessive region allocation during restore process causing oneshot cancellation and split region failures",
    "issue_number": 14641,
    "title": "br restore fail for \"split region failed\""
  },
  {
    "bug_location": "pd_client/resolve",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Invalid handling of store ID 0 during region split or address resolution, causing repeated failed resolution attempts",
    "issue_number": 14644,
    "title": "[Dynamic Regions] TiKV keeps resolving store 0 address after split"
  },
  {
    "bug_location": "Metrics/Monitoring",
    "severity": 2,
    "categories": [
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Missing instrumentation for `check_leader` gRPC request metrics in TiKV monitoring system",
    "issue_number": 14658,
    "title": "`check_leader` requests are missing in the metrics"
  },
  {
    "bug_location": "raftstore-v2/operation/command/write/ingest",
    "severity": 5,
    "categories": [
      "Storage",
      "Recovery",
      "CodeBug"
    ],
    "root_cause": "SST file corruption or missing file during dynamic region recovery after TiKV node failure",
    "issue_number": 14663,
    "title": "[Dynamic Regions] tikv panic repeatedly after one tikv failure for 10m and recover"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 14664,
    "title": "[Dynamic Regions] stale read is not working in raftstore v2"
  },
  {
    "bug_location": "Raft Engine / Write Performance",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Potential configuration parameter changes in 7.1.0-rc that negatively impact write performance, specifically in write duration and QPS metrics",
    "issue_number": 14667,
    "title": "[dbaas] The performance of version 7.1.0-rc has regressed by 13% in the sysbench insert scenario,compared with version 7.0.0"
  },
  {
    "bug_location": "PessimisticLock",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Potential performance degradation in pessimistic locking mechanism during long-running TPCC workload, causing latency spikes",
    "issue_number": 14675,
    "title": "[Dynamic Regions] latency spike caused by PessimisticLock when running tpcc stability workload with v2"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 14677,
    "title": "simplify the process of running tikv test in docker"
  },
  {
    "bug_location": "Raftstore",
    "severity": 4,
    "categories": [
      "Replication",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Incomplete implementation of flashback feature in raftstore v2 configuration",
    "issue_number": 14684,
    "title": "[Dynamic Regions] flashback is not working in raftstore v2"
  },
  {
    "bug_location": "Region Split Component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Incomplete handling of peer removal metadata during region split operation, causing unintended data loss of removed_records tracking",
    "issue_number": 14689,
    "title": "[Dynamic Regions] Region split removes records unexpectedly "
  },
  {
    "bug_location": "Raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Missing epoch validation during read requests in Raftstore v2, allowing potential stale or incorrect data reads after region split",
    "issue_number": 14699,
    "title": "[Dynamic Regions] Response read without checking epoch"
  },
  {
    "bug_location": "raftstore/v2",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Incomplete implementation of follower read functionality in raftstore v2",
    "issue_number": 14701,
    "title": "[Dynamic Regions] Follower read is not working in raftstore v2"
  },
  {
    "bug_location": "DR Auto-Sync Component",
    "severity": 3,
    "categories": [
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Incomplete state transition logic when a single voter node is down in a multi-datacenter configuration",
    "issue_number": 14704,
    "title": "[dr-autosync] after down one voter tikv in backup dc\uff0creplicatiton state didn't change to async"
  },
  {
    "bug_location": "Transaction",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Incompatibility between autocommit point get read and follower read mechanisms causing potential linearizability violation",
    "issue_number": 14715,
    "title": "txn: autocommit point get read is not compatible with follower read"
  },
  {
    "bug_location": "Region Leader Transfer",
    "severity": 4,
    "categories": [
      "Replication",
      "Network",
      "LoadBalance"
    ],
    "root_cause": "Large log gap between leader and follower during network partition prevents successful leader transfer, with pending configuration changes blocking transfer command execution",
    "issue_number": 14740,
    "title": "[Dynamic Regions] tikv leader can not balance after one tikv network partition for 50m and recover"
  },
  {
    "bug_location": "raftstore_v2/apply_trace",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Recovery"
    ],
    "root_cause": "Option unwrap on a None value during Raft log engine recovery, likely due to incorrect handling of flushed index in atomic group processing",
    "issue_number": 14743,
    "title": "[Dynamic Regions]: tikv panic during the restart"
  },
  {
    "bug_location": "sst_importer",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "SST file cleanup mechanism fails when region topology changes during lightning import process, causing potential file leakage",
    "issue_number": 14745,
    "title": "sst_importer may leak sst files once lightning encounters error due to wrong range loaded"
  },
  {
    "bug_location": "raftstore-v2",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Unimplemented functionality in Raft message handling when using TiFlash, causing a panic in the Raft peer operation",
    "issue_number": 14749,
    "title": "[Dynamic Regions] tikv panic when using tiflash"
  },
  {
    "bug_location": "Region Management",
    "severity": 3,
    "categories": [
      "Storage",
      "Replication",
      "Performance"
    ],
    "root_cause": "Potential issue with region splitting or merging logic causing excessive empty regions",
    "issue_number": 14752,
    "title": "Empty Regions are too much"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Potential timeout configuration issue with long-running analytical queries on learner nodes with high CPU utilization",
    "issue_number": 14753,
    "title": "Coprocessor task terminated due to exceeding the deadline in tispark scenario"
  },
  {
    "bug_location": "raftstore-v2/apply_trace.rs",
    "severity": 4,
    "categories": [
      "Storage",
      "Recovery",
      "CodeBug"
    ],
    "root_cause": "Unable to retrieve region state during TiKV node restart, potentially due to data corruption or inconsistent region metadata",
    "issue_number": 14756,
    "title": "[Dynamic Regions] TiKV restart fails \"failed to get region state\""
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Network"
    ],
    "root_cause": "Accumulation of in-flight coprocessor tasks during network latency causing excessive memory consumption",
    "issue_number": 14759,
    "title": "[Read-Only Replica] tikv oom when network delay between read-only instance and tispark"
  },
  {
    "bug_location": "Raft Engine / Encryption Module",
    "severity": 4,
    "categories": [
      "Storage",
      "Recovery",
      "CodeBug"
    ],
    "root_cause": "Race condition during file deletion and encryption metadata management, where file system deletion and metadata removal are not atomic, potentially causing inconsistent state during process interrupti",
    "issue_number": 14761,
    "title": "Raft engine encounters log file magic header mismatch"
  },
  {
    "bug_location": "Lightning Import Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "SST file leakage during parallel large-scale data import, causing unexpected disk space consumption in /var/lib/tikv/data/import directory",
    "issue_number": 14763,
    "title": "lightning import fails due to sst leakake in /var/lib/tikv/data/import "
  },
  {
    "bug_location": "Transaction/Write CF",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Upgrade"
    ],
    "root_cause": "Incomplete handling of lock record versioning during cluster upgrades, causing potential performance degradation due to accumulated lock records without proper last change timestamp tracking",
    "issue_number": 14780,
    "title": "Clusters upgraded from <6.5 to >=6.5 may suffer additional bad performance from accumulated lock records"
  },
  {
    "bug_location": "Region Leader Transfer",
    "severity": 3,
    "categories": [
      "CodeBug",
      "LoadBalance",
      "Replication"
    ],
    "root_cause": "Leader transfer blocked by pending admin commands that modify region epoch",
    "issue_number": 14785,
    "title": "[Dynamic Regions] transfer leader may be blocked on a slow TiKV node"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 3,
    "categories": [
      "Performance",
      "Replication"
    ],
    "root_cause": "Leader fails to set 'ready' state after serving read index request, causing follower read response to be delayed until next heartbeat",
    "issue_number": 14786,
    "title": "[Dynamic Regions] Follower read may be slow when leader is in idle."
  },
  {
    "bug_location": "Region Split/Snapshot Generation Component",
    "severity": 3,
    "categories": [
      "Performance",
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Inefficient handling of slow region splits in follower replica, triggering unnecessary snapshot generation when split operation takes longer than expected",
    "issue_number": 14787,
    "title": "[Dynamic Regions] A slow (>1s) split in follower may cause leader to generate a snapshot"
  },
  {
    "bug_location": "RaftStore",
    "severity": 5,
    "categories": [
      "Memory",
      "Performance",
      "Replication"
    ],
    "root_cause": "Memory leak in entry cache with potential race condition during dynamic region operations, causing unbounded memory growth during high-load TPC-C workload",
    "issue_number": 14798,
    "title": "[Dynamic Regions] memory of entry cache keep increasing and raftstore race condition and oom"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 14808,
    "title": "[storage]: Raftstore error's err_other is confusing"
  },
  {
    "bug_location": "CDC (Change Data Capture) Old Value Cache",
    "severity": 3,
    "categories": [
      "Memory",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "The old value cache does not properly enforce memory quota limits when inserting large or numerous values, allowing cache size to significantly exceed configured memory constraints",
    "issue_number": 14815,
    "title": "cdc's old value cache sometimes exceeds the quota"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 14827,
    "title": "Crashing due to index out of bounds in elt function"
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Insufficient flow control mechanism when ingesting multiple SST files during snapshot, leading to potential L0 file count explosion and triggering unnecessary flow control",
    "issue_number": 14828,
    "title": "Multiple SST files snapshot may trigger flow control"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 14831,
    "title": "TiKV resolved ts gap keep increasing"
  },
  {
    "bug_location": "Scheduler/Metrics",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction",
      "Monitoring"
    ],
    "root_cause": "Inconsistent duration tracking for write commands, particularly during latch conflicts where task timeouts do not immediately release resources or update metrics accurately",
    "issue_number": 14838,
    "title": "metric: the scheduler and latch duration may not be recorded with the grpc duration"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 14839,
    "title": "storage: backport the stale read enhancement and bug fix to release 6.5"
  },
  {
    "bug_location": "Raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication",
      "Performance"
    ],
    "root_cause": "Entry cache not being pre-warmed during leader transfer in Raftstore v2",
    "issue_number": 14845,
    "title": "[Dynamic Regions] Entry cache does not warm up before transferring leader"
  },
  {
    "bug_location": "Stale Read Component",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "Unbounded memory growth of resolved timestamp map for tracking locks during large transactions, causing potential out-of-memory conditions",
    "issue_number": 14864,
    "title": "Resolved ts in stale read may cause TiKV OOM"
  },
  {
    "bug_location": "CDC (Change Data Capture)",
    "severity": 3,
    "categories": [
      "Performance",
      "Replication"
    ],
    "root_cause": "Potential performance bottleneck during region split causing resolved timestamp tracking delay",
    "issue_number": 14870,
    "title": "CDC resolved ts has 20s+ spike after region split"
  },
  {
    "bug_location": "Raft Message Handling",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Incorrect handling of local MsgUnreachable Raft messages during leader eviction, causing unnecessary log messages without significant system impact",
    "issue_number": 14874,
    "title": "Too many \"cannot step raft local message\" when RejectRaftAppend occurs"
  },
  {
    "bug_location": "PD (Placement Driver) and Store Communication",
    "severity": 4,
    "categories": [
      "Network",
      "Recovery",
      "Transaction"
    ],
    "root_cause": "Store (specifically store 5) failed to report status to PD during unsafe recovery process, causing recovery procedure timeout and failure",
    "issue_number": 14883,
    "title": "[dr-autosync]online recover failed due to one store have not reported to PD"
  },
  {
    "bug_location": "RocksDB Compaction Layer",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Default compaction guard minimum output file size (8MB) is too large, causing ineffective compaction filtering and potentially triggering massive data compaction when Time-To-Live (TTL) is enabled",
    "issue_number": 14888,
    "title": "compaction-guard-min-output-file-size default value 8MB is too big that could leads to huge compaction"
  },
  {
    "bug_location": "RaftStore",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "CheckLongUncommitted tick mechanism is incorrectly disabled in raftstore v1 implementation",
    "issue_number": 14893,
    "title": "CheckLongUncommitted tick is missing"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 14898,
    "title": "[Dynamic Regions] online qps drop a lot when drop other database"
  },
  {
    "bug_location": "RocksDB/Storage Layer",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Inconsistent state between memtable and flushed index during delete range operation, causing potential blocking of admin flush operations",
    "issue_number": 14904,
    "title": "[Dynamic Regions] after delete range, admin flushed index may be blocked"
  },
  {
    "bug_location": "backup-stream",
    "severity": 5,
    "categories": [
      "Network",
      "CodeBug",
      "Recovery"
    ],
    "root_cause": "Unhandled future polling after resolution, causing panic during network partition in backup stream component",
    "issue_number": 14910,
    "title": "one tikv panic when inject one tidb network partition"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Performance",
      "Upgrade",
      "Replication"
    ],
    "root_cause": "Potential issue with dynamic region handling during rolling update process causing significant QPS degradation",
    "issue_number": 14912,
    "title": "[Dynamic Regions] online qps almost drop to zero when rolling update"
  },
  {
    "bug_location": "Import Configuration Module",
    "severity": 3,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Configuration parameter 'import.num-threads' does not support dynamic online reconfiguration, despite being marked as changeable",
    "issue_number": 14934,
    "title": "Import: num-thread is not onlineConfig"
  },
  {
    "bug_location": "resource_control/resource_group.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential integer overflow or incorrect threshold calculation in resource group virtual token reset logic",
    "issue_number": 14943,
    "title": "test_reset_resource_group_vt_overflow failed when executing `make test`"
  },
  {
    "bug_location": "raftstore-v2/operation/command/write/ingest",
    "severity": 5,
    "categories": [
      "Storage",
      "Recovery",
      "CodeBug"
    ],
    "root_cause": "SST file missing during region restoration after backup and restore process, likely due to incomplete or interrupted file transfer during backup/restore operation",
    "issue_number": 14946,
    "title": "[Dynamic Regions] after br full restored to multirocksdb cluster failed and tikv restarted, tikv panic"
  },
  {
    "bug_location": "RocksDB Lock ColumnFamily",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Reduced compaction frequency in dynamic regions causing increased delete_skipped-index and performance degradation, specifically in lock column family with larger write buffer size",
    "issue_number": 14947,
    "title": "[Dynamic Regions] v2 60% QPS drop compare with v1 with ~5x delete_skipped-index"
  },
  {
    "bug_location": "backup-stream",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Removed failpoints causing test failures in integration tests",
    "issue_number": 14958,
    "title": "backup-stream: failpoint tests may fail"
  },
  {
    "bug_location": "KV Get Operation",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Potential performance regression in dynamic regions implementation for YCSB workloads, specifically impacting kv_get operation efficiency",
    "issue_number": 14966,
    "title": "[Dynamic Regions] v2 ~5% QPS drop compare with v1, ycsb workload"
  },
  {
    "bug_location": "RaftLogEngine",
    "severity": 5,
    "categories": [
      "Storage",
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Checksum mismatch in Raft log engine, indicating potential data corruption during log entry storage or retrieval",
    "issue_number": 14969,
    "title": "TiFlash proxy panic with \"Corruption: Checksum expected 369222644 but got 1144459145\""
  },
  {
    "bug_location": "PD Client / Region Metadata Management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Recovery",
      "Replication"
    ],
    "root_cause": "Incomplete region metadata cleanup during online recovery, causing inconsistent region state between PD and TiKV stores",
    "issue_number": 14974,
    "title": "[dr-autosync] tikv flashback to version panic due to online recover didn't clean region meta data in etcd"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Replication",
      "Recovery",
      "Transaction"
    ],
    "root_cause": "Potential race condition during sync recovery process causing region unavailability, possibly related to region split/merge during async mode replication",
    "issue_number": 14975,
    "title": "[dr-autosync] during sync_recover, workload report error: Region is unavailable"
  },
  {
    "bug_location": "raftstore-v2/region_merge",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Invalid argument during tablet merge with overlapping database ranges, causing panic in region merge process",
    "issue_number": 14992,
    "title": "[Dynamic Regions] panic during region merge fails to merge tablet"
  },
  {
    "bug_location": "Region Statistics Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Potential issue with dynamic region bucket statistics calculation during coprocessor requests, causing incorrect or zero statistics tracking",
    "issue_number": 14998,
    "title": "[Dynamic Regions]: the statistics of region buckets can't work well for coprocessor request "
  },
  {
    "bug_location": "Import SST Service",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "Recovery"
    ],
    "root_cause": "Race condition between import job cancellation and TiKV import mode exit, causing premature SST file cleanup before import process fully stops",
    "issue_number": 15003,
    "title": "tikv panic when tikv exit the import mode but import process is still not stopped during \"import into\" command is cancelled"
  },
  {
    "bug_location": "sysinfo/src/linux/cpu.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance",
      "Memory"
    ],
    "root_cause": "Index out of bounds error when dynamically detecting CPU cores without proper array bounds checking during hardware information retrieval",
    "issue_number": 15006,
    "title": "diagnostic: non-stop cpu expansion causes index out of bounds error"
  },
  {
    "bug_location": "Region Scheduler",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Dynamic region scheduling mechanism fails to evenly distribute write load across TiKV stores during TPCC workload preparation",
    "issue_number": 15011,
    "title": "[Dynamic Region] Write hot can't be scheduled with the tpcc preapre insert"
  },
  {
    "bug_location": "tikv_util component",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Multiple compilation errors related to import conflicts, type mismatches, and missing dependencies when building on FreeBSD platform",
    "issue_number": 15015,
    "title": "tikv_util: Building on FreeBSD fails"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Storage",
      "Performance"
    ],
    "root_cause": "Uneven data distribution during network partition recovery, causing capacity imbalance between TiKV nodes",
    "issue_number": 15018,
    "title": "[Dynamic Regions] the available size is a big gap between tikvs and leading to the capacity of one tikv is insufficient "
  },
  {
    "bug_location": "server/raftkv",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Async write callback not properly handled when encountering errors from failpoints, causing potential panic in transaction processing",
    "issue_number": 15020,
    "title": "Enabling failpoint `raftkv_early_error_report`, `cleanup`, `pessimistic_prewrite` sometimes leads to tikv panic"
  },
  {
    "bug_location": "Scheduling",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Potential type conversion issue causing significant performance degradation during version upgrade from 4.0.8 to 6.5.2, resulting in tp999 latency increasing from 50ms to 800+ms",
    "issue_number": 15021,
    "title": "Will upgrading from v4.0.8 to 6.5.2 meet this bug #14780 ?"
  },
  {
    "bug_location": "Backup and Restore (BR) Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Replication"
    ],
    "root_cause": "Potential timeout or performance issue during checksum calculation during database restore operation, possibly related to large dataset or concurrency configuration",
    "issue_number": 15022,
    "title": "[Dynamic Regions] br restore checksum failure on v2"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Checkpoint Mechanism",
    "severity": 4,
    "categories": [
      "Network",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Network partition causing resolved timestamp synchronization disruption, leading to prolonged checkpoint lag recovery",
    "issue_number": 15031,
    "title": "[Dynamic Regions] pitr checkpoint ts lag keep increasing after simulate tikv network partition for 50m and recover"
  },
  {
    "bug_location": "Raftstore/LeaseRead",
    "severity": 5,
    "categories": [
      "Replication",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Lease invalidation mechanism fails during node isolation and restart scenarios, allowing potential concurrent leaders with invalid lease validation",
    "issue_number": 15035,
    "title": "Leader holds an invalid lease if one node is isolated and one node restarts"
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Excessive L0 file compaction causing rapid disk space consumption during data import, leading to 'No space left on device' error",
    "issue_number": 15036,
    "title": "tikv exit \"IO error: No space left on device: While appending to file: /var/lib/tikv/data/db/2355792.log: No space left on device\""
  },
  {
    "bug_location": "raftstore/store/peer_storage.rs",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Snapshot file corruption during I/O hang and recovery, causing checksum mismatch in snapshot files",
    "issue_number": 15044,
    "title": "tikv panic repeatedly after inject tikv io hang and recover"
  },
  {
    "bug_location": "Transaction",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "Potential performance regression in commit log duration for dynamic regions in TiKV master version, causing approximately 8% QPS drop during sysbench oltp_read_write workload",
    "issue_number": 15051,
    "title": "[Dynamic Regions] v2 ~8% QPS drop compare with v1, sysbench oltp_read_write"
  },
  {
    "bug_location": "Storage/Encryption",
    "severity": 5,
    "categories": [
      "Storage",
      "Network",
      "Recovery"
    ],
    "root_cause": "Encryption key management failure during network isolation and recovery scenario, potentially causing data corruption or key lookup issues",
    "issue_number": 15052,
    "title": "[Dynamic Region] start ticdc changefeed and pitr task, inject one tikv network isolation 10min and recovery, 3 hours later, one tikv panic repeatedly"
  },
  {
    "bug_location": "SST Importer",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Incorrect SST file deletion logic during region split and import process, causing premature deletion of SST files needed for Raft ingest",
    "issue_number": 15053,
    "title": "tikv panic because SST files of sst_importer is deleted before raft ingest"
  },
  {
    "bug_location": "Region Request/Client-Go",
    "severity": 4,
    "categories": [
      "Transaction",
      "LoadBalance",
      "Network"
    ],
    "root_cause": "Client-side leader redirection logic fails to correctly switch to the current region leader, causing request retry and potential performance degradation",
    "issue_number": 15056,
    "title": "[Dynamic Region] region unavailable when prepare tpcc data"
  },
  {
    "bug_location": "Raft Replication Component",
    "severity": 5,
    "categories": [
      "Replication",
      "Network",
      "Transaction"
    ],
    "root_cause": "Potential issue with log synchronization and recovery mechanism after network partition, causing persistent log lag that cannot self-heal",
    "issue_number": 15059,
    "title": "[Dynamic Regions] raft log lag more and more after inject one tikv network partition for 50m and recover"
  },
  {
    "bug_location": "Storage/Memory Management",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "Potential memory pressure and inefficient resource management under high IO latency and large data workload conditions",
    "issue_number": 15061,
    "title": "[Dynamic Region] tikv oom when inject 100ms io delay chaos"
  },
  {
    "bug_location": "Lock Manager / Transaction Layer",
    "severity": 3,
    "categories": [
      "Transaction",
      "Storage",
      "Performance"
    ],
    "root_cause": "In-memory pessimistic locks are stored in a hash table that is not scanned during lock resolution, causing potential lock visibility and garbage collection issues",
    "issue_number": 15066,
    "title": "In-memory pessimistic locks are invisible to scan_lock requests"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 15074,
    "title": "[Dynamic Region] TiFlash proxy crash for failed to get regions state"
  },
  {
    "bug_location": "Build System",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Potential dependency or build configuration issue with tipb library during build process",
    "issue_number": 15075,
    "title": "build error failed to run custom build command"
  },
  {
    "bug_location": "components/encryption/src/io.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Security"
    ],
    "root_cause": "Partial write implementation in encryption writer breaks block encryption integrity, potentially causing data corruption during encryption process",
    "issue_number": 15080,
    "title": "encryption: encryption may corrupt data when partial write"
  },
  {
    "bug_location": "Resolved-ts Tracking Component",
    "severity": 3,
    "categories": [
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Limited observability of resolved timestamp progression and potential blocking factors",
    "issue_number": 15082,
    "title": "Improve observability of resolved-ts and safe_ts"
  },
  {
    "bug_location": "Region Management Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Potential incompatibility or error handling issue with multi-rocksdb configuration during dynamic region processing",
    "issue_number": 15091,
    "title": "[Dynamic Region] TiDB fails to start, \"Internal error: received illegal TiKV response\""
  },
  {
    "bug_location": "Raft Replication Component",
    "severity": 4,
    "categories": [
      "Network",
      "Replication",
      "Performance"
    ],
    "root_cause": "Network partition causing unexpected Raft log synchronization delays without proper snapshot recovery mechanism",
    "issue_number": 15105,
    "title": "[Dynamic Regions] : raft log lag increased after network partition."
  },
  {
    "bug_location": "Unified Read Pool",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Potential resource exhaustion or task queue bottleneck during large-scale data checksum operation after restore",
    "issue_number": 15106,
    "title": "[Dynamic Regions] running task in unified read pool keep increasing during checksum after restore 1TB sysbench data"
  },
  {
    "bug_location": "tikv/server/debug.rs",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Recovery",
      "Transaction"
    ],
    "root_cause": "Retry mechanism in flashback process creates an infinite loop when some regions have already completed flashback state, preventing successful recovery",
    "issue_number": 15107,
    "title": "[dr-autosync] tikv flashback hung more than 12h"
  },
  {
    "bug_location": "storage/txn/actions/common.rs",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Incorrect assumption about RocksDB snapshot repeatability during compaction, causing unexpected behavior when tracking transaction write versions",
    "issue_number": 15109,
    "title": "Tikv panic for assertion failed: estimated_versions_to_last_change > 0"
  },
  {
    "bug_location": "Recovery/Logging",
    "severity": 3,
    "categories": [
      "Recovery",
      "Performance"
    ],
    "root_cause": "Excessive log generation during online recovery process in disaster recovery mode, potentially causing storage and performance overhead",
    "issue_number": 15112,
    "title": "[dr-autosync] too much logs during online recovery after switching to back dc in sync_recovery mode"
  },
  {
    "bug_location": "Flow Control / Region Management",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Unexpected dynamic region flow control mechanism causing QPS instability during read-only workloads",
    "issue_number": 15113,
    "title": "[Dynamic Regions] online qps is unstable for the unexpected flow control"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 15114,
    "title": "test_gc_removed_peer fails"
  },
  {
    "bug_location": "Follower Read/Stale Read Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Replication",
      "Transaction"
    ],
    "root_cause": "When a TiKV node recovers after a long network partition, follower-read requests cannot be served due to significant log replication lag, causing extended QPS drop while waiting for log synchronizatio",
    "issue_number": 15120,
    "title": "enable stale read and closest-replicas\uff0csimulate one of tikv network partition last for 50min\uff0cqps drop to zero for more than 1h after fault recover"
  },
  {
    "bug_location": "Snapshot Restore",
    "severity": 4,
    "categories": [
      "Replication",
      "Network",
      "Recovery"
    ],
    "root_cause": "Message dropping during snapshot restoration process, potentially causing election message loss and restoration stalling when handling large number of regions",
    "issue_number": 15122,
    "title": "snap_restore: when there are message dropping, snap_restore may fail"
  },
  {
    "bug_location": "Region Split/Memory Management",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Lack of memory pressure control during region split operations, allowing excessive region creation that can lead to out-of-memory conditions",
    "issue_number": 15125,
    "title": "region count should not be throttled when memory usage exceeds a threshold "
  },
  {
    "bug_location": "Performance/Dynamic Regions",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Potential performance regression in dynamic region handling causing significant QPS drop in YCSB workload",
    "issue_number": 15132,
    "title": "[Dynamic Regions] v2 62% QPS drop compare with v1, ycsb workloade"
  },
  {
    "bug_location": "GitHub Issue Template",
    "severity": 2,
    "categories": [
      "Human",
      "Config"
    ],
    "root_cause": "Incomplete issue reporting template preventing clear problem documentation",
    "issue_number": 15139,
    "title": "TiKV repo's issue template is not correct"
  },
  {
    "bug_location": "RawKV API v2",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "TTL check mechanism not properly implemented for RawKV API v2, causing expired data not being cleaned up as configured",
    "issue_number": 15142,
    "title": "rawkv: `ttl-check-poll-interval` does not take effect on RawKV API v2"
  },
  {
    "bug_location": "Flow Controller",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "Replication"
    ],
    "root_cause": "Network partition recovery mechanism causing persistent performance degradation in QPS after fault injection and recovery",
    "issue_number": 15145,
    "title": "[Dynamic Region] QPS continues to decrease after inject one tikv network partition for 50m and recover due to flow controller"
  },
  {
    "bug_location": "Region/Bucket Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Replication"
    ],
    "root_cause": "Delayed bucket updates after SST file ingestion causing key-value range metadata to become stale",
    "issue_number": 15147,
    "title": "[Dynamic Regions] Let TiKV update buckets ASAP after ingesting SST files"
  },
  {
    "bug_location": "raftstore-v2/operation/command/admin/merge",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Assertion failure when merging regions due to missing availability context, likely caused by version incompatibility between TiKV and TiFlash",
    "issue_number": 15151,
    "title": "[Dynamic Regions] tikv crash for assertion failed: resp.has_availability_context() with tiflash"
  },
  {
    "bug_location": "async-read-worker",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Race condition between peer destruction and snapshot generation, causing RocksDB checkpoint generation to get stuck when background work is paused during peer destruction",
    "issue_number": 15153,
    "title": "[Dynamic Regions] async-read-worker may get stuck in generating snapshot when a region is destroying"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 15175,
    "title": "Delay flushing raft message may cause high raft log commit duration"
  },
  {
    "bug_location": "Import/SST Service",
    "severity": 4,
    "categories": [
      "Storage",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Region replica synchronization failure during backup and restore process, causing region not found error",
    "issue_number": 15176,
    "title": "[Dynamic Regions] br full restore meets error \"region xxx not found \""
  },
  {
    "bug_location": "TiKV Storage/Transaction Layer",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Performance regression in transaction processing and storage write operations between TiDB versions 7.1.0 and 7.1.1, likely due to changes in async write and commit mechanisms",
    "issue_number": 15177,
    "title": "During a large-scale table test of TiDB, a noticeable regression in insertion performance was observed for version 7.1.1, accompanied by significant tidb duration"
  },
  {
    "bug_location": "Block Cache Configuration",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Config"
    ],
    "root_cause": "Oversized block cache configuration causing excessive memory consumption during fault testing scenarios, leading to out-of-memory (OOM) conditions",
    "issue_number": 15181,
    "title": "[Dynamic Regions] tikv oom when simulate fault testing due to the block cache configuration is too large"
  },
  {
    "bug_location": "PD (Placement Driver) Store Heartbeat Handler",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "LoadBalance"
    ],
    "root_cause": "Inefficient retry mechanism for store heartbeats causing increased load and potential resource exhaustion in large clusters",
    "issue_number": 15184,
    "title": "Store heartbeat cannot be consumed, heartbeat storm in big cluster"
  },
  {
    "bug_location": "Transaction/Replication",
    "severity": 4,
    "categories": [
      "Transaction",
      "Replication",
      "Network"
    ],
    "root_cause": "Potential issue with stale read and replica selection strategy during network partition, causing significant QPS degradation across availability zones",
    "issue_number": 15187,
    "title": "enable stale read and closest-replicas\uff0cqps of all az drop to bottom during simulating network partition between two az "
  },
  {
    "bug_location": "Scheduling/Replication",
    "severity": 4,
    "categories": [
      "Network",
      "Replication",
      "Performance"
    ],
    "root_cause": "Potential issue with stale read and replica selection strategy during network partition, causing prolonged QPS degradation in specific availability zone",
    "issue_number": 15188,
    "title": "enable stale read and closest-replicas\uff0cqps drop last for 10min on us-west-2a when injection network partition between one of tikv of us-west-2a and all other pods in cluster"
  },
  {
    "bug_location": "Raft/Follower Read",
    "severity": 4,
    "categories": [
      "Replication",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Follower log synchronization delay causing prolonged read performance degradation after node recovery",
    "issue_number": 15189,
    "title": "[Dynamic Regions] enable follower read, qps drop more than 90% lasts for 50m after fault recover\uff08one of tikv failure for 10mins\uff09"
  },
  {
    "bug_location": "RocksDB/Storage",
    "severity": 4,
    "categories": [
      "Storage",
      "Memory",
      "CodeBug"
    ],
    "root_cause": "Potential memory management issue during table creation with multi-rocksdb configuration, causing out-of-memory (OOM) conditions",
    "issue_number": 15190,
    "title": "[Dynamic Regions] tikv oom frequently when create table"
  },
  {
    "bug_location": "PITR (Point-in-Time Recovery) Checkpoint Mechanism",
    "severity": 4,
    "categories": [
      "Network",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Resolved timestamp fails to progress during network partition, causing significant checkpoint lag when network is restored",
    "issue_number": 15192,
    "title": "[Dynamic Regions] pitr checkpoint ts lag reached 88mins after fault recover when injection network partition between one of tikv and others tikv due to resolved ts do not move forward"
  },
  {
    "bug_location": "PD (Placement Driver) / Min Resolved Timestamp",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Replication"
    ],
    "root_cause": "Potential issue with min-resolved-ts tracking during DR auto-sync mode, possibly related to store heartbeat interval configuration affecting stats monitoring",
    "issue_number": 15194,
    "title": "[dr-autosync] set cluster to dr-autosync mode,  min-resolved-ts is still 0 after run some transactions"
  },
  {
    "bug_location": "raftstore-v2/operation/command/admin/merge",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Assertion failure during region merge operation when availability context is not properly populated",
    "issue_number": 15197,
    "title": "[Dynamic Regions] tikv panic for [FATAL] [lib.rs:510] [\"assertion failed: resp.has_availability_context()\"] "
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "Replication",
      "Recovery"
    ],
    "root_cause": "Read index not ready during region merge, preventing snapshot retrieval in point-in-time recovery (PITR) task",
    "issue_number": 15198,
    "title": "[Dynamic Regions] br: pitr task in error \"retry time exceeds: error failed to get initial snapshot\""
  },
  {
    "bug_location": "TxnScheduler",
    "severity": 3,
    "categories": [
      "Transaction",
      "CodeBug",
      "Recovery"
    ],
    "root_cause": "Response channel unexpectedly dropped during TiKV shutdown, causing a panic in the transaction scheduler",
    "issue_number": 15202,
    "title": "[Dynamic Regions] Response channel may be unexpectedly dropped during tikv stop"
  },
  {
    "bug_location": "raftstore-v2/region_merge",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Missing source peer during region merge checkpoint retrieval, indicating a potential synchronization or metadata tracking issue in dynamic region management",
    "issue_number": 15209,
    "title": "[Dynamic Regions] tikv panic for \"[FATAL] [lib.rs:510] [\"source peer is missing when getting checkpoint for merge [peer_id=6259] [region_id=6258]\""
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "LoadBalance"
    ],
    "root_cause": "Potential race condition or inconsistency in dynamic region splitting and metadata tracking during frequent region operations",
    "issue_number": 15210,
    "title": "[Dynamic Regions] region split may led PD returned no region"
  },
  {
    "bug_location": "Flashback/Key Scanning Logic",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Insufficient filtering of non-user write records during key scanning, causing inconsistent key retrieval across flashback phases",
    "issue_number": 15219,
    "title": "scan_latest_user_keys should skip the type of non-user write"
  },
  {
    "bug_location": "Unknown",
    "severity": 2,
    "categories": [
      "Unknown"
    ],
    "root_cause": "Insufficient information provided to definitively diagnose the specific cargo update error",
    "issue_number": 15221,
    "title": "cargo update error"
  },
  {
    "bug_location": "storage/txn/actions/check_txn_status.rs",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Unexpected transaction record state during transaction status checking, causing a panic in the transaction rollback lock handling logic",
    "issue_number": 15223,
    "title": "[Dynamic Regions] tikv panic for \"txn record found but not expected: Write { write_type: Put\""
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 15227,
    "title": "post_apply_snapshot may fail to be informed if the snapshot is cancelled"
  },
  {
    "bug_location": "Storage/CDC Component",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Replication"
    ],
    "root_cause": "Potential memory management issue during dynamic scaling of TiKV nodes, possibly related to snapshot transfer or CDC incremental scan concurrency under high load",
    "issue_number": 15240,
    "title": "[Dynamic Regions] TiKV OOM after scaling TiKV"
  },
  {
    "bug_location": "raftstore-v2/fsm/store.rs",
    "severity": 4,
    "categories": [
      "Replication",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Race condition during concurrent region merge and leader transfer operations",
    "issue_number": 15242,
    "title": "[Dynamic Regions] panic with \"region corrupted\""
  },
  {
    "bug_location": "RawStorage coprocessor plugin",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect implementation of scan function in RawStorage trait, returning only Value instead of expected KvPair",
    "issue_number": 15246,
    "title": "The scan function return in RawStorage in coprocessor_plugin does not meet expectations"
  },
  {
    "bug_location": "raftstore/coprocessor/region_info_accessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Assertion failure during region range checking, likely due to inconsistent region metadata or concurrent region changes during Jepsen monotonic test",
    "issue_number": 15253,
    "title": "[Dynamic Regions]tikv panic for \"assertion failed: `(left != right)`\\n  left: `51`,\\n right: `51`: id: 1376 start_key\""
  },
  {
    "bug_location": "Snapshot Transfer/Receive Module",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "Insufficient memory quota management during snapshot send/receive operations, causing potential Out of Memory (OOM) scenarios on systems with 16GB or less total memory",
    "issue_number": 15255,
    "title": "[Dynamic Regions] limit the memory quota in snapshot send/recv"
  },
  {
    "bug_location": "TiKV Memory Management",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "Potential memory leak or inefficient memory management during large data restore and high-concurrency workload scenarios, especially with partitioned-raft-kv engine",
    "issue_number": 15257,
    "title": "[Dynamic Regions] TiKV OOM seen after restore data of 4k tables and run workload"
  },
  {
    "bug_location": "Region Merge / Flashback Mechanism",
    "severity": 4,
    "categories": [
      "Replication",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "PD does not properly handle region flashback state changes during region merge, causing resolved timestamp blocking and preventing commit merge operations",
    "issue_number": 15258,
    "title": "[Dynamic Regions] resolved ts blocked after region merge"
  },
  {
    "bug_location": "Transaction",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Inconsistent transaction ordering and timestamp management in dynamic region transactions, causing potential causal consistency violations",
    "issue_number": 15261,
    "title": "[Dynamic Region] transaction check inconsistent "
  },
  {
    "bug_location": "raftstore-v2/operation/command/admin/merge",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Unexpected state during region merge operation, causing an assertion failure when checking merge state",
    "issue_number": 15262,
    "title": "[Dynamic Region]tikv panic for \"assertion failed: !state.has_merge_state()\""
  },
  {
    "bug_location": "Scheduler/LoadBalancer",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance",
      "Network"
    ],
    "root_cause": "Potential instability in evict-slow-trend-scheduler when network latency is introduced, causing QPS jitter and performance degradation",
    "issue_number": 15267,
    "title": "[Dynamic Regions] QPS is severe jitter after inject one of tikv network latency with enable evict-slow-trend-scheduler"
  },
  {
    "bug_location": "Dynamic Regions / Multi-RocksDB Component",
    "severity": 3,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Slow node handling limitations in distributed transaction system, where client-side request routing and transaction continuity can cause prolonged QPS drops during node performance degradation",
    "issue_number": 15268,
    "title": "[Dynamic Regions] QPS drop to zero during injection io hang to one of tikv"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "Excessive region creation causing memory exhaustion during table partitioning",
    "issue_number": 15269,
    "title": "[Dynamic Regions]tikv oom when table has a large number of partition"
  },
  {
    "bug_location": "Backup and Restore (BR) component",
    "severity": 4,
    "categories": [
      "Replication",
      "Performance",
      "Network"
    ],
    "root_cause": "Potential checkpoint timestamp synchronization issue during network partition or datacenter downtime",
    "issue_number": 15279,
    "title": "[dr-autosync] pitr checkpoint ts lag up to 15min after down backup dc "
  },
  {
    "bug_location": "RocksDB Compaction/Region Management",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Current compaction threshold mechanism does not adequately handle regions with large MVCC versions and large key-value sizes, preventing effective region splitting and compaction",
    "issue_number": 15282,
    "title": "Improve compaction check mechanism"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Large region size causing flow control during region balancing due to oversized keys and frequent updates",
    "issue_number": 15286,
    "title": "have a big key more than a half region size"
  },
  {
    "bug_location": "raftstore/entry_storage",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Potential issue with entry cache warmup logic when handling disk-related constraints or limited storage conditions",
    "issue_number": 15289,
    "title": "tikv crash for the warmup range should still be valid"
  },
  {
    "bug_location": "storage/mvcc/reader.rs",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Missing default column family data during key retrieval, causing panic when attempting to load non-existent key",
    "issue_number": 1529,
    "title": "tikv-server panics due to default cf data missing."
  },
  {
    "bug_location": "raftstore/peer_storage",
    "severity": 4,
    "categories": [
      "Storage",
      "Recovery",
      "CodeBug"
    ],
    "root_cause": "Snapshot file corruption during recovery process, causing checksum validation failure when applying snapshot after IO hang",
    "issue_number": 15292,
    "title": "tikv panic repeatedly after this tikv recover from io hang"
  },
  {
    "bug_location": "RaftStore",
    "severity": 3,
    "categories": [
      "Memory",
      "Monitoring"
    ],
    "root_cause": "Incomplete memory tracing implementation in TiKV v2 for Raft store components",
    "issue_number": 15293,
    "title": "[Dynamic Regions] Memory Trace in v2 is missing some raft store related entries"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 15296,
    "title": "snap_restore: cannot elect leader when start up."
  },
  {
    "bug_location": "resource_metering/cpu_recorder",
    "severity": 2,
    "categories": [
      "Memory",
      "CodeBug"
    ],
    "root_cause": "Memory leak in thread statistics tracking where destroyed threads are not properly cleaned up from CpuRecorder.thread_stats",
    "issue_number": 15304,
    "title": "CpuRecorder may leak memory for destroyed threads"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 15316,
    "title": "raft-store-v2: tikv does not report down peers after 1 tikv is down"
  },
  {
    "bug_location": "RocksDB Compaction",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "Excessive memory allocation during RocksDB compaction, potentially related to Ribbon filter implementation causing memory surge of approximately 3GB",
    "issue_number": 15324,
    "title": "3GB memory spike caused by RocksDB compaction"
  },
  {
    "bug_location": "Region Split/Merge Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Incorrect region size estimation during split operation, causing regions to retain previous data size instead of being reset to empty",
    "issue_number": 15326,
    "title": "Region approximate size maybe wrong after split"
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage",
      "Config"
    ],
    "root_cause": "Performance bottleneck in table creation and analysis when using multi-RocksDB configuration with split-table enabled",
    "issue_number": 15330,
    "title": "[Dynamic Regions] create table and anlyze is slow "
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "Replication",
      "Transaction",
      "Recovery"
    ],
    "root_cause": "Race condition between snapshot application and destroy peer message during region merge, causing potential node state inconsistency for learner nodes",
    "issue_number": 15333,
    "title": "TiKV may stuck/panic if a destroy peer message is handled after apply snapshot on raftstore v1"
  },
  {
    "bug_location": "TiKV Unsafe Recovery",
    "severity": 5,
    "categories": [
      "Recovery",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Timeout during unsafe recovery process when switching to backup cluster, preventing successful region leader force assignment and cluster recovery",
    "issue_number": 15346,
    "title": "[dr-autosync] online recover time out after switching to backup cluster in sync_recover mode"
  },
  {
    "bug_location": "TiKV Region",
    "severity": 4,
    "categories": [
      "Replication",
      "Transaction",
      "Recovery"
    ],
    "root_cause": "Region unavailability during sync recovery process, potentially causing transaction failures and data access interruptions",
    "issue_number": 15347,
    "title": "[dr-autosync] after running in sync_recovery for 10min, hit 9005 error "
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Global mutex causing write performance bottleneck during manifest synchronization",
    "issue_number": 15351,
    "title": "titan: avoid to hold global mutex while syncing titan manifest"
  },
  {
    "bug_location": "storage/raw/raw_mvcc.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect prefix seek implementation in RawMvccSnapshot that could potentially extract wrong prefix when using keys without timestamps, which is currently masked by the absence of PrefixExtractor in D",
    "issue_number": 15354,
    "title": "RawMvccSnapshot use prefix seek in defaultcf which not setup PrefixExtrator"
  },
  {
    "bug_location": "Monitoring/Metrics",
    "severity": 3,
    "categories": [
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Metrics reporting mechanism for raft_router and apply_router memory tracing is not correctly implemented, preventing proper tracking of alive and leak data",
    "issue_number": 15357,
    "title": "Memory Trace does not include alive or leak data for raft_router/apply_router"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 1536,
    "title": "pd: support \"http://127.0.0.1:2379\" scheme "
  },
  {
    "bug_location": "Raftstore",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "Replication"
    ],
    "root_cause": "Potential performance degradation during network partition recovery and sync mode switching, likely related to I/O pool configuration and workload handling",
    "issue_number": 15366,
    "title": "[dr-autosync] v6.5.4 QPS drop to 0 during switch sync_recovery  to sync"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Storage"
    ],
    "root_cause": "Potential metadata corruption during region heartbeat and peer creation process, likely triggered by network failures or concurrent region split/merge operations",
    "issue_number": 15370,
    "title": "v6.5.4 tikv panic for \"meta corrupted: no region for 51902 7A7480000000000000FFA45F728000000000FF00192A0000000000FA \""
  },
  {
    "bug_location": "Region Merge/Split Component",
    "severity": 3,
    "categories": [
      "Memory",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Memory trace counter not being properly reset or cleaned during repeated region merge and split operations",
    "issue_number": 15371,
    "title": "The applys count of memory trace keeps growing "
  },
  {
    "bug_location": "storage/txn/scheduler",
    "severity": 3,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Unexpected response channel drop during transaction processing, likely due to asynchronous communication failure in the transaction scheduler",
    "issue_number": 15382,
    "title": "[Dynamic Regions] response channel is unexpectedly dropped"
  },
  {
    "bug_location": "RaftStore V2 / Snapshot Processing",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Excessive file descriptor usage during snapshot processing, leading to system resource exhaustion",
    "issue_number": 15383,
    "title": "[Dynamic Regions] Jepsen test reported \"too many open files\""
  },
  {
    "bug_location": "Azure Authentication Module",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Error handling mechanism suppresses detailed error information during Azure AD token retrieval, masking the underlying authentication failure",
    "issue_number": 15384,
    "title": "the error, which is occurred when try to get azure ad token, is hidden by upper error."
  },
  {
    "bug_location": "Region Merge Component",
    "severity": 4,
    "categories": [
      "Replication",
      "Transaction",
      "Recovery"
    ],
    "root_cause": "Resolved timestamp blocking during dynamic region merge, preventing point-in-time recovery (PITR) and flashback operations",
    "issue_number": 15386,
    "title": "[Dynamic Regions] resolved ts blocked after region merge"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 15403,
    "title": "[Dynamic Regions] Load base split config can't be updated online"
  },
  {
    "bug_location": "CDC Resolver",
    "severity": 4,
    "categories": [
      "Memory",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Memory consumption by CDC resolver during large transaction processing exceeds available memory, leading to out-of-memory condition",
    "issue_number": 15412,
    "title": "CDC resolver may cause TiKV OOM"
  },
  {
    "bug_location": "Monitoring",
    "severity": 3,
    "categories": [
      "Monitoring",
      "CodeBug"
    ],
    "root_cause": "Parsing failure in procinfo package due to unexpected format in status task file, preventing context switch metrics from being correctly extracted",
    "issue_number": 15413,
    "title": "No data in Thread Voluntary/Nonvoluntary Context Switches panel"
  },
  {
    "bug_location": "backup-restore component",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Transaction"
    ],
    "root_cause": "PiTR (Point-in-Time Recovery) resolver consuming excessive memory during large transaction processing, potentially causing memory exhaustion",
    "issue_number": 15414,
    "title": "PiTR resolver may cause TiKV OOM"
  },
  {
    "bug_location": "Transaction",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Potential inconsistency in transaction timestamp ordering and commit protocol, causing observable contradictions in transaction sequences",
    "issue_number": 15420,
    "title": "[Dynamic Regions]  Jepsen reported contradiction "
  },
  {
    "bug_location": "RocksDB Configuration Management",
    "severity": 2,
    "categories": [
      "Config",
      "Performance"
    ],
    "root_cause": "Dynamic background thread configuration not properly synchronized in DBConfigManager when max_background_jobs or max_background_flushes are modified",
    "issue_number": 15424,
    "title": "can't dynamic adjust rocksdb background_compaction threads"
  },
  {
    "bug_location": "PD Client/Cluster Management",
    "severity": 4,
    "categories": [
      "Network",
      "Replication",
      "Performance"
    ],
    "root_cause": "Inefficient leader election recovery mechanism causing excessive MemberList RPC requests when PD leader is temporarily lost",
    "issue_number": 15428,
    "title": "Too many MemberList request after PD lost leader for a while"
  },
  {
    "bug_location": "RocksDB Block Cache",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "Memory leak or inefficient memory management in block cache when region count decreases, causing sustained memory growth despite reduced workload",
    "issue_number": 15430,
    "title": "TiKV's memory usage keeps increases even when region count decreases and block cache cap reaches. "
  },
  {
    "bug_location": "components/raftstore/src/store/config.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Upgrade",
      "Config"
    ],
    "root_cause": "Potential configuration parsing error during upgrade process, causing an unwrap() on a None value at line 777 of config.rs",
    "issue_number": 15438,
    "title": "v5.2.4 upgrade to master with tiup\uff0ctikv report check config fail"
  },
  {
    "bug_location": "raftstore-v2",
    "severity": 5,
    "categories": [
      "Replication",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Peer ID conflict during region merge causing unexpected peer destruction and leader election failure",
    "issue_number": 15439,
    "title": "raftstore v2: A region's peer may be stolen by another region and cause destruction of its Leader peer"
  },
  {
    "bug_location": "Storage Engine / Titan",
    "severity": 5,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Blob file corruption during data import and cluster suspend/resume operation, causing multiple deletion of blob file 49782",
    "issue_number": 15454,
    "title": "tikv occur `CrashLoopBackOff` after lighning import data, suspend cluster then cancel suspend cluster"
  },
  {
    "bug_location": "resolved_ts/resolver",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance"
    ],
    "root_cause": "HashMap not being shrunk after size reduction, causing memory accumulation over time",
    "issue_number": 15458,
    "title": "Resolver memory is not reclaimed and may cause OOM"
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Recovery",
      "CodeBug"
    ],
    "root_cause": "Blob file corruption during restart, specifically a blob file being deleted twice causing storage engine initialization failure",
    "issue_number": 15459,
    "title": "titan: tikv start fails with error"
  },
  {
    "bug_location": "Import/SST Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Recovery"
    ],
    "root_cause": "Inefficient SST file cleanup mechanism after backup and restore operations, potentially related to tablet index management in partition-raft-kv mode",
    "issue_number": 15461,
    "title": "[Dynamic Regions] many sst files left in the import directory after br restore "
  },
  {
    "bug_location": "raft-engine",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Performance jitter caused by log recycling mechanism when append-recycle files are exhausted, leading to increased write latency during log compaction",
    "issue_number": 15462,
    "title": "[Dynamic Regions] periodical performance jitter due to raft-engine log recycle"
  },
  {
    "bug_location": "Region Read Path",
    "severity": 4,
    "categories": [
      "Performance",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Race condition between read operations and region destruction causing unnecessary back-off delays when checking witness status",
    "issue_number": 15468,
    "title": "possible slow query when reading racing with region destroy"
  },
  {
    "bug_location": "IO/Storage",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Network"
    ],
    "root_cause": "Performance degradation due to IO latency injection causing significant QPS drop, potentially indicating sensitivity in dynamic region handling or IO path resilience",
    "issue_number": 15474,
    "title": "[Dynamic Regions] qps drops more than 90% during injection one of tikv io delay 10ms "
  },
  {
    "bug_location": "Coprocessor/Regexp",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Inconsistent regexp library implementation between Golang and Rust, specifically around escape character handling",
    "issue_number": 15478,
    "title": "[regexp lib] TiKV's regexp is inconsistent with that of Tidb"
  },
  {
    "bug_location": "Raftstore V2",
    "severity": 4,
    "categories": [
      "Storage",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Inconsistent index handling during stale read operations, potentially related to dynamic region management in Raftstore V2",
    "issue_number": 15480,
    "title": "[Dynamic Regions] raftstore v2: TiKV has inconsistent index"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 15483,
    "title": "sst_importer: file may not be rewritten properly while repeating restoration"
  },
  {
    "bug_location": "online_config",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Type conversion error when setting write-buffer-limit configuration, causing unexpected panic during config update",
    "issue_number": 15503,
    "title": "panic when set write-buffer-limit "
  },
  {
    "bug_location": "raftstore/store/util.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Inaccurate min resolved timestamp calculation that fails to account for uninitialized peers and newly created peers with potentially older locks",
    "issue_number": 15506,
    "title": "Store min resolved ts may not be accurate"
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Incomplete blob file cleanup mechanism during Titan fallback mode, preventing reduction of live blob files to zero",
    "issue_number": 15507,
    "title": "[Titan] the number of live blob file can not be reduced to 0 when enable fallback mode"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of multi-value index restoration logic, causing potential error when processing JSON-type indexes",
    "issue_number": 15531,
    "title": "The `need_restored_data` for multi value index should return false"
  },
  {
    "bug_location": "RaftStore/ApplyFSM",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Potential race condition or synchronization issue during memtable sealing and sequence number management in RocksDB event listener",
    "issue_number": 15534,
    "title": "[Dynamic Regions] tikv panic with error \u201csealed seqno has been flushed lock 469114 11526377 <= 11526405\\\"] [backtrace=\\\"   0: tikv_util::set_panic_hook::{{closure}}\u201d"
  },
  {
    "bug_location": "TiDB Schema Lease",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Network"
    ],
    "root_cause": "High latency in TiKV write operations causing schema lease update failure, likely due to slow disk performance",
    "issue_number": 15535,
    "title": "[Dynamic Regions] Sysbench prepare fails with error \"schema failed to update in 1 lease\""
  },
  {
    "bug_location": "raftstore/peer",
    "severity": 3,
    "categories": [
      "LoadBalance",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Follower peers cannot propose region split commands, requiring leader-only proposal processing",
    "issue_number": 15539,
    "title": "Load base split cannot work with stale read if all reads occurs on the follower"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Replication",
      "LoadBalance",
      "Recovery"
    ],
    "root_cause": "Dynamic region unavailability during cluster restart after large data import, causing region epoch mismatch and inability to start TiDB",
    "issue_number": 15542,
    "title": "[Dynamic Region] TiDB can't start for region unavailable"
  },
  {
    "bug_location": "Backup and Restore (BR) External Storage Module",
    "severity": 4,
    "categories": [
      "Storage",
      "Network",
      "CodeBug"
    ],
    "root_cause": "Conflict in multipart upload initialization with KS3 external storage, causing upload failures during large backup operations",
    "issue_number": 15555,
    "title": "Br failed backup to ks3 for The operation generates conflict uploadId, try to initiate multipart upload later"
  },
  {
    "bug_location": "Transaction/Endpoint",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Potential deadlock or ineffective lock resolution mechanism in transaction handling, causing repeated key lock warnings and potential TiKV store hang",
    "issue_number": 15561,
    "title": "[Dynamic Regions] Tikv hang for Key is locked"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction"
    ],
    "root_cause": "Lack of proper handling of max_execution_duration_ms in coprocessor task execution",
    "issue_number": 15565,
    "title": "Coprocessor task use the deadline specified in kvrpcpb::Context.max_execution_duration_ms"
  },
  {
    "bug_location": "Region Request Handling",
    "severity": 4,
    "categories": [
      "Transaction",
      "Replication",
      "Performance"
    ],
    "root_cause": "Potential region epoch synchronization issue causing stale command errors during large table analyze operation, with multiple region replicas experiencing leadership and term inconsistencies",
    "issue_number": 15566,
    "title": "[Dynamic Regions] Analyze table failed: [tikv:9005] Region is unavailable"
  },
  {
    "bug_location": "TiDB Collation/Character Set Handling",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect handling of utf8mb4_0900_bin collation during data restoration process, causing potential data access or compatibility issues",
    "issue_number": 15571,
    "title": "`utf8mb4_0900_bin` shouldn't try to read RestoredData"
  },
  {
    "bug_location": "TiDB Auto Analyze",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Context timeout during large dataset analysis, possibly due to excessive data processing time or network/resource constraints",
    "issue_number": 15575,
    "title": "[Dynamic Regions] Auto Analyze failed for context deadline exceeded"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 15579,
    "title": "Cargo reference should not point to personal repo. "
  },
  {
    "bug_location": "PD (Placement Driver) Unsafe Recovery Controller",
    "severity": 5,
    "categories": [
      "Recovery",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Timeout during unsafe store removal and region recovery process in disaster recovery scenario, preventing successful cluster restoration",
    "issue_number": 15580,
    "title": "[dr-autosync] online recover time out after switching to backup cluster in sync_recover mode"
  },
  {
    "bug_location": "backup-stream component",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Race condition in file generation and flush process with millisecond-precision timestamp causing file name collision and potential file deletion errors",
    "issue_number": 15602,
    "title": "Log Backup stopped due to \"No such file or directory\" error"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 15607,
    "title": "freaky test case test_store_disconnect_with_hibernate "
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "Replication"
    ],
    "root_cause": "Potential network partition handling issue causing persistent performance degradation in dynamic region management during large-scale data operations",
    "issue_number": 15613,
    "title": "[Dynamic Regions] Qps continuously dropped by 10% for 20 minutes in the scenario tikv network partition 50m"
  },
  {
    "bug_location": "raftstore-v2/worker/tablet",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Assertion failure in tablet destruction test, indicating potential race condition or incorrect path handling during tablet removal",
    "issue_number": 15615,
    "title": "unstable test test_destroy_missing"
  },
  {
    "bug_location": "backup-restore",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Network"
    ],
    "root_cause": "Excessive file read frequency overwhelming S3 rate limits during Point-in-Time Recovery (PITR) log restore, causing potential throttling and restore failures",
    "issue_number": 15620,
    "title": "PiTR restore failed due to reading S3 files too frequently"
  },
  {
    "bug_location": "Dependencies",
    "severity": 3,
    "categories": [
      "Security",
      "Upgrade"
    ],
    "root_cause": "Outdated third-party Rust crate dependencies with known security vulnerabilities",
    "issue_number": 15621,
    "title": "Update third-party crates to fix insecure issues."
  },
  {
    "bug_location": "RaftStore V2 Component",
    "severity": 4,
    "categories": [
      "Replication",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Incorrect region ID generation during peer garbage collection after region merge, causing source peers to be abandoned and resolved timestamp to become stuck",
    "issue_number": 15623,
    "title": "[Dynamic Regions] resolved ts can't  advance for long time"
  },
  {
    "bug_location": "Raftstore/PeerFsm",
    "severity": 4,
    "categories": [
      "Recovery",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Timeout during unsafe recovery process, potentially caused by TiKV node panic during leader election or state transition",
    "issue_number": 15629,
    "title": "[dr-autosync] online recover time out after switching to backup cluster in sync_recover mode"
  },
  {
    "bug_location": "Storage/RocksDB",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Config"
    ],
    "root_cause": "Reduced default lock-cf write-buffer-size causing increased flush and level-0 compaction, consuming more CPU and IO resources",
    "issue_number": 15630,
    "title": "[Dynamic Regions] Compared with 7.3.0, write-heavy workloads have 7.6% - 31% performance regression"
  },
  {
    "bug_location": "components/tidb_query_datatype/src/codec/mysql/decimal.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect implementation of decimal division and modulo operations for zero numerator, causing incorrect mathematical behavior",
    "issue_number": 15631,
    "title": "The result of `0 / decimal` and `0 % decimal` is wrong."
  },
  {
    "bug_location": "raftstore-v2/merge",
    "severity": 4,
    "categories": [
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Failed to retrieve merge entries due to log compaction, possibly indicating a race condition or log truncation issue during region merge process",
    "issue_number": 15633,
    "title": "[Dynamic Regions] failed to get merge entires"
  },
  {
    "bug_location": "raftstore-v2/operation/life.rs",
    "severity": 4,
    "categories": [
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Inconsistent region peer state where a peer is reported as non-existent despite having a valid local state region",
    "issue_number": 15634,
    "title": "[Dynamic Regions] peer doesn't exist but has valid local state region"
  },
  {
    "bug_location": "Region/Bucket Management",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Dynamic region splitting mechanism appears to be ineffective, causing excessive data scanning in a single region/bucket despite high write volume",
    "issue_number": 15636,
    "title": "[Dynamic Regions]: buckets maybe not split even if there are many writes."
  },
  {
    "bug_location": "TiKV Flashback/Recovery Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Incorrect min-resolved-ts retrieval during disaster recovery, leading to potential data inconsistency when performing table flashback",
    "issue_number": 15639,
    "title": "[dr-autosync] after do tikv flashback to  min-resolved-ts, admin check table hit  ERROR 8223 (HY000): data inconsistency in table "
  },
  {
    "bug_location": "raftstore-v2/operation/command/admin/merge",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Loss of PrepareMerge state during TiKV restart causing inconsistent region merge process",
    "issue_number": 15642,
    "title": "[Dynamic Regions] tikv panic at \"state: Merging region\""
  },
  {
    "bug_location": "components/raftstore-v2/src/operation/life.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Followers do not report GcPeerResponse after commit merge, preventing cleanup of merged_records",
    "issue_number": 15644,
    "title": "[Dynamic Regions] `merged_records` will never be cleaned up"
  },
  {
    "bug_location": "TiKV Store Status Metadata",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Stale label metadata not automatically updated after configuration changes",
    "issue_number": 15648,
    "title": "information_schema.tikv_store_status  bug"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Lack of retry mechanism for MsgAvailabilityRequest during region merge, causing merge process to stall when a critical message is dropped",
    "issue_number": 15651,
    "title": "[Dynamic Regions] merge is blocked if MsgAvailabilityRequest is dropped"
  },
  {
    "bug_location": "TableScan/Query Execution",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Performance regression in table full scan operations, likely due to a code change that introduced inefficient scanning logic",
    "issue_number": 15653,
    "title": "TableFullScan gets slower resulting in a 5% performance regression in TPCDS "
  },
  {
    "bug_location": "endpoint.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incomplete request source metadata handling during table analysis operation",
    "issue_number": 15663,
    "title": "Background control cannot get the stats type"
  },
  {
    "bug_location": "information_schema",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Monitoring"
    ],
    "root_cause": "Incorrect calculation of PEER_COUNT in table_storage_stats, showing 6 peers instead of expected 3 peers based on max-replicas configuration",
    "issue_number": 15665,
    "title": "infromation_schema.table_storage_stats  bug"
  },
  {
    "bug_location": "RaftStore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Leader continues sending GcPeerRequests to peers on tombstone stores instead of directly removing them, causing unnecessary network communication and potential resource waste",
    "issue_number": 15669,
    "title": "[Dynamic Regions] Leader keeps sending GcPeerRequests to peers on tombstone store"
  },
  {
    "bug_location": "Region/Transaction",
    "severity": 4,
    "categories": [
      "Transaction",
      "Replication",
      "Storage"
    ],
    "root_cause": "Potential data inconsistency during index lookup across dynamic regions, likely related to MVCC (Multi-Version Concurrency Control) synchronization issues",
    "issue_number": 15670,
    "title": "[Dynamic Regions] indexLookup found data inconsistency"
  },
  {
    "bug_location": "Region Management / Merge Process",
    "severity": 4,
    "categories": [
      "Replication",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Orphan peer remains unhandled after region merge, preventing resolved timestamp advancement in large clusters with dynamic regions",
    "issue_number": 15672,
    "title": "[Dynamic Regions] resolved ts can't advance in big cluster scenario"
  },
  {
    "bug_location": "Build System",
    "severity": 3,
    "categories": [
      "Upgrade",
      "CodeBug"
    ],
    "root_cause": "Incompatibility introduced when upgrading raft-engine dependency from version 0.3.0 to 0.4.1",
    "issue_number": 15673,
    "title": "fail to build fail_release"
  },
  {
    "bug_location": "TiCDC ResolvedTs",
    "severity": 3,
    "categories": [
      "Network",
      "Replication",
      "Performance"
    ],
    "root_cause": "Network partition causing ResolvedTs synchronization disruption",
    "issue_number": 15679,
    "title": "ResolvedTs lag increases after a TiKV store is partitioned"
  },
  {
    "bug_location": "raftstore-v2/operation/command/admin/merge",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Non-deterministic merge commit and rollback process that can lead to race conditions during region merge operations, potentially causing region corruption",
    "issue_number": 15682,
    "title": "[Dynamic Regions] panic with \"region corrupted\""
  },
  {
    "bug_location": "Region Management",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect handling of region size metadata during region splitting, where new region sizes are set to None instead of zero",
    "issue_number": 15696,
    "title": "After spliting regions, the size of the new regions are not updated to zero but None."
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Unbounded channel buffers for peer communication can accumulate unlimited memory, potentially causing out-of-memory (OOM) conditions when many regions are created",
    "issue_number": 15701,
    "title": "The total buffer of peers' channels is essentially unlimited and may cause OOM  "
  },
  {
    "bug_location": "Backup & Restore (BR) component",
    "severity": 3,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Potential file permission or file transfer issue during SST file restoration process, possibly related to file path handling or access rights when restoring backup files",
    "issue_number": 15705,
    "title": "Problem cannot read sst file when running br restore"
  },
  {
    "bug_location": "TiKV Flashback Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Retry mechanism fails when region leader transfers during flashback, causing infinite retry without proper backoff or peer identification strategy",
    "issue_number": 15712,
    "title": "[dr-autosync] flashback hang for more than 30min on second call"
  },
  {
    "bug_location": "backup-stream",
    "severity": 4,
    "categories": [
      "Performance",
      "Memory",
      "Transaction"
    ],
    "root_cause": "Memory quota semaphore gets stuck when processing large event sizes, potentially causing initial scanning queue to block",
    "issue_number": 15714,
    "title": "br: pitr-task stuck and log shows \"failed to get initial snapshot: failed to get the snapshot\""
  },
  {
    "bug_location": "TiKV Server Component",
    "severity": 3,
    "categories": [
      "Storage",
      "Config",
      "Performance"
    ],
    "root_cause": "Insufficient disk space due to large reserved space configuration preventing place holder file creation",
    "issue_number": 15718,
    "title": "Need a standalone space holder file for raft engine"
  },
  {
    "bug_location": "Region Cache / PD Client",
    "severity": 5,
    "categories": [
      "Performance",
      "LoadBalance",
      "Network"
    ],
    "root_cause": "Excessive GetRegionByID requests triggered by bucket update mechanism in large clusters, causing PD server overload",
    "issue_number": 15719,
    "title": "Bucket update triggers too many GetRegionByID requests and make PD overwhelmed "
  },
  {
    "bug_location": "store.rs",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Network",
      "Performance"
    ],
    "root_cause": "Potential deadlock in PD worker latency inspector causing store communication failures, likely related to RocksDB synchronization issue",
    "issue_number": 15721,
    "title": "[Dynamic Regions] some tikv stores status are always down"
  },
  {
    "bug_location": "TiFlash Replica Management",
    "severity": 3,
    "categories": [
      "Replication",
      "Storage",
      "Recovery"
    ],
    "root_cause": "Potential issue with replica availability tracking during backup and restore process for TiFlash data",
    "issue_number": 15724,
    "title": "[Dynamic Regions] AVAILABLE is always 0 when query information_schema.tiflash_replica after br restore tiflash data with 2 replicas "
  },
  {
    "bug_location": "raftstore-v2/operation/command/admin/merge/commit.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Replication"
    ],
    "root_cause": "Potential race condition or error handling issue during region merge process under network loss conditions",
    "issue_number": 15725,
    "title": "[Dynamic Regions] tikv panic after injection network loss for one of tikv repeatedly"
  },
  {
    "bug_location": "Storage",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Lockcf write buffer limit configuration causing performance regression in write-heavy and read-write workloads with multiple MVCC versions",
    "issue_number": 15735,
    "title": "[Dynamic Regions] Compared with v7.3.0, TPCC has a 8% ~ 29% performance regression"
  },
  {
    "bug_location": "Storage/LockCF",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Increased write_buffer_size and write_buffer_limit for lockcf negatively impacts read-write workloads with multiple MVCC versions, causing performance regression in insert/update + scan operations",
    "issue_number": 15736,
    "title": "[Dynamic Regions] Compared with v7.3.0, workloads with insert/update + scan operations have a 32% ~ 77% performance regression in v7.4.0"
  },
  {
    "bug_location": "raftstore/peer",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Inconsistent Raft log state during region merge when restoring from EBS snapshot, causing out-of-bounds slice access during log catch-up",
    "issue_number": 15739,
    "title": "TiKV panic due to catch up logs during ebs restore"
  },
  {
    "bug_location": "CDC/Resolved-TS",
    "severity": 4,
    "categories": [
      "Transaction",
      "Replication",
      "Performance"
    ],
    "root_cause": "Stale transaction lock preventing resolved timestamp progression, potentially caused by large row handling in CDC changefeed with claim-check option",
    "issue_number": 15741,
    "title": "TiKV resolved ts not stable when running workload with >1MB large rows"
  },
  {
    "bug_location": "backup-restore",
    "severity": 3,
    "categories": [
      "Network",
      "Config"
    ],
    "root_cause": "Lack of retry mechanism when obtaining Azure AD authentication token fails",
    "issue_number": 15744,
    "title": "need retry logic when failed to get azure ad token from azure blob server"
  },
  {
    "bug_location": "RaftStore/ApplyThread",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Replication"
    ],
    "root_cause": "Deadlock in apply thread preventing lock release, blocking transaction scheduler and preventing resolved timestamp and GC safepoint advancement",
    "issue_number": 15745,
    "title": "[Dynamic Regions] resolved ts and gc safepoint can't advance"
  },
  {
    "bug_location": "raftstore-v2/query",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Incorrect handling of stale read requests in raft-kv2, where readIndex is triggered inappropriately when snapshot retrieval fails for stale reads",
    "issue_number": 15748,
    "title": "[Dynamic Regions] stale read with raft-kv2 returns unexpected region error occasionally"
  },
  {
    "bug_location": "raft_log_engine",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Potential file sequence number corruption during disk operations, likely related to storage engine state inconsistency",
    "issue_number": 15751,
    "title": "TiKV panic for Corruption: file seqno out of range"
  },
  {
    "bug_location": "Raft Metrics Component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Metrics tracking for dropped Raft messages is incorrectly implemented, only capturing messages when a rare condition (MsgTimeoutNow) is met, leading to inaccurate representation of disk-full related m",
    "issue_number": 15753,
    "title": "dropped raft message metrics on disk_full type is misleading"
  },
  {
    "bug_location": "Raft Write Flow / Storage Management",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Lack of robust disk space exhaustion prevention mechanism in Raft write flow, allowing minor writes to continue even when disk is critically low on space",
    "issue_number": 15754,
    "title": "when disk almost full happens, there's still minor raft write flow which could further consume up tikv free space"
  },
  {
    "bug_location": "raft-engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Recovery",
      "Performance"
    ],
    "root_cause": "Raft engine attempting log rewrite when disk space is exhausted, causing unhandled fatal error instead of graceful degradation",
    "issue_number": 15755,
    "title": "raft-engine should not start rewrite when there's no available space"
  },
  {
    "bug_location": "Profiling/Debug Module",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Monitoring"
    ],
    "root_cause": "Concurrent profiling modes are not properly handled, causing mutual exclusion of profiling operations",
    "issue_number": 15760,
    "title": "Can't perform CPU profiling when heap profiling is activated"
  },
  {
    "bug_location": "Replication",
    "severity": 4,
    "categories": [
      "Replication",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Potential data consistency issue during multi-zone failure scenario with DR auto-sync configuration, causing incomplete data preservation",
    "issue_number": 15762,
    "title": "[dr-autosync] After down 2 zone, got of total bank account  does not meet expectations"
  },
  {
    "bug_location": "raftstore-v2/merge",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Sibling region check failure during commit merge operation, likely due to inconsistent region metadata or epoch validation",
    "issue_number": 15764,
    "title": "[Dynamic Regions] sibling check failed during commit merge"
  },
  {
    "bug_location": "DataKeyImporter",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Recovery"
    ],
    "root_cause": "Non-atomic file metadata update during key management, causing potential key inconsistency when nodes crash and restart",
    "issue_number": 15768,
    "title": "[Dynamic Regions] tikv panic with fatal error \"Corruption: Encryption key manager get file failure: key not found\" after kill one tikv repeatedly"
  },
  {
    "bug_location": "raft-rs component",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Unbounded log scanning during raft configuration change check, causing memory consumption to grow uncontrollably when many unapplied log entries exist",
    "issue_number": 15770,
    "title": "TiKV OOM causes by reading raft conf changes during raft.hup"
  },
  {
    "bug_location": "Configuration Management",
    "severity": 2,
    "categories": [
      "Config",
      "Upgrade"
    ],
    "root_cause": "Lack of automatic configuration preservation during TiKV upgrade process, specifically for raft-engine directory settings",
    "issue_number": 15771,
    "title": "Can't set raft-engine.dir as the same as raftstore.raftdb-path when upgrading"
  },
  {
    "bug_location": "PD Replication Mode",
    "severity": 5,
    "categories": [
      "Replication",
      "Transaction",
      "Recovery"
    ],
    "root_cause": "Potential synchronization failure during dynamic replication mode transition, causing recovery process to hang when switching between dr-auto-sync and majority modes",
    "issue_number": 15784,
    "title": "[dr-autosync] hang in sync_recover when change from majority to dr-auto-sync"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 15791,
    "title": "lightning (sst_importer client) integration tests fail after merge #15013"
  },
  {
    "bug_location": "GC Worker / Compaction Filter",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Monitoring",
      "Transaction"
    ],
    "root_cause": "Race condition in metric collection where compaction event metrics are collected before compaction filter metrics, triggering a false 'GC not working' alert when single-version data exists",
    "issue_number": 15796,
    "title": "Avoid false \"GC can not work\" alert when there is only one version of all the data."
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Replication",
      "Transaction",
      "LoadBalance"
    ],
    "root_cause": "Inconsistent region leadership and replica availability during placement rule reconfiguration from 5 to 3 replicas, causing region unavailability",
    "issue_number": 15799,
    "title": "Change placement rule from 5 replicas to 3 replicas with some tikv down, tpcc report error: [tikv:9005]Region is unavailable"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "High memory usage caused by excessive raft log caching, triggered by TiFlash node disk exhaustion, preventing log appending and causing server busy state",
    "issue_number": 15800,
    "title": "TiKV reports ServerIsBusy error because it can not append raft log"
  },
  {
    "bug_location": "Backup and Restore (BR) Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Upgrade"
    ],
    "root_cause": "Schema incompatibility during backup and restore process, causing incorrect key decoding when clustered index configuration changes",
    "issue_number": 15801,
    "title": "Panic every 5 minute after restore from backup"
  },
  {
    "bug_location": "backup_stream",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Excessive memory consumption in Point-in-Time Recovery (PiTR) components, specifically from EventLoader iterator creation and Zstd compression context allocation",
    "issue_number": 15805,
    "title": "tikv oom when running large-row-values workload"
  },
  {
    "bug_location": "Raft Consensus Module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Commit condition failure in joint consensus state when peers are down in disaster recovery auto-sync scenario, specifically when demoting voter is on a failed store preventing log commit progression",
    "issue_number": 15817,
    "title": "Commit can't advance for region in joint state when dr auto sync is enabled"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 15820,
    "title": "Cast bytes as time gets encoding failed"
  },
  {
    "bug_location": "Monitoring",
    "severity": 2,
    "categories": [
      "Monitoring",
      "Config"
    ],
    "root_cause": "Grafana dashboard configuration issue in cloud environment causing incorrect display of scheduler command variables",
    "issue_number": 15832,
    "title": "Scheduler command variables of grafana are not correct on cloud "
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 15835,
    "title": "dynamic modify `resolved-ts.advance-ts-interval` from 5s to 2s is not work"
  },
  {
    "bug_location": "GC Worker",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Recovery"
    ],
    "root_cause": "TiKV cannot perform garbage collection when disk capacity is near full, causing write operations and GC processes to fail due to resource constraints",
    "issue_number": 15846,
    "title": "gc can not work when disk capacity is near full"
  },
  {
    "bug_location": "Build System",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incompatibility of AFL dependency configuration in DEBUG build mode on MacOS",
    "issue_number": 15847,
    "title": "Build fails due to `afl` dependency in DEBUG mode"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 15851,
    "title": "release failed"
  },
  {
    "bug_location": "Monitoring/Metrics",
    "severity": 3,
    "categories": [
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Incorrect metric calculation using rate() on a gauge metric instead of a counter metric",
    "issue_number": 15859,
    "title": "tikv's worker pending tasks under task result is not correct"
  },
  {
    "bug_location": "RaftStore",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Potential race condition or synchronization issue in region bucket split logic",
    "issue_number": 15862,
    "title": "flaky test case: test_gen_split_check_bucket_ranges"
  },
  {
    "bug_location": "Region Split Checker",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Incorrect handling of size_diff_hint after split check scheduling, causing regions to remain unsplit even when exceeding size limits",
    "issue_number": 15863,
    "title": "A 12GB region is not splitted automatically, cause TiSpark's failure due to too large coproc response. "
  },
  {
    "bug_location": "RocksDB Block Cache / Raft Log Replay",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Replication"
    ],
    "root_cause": "Performance degradation during TiKV restart due to block cache warm-up and pending Raft log application, causing temporary high IO burst and reduced query performance",
    "issue_number": 15864,
    "title": "Write QPS drops to 0 after TiKV Restart"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Blocking semaphore acquisition prevents timely cancellation of incremental scan tasks during region split or merge events",
    "issue_number": 15866,
    "title": "cdc incremental scan tasks can't be canceld by region split or merge correctly"
  },
  {
    "bug_location": "Raftstore",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Leader transfer process becomes slow due to synchronous I/O blocking and Raftstore log write queue congestion when experiencing I/O delays",
    "issue_number": 15875,
    "title": "transfer leader too slow when inject one of tikv io delay with \u201cevict-slow-trend-scheduler\u201d or \u201cevict-slow-store-scheduler\u201d"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect string comparison handling of different collation types in TiKV's comparison logic",
    "issue_number": 15878,
    "title": "`field` function doesn't consider the collation"
  },
  {
    "bug_location": "endpoint.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Inappropriate log level for expected lock contention scenario, causing unnecessary log noise",
    "issue_number": 15881,
    "title": "server: reduce unnecesary logs"
  },
  {
    "bug_location": "TiKV Transaction Layer",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Potential data encoding/decoding corruption during key generation or transaction processing, possibly related to region scaling or server shutdown",
    "issue_number": 15896,
    "title": "We have some bad encoded-keys like: aaaa FF bbbb FF cccc 06 dd00 FE"
  },
  {
    "bug_location": "CDC (Change Data Capture) Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Memory",
      "Transaction"
    ],
    "root_cause": "Resuming changefeed with an older timestamp triggers excessive snapshot generation, likely due to inefficient data reconciliation mechanism when backfilling large historical data ranges",
    "issue_number": 15910,
    "title": "Number of snapshots  increases tremendously and TiKV OOM when resuming changefeed with a ts 6 days before."
  },
  {
    "bug_location": "Lightning/SST Import",
    "severity": 5,
    "categories": [
      "Storage",
      "Upgrade",
      "CodeBug"
    ],
    "root_cause": "Filename pattern change in SST file generation causing compatibility issue between old and new TiKV versions during restart",
    "issue_number": 15912,
    "title": "lightning Import sst lost after tikv restart"
  },
  {
    "bug_location": "Transaction",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Storage"
    ],
    "root_cause": "Resolved timestamp advancement issue during batch insert operations with wide tables, likely related to transaction metadata management",
    "issue_number": 15916,
    "title": "resolved ts can't advance in the batch insert wide table scenario"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 15917,
    "title": "Priority Scheduling Optimization"
  },
  {
    "bug_location": "raftstore/pd_worker",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Storage"
    ],
    "root_cause": "Incomplete implementation of peer cleanup logic during region merge, causing stale peers to remain in the system and consume resources",
    "issue_number": 15919,
    "title": "Stale peers are left forever after region merge"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 15920,
    "title": "tikv connecto to pd failed, but pd_ctl is ok"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 15926,
    "title": "cb27f2 causes 6%- 10% QPS regression in CH benchmark on ARM64"
  },
  {
    "bug_location": "Follower Read Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "Replication"
    ],
    "root_cause": "When disk capacity is near full, followers refuse to apply new logs and cannot handle read requests with updated region epochs, causing consistent read failures",
    "issue_number": 15932,
    "title": "tikv always report \u201cRegion error (will back off and retry)\u201d when disk capacity is near full with follower read"
  },
  {
    "bug_location": "Monitoring",
    "severity": 3,
    "categories": [
      "Memory",
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Lack of built-in heap profiling and memory usage visualization tools for troubleshooting out-of-memory (OOM) scenarios",
    "issue_number": 15958,
    "title": "Can't investigate OOM issues easily"
  },
  {
    "bug_location": "Raft/Replication",
    "severity": 3,
    "categories": [
      "Replication",
      "LoadBalance"
    ],
    "root_cause": "Potential synchronization or state tracking issue between TiKV and PD regarding pending peer status, causing inconsistent peer state representation",
    "issue_number": 15969,
    "title": "high pending peer duration"
  },
  {
    "bug_location": "Titan Storage Metrics",
    "severity": 3,
    "categories": [
      "Storage",
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Potential inaccurate calculation or reporting of Titan store size metrics during sysbench workload",
    "issue_number": 15971,
    "title": "Titan store size metric is not accurate"
  },
  {
    "bug_location": "snapshot_transfer_module",
    "severity": 3,
    "categories": [
      "Replication",
      "Performance"
    ],
    "root_cause": "Current snapshot transfer mechanism lacks dynamic backoff or delay strategy when receiving snapshot tasks reach system-defined limit",
    "issue_number": 15972,
    "title": "TiKV should delay request snapshot if it exceeds receiving snapshot  limit"
  },
  {
    "bug_location": "engine_rocks/cf_options.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incorrect validation of Titan blob run mode configuration, causing invalid argument errors when attempting to modify configuration via tikv-ctl",
    "issue_number": 15978,
    "title": "tikv-ctl modify titan config Invalid argument error"
  },
  {
    "bug_location": "Makefile",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incorrect usage of `basename` command without providing a required argument in the Makefile build script",
    "issue_number": 15980,
    "title": "make: `basename` prints errors."
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 5,
    "categories": [
      "Storage",
      "CodeBug",
      "Recovery"
    ],
    "root_cause": "Block-level CRC checksum verification failure during RocksDB background compaction, causing unhandled corruption in SST files",
    "issue_number": 15986,
    "title": "Isolate the damaged ssts and regions and avoid tikv panic when block crc mismatches "
  },
  {
    "bug_location": "Titan storage engine configuration",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config",
      "Storage"
    ],
    "root_cause": "Invalid configuration parameter validation in Titan blob run mode configuration update mechanism",
    "issue_number": 15987,
    "title": "fail to update titan config by tikv-ctl or tikv status API"
  },
  {
    "bug_location": "Transaction/ResolvedTS",
    "severity": 4,
    "categories": [
      "Transaction",
      "Memory",
      "CodeBug"
    ],
    "root_cause": "Uncommitted transactions causing memory leaks when transactions are not properly committed, leading to memory accumulation in the resolved timestamp worker",
    "issue_number": 15992,
    "title": "TiKV OOM"
  },
  {
    "bug_location": "Performance Layer",
    "severity": 4,
    "categories": [
      "Performance"
    ],
    "root_cause": "Performance degradation between TiKV 7.1.2 and 7.5.0 patches, likely due to changes in transaction processing or resource management",
    "issue_number": 15994,
    "title": "release 7.5.0 Compared with the previous commit, oltp_read_write has a 3.65% regression under 200 concurrency and oltp_insert has a 1.5+% regression under 100 concurrency"
  },
  {
    "bug_location": "Titan storage engine",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance",
      "Config"
    ],
    "root_cause": "Inconsistent compression rates of RocksDB and Titan data across different TiKV nodes, leading to uneven store sizes",
    "issue_number": 15996,
    "title": "TiKV store size among TiKV nodes are very different after Titan is enabled"
  },
  {
    "bug_location": "Resolve Timestamp (resolve_ts) Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Potential cascading performance issue where a single store's check_leader slowdown propagates resolve timestamp advancement across multiple stores",
    "issue_number": 15999,
    "title": "1 store check_leader slow may affect may store to advance resolve_ts"
  },
  {
    "bug_location": "sst_importer",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Empty SST files are not being cleaned up after snapshot restore on split regions, causing unnecessary file accumulation in the import directory",
    "issue_number": 16005,
    "title": "sst_importer needs to clean the sst if no kv need to be saved and rewritten"
  },
  {
    "bug_location": "Storage",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential memory leak or inefficient garbage collection in Titan storage engine when enabled, causing continuous store size growth",
    "issue_number": 16006,
    "title": "TiKV store size can't converges stablly after titan enable"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16011,
    "title": "slow score sometimes has false positive which prevent its adoption in production environment"
  },
  {
    "bug_location": "Performance Layer",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Potential regression in performance optimization between TiKV commits, causing 3-5% throughput decrease in YCSB workloads under 200 concurrency",
    "issue_number": 16015,
    "title": "Daily run , compared with the previous tikv commit  , commit 7be1b1 leads ycsb workloada/workloadc has a 3% ~ 5% performance regression under 200 concurrency"
  },
  {
    "bug_location": "RaftStore/ApplySystem",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "High apply wait duration caused by potential queue bottleneck and write contention, possibly related to RocksDB mutex conflicts and entry loading overhead",
    "issue_number": 16016,
    "title": "High apply wait tail latency  "
  },
  {
    "bug_location": "raftstore",
    "severity": 2,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Concurrent SST file cleanup and listing causing file not found error during import operation",
    "issue_number": 16018,
    "title": "tikv report \"cleanup import sst failed\" during import into command"
  },
  {
    "bug_location": "coprocessor/endpoint.rs",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Incorrect handling of request execution duration deadline, causing potential resource waste for long-running requests",
    "issue_number": 16021,
    "title": "Coprocessor deadlink check ignore `max_execution_duration_ms`"
  },
  {
    "bug_location": "Expression Evaluation",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incomplete implementation of integer division for decimal data types, requiring a port from TiDB's existing fix",
    "issue_number": 16024,
    "title": "intdiv for decimal is not handle correctly"
  },
  {
    "bug_location": "unified-read-pool",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Async quota-limiter incorrectly calculates task availability, causing false positive queue-is-full errors when tasks are pending in future-timer",
    "issue_number": 16026,
    "title": "unified-read-pool may return false positive queue-is-full error"
  },
  {
    "bug_location": "storage/txn/actions/prewrite.rs",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Assertion failure in transaction prewrite process related to last change timestamp tracking, potentially causing concurrent transaction processing issues during index addition",
    "issue_number": 16030,
    "title": "more than one tikv panic repeatedly and service is unavailable when run workload and add index"
  },
  {
    "bug_location": "CDC Component",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Excessive pending tasks during CDC initial scan causing memory exhaustion, with approximately 1.6M tasks consuming around 8.9 GB memory",
    "issue_number": 16035,
    "title": "TiKV OOM during CDC initial scan"
  },
  {
    "bug_location": "RocksDB Compaction Thread",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential improper handling of RocksDB database closure or thread synchronization during compaction, leading to a panic when stopping the compaction process",
    "issue_number": 1604,
    "title": "rocksdb compaction thread panic when stop "
  },
  {
    "bug_location": "PD Balance Scheduler",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Storage",
      "Performance"
    ],
    "root_cause": "Titan storage format not properly handled during store rebalancing, causing uneven data distribution when scaling out TiKV nodes",
    "issue_number": 16055,
    "title": "The scaled out tikv store size can't balance when titan is enable"
  },
  {
    "bug_location": "backup-restore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Recovery"
    ],
    "root_cause": "Initialization failure during log backup task when a store has connectivity issues with PD, causing task to silently break without proper error handling or recovery mechanism",
    "issue_number": 16056,
    "title": "log-backup: log backup task may silently broken by a failed initialization"
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Insufficient disk space management during manual cluster compaction when Titan is enabled, potentially caused by incorrect configuration or lack of space reservation checks",
    "issue_number": 16062,
    "title": "Manual  compaction raises diskfull when titan is enable"
  },
  {
    "bug_location": "RaftStore",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Outdated apply state during peer removal causing inconsistent snapshot version checks, potentially violating linearizability when processing follower read requests",
    "issue_number": 16069,
    "title": "raftstore: linearibilizy is broken when follower read is used"
  },
  {
    "bug_location": "log-backup module",
    "severity": 3,
    "categories": [
      "Memory",
      "Storage"
    ],
    "root_cause": "Memory not properly deallocated after stopping log backup task, causing memory leak in template file handling",
    "issue_number": 16070,
    "title": "log-backup: memory leak after stopping a task."
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 5,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Compaction filter mechanism appears to be malfunctioning in Titan storage engine, preventing garbage collection and causing storage size to continuously grow",
    "issue_number": 16091,
    "title": "Titan can't execute compaction filter"
  },
  {
    "bug_location": "tikv-ctl encryption metadata handling",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Lack of robust error handling when data directory is not explicitly specified in configuration",
    "issue_number": 16094,
    "title": "(encryption) tikv-ctl dump encryption metadata panic without data-dir in config.toml"
  },
  {
    "bug_location": "GarbageCollection",
    "severity": 3,
    "categories": [
      "Performance",
      "Config",
      "CodeBug"
    ],
    "root_cause": "Current GC implementation is single-threaded, causing potential performance bottlenecks during garbage collection process",
    "issue_number": 16101,
    "title": "Make TiKV GC support configurable thread count "
  },
  {
    "bug_location": "Monitoring",
    "severity": 2,
    "categories": [
      "Monitoring",
      "CodeBug"
    ],
    "root_cause": "Incorrect configuration or generation of Grafana dashboard metrics panels in tikv_details.json",
    "issue_number": 16102,
    "title": "metrics: some minor bugs have side effects to observability"
  },
  {
    "bug_location": "raftstore/peer",
    "severity": 4,
    "categories": [
      "Transaction",
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Potential race condition or data inconsistency during complex transaction scenarios with network partitions and leader changes",
    "issue_number": 16111,
    "title": "tikv panic for \"no entry found for key\""
  },
  {
    "bug_location": "raftstore",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Monitoring"
    ],
    "root_cause": "Lack of specific error type recording for data not ready scenarios in stale read processing",
    "issue_number": 16113,
    "title": "raftstore: record the data not ready error types more clearly"
  },
  {
    "bug_location": "Monitoring",
    "severity": 2,
    "categories": [
      "Monitoring",
      "Config"
    ],
    "root_cause": "Potential compatibility issue between Grafana 9+ dashboard configuration and heatmap visualization rendering",
    "issue_number": 16144,
    "title": "Grafana heatmap does not display any data"
  },
  {
    "bug_location": "Monitoring",
    "severity": 2,
    "categories": [
      "Monitoring"
    ],
    "root_cause": "Missing label filter for QPS (Queries Per Second) metric in Grafana dashboard, preventing granular view of request types",
    "issue_number": 16148,
    "title": "grafana: QPS lacks the filter label of `type`."
  },
  {
    "bug_location": "TiKV Client/Error Handling",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Inconsistent error reporting for deadline exceeded scenarios across different TiKV response types",
    "issue_number": 16154,
    "title": "Uniform deadline exceeded error types"
  },
  {
    "bug_location": "CDC",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Replication"
    ],
    "root_cause": "Resolved timestamp not advancing during changefeed pause and resume operations, potentially related to synchronization or state tracking mechanism",
    "issue_number": 16156,
    "title": "ddl resolved ts not advance for 2 min "
  },
  {
    "bug_location": "CDC (Change Data Capture) and RocksDB",
    "severity": 4,
    "categories": [
      "Performance",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Performance degradation during CDC incremental scan resumption, causing increased RocksDB apply-write_memtable_time and subsequent TiDB latency increases",
    "issue_number": 16164,
    "title": "apply-write_memtable_time and TiDB latency increases during cdc initial scan"
  },
  {
    "bug_location": "Profiler Module",
    "severity": 3,
    "categories": [
      "Performance",
      "Monitoring"
    ],
    "root_cause": "Unable to create or access temporary files for heap profiling, possibly due to filesystem permissions or temporary directory configuration issues",
    "issue_number": 16169,
    "title": "dump or get heap profile failed"
  },
  {
    "bug_location": "TiKV Control (tikv-ctl)",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incomplete implementation of compact-cluster RPC handling when TiFlash nodes are present in the cluster",
    "issue_number": 16189,
    "title": "tikv-ctl compact-cluster doesn't work when TiFlash nodes exist"
  },
  {
    "bug_location": "Storage",
    "severity": 3,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential race condition or timeout issue in garbage collection (GC) test scenarios, causing test failures in Travis CI environment",
    "issue_number": 1620,
    "title": "storage GC test may fail too frequently in travis CI"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16211,
    "title": "Wrong result of `cast_duration_as_time` "
  },
  {
    "bug_location": "TiKV Timezone Library",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Bundled tzdata in TiKV binary is not regularly updated, leading to potential timezone information inaccuracies",
    "issue_number": 16220,
    "title": "TiKV tzdata out of date and may cause correctness issues"
  },
  {
    "bug_location": "TiKV Import Stage",
    "severity": 4,
    "categories": [
      "Memory",
      "Storage",
      "Performance"
    ],
    "root_cause": "Excessive memory consumption during large-scale table creation, potentially due to high region count and peer channel memory allocation",
    "issue_number": 16229,
    "title": "tikv oom crash during lightning import 80k tables"
  },
  {
    "bug_location": "Transaction Scheduler",
    "severity": 4,
    "categories": [
      "Transaction",
      "Memory",
      "Performance"
    ],
    "root_cause": "Uncontrolled memory accumulation of blocked `check_secondary_locks` RPCs on transaction latches during slow raftstore operations",
    "issue_number": 16234,
    "title": "TiKV txn scheduler may cause OOM when raftstore runs slow"
  },
  {
    "bug_location": "Resolved Transaction Service / Log Backup",
    "severity": 5,
    "categories": [
      "Network",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Network partition disrupting transaction and log backup synchronization mechanisms, causing system-wide service interruption",
    "issue_number": 16235,
    "title": "cluster can not provide service and tikv resolved ts can not go on after inject minio(pitr log backup path) network partition lasts for 10mins "
  },
  {
    "bug_location": "thread_group",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Thread group properties not properly initialized for gRPC threads, causing unexpected panic during thread lifecycle",
    "issue_number": 16236,
    "title": "tikv v7.4.0 crash"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Unbalanced region distribution causing excessive memory allocation in specific TiKV nodes, with over 4 million regions created for only 130 tables, leading to Out of Memory (OOM) conditions",
    "issue_number": 16243,
    "title": "TiKV OOM when importing 130k tables via lightning"
  },
  {
    "bug_location": "Metrics/Monitoring",
    "severity": 2,
    "categories": [
      "Monitoring"
    ],
    "root_cause": "Missing instance-level filtering in Raft IO and Raft Propose metrics, preventing granular performance monitoring",
    "issue_number": 16251,
    "title": "metrics: Raft IO and Raft Propose sections missing filters `by instance`."
  },
  {
    "bug_location": "TitanDB Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Recovery",
      "CodeBug"
    ],
    "root_cause": "Incomplete blob file cleanup during Titan storage engine fallback, causing orphaned blob files when manifest is not properly updated before restart",
    "issue_number": 16256,
    "title": "titan blob file can't be cleared when fallback"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16268,
    "title": "executor:  different result between tikv and tidb when handling decimal multiplication truncating"
  },
  {
    "bug_location": "Client Interface",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction",
      "Config"
    ],
    "root_cause": "Incompatible key encoding between raw and transactional clients when using different client types simultaneously without API v2 enabled",
    "issue_number": 16284,
    "title": "TiKV can't  read and write due to use rawclient and txnclient at the same time"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16290,
    "title": "Resolve ts can't advance for long time"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16295,
    "title": "TiKV fails to restart after making Titan default on"
  },
  {
    "bug_location": "Config Module",
    "severity": 4,
    "categories": [
      "Config",
      "CodeBug"
    ],
    "root_cause": "Invalid parsing of periodic-full-compact-start-times configuration parameter, causing configuration file generation failure during restart",
    "issue_number": 16299,
    "title": "tikv cannot restart twice after enable raftstore.periodic-full-compact-start-times"
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 5,
    "categories": [
      "Storage",
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Compatibility issue with compaction filter on write column family when Titan is enabled, causing potential panic and region unavailability during network partition or restart scenarios",
    "issue_number": 16301,
    "title": "others tikv abnormally restart and workload report \u201cRegion is unavailable\u201d after inject one of tikv network partition or failure or rolling restart with titan enable"
  },
  {
    "bug_location": "raftstore/peer.rs",
    "severity": 3,
    "categories": [
      "Replication",
      "Transaction",
      "Recovery"
    ],
    "root_cause": "Snapshot recovery process can get blocked when logs from a previous term prevent applying committed logs, due to synchronization constraints during leader election and log replication",
    "issue_number": 16307,
    "title": "Snapshot recovery wait apply may be blocked by logs from a pervious term"
  },
  {
    "bug_location": "engine_rocks_helper/sst_recovery",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Recovery"
    ],
    "root_cause": "Incomplete SST file deletion mechanism when handling multi-region corrupted files, causing panic during file recovery process",
    "issue_number": 16308,
    "title": "TiKV panic because it detects damaged SST file was not removed"
  },
  {
    "bug_location": "Transaction",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Potential transaction state inconsistency after flashback operation, likely related to timestamp management and key assertion during update after data recovery",
    "issue_number": 16313,
    "title": "enable titan, update report error after flashback "
  },
  {
    "bug_location": "Transaction",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential monotonic transaction consistency violation during concurrent read operations, possibly related to transaction timestamp management or read isolation mechanism",
    "issue_number": 16314,
    "title": "Jepsen monotonic test failed with anomalies"
  },
  {
    "bug_location": "Transaction",
    "severity": 5,
    "categories": [
      "Transaction",
      "Replication",
      "Performance"
    ],
    "root_cause": "Potential timeline inconsistency in transaction commit ordering, causing a logical contradiction in transaction sequence and visibility",
    "issue_number": 16315,
    "title": "Jepsen monotonic test failed with timeline contradiction"
  },
  {
    "bug_location": "Storage/Titan",
    "severity": 3,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Unable to collect min/max timestamp properties when Titan blob storage is enabled due to limited access to blob index instead of full value",
    "issue_number": 16319,
    "title": "Some table properties are incorrect if titan is enabled"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 1632,
    "title": "Fix read only request handling for new leader"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16324,
    "title": "Raft-Engine: reduce EBS jitter impact by the Raft-Engine rewrite"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16336,
    "title": "Test test_turnoff_titan failed"
  },
  {
    "bug_location": "Transaction",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Storage"
    ],
    "root_cause": "Potential write command blocking or deadlock causing QPS to drop to zero during long-running workload with large dataset and multiple indexes",
    "issue_number": 16337,
    "title": "tidb qps drop to 0 for write command can't end "
  },
  {
    "bug_location": "raftstore/txn_ext.rs",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Concurrent lock access in PeerPessimisticLocks with improper synchronization between raftstore and scheduler workers, causing mutual waiting and deadlock",
    "issue_number": 16340,
    "title": "Deadlock on PeerPessimisticLocks between raftstore and shceduler workers"
  },
  {
    "bug_location": "Raftstore",
    "severity": 4,
    "categories": [
      "Performance",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Peers entering PreChaos state and ignoring WakeUp messages during IO hang, causing extended region recovery time due to interaction between Hibernate Region feature and peer state management",
    "issue_number": 16368,
    "title": "qps recover after 13mins when inject one of tikv io hang"
  },
  {
    "bug_location": "Titan Storage Engine",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Config"
    ],
    "root_cause": "Suboptimal default configuration for `min-blob-size` causing performance regression in Titan storage engine during data dumpling and point-in-time recovery operations",
    "issue_number": 16370,
    "title": "titan =true dumpling and pitr has performance regression,which is 24.26% and 11.35%"
  },
  {
    "bug_location": "tidb_query_datatype",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Unexpected None value when processing row checksum during data retrieval, likely due to row encoding/decoding issue when row-level checksum is enabled",
    "issue_number": 16371,
    "title": "tikv panic when enable row level checksum and insert some data"
  },
  {
    "bug_location": "Monitoring/Metrics",
    "severity": 3,
    "categories": [
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Potential timezone discrepancy in resolved timestamp metrics calculation, causing incorrect max gap representation",
    "issue_number": 16390,
    "title": "tikv max gap of resolved ts metrics may be incorrect"
  },
  {
    "bug_location": "backup/endpoint",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Unexpected None value during Option::unwrap() in backup progress handling, likely due to race condition or unhandled edge case in backup process",
    "issue_number": 16394,
    "title": "TiKV panic when running br backup due to \"called `Option::unwrap()` on a `None` value\""
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16396,
    "title": "tikv conprof failed"
  },
  {
    "bug_location": "logging/thread_id",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Monitoring"
    ],
    "root_cause": "Incorrect thread ID generation or logging mechanism that always returns fixed value 0x5 instead of actual thread identifier",
    "issue_number": 16398,
    "title": "\"thread_id\" field in log is wrong and always 0x5"
  },
  {
    "bug_location": "Scheduler/LoadBalancer",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Performance",
      "Recovery"
    ],
    "root_cause": "Potential scheduling disruption after IO delay and fault recovery, preventing leader rebalancing",
    "issue_number": 16403,
    "title": "leader can not balance again after fault recover from one of tikv io delay 100ms which lasts for 10mins with evict-slow-trend-schedule"
  },
  {
    "bug_location": "Expression Evaluation",
    "severity": 2,
    "categories": [
      "CodeBug"
    ],
    "root_cause": "Inconsistent error message formatting during power function overflow handling between TiDB and TiKV",
    "issue_number": 16407,
    "title": "Error message is different from TiDB when `pow` is out of bound"
  },
  {
    "bug_location": "raftstore/raft-rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Monitoring"
    ],
    "root_cause": "Logging modification in raft-rs library removed printing of raft-id and region-id during log events",
    "issue_number": 16410,
    "title": "raftstore: Peer does not print `raft-id` and `peer-id` in `raft-rs` lib"
  },
  {
    "bug_location": "Profiling/Monitoring",
    "severity": 3,
    "categories": [
      "Monitoring",
      "Performance",
      "Unknown"
    ],
    "root_cause": "Unable to definitively determine root cause due to insufficient reproduction details and inconclusive investigation",
    "issue_number": 16421,
    "title": "In a cluster of 100 tikv nodes, conprofiling continues to fail"
  },
  {
    "bug_location": "raftstore/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Election mechanism fails to handle leader selection under network partition when hibernate region is enabled, especially when election timeout is configured with large values",
    "issue_number": 16429,
    "title": "hibernate region may not able to reelection a new leader for a long when leader is partitioned"
  },
  {
    "bug_location": "log_backup",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance",
      "Storage"
    ],
    "root_cause": "Potential u64 integer overflow during initial scanning causing work queue to get stuck in nanosleep",
    "issue_number": 16430,
    "title": "log_backup: u64 overflow may make initial scanning stuck "
  },
  {
    "bug_location": "raftstore/snapshot_sender",
    "severity": 4,
    "categories": [
      "Network",
      "Performance",
      "Replication"
    ],
    "root_cause": "Lack of timeout mechanism for snapshot transfer, potentially causing leader transfer and replication to stall in slow network conditions",
    "issue_number": 16435,
    "title": "raftstore: Add a timeout mechanism on snapshot send"
  },
  {
    "bug_location": "Backup & Restore (BR) component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Storage"
    ],
    "root_cause": "AWS SDK does not support IMDSv2 metadata retrieval mechanism for EC2 instance credentials",
    "issue_number": 16443,
    "title": "TiKV aws storage sdk don't support retrieve instance metadata via IMDSv2"
  },
  {
    "bug_location": "CDC (Change Data Capture) Endpoint",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Network"
    ],
    "root_cause": "Unhandled Option::unwrap() on a None value during CDC connection handling, likely caused by improper connection state management",
    "issue_number": 16444,
    "title": "cdc endpoint thread panics"
  },
  {
    "bug_location": "Build System",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Dependency conflict with multiple sources of the same package version",
    "issue_number": 16445,
    "title": "Cargo vendor fails with duplicate version of package \"encoding_rs\""
  },
  {
    "bug_location": "Replication",
    "severity": 4,
    "categories": [
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Potential blocking issue during node scale-in when replication mode switches to async, likely related to peer health and learner node counting logic",
    "issue_number": 16465,
    "title": "(dr-autosync) v6.5.8 scale in one node from primary datacenter failed during async mode"
  },
  {
    "bug_location": "Backup and Restore (BR) component",
    "severity": 4,
    "categories": [
      "Replication",
      "Network",
      "Performance"
    ],
    "root_cause": "Stale checking mechanism in StartObserve command causing checkpoint timestamp lag during network partition, potentially due to dropped subscription records when region epochs change",
    "issue_number": 16469,
    "title": "pitr checkpoint ts lag reached more than 8h after inject network partition between one of tikv and pd leader"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16478,
    "title": "Fix style_doc link"
  },
  {
    "bug_location": "Placement Rules",
    "severity": 4,
    "categories": [
      "Replication",
      "LoadBalance",
      "Config"
    ],
    "root_cause": "Placement rules policy interferes with down-peer recovery mechanism, preventing regions from automatically healing when nodes are scaled down",
    "issue_number": 16480,
    "title": "Down-peer-region can't recover when enable placement-rule policy"
  },
  {
    "bug_location": "Region Load Balancer",
    "severity": 4,
    "categories": [
      "LoadBalance",
      "Replication",
      "Performance"
    ],
    "root_cause": "Incomplete border check in `check_store_is_busy_on_apply` causing incorrect store busy state during data restoration, preventing proper leader and region distribution",
    "issue_number": 16491,
    "title": "leader and region are not balanced during br restore data and losts of miss-peer region after restore finished"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Precision loss during datetime type casting with time zone information",
    "issue_number": 16492,
    "title": "cast string as datetime with time zone results in loss of precision"
  },
  {
    "bug_location": "TiKV JSON Function Processing",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Insufficient validation of JSON path expressions during json_length() function processing, allowing invalid wildcard path syntax",
    "issue_number": 16495,
    "title": "An exception should be thrown for `json_length(col, '$.*')`"
  },
  {
    "bug_location": "raftstore/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Incorrect peer comparison logic in GC message handling causes stale peer to ignore garbage collection message when peer role changes, leading to resolve timestamp blocking",
    "issue_number": 16504,
    "title": "Stale peer may cause resolve ts doesn't push"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect JSON comparison logic for large integer values, causing inconsistent results between TiKV and TiDB when comparing JSON arrays with large numbers",
    "issue_number": 16512,
    "title": "Incorrect result for json compare and `json_contains`"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16518,
    "title": "disk_snapshot_backup: The lag between taking snapshot will slow down the restoration"
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Upgrade",
      "CodeBug"
    ],
    "root_cause": "Incompatible API version migration with existing data keys, preventing storage engine bootstrap when switching from V1 to V2 API",
    "issue_number": 16524,
    "title": "Failure with booting up TiKV with API v2 causes Fatal crash, which is probably a bit too severe."
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Incorrect lock tracking and resolved timestamp handling when partially subscribing to a region, causing potential race condition during transaction commit",
    "issue_number": 16526,
    "title": "other tikv panic when inject one of tikv failure with ticdc changfeed running"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16539,
    "title": "TiCDC scan speed limiter seems doesn't work if Titan is enabled"
  },
  {
    "bug_location": "CDC Endpoint",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "TiCDC disk becoming full, causing changefeed initialization blockage and region scanning errors",
    "issue_number": 16543,
    "title": "CDC resolved ts stucks, CDC incremental scan encountered lots of RegionNotFound error"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16549,
    "title": "TiKV crash loop during EBS recovery"
  },
  {
    "bug_location": "backup-stream/event_loader",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Race condition during region merge causing snapshot retrieval failure in log backup task, preventing consistent snapshot capture",
    "issue_number": 16554,
    "title": "Log Backup Task Randomly Paused due to Failed to get Initial Snapshot"
  },
  {
    "bug_location": "Raft Peer Management",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Orphan peer not properly destroyed after snapshot application is cancelled, leading to a stale peer that prevents timestamp advancement",
    "issue_number": 16561,
    "title": "Resolve ts can't not advance"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Performing snapshot checksum calculation and IO operations directly in the raftstore thread, causing high process-ready duration and blocking critical path processing",
    "issue_number": 16564,
    "title": "High peer msg duration, should avoid snapshot IO in raftstore"
  },
  {
    "bug_location": "Monitoring/Grafana Dashboard",
    "severity": 2,
    "categories": [
      "Monitoring",
      "Config"
    ],
    "root_cause": "Incorrect metric type selection in Grafana dashboard configuration for tracking memory allocation metrics",
    "issue_number": 16572,
    "title": "v7.5 tikv grafana [TiKV-Details] [Recently Released Bytes by Thread] incorrect configuration"
  },
  {
    "bug_location": "Memory Allocator/Metrics",
    "severity": 3,
    "categories": [
      "Memory",
      "Performance",
      "Monitoring"
    ],
    "root_cause": "Potential memory tracking inconsistency in thread allocation metrics where deallocated memory appears larger than allocated memory, suggesting possible metric calculation or tracking error in memory m",
    "issue_number": 16573,
    "title": "v7.5 web(status_port) metrics tikv_allocator_thread_allocation values issue"
  },
  {
    "bug_location": "Backup & Restore (BR) Component",
    "severity": 3,
    "categories": [
      "Performance",
      "Monitoring"
    ],
    "root_cause": "Lack of metrics tracking concurrent SST import tasks, potentially causing task queue bottlenecks during backup/restore operations",
    "issue_number": 16583,
    "title": "need a metrics to show the number of inflight sst importer task"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16589,
    "title": "Projection executor crashes when expression returns RpnStackNodeVectorValue::Generated value"
  },
  {
    "bug_location": "Compaction Filter",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Inconsistent compaction filter behavior across different TiKV peers, potentially related to GC safe point synchronization and tombstone marking",
    "issue_number": 16591,
    "title": "compaction-filter sometimes stably fails to work"
  },
  {
    "bug_location": "RocksDB build system",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Compatibility issues with GCC 13 in RocksDB's performance monitoring code and missing gflags dependency",
    "issue_number": 16593,
    "title": "TiKV built failed with GCC13"
  },
  {
    "bug_location": "Replication",
    "severity": 4,
    "categories": [
      "Replication",
      "Transaction",
      "Recovery"
    ],
    "root_cause": "Data inconsistency during disaster recovery switchover, potentially caused by synchronization issues between primary and backup data centers in dr-autosync mode",
    "issue_number": 16595,
    "title": "(dr-autosync)after switch to backup datacenter in sync mode, tpcc report data inconsistency in table"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16600,
    "title": "High store message duration"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16601,
    "title": "TiCDC incremental scan can usage too much CPU under some unkown conditions"
  },
  {
    "bug_location": "JSON/Type Casting Module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Improper handling of type conversion when casting a temporary binary value to JSON during query execution, leading to unexpected panic",
    "issue_number": 16616,
    "title": "TiKV will panic if cast a temporary binary to json"
  },
  {
    "bug_location": "storage/txn/actions/check_txn_status.rs",
    "severity": 5,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Unprotected rollback record for primary key in optimistic transactions can break transaction atomicity during async commit/1PC scenarios, allowing potential transaction retry and inconsistent state",
    "issue_number": 16620,
    "title": "`CheckTxnStatus` on primary key of optimistic transaction writing non-protectecd rollback can break transaction atomicity"
  },
  {
    "bug_location": "Timer/Clock Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Unknown"
    ],
    "root_cause": "Potential monotonic clock regression causing unexpected time jumps, leading to TiKV timer/thread panic",
    "issue_number": 16623,
    "title": "tikv crash or panic"
  },
  {
    "bug_location": "ReadPool",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config",
      "Monitoring"
    ],
    "root_cause": "Metric initialization logic fails to set thread count metric until configuration is explicitly modified",
    "issue_number": 16629,
    "title": "`tikv_unified_read_pool_thread_count` is always zero until online modifying `readpool.unified.max-thread-count`."
  },
  {
    "bug_location": "resolved_ts module",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect handling of rewritten transaction records with gc_fence set, causing rollback information to be lost and potentially blocking resolved_ts for stale read operations",
    "issue_number": 16634,
    "title": "Writing protected rollback directly to rollback flag of another transaction's commit record may cause resolved_ts being blocked"
  },
  {
    "bug_location": "raftstore/store/fsm/store",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Memory"
    ],
    "root_cause": "Index out of bounds access causing a panic in the Raft store finite state machine handling messages",
    "issue_number": 16639,
    "title": "TiKV panic index out of bounds"
  },
  {
    "bug_location": "TiKV Coprocessor",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Potential memory management issue with high concurrency coprocessor requests during TPCC check operation",
    "issue_number": 16653,
    "title": "TPCC check with -T 1000 causes TiKV OOM "
  },
  {
    "bug_location": "Monitoring/Grafana Dashboard",
    "severity": 2,
    "categories": [
      "Monitoring",
      "Config"
    ],
    "root_cause": "Missing default metric configuration for log backup initial scan in Grafana dashboard template",
    "issue_number": 16656,
    "title": "br: add metrics tikv_log_backup_pending_initial_scan to grafana panel"
  },
  {
    "bug_location": "TiKV Server Memory Management",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Potential memory leak in transaction processing, possibly related to unbounded transaction request handling",
    "issue_number": 16658,
    "title": "Tikv OOM "
  },
  {
    "bug_location": "RaftStore/Storage",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "LoadBalance"
    ],
    "root_cause": "High IO bandwidth consumption during TiKV scale out causing slow store performance and increased write latency, which impacts CDC and resolve-ts progress",
    "issue_number": 16661,
    "title": "slow store during TiKV scale out, and CDC lag up to 15s"
  },
  {
    "bug_location": "Storage/Scheduler",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Potential memory leak or uncontrolled memory growth in transaction execution, likely related to long-running transaction timeout settings in Java client",
    "issue_number": 16676,
    "title": "8.0.0-alpha OOM"
  },
  {
    "bug_location": "RocksDB/TitanDB Testing Component",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential performance bottleneck or synchronization issue in proptest comparison between RocksDB and TitanDB causing test timeout",
    "issue_number": 16681,
    "title": "test: test_rocks_titan_basic_ops timeouts"
  },
  {
    "bug_location": "CDC/ResolvedTS",
    "severity": 4,
    "categories": [
      "Performance",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Multiple potential issues causing resolved timestamp lag during TiKV rolling restart, including kv-client regression, region cache updates, and leader transfer timeout handling",
    "issue_number": 16698,
    "title": "TiKV min resolved ts lag up to 30s during tikv rolling restart, resulting in cdc lag increase"
  },
  {
    "bug_location": "storage.io-rate-limit",
    "severity": 3,
    "categories": [
      "Config",
      "Performance",
      "Storage"
    ],
    "root_cause": "IO rate limiter does not restrict high-priority operations, which can lead to uncontrolled disk throughput during operations like scaling out TiKV with Titan enabled",
    "issue_number": 16701,
    "title": "storage.io-rate-limit max-bytes-per-sec doesn't work perfectly"
  },
  {
    "bug_location": "RocksDB Storage Engine",
    "severity": 5,
    "categories": [
      "Storage",
      "Recovery",
      "CodeBug"
    ],
    "root_cause": "Potential data inconsistency during EBS snapshot backup and restore process causing WAL (Write-Ahead Log) corruption",
    "issue_number": 16705,
    "title": "tikv failed at bootstrap complaining wal corruption "
  },
  {
    "bug_location": "PD Worker / Store Heartbeat",
    "severity": 5,
    "categories": [
      "Performance",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Potential task queue blocking or processing issue with store heartbeat tasks during high-load workloads, causing TiKV store communication disruption",
    "issue_number": 16716,
    "title": "TiKV store down when running workload"
  },
  {
    "bug_location": "backup-stream/utils/wait_group",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Race condition in CallbackWaitGroup implementation where a stale notification can incorrectly signal task completion before all tasks are actually finished, potentially causing data loss during backup",
    "issue_number": 16739,
    "title": "log_backup: the type `CallbackWaitGroup` is error-prone"
  },
  {
    "bug_location": "tikv-server.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Recovery"
    ],
    "root_cause": "Unhandled file creation error with `.unwrap()` causing panic during TiKV cluster restart, specifically when attempting to create a lock file",
    "issue_number": 1674,
    "title": "tikv cluster restart failed with opaque error"
  },
  {
    "bug_location": "CDC/Resolved Timestamp",
    "severity": 3,
    "categories": [
      "Performance",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Potential synchronization issue during TiKV node recovery causing resolved timestamp to get stuck",
    "issue_number": 16741,
    "title": "resolved ts stuck for 10m when TiKV failure restored. "
  },
  {
    "bug_location": "Storage",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Potential performance regression in async I/O implementation causing unexpected performance degradation compared to synchronous I/O mode",
    "issue_number": 16742,
    "title": "enable async io, hxxxk poc workload has 3.6%~6.0%+ regression in performance"
  },
  {
    "bug_location": "backup-stream/subscription_manager",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "File or directory not found during test execution, likely related to file path handling or test setup",
    "issue_number": 16747,
    "title": "PiTR: unstable tests"
  },
  {
    "bug_location": "Network",
    "severity": 4,
    "categories": [
      "Network",
      "Security"
    ],
    "root_cause": "Vulnerability in h2 library allowing CONTINUATION frame flood that increases CPU usage and potentially degrades service performance",
    "issue_number": 16766,
    "title": "h2 vulnerability issue"
  },
  {
    "bug_location": "Memory Profiling Component",
    "severity": 2,
    "categories": [
      "Memory",
      "Config"
    ],
    "root_cause": "Inconsistent jemalloc sampling rate configuration between runtime options and actual profiling settings",
    "issue_number": 16774,
    "title": "Wrong jemalloc profiling sample rate"
  },
  {
    "bug_location": "CDC (Change Data Capture) Component",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Potential race condition or unexpected state in CDC delegate sink processing, causing a panic during data sink operation",
    "issue_number": 16776,
    "title": "TiKV Server Crashed And Restart With Panic Info"
  },
  {
    "bug_location": "RaftStore/RawKV Service",
    "severity": 2,
    "categories": [
      "Transaction",
      "Replication"
    ],
    "root_cause": "MaxTimestampNotSynced error during leader transfer in RawKV test, indicating a timestamp synchronization issue during region leadership transition",
    "issue_number": 16789,
    "title": "[flay test]  test_rawkv::test_leader_transfer "
  },
  {
    "bug_location": "Coprocessor/Storage",
    "severity": 3,
    "categories": [
      "Storage",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Potential range boundary mismatch during flashback operation causing request range to exceed physical storage bounds",
    "issue_number": 16790,
    "title": "sql report \"Error 1105 (HY000): other error: Request range exceeds bound\" after flashback"
  },
  {
    "bug_location": "Raft Log Replication/Recovery Module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Inconsistent log index tracking when applying Raft logs before persistence, causing potential data recovery misalignment during online unsafe recovery",
    "issue_number": 16796,
    "title": "Apply raft log before persistence is not compatible with online unsafe recovery"
  },
  {
    "bug_location": "Configuration",
    "severity": 1,
    "categories": [
      "Config",
      "Human"
    ],
    "root_cause": "Typographical error in configuration file spelling of 'hmac'",
    "issue_number": 16797,
    "title": "Typo in deny.toml"
  },
  {
    "bug_location": "Backup and Restore (PITR) Component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Concurrent progress tracking and file flushing causing inconsistent checkpoint timestamp management, leading to potential data loss during TiKV restart and external storage synchronization",
    "issue_number": 16809,
    "title": "upstream and downstream data are inconsistent after running some fault test and then pitr restore and checksum"
  },
  {
    "bug_location": "Transaction Resolution Mechanism",
    "severity": 3,
    "categories": [
      "Transaction",
      "Performance",
      "Recovery"
    ],
    "root_cause": "Incorrect lock resolution strategy during pipelined DML operations, using a lightweight lock resolution method instead of comprehensive region-wide lock scanning",
    "issue_number": 16810,
    "title": "Pipelined-DML uses resolve_lock_lite instead of resolve_lock when there is conflict"
  },
  {
    "bug_location": "resolved_ts",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Potential race condition in handling multiple pessimistic locks for the same key within a single RaftCmdRequest, causing unexpected lock state during transaction processing",
    "issue_number": 16818,
    "title": "tikv panic for \"assertion failed: row.lock.is_none()\""
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16820,
    "title": "missing grafana panels for \"apply unpersisted raft logs\""
  },
  {
    "bug_location": "RawKV",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Unknown"
    ],
    "root_cause": "Attempted to unwrap a None value in test_raw_put_key_guard, indicating a potential nil/null handling issue",
    "issue_number": 16825,
    "title": "[flay test] test_rawkv::test_raw_put_key_guard"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16871,
    "title": "The integration tests are not stable"
  },
  {
    "bug_location": "Transaction",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Pipelined DML locks are not being properly handled during read requests, causing unnecessary blocking of read transactions when locks should be ignorable based on timestamp semantics",
    "issue_number": 16880,
    "title": "Pipelined DML locks blocks read"
  },
  {
    "bug_location": "RaftStore/BufferBatchGet",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Potential issue with key ordering or processing in buffer batch get operation that causes inconsistent result retrieval when keys are not sorted or have duplicates",
    "issue_number": 16882,
    "title": "some results missing when keys of buffer batch get are unsorted"
  },
  {
    "bug_location": "PD (Placement Driver)",
    "severity": 3,
    "categories": [
      "Config",
      "Replication"
    ],
    "root_cause": "Incorrect configuration of single-node PD cluster with peer-urls and initial-cluster parameters causing leader election failure",
    "issue_number": 16884,
    "title": "README.md About Deploy a playground with binary part Incorrect"
  },
  {
    "bug_location": "raftstore/store/fsm/store",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Index out of bounds error during table splitting operation, likely caused by an array access violation in the StoreFsmDelegate handler",
    "issue_number": 16885,
    "title": "tikv panic when run sysbench and split table"
  },
  {
    "bug_location": "Transaction/MVCC",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Race condition during concurrent read index and write lock operations where max_ts and min_commit_ts are not properly synchronized, potentially causing stale transaction metadata",
    "issue_number": 16892,
    "title": "ReadIndex raft message may fail to advance max_ts in the middle of write_lock"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16913,
    "title": "expr: cast as decimal function is inconsistent with TiDB"
  },
  {
    "bug_location": "TiKV SQL Pushdown Layer",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Potential memory corruption or invalid handling during JSON array append operation during SQL pushdown to TiKV",
    "issue_number": 16930,
    "title": "json_array_append pushdown to tikv led tikv crash"
  },
  {
    "bug_location": "Request Processing Layer",
    "severity": 3,
    "categories": [
      "Performance",
      "Monitoring",
      "Transaction"
    ],
    "root_cause": "Lack of detailed tracing and logging for request lifecycle bottlenecks",
    "issue_number": 16941,
    "title": "Add observability on why tikv requests are timing out"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16942,
    "title": "Add observability on GRPC server connections"
  },
  {
    "bug_location": "backup-stream/endpoint",
    "severity": 4,
    "categories": [
      "Recovery",
      "Network",
      "CodeBug"
    ],
    "root_cause": "Task pause triggered by PD connection failure during TiKV registration of backup stream ranges",
    "issue_number": 16946,
    "title": "pitr status became paused after restart all pd\u3001tikv\u3001tidb"
  },
  {
    "bug_location": "In-Memory Engine (IME)",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Performance regression when In-Memory Engine is enabled during OLTP insert workloads, causing more than 10% throughput reduction compared to standard configuration",
    "issue_number": 16947,
    "title": "in-memory-engine: OLTP-insert has more than 10% regression when in-memory-engine is enabled in a test environment"
  },
  {
    "bug_location": "backup-stream/endpoint",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect safe point handling during log backup task pause operation, causing misleading error message when attempting to remove service GC safe point",
    "issue_number": 16956,
    "title": "log_backup: misleading error message reported when pausing a task"
  },
  {
    "bug_location": "raftstore/peer_storage",
    "severity": 4,
    "categories": [
      "Replication",
      "Recovery",
      "Storage"
    ],
    "root_cause": "Snapshot application failure during TiKV node recovery after being down for an extended period",
    "issue_number": 16958,
    "title": "tikv panic repeatedly with \u201c\\\"[region 16697056] 19604003 applying snapshot failed\\\"\u201d after down this tikv for 20mins and recover"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16962,
    "title": "Different behavior of `cast_string_to_decimal` between TiDB and TiKV"
  },
  {
    "bug_location": "In-memory engine",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Incorrect implementation of manual table load mechanism using region labels, causing single-load limitation and range overlap issues",
    "issue_number": 16966,
    "title": "In-memory engine: manual load table does work properly"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 16969,
    "title": "Overflow panic from `conv` function"
  },
  {
    "bug_location": "TiDB SQL Type Casting",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Precision loss during float to string type conversion, likely due to floating-point representation and rounding issues",
    "issue_number": 16974,
    "title": "unexpected result: casting real to string "
  },
  {
    "bug_location": "Raft Snapshot Management",
    "severity": 2,
    "categories": [
      "Storage",
      "Replication"
    ],
    "root_cause": "Incomplete snapshot file cleanup mechanism when leader snapshot transmission fails",
    "issue_number": 16976,
    "title": "Raft snapshot file is not deleted if the leader fails to send it out"
  },
  {
    "bug_location": "region_cache_memory_engine/range_manager",
    "severity": 4,
    "categories": [
      "Memory",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Assertion failure during range eviction when memory usage reaches hard limit, likely due to incorrect range management logic in memory cache engine",
    "issue_number": 16994,
    "title": "in-memory-engine: tikv crash due to an assert failure"
  },
  {
    "bug_location": "Monitoring",
    "severity": 2,
    "categories": [
      "Monitoring",
      "Performance",
      "Memory"
    ],
    "root_cause": "Missing per-thread memory usage metrics, which prevents effective identification of memory-related performance issues",
    "issue_number": 17006,
    "title": "Lack of per thread metric for release-8.1 "
  },
  {
    "bug_location": "In-memory Engine",
    "severity": 4,
    "categories": [
      "Transaction",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Potential write atomicity violation and incorrect sequence number handling during batch operations, causing inconsistent read results in Jepsen testing",
    "issue_number": 17018,
    "title": "In-memory engine: wrong-total in jepsen test"
  },
  {
    "bug_location": "backup-stream/endpoint",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Tokio runtime context not properly initialized when spawning tasks in backup stream endpoint",
    "issue_number": 17020,
    "title": "tikv panic due to: there is no reactor running, must be called from the context of a Tokio 1.x runtime"
  },
  {
    "bug_location": "SST Importer",
    "severity": 4,
    "categories": [
      "Storage",
      "Recovery"
    ],
    "root_cause": "Block checksum mismatch during SST file restoration, indicating potential data corruption during backup or storage transfer",
    "issue_number": 17022,
    "title": "BR restore failed: `Corruption: block checksum mismatch`"
  },
  {
    "bug_location": "Build System / Linker",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Missing Snappy library linkage for ARM64 architecture, likely due to incomplete cross-compilation support",
    "issue_number": 17031,
    "title": "Build failure: Undefined symbols for architecture arm64: \"snappy::RawCompress\""
  },
  {
    "bug_location": "table_scan_executor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Slice index out of bounds error during table scan operation, likely caused by incorrect slice indexing or memory corruption during large data operations",
    "issue_number": 17033,
    "title": "TiKV panic: range end index 182 out of range for slice of length 180 in table_scan_executor"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Potential performance regression in multi-threaded I/O handling for AArch64 architecture, specifically on openEuler 22.03 with Kunpeng 920 CPUs",
    "issue_number": 17036,
    "title": "Compared with v7.5.1, v8.1.0 has a 7%-10% performance regression in oltp_read_write on openEuler 22.03 (LTS-SP1)"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 17037,
    "title": "tikv-ctl raft region command output should include the region state"
  },
  {
    "bug_location": "raftstore/apply",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Unexpected error handling during entries retrieval in Raft apply process, causing an unwrap() panic on an Err value of 'EntriesUnavailable'",
    "issue_number": 17040,
    "title": "tikv panic when run sysbench prepare"
  },
  {
    "bug_location": "raft_server",
    "severity": 4,
    "categories": [
      "Network",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Potential leadership election or communication failure during Raft server initialization, resulting in 'not leader' gRPC error",
    "issue_number": 17053,
    "title": "In-memory engine: failed to start raft_server in jepsen test"
  },
  {
    "bug_location": "pd-client",
    "severity": 3,
    "categories": [
      "Performance",
      "Network",
      "Config"
    ],
    "root_cause": "Potential excessive retry or reconnection logic when PD instances are down, causing high CPU utilization in pd-worker and timer threads",
    "issue_number": 17054,
    "title": "the pd-work cpu is nearly 100% when tikv disconnected from pd"
  },
  {
    "bug_location": "Memory Allocation",
    "severity": 4,
    "categories": [
      "Memory",
      "CodeBug"
    ],
    "root_cause": "Deadlock in jemalloc profiling configuration when enabled with specific settings",
    "issue_number": 17057,
    "title": "Deadlock when enable jemalloc prof."
  },
  {
    "bug_location": "In-memory Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Potential data corruption or inconsistent state during long-running TPCC benchmark, causing default key/table not found error",
    "issue_number": 17060,
    "title": "In-memory engine: default not found in long tpcc run"
  },
  {
    "bug_location": "config/mod.rs",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Unhandled config value type for 'raftstore.periodic-full-compact-start-times' in config change processing, causing panic when parsing Schedule type",
    "issue_number": 17066,
    "title": "tikv panic when update config online for \u201craftstore.periodic-full-compact-start-times\u201d"
  },
  {
    "bug_location": "Region Management",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "Excessive memory consumption when handling a large number of regions, causing memory exhaustion before reaching system memory limit",
    "issue_number": 17072,
    "title": "TiKV may OOM if too many regions on a TiKV server "
  },
  {
    "bug_location": "MVCC Transaction Layer",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Potential race condition or inconsistent state during transaction prewrite involving deleted keys and region split operations",
    "issue_number": 17074,
    "title": " In-memory engine: assertion failed during prewrite"
  },
  {
    "bug_location": "Build/Docker Configuration",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Dockerfile references outdated `rust-toolchain` file instead of new `rust-toolchain.toml` configuration file introduced in a recent commit",
    "issue_number": 17075,
    "title": "make docker fails in v8.1.0"
  },
  {
    "bug_location": "Region Worker / Snapshot Apply",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Replication"
    ],
    "root_cause": "High number of L0 files blocking snapshot ingestion, causing significant delay in snapshot application process",
    "issue_number": 17078,
    "title": "Region is applying snapshot for a long time"
  },
  {
    "bug_location": "region_cache_memory_engine/read.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Iterator direction handling issue in region cache memory engine, causing assertion failure when attempting to reverse iteration direction",
    "issue_number": 17079,
    "title": "In-memory Engine: assertion failed: self.direction == Direction::Backward"
  },
  {
    "bug_location": "region_cache_memory_engine/keys.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect endian encoding of sequence number and value type in key encoding, causing mismatch with RocksDB's internal key encoding format",
    "issue_number": 17082,
    "title": "In-memory Engine: u64 endian in key encoding is wrong"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Log synchronization gap during region merge process, likely caused by inconsistent replication state after mode change from sync_recover to sync",
    "issue_number": 17084,
    "title": "(dr-autosync)merge region hang for 28min after replication mode change from sync_recover to sync"
  },
  {
    "bug_location": "src/coprocessor_v2/plugin_registry.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Missing function definition for `is_library_file` during FreeBSD build, likely a compilation scope/import issue",
    "issue_number": 17090,
    "title": "Building on FreeBSD fails"
  },
  {
    "bug_location": "tikv_alloc",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Unstable Rust feature `core_intrinsics` used on stable release channel, causing compilation failure on NixOS",
    "issue_number": 17098,
    "title": "Compilation and packaging issues on NixOS"
  },
  {
    "bug_location": "RocksDB Mutex / Write Batch",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Deadlock during concurrent SST ingestion and write operations due to mutex acquisition order and waiting for pending writes",
    "issue_number": 17100,
    "title": "In-memory Engine: Deadlock during TPCC prepare "
  },
  {
    "bug_location": "Raft/Region Hibernation",
    "severity": 4,
    "categories": [
      "Performance",
      "Replication",
      "Network"
    ],
    "root_cause": "Excessive message traffic when regions awaken, causing potential message blocking and delayed heartbeats during cluster state transitions",
    "issue_number": 17101,
    "title": "Commit log duration is high when hibernate regions are awake"
  },
  {
    "bug_location": "In-memory Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Write batch not properly cleared after load failure, potentially causing data inconsistency in lock column family",
    "issue_number": 17103,
    "title": "In-memory Engine: cached_write_batch is not cleared after load failure"
  },
  {
    "bug_location": "In-memory Engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Missing concurrency control mechanism between delete range and write operations, causing potential race condition and data inconsistency",
    "issue_number": 17104,
    "title": "In-memory Engine: race condition among delete range and write to memory engine"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 17106,
    "title": "In-memory Engine: memory usage goes to negative number"
  },
  {
    "bug_location": "CDC (Change Data Capture) and Log Backup components",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "Transaction"
    ],
    "root_cause": "Default 5-second timeout for leader checking in resolved_ts mechanism, causing potential significant lag during node unreachability",
    "issue_number": 17107,
    "title": "check_leader of resolved_ts used in CDC and log-backup always uses default timeout which is 5s"
  },
  {
    "bug_location": "region_cache_memory_engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Incorrect sequence number management in cache engine causing potential write operation conflicts",
    "issue_number": 17114,
    "title": "In-memory Engine: cache engine's sequence number may differ from rocksdb "
  },
  {
    "bug_location": "region_cache_memory_engine",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Local clock used for safe_point instead of centralized PD timestamp, potentially causing cache unavailability due to clock drift",
    "issue_number": 17123,
    "title": "In-memory Engine: `safe_point` in `gc_range` should be obtained from PD"
  },
  {
    "bug_location": "CDC (Change Data Capture)",
    "severity": 4,
    "categories": [
      "Performance",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Potential resolved timestamp tracking instability during high-volume data replication with large number of tables and regions",
    "issue_number": 17124,
    "title": "resolved ts lag jiter to 40s"
  },
  {
    "bug_location": "In-memory Engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Direct deletion of lock column family without writing tombstone, violating write atomicity in RocksDB and in-memory engine write process",
    "issue_number": 17127,
    "title": "In-memory Engine: lock should not be deleted directly"
  },
  {
    "bug_location": "In-memory Engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Current implementation of `evict_range` does not handle scenarios where multiple cached ranges are present during concurrent delete and merge operations",
    "issue_number": 17131,
    "title": "In-memory Engine: `range` to evict may contain multiple cached range"
  },
  {
    "bug_location": "Monitoring",
    "severity": 2,
    "categories": [
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Inaccurate gRPC request source duration metrics calculation in Grafana dashboard",
    "issue_number": 17133,
    "title": "TiDB Grafana gRPC request source duration is inaccurate"
  },
  {
    "bug_location": "In-memory Engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Race condition during range eviction where concurrent data loading is not properly handled",
    "issue_number": 17139,
    "title": "In-memory Engine: eviction should also consider loading ranges"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 17140,
    "title": "In-memory Engine: encoding keys for lock cf should not contain mvcc in delete range"
  },
  {
    "bug_location": "region_cache_memory_engine/background.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Missing boundary specification during garbage collection process in memory engine, potentially leading to incomplete or incorrect data cleanup",
    "issue_number": 17143,
    "title": "In-memory Engine: boundaries are not specified when GC"
  },
  {
    "bug_location": "In-memory Engine",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Incorrect tag management during region split, causing potential debugging inaccuracies",
    "issue_number": 17147,
    "title": "In-memory Engine: the tag in `CacheRange` may be inaccurate "
  },
  {
    "bug_location": "region_cache_memory_engine/background",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential data inconsistency between in-memory engine and RocksDB during background audit process, possibly related to handling of dropped tables or UnsafeDestroyRange",
    "issue_number": 17148,
    "title": "In-memory Engine: entered unreachable code in `next_to_match`"
  },
  {
    "bug_location": "log backup component",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Network"
    ],
    "root_cause": "Potential I/O bottleneck with single thread flushing to object storage, causing significant backup lag and reduced flush speed",
    "issue_number": 17150,
    "title": "Log backup flush speed only ~20MB/s and log backup lag up to 13m"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 17153,
    "title": "In-memory Engine: loading range should be considered in eviction"
  },
  {
    "bug_location": "PD (Placement Driver) Region Check Component",
    "severity": 4,
    "categories": [
      "Performance",
      "LoadBalance",
      "Replication"
    ],
    "root_cause": "Potential regression in region down-peer checking mechanism between TiKV versions 6.5.9 and 6.5.10, causing increased processing time",
    "issue_number": 17162,
    "title": "Compared to version 6.5.9, the duration for completing the region check down-peer in version 6.5.10 has significantly increased."
  },
  {
    "bug_location": "In-Memory Engine (IME)",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential cache corruption or inconsistent loading of system tables during in-memory engine operations, causing user authentication records to be improperly cached or discarded",
    "issue_number": 17163,
    "title": "In-memory Engine: SELECT command denied to user 'root'@'%"
  },
  {
    "bug_location": "backup-restore component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Premature termination of backup task when initial batch of regions do not intersect with specified subranges",
    "issue_number": 17168,
    "title": "backup: requests for subranges potentially being terminated prematurely."
  },
  {
    "bug_location": "CDC Component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Unreachable code path in CDC delegate finish_scan_locks method, likely triggered by I/O hang injection during changefeed operation",
    "issue_number": 17172,
    "title": "tikv panic with \"[lib.rs:478] [\\\"internal error: entered unreachable code\\\"] \u201c when injection ticdc owner io hang"
  },
  {
    "bug_location": "Channel Communication Layer",
    "severity": 4,
    "categories": [
      "Memory",
      "CodeBug"
    ],
    "root_cause": "Race condition in channel message handling causing memory retention after receiver disconnection",
    "issue_number": 17178,
    "title": "Occasional memory leak in std mpsc channels and crossbeam channels "
  },
  {
    "bug_location": "Storage Engine",
    "severity": 3,
    "categories": [
      "Storage",
      "Performance"
    ],
    "root_cause": "Delayed SST file loading after ingestion causing checksum verification failures during data restore operations",
    "issue_number": 17182,
    "title": "In-memory Engine: range should be evicted after sst ingestion "
  },
  {
    "bug_location": "Region Transfer/Scheduling",
    "severity": 5,
    "categories": [
      "Replication",
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "Region transfer failure during TiKV cluster scale-in operation, preventing complete node removal",
    "issue_number": 17191,
    "title": "tikv scaling in blocked due to one region can not transfer"
  },
  {
    "bug_location": "Table Metadata Management",
    "severity": 4,
    "categories": [
      "Memory",
      "Storage",
      "Performance"
    ],
    "root_cause": "Inefficient memory allocation and metadata handling when creating a large number of tables, causing excessive memory consumption",
    "issue_number": 17199,
    "title": "Creating tons of tables make TiKV OOM"
  },
  {
    "bug_location": "CDC (Change Data Capture)",
    "severity": 4,
    "categories": [
      "Transaction",
      "Performance",
      "Replication"
    ],
    "root_cause": "Transaction rollback mechanism interfering with CDC change tracking process",
    "issue_number": 17211,
    "title": "rollback a txn_source specified transaction may blocks cdc"
  },
  {
    "bug_location": "components/raftstore/src/store/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Snapshot generation process does not handle peer removal gracefully, causing blocking of snapshot and subsequent configuration changes when a peer is removed immediately after requesting a snapshot",
    "issue_number": 17213,
    "title": "Raft snapshot blocked when peer is removed right after requesting snapshot"
  },
  {
    "bug_location": "In-memory engine",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential snapshot inconsistency during RocksDB write operations, causing delete operation to be lost during cross-check task",
    "issue_number": 17216,
    "title": "In-memory engine: panic in cross check task"
  },
  {
    "bug_location": "raftstore/peer_storage",
    "severity": 5,
    "categories": [
      "Network",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Failed to retrieve snapshot during network partition, causing Raft leader to panic when attempting to send append entries",
    "issue_number": 17226,
    "title": "tikv panic with \"unexpected error: Store(Other(\\\\\\\"[components/raftstore/src/store/peer_storage.rs:545]: failed to get snapshot after 5 times\\\\\\\"  when injection network partition"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Negative lock count modification in CDC delegate, indicating a synchronization or state tracking error during data replication",
    "issue_number": 17229,
    "title": "TiKV Panic: lock_count_modify should never be negative, start_ts:xxx"
  },
  {
    "bug_location": "CDC Component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Unexpected None value during lock count modification in CDC delegate processing, causing an unwrap() panic",
    "issue_number": 17230,
    "title": "tikv panic: called `Option::unwrap()` on a `None` value"
  },
  {
    "bug_location": "CDC (Change Data Capture)",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Improper event handling after encountering an EventError, allowing unfinished incremental scans to continue emitting events to downstream",
    "issue_number": 17233,
    "title": "cdc: no more any events should be emit to a downstream after it retrieves an event error"
  },
  {
    "bug_location": "Profiling/Monitoring Component",
    "severity": 3,
    "categories": [
      "Monitoring",
      "CodeBug"
    ],
    "root_cause": "Persistent error state in CPU profiling mechanism preventing proper profiling reset or termination",
    "issue_number": 17234,
    "title": "Always keep reporting `Already in CPU Profiling`"
  },
  {
    "bug_location": "Metrics/Instrumentation",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Inconsistent documentation and implementation of `ingest_external_file_optimized` function, leading to misleading metrics reporting about SST file ingestion blocking behavior",
    "issue_number": 17239,
    "title": "metrics: contradictory statistics on `non-block` and `block` flushing when `INGEST`."
  },
  {
    "bug_location": "In-memory engine / Range Cache",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Race condition in delete_range operation where snapshot guarantees are violated, potentially allowing access to data that should have been deleted",
    "issue_number": 17243,
    "title": "In-memory engine: evict in delete_range is too late"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 17244,
    "title": "[client-bug] java-client test with tikv 6.9.3 failed. "
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 17245,
    "title": "In-memory Engine: Duplicate error when range cache enabled"
  },
  {
    "bug_location": "backup-restore",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Replication",
      "Storage"
    ],
    "root_cause": "Read index not ready during region merge, preventing log backup snapshot retrieval during TiKV upgrade",
    "issue_number": 17249,
    "title": "log backup task gets stuck and reports error when tikv upgrade"
  },
  {
    "bug_location": "Transaction",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Potential transaction conflict or inconsistent transaction state during concurrent read-write operations with in-memory engine",
    "issue_number": 17252,
    "title": "In-memory Engine: sysbench oltp_read_write fails with \"assertion failed\""
  },
  {
    "bug_location": "Raft/Region",
    "severity": 5,
    "categories": [
      "Replication",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Incorrect data deletion after snapshot application, potentially caused by a race condition in range deletion logic introduced in PR 17121",
    "issue_number": 17258,
    "title": "tpcc report \"failed Error 8133: data inconsistency in table: customer, index: PRIMARY, index-count:1 != record-count:0\\r\\n\" during tikv balance"
  },
  {
    "bug_location": "In-memory Engine Range Management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Incomplete range deletion logic that fails to properly handle canceled or overlapping range deletion tasks, leading to potential data inconsistency during snapshot and range eviction processes",
    "issue_number": 17259,
    "title": "In-memory Engine: delete range task has been scheduled multiple times"
  },
  {
    "bug_location": "RocksDB/RangeCacheEngine",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Circular mutex/lock dependency between cleanup-worker, apply threads, and RocksDB mutex causing thread deadlock during concurrent write operations",
    "issue_number": 17260,
    "title": "In-memory Engine: Deadlock during TPCC run"
  },
  {
    "bug_location": "Documentation",
    "severity": 2,
    "categories": [
      "Human",
      "Config"
    ],
    "root_cause": "Inaccurate build prerequisites documentation for MacOS platform, where gcc actually refers to clang",
    "issue_number": 17265,
    "title": "Make CONTRIBUTING.md prerequisites more accurate"
  },
  {
    "bug_location": "backup-stream/subscription_manager",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Concurrent test execution causing file system race condition with temp file creation, resulting in 'File exists' error when multiple tests attempt to create the same temporary file",
    "issue_number": 17268,
    "title": "test_unretryable_failure failed when \"make test\""
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "Performance",
      "Replication",
      "LoadBalance"
    ],
    "root_cause": "Performance degradation during TiFlash recovery, likely due to high CPU load in RaftStore during node recovery and rebalancing",
    "issue_number": 17270,
    "title": "tikv stability: qps drop 67% when one of tiflash recovers from failure due to high cpu of raftstore"
  },
  {
    "bug_location": "RocksDB Storage Layer",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Storage",
      "Performance"
    ],
    "root_cause": "Unexpected change in bloom filter naming convention during RocksDB upgrade, causing existing SST file filters to become invalidated",
    "issue_number": 17272,
    "title": "Bloom filter is invalidated after upgrade from <=7.1 to >=7.2"
  },
  {
    "bug_location": "components/raftstore/src/store/fsm/apply.rs",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Inefficient handling of priority and SST ingestion in apply context, causing unnecessary commits and potential performance overhead during batch processing of Raft entries",
    "issue_number": 17278,
    "title": "Reduce `apply_ctx.commit(self)` if there are ingest SST commands applying"
  },
  {
    "bug_location": "In-memory engine safe point management",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Incorrect safe point inheritance and tracking during range eviction, which can cause snapshot read consistency violations",
    "issue_number": 17282,
    "title": "In-memory engine: safe point may be larger than snapshot read ts"
  },
  {
    "bug_location": "OpenSSL Dependency",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Security"
    ],
    "root_cause": "Undefined behavior in OpenSSL's MemBio buffer handling when buffer is empty",
    "issue_number": 17291,
    "title": "cargo deny warns RUSTSEC-2024-0357"
  },
  {
    "bug_location": "Raft Log Handling",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Excessive logging of large Raft log entries causing performance overhead and potential log bloat",
    "issue_number": 17294,
    "title": "Slow log is too large"
  },
  {
    "bug_location": "Flow Control Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Unexpected flow control mechanism triggered during range destruction operation, potentially causing performance degradation or operation interruption",
    "issue_number": 17304,
    "title": "Flow control is triggered when performing destroy range"
  },
  {
    "bug_location": "Backup and Restore (PITR) component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Recovery"
    ],
    "root_cause": "Safepoint cleanup mechanism fails to remove store-level safepoints when PITR log backup task is stopped or paused",
    "issue_number": 17316,
    "title": "pitr: store level safepoint not cleaned"
  },
  {
    "bug_location": "raft-engine",
    "severity": 3,
    "categories": [
      "Storage",
      "CodeBug",
      "Recovery"
    ],
    "root_cause": "Incorrect decoding of log headers during disk flush, causing unexpected offset and potential panic when starting TiKV",
    "issue_number": 17325,
    "title": "raft-engine: panic on decoding unexpected bytes when starting."
  },
  {
    "bug_location": "Backup and Restore (BR) Import Component",
    "severity": 4,
    "categories": [
      "Memory",
      "Performance",
      "Storage"
    ],
    "root_cause": "Insufficient memory allocation for log restore process, causing repeated download and apply file failures during Point-in-Time Recovery (PiTR)",
    "issue_number": 17327,
    "title": "PiTR log restore is very slow, lots of \"download and apply file failed\" due to \"memory is limited\" and OOM seen."
  },
  {
    "bug_location": "Coprocessor",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect string comparison handling for Unicode whitespace characters in LIKE operation",
    "issue_number": 17332,
    "title": "copr: the behavior of `like` is not correct for string with characters equal with space"
  },
  {
    "bug_location": "Raftstore/Peer Transfer",
    "severity": 4,
    "categories": [
      "Transaction",
      "Replication",
      "Performance"
    ],
    "root_cause": "Region configuration change (ConfChange) gets stuck during IO hang, preventing leader transfer and causing QPS to drop to zero",
    "issue_number": 17363,
    "title": "qps continued to drop to zero during one of tikv io hang"
  },
  {
    "bug_location": "Backup & Restore (BR) Dashboard",
    "severity": 3,
    "categories": [
      "Monitoring",
      "Config"
    ],
    "root_cause": "Missing configuration of recently added stream backup metrics in Grafana dashboard",
    "issue_number": 17369,
    "title": "br: recently added metrics are not configured in the grafana dashboard, audit and sync all br metrics "
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 17370,
    "title": "TiKV build fails in MacOS"
  },
  {
    "bug_location": "Region Management",
    "severity": 3,
    "categories": [
      "Performance",
      "LoadBalance",
      "Storage"
    ],
    "root_cause": "Performance regression when handling massive number of empty regions, likely due to inefficient region management or metadata processing overhead in V2 implementation",
    "issue_number": 17375,
    "title": "Although performance degradation under massive regions is a known issue, in addition to merging regions, are there other methods available?"
  },
  {
    "bug_location": "Region Merge Scheduler",
    "severity": 4,
    "categories": [
      "Performance",
      "Memory",
      "LoadBalance"
    ],
    "root_cause": "Inefficient region merge scheduling causing excessive memory consumption and task backlog during large-scale database restoration",
    "issue_number": 17376,
    "title": "During the process of merging regions, TiKV's execution speed is very slow, and it occurs multipl OOM events."
  },
  {
    "bug_location": "raft_log_engine",
    "severity": 5,
    "categories": [
      "Storage",
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Potential corruption in file sequence number handling during log rewriting, causing unexpected index range errors in Raft log engine",
    "issue_number": 17383,
    "title": "tikv panic with \u201c[\\\"unexpected error: Store(Other(Other(\\\\\\\"[components/raft_log_engine/src/engine.rs:807]: Corruption: file seqno out of range\\\\\\\")))\u201d when run sysbench prepare"
  },
  {
    "bug_location": "Monitoring",
    "severity": 2,
    "categories": [
      "Monitoring",
      "CodeBug"
    ],
    "root_cause": "Incorrect metric calculation using 'Replicate Raft Log' instead of commit log duration",
    "issue_number": 17390,
    "title": "Metric: should use commit log instead of \"Replicate Raft Log\" in Duration Panel"
  },
  {
    "bug_location": "lock_manager/waiter_manager.rs",
    "severity": 5,
    "categories": [
      "Transaction",
      "Memory",
      "Performance"
    ],
    "root_cause": "Unbatched RPC requests during lock contention scenarios causing excessive message overhead and potential message channel accumulation",
    "issue_number": 17394,
    "title": "Memory usage may grow unexpectedly and causes OOM in some high-concurrency lock contention scenarios"
  },
  {
    "bug_location": "range_cache_memory_engine/background.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Race condition during region split where region state tracking fails to update correctly for split regions before snapshot loading",
    "issue_number": 17403,
    "title": "In-memory engine: pending region split may cause tikv panic"
  },
  {
    "bug_location": "range_cache_memory_engine/range_manager.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Incorrect handling of region epoch changes during snapshot application, causing state mismatch between expected and actual region state during split operation",
    "issue_number": 17405,
    "title": "In-memory Engine: assert failed \"left: Active != right: Pending\" on RegionManager::split_region"
  },
  {
    "bug_location": "server/encryption",
    "severity": 5,
    "categories": [
      "Security",
      "Config",
      "CodeBug"
    ],
    "root_cause": "Failure in decrypting master key during key rotation with Google Cloud KMS, likely due to authentication or key access issues",
    "issue_number": 17410,
    "title": "encryption: error occurs when rotating master key"
  },
  {
    "bug_location": "Backup and Restore (BR) Component",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Flashback operation incorrectly handles transaction locks and commit records, potentially breaking transaction atomicity by partially rolling back multi-key transactions",
    "issue_number": 17415,
    "title": "Flashback may break transaction atomicity"
  },
  {
    "bug_location": "TiKV Load Balance / Node Detection",
    "severity": 3,
    "categories": [
      "LoadBalance",
      "Performance"
    ],
    "root_cause": "False-positive slow node detection algorithm during TiKV scaling, causing unexpected leader eviction",
    "issue_number": 17444,
    "title": "the tikv that is being scaled is incorrectly identified as a slow node"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 17456,
    "title": "In-memory engine: prop test failed"
  },
  {
    "bug_location": "Log Backup Component",
    "severity": 4,
    "categories": [
      "Performance",
      "Network",
      "Storage"
    ],
    "root_cause": "Single-threaded log file upload process becomes bottlenecked under network latency, causing significant backup checkpoint advancement delays",
    "issue_number": 17464,
    "title": "Log flush is very slow when injecting S3 network latency 50ms for 10m"
  },
  {
    "bug_location": "raftstore/store/fsm/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Race condition during peer destruction and snapshot processing, where a peer's metadata is removed while a snapshot message is still being processed",
    "issue_number": 17469,
    "title": "raftstore: TiKV panics due to self was destroyed from meta during receiving snapshot"
  },
  {
    "bug_location": "range_cache_memory_engine/range_manager.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Memory"
    ],
    "root_cause": "Unexpected missing key during region iteration in the in-memory engine's region manager, causing a panic when attempting to access a non-existent entry",
    "issue_number": 17483,
    "title": "In-memory Engine: `iter_overlapped_regions` panics no entry found for key"
  },
  {
    "bug_location": "range_cache_memory_engine",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Assertion failure in region range containment check during snapshot loading, likely due to inconsistent region metadata or range management",
    "issue_number": 17484,
    "title": "In-memory Engine: `on_snapshot_load_finished` assertion failed: region.contains_range()"
  },
  {
    "bug_location": "range_cache_memory_engine/background",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Potential race condition or incorrect state management during region loading, causing an unexpected state transition from 'Active' to 'LoadingCanceled'",
    "issue_number": 17487,
    "title": "In-memory Engine: `BackgroundTask::LoadRegion` assert fails"
  },
  {
    "bug_location": "range_cache_memory_engine",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Memory"
    ],
    "root_cause": "Potential assertion failure during snapshot loading in the in-memory engine's background processing, likely due to unexpected region metadata state",
    "issue_number": 17493,
    "title": "In-memory Engine: `on_snapshot_load_finished` assert fails \"region meta: CacheRegionMeta ...\""
  },
  {
    "bug_location": "raftstore/failpoints",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Race condition in stale read test with failpoint, causing async write callback to be dropped unexpectedly",
    "issue_number": 17497,
    "title": "Flaky test tests::failpoints cases::test_replica_stale_read::test_update_apply_index_before_sync_read_state"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 17502,
    "title": "prewrite generation should be sent to TiCDC"
  },
  {
    "bug_location": "Backup and Restore (BR) Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Disk space exhaustion during log restore due to suspended garbage collection, causing excessive MVCC versions and compaction space consumption",
    "issue_number": 17508,
    "title": "TiKV disk full during PiTR log restore"
  },
  {
    "bug_location": "Backup & Restore (PiTR)",
    "severity": 4,
    "categories": [
      "Recovery",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Potential region unavailability during log replay and recovery process, causing extended service interruption after Point-in-Time Recovery",
    "issue_number": 17511,
    "title": " \"[tikv:9005]Region is unavailable\" seen for ~1.5h after PiTR restore"
  },
  {
    "bug_location": "range_cache_memory_engine/range_manager.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Memory"
    ],
    "root_cause": "Assertion failure in `new_region_meta` method during region metadata management, likely due to unexpected state or invalid region metadata during background eviction process",
    "issue_number": 17548,
    "title": "In-memory Engine: `new_region_meta` assert fails"
  },
  {
    "bug_location": "range_cache_engine",
    "severity": 2,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Potential race condition or timeout issue in range cache engine test causing intermittent failures",
    "issue_number": 17556,
    "title": "flaky test: test_load_with_split"
  },
  {
    "bug_location": "In-Memory Engine (IME)",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Region loading mechanism only triggered by write operations, causing read-heavy regions to remain uncached",
    "issue_number": 17561,
    "title": "In-memory engine: pending region may never load"
  },
  {
    "bug_location": "In-memory engine hotspot region identification",
    "severity": 3,
    "categories": [
      "Performance",
      "LoadBalance"
    ],
    "root_cause": "Slow automatic hotspot region detection mechanism when manual load is disabled, causing extended performance ramp-up time",
    "issue_number": 17562,
    "title": "In-memory engine: auto load is slow to identify hotspot regions"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 17568,
    "title": "PR#17555 may cause 7.4% performance regression in benchmarksql"
  },
  {
    "bug_location": "RangeCacheMemoryEngine",
    "severity": 2,
    "categories": [
      "Config",
      "CodeBug"
    ],
    "root_cause": "Configuration changes are not dynamically tracked due to using a raw config value instead of a tracked version, preventing runtime configuration updates from taking effect",
    "issue_number": 17578,
    "title": "In-memory engine: online configuration change does not take effect for some configs"
  },
  {
    "bug_location": "src/server/raftkv/mod.rs",
    "severity": 3,
    "categories": [
      "Performance",
      "Monitoring"
    ],
    "root_cause": "Incorrectly including task wait time in scheduler worker pool duration metric, causing metric inflation during high load scenarios",
    "issue_number": 17579,
    "title": "Storage async write duration is inaccurate"
  },
  {
    "bug_location": "metrics/grafana/common.py",
    "severity": 2,
    "categories": [
      "Monitoring",
      "CodeBug"
    ],
    "root_cause": "Incorrect metric calculation method using rate() instead of increase() for heatmap panel, which prevents accurate count representation",
    "issue_number": 17586,
    "title": "heatmap panel should use increase instead of rate"
  },
  {
    "bug_location": "Flow Control Priority Limiter",
    "severity": 4,
    "categories": [
      "Performance",
      "Config"
    ],
    "root_cause": "Async speed limit mechanism with fixed 0.1 second wait time causing excessive task blocking and high tail latency",
    "issue_number": 17589,
    "title": "tail lantency is too high when task is limited by flow control priority limiter"
  },
  {
    "bug_location": "In-memory Engine Coprocessor",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incomplete handling of region processing when process count is zero in coprocessor statistics auto-load mechanism",
    "issue_number": 17592,
    "title": "In-memory Engine: auto load does not handle the case that process=0"
  },
  {
    "bug_location": "Raft/Region Split Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Region split operation not handling joint consensus state correctly, causing potential deadlock during concurrent leader transfer and region split operations",
    "issue_number": 17602,
    "title": "BatchSplit hang at joint state when execute `BatchSplit` and `move-hot-read-leader` at same time"
  },
  {
    "bug_location": "Resource Control Module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Duplicate resource consumption and potential unnecessary sleep in transaction task handling, causing inefficient resource management",
    "issue_number": 17603,
    "title": "resource_control: txn task may consume and sleep(if reached quota) twice"
  },
  {
    "bug_location": "CDC (Change Data Capture) Initial Scan Task",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Initial scan tasks are not properly terminated or cancelled when changefeeds are paused, failed, or deleted, causing lingering background processes that block subsequent changefeed initializations",
    "issue_number": 17606,
    "title": "TiKV initial scan task not cancel/termicated when changefeed paused or failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 17607,
    "title": "[status_server] no heap dump result via http"
  },
  {
    "bug_location": "Expression Evaluation Engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Insufficient handling of integer overflow during date arithmetic operations, potentially causing incorrect calculation results",
    "issue_number": 17608,
    "title": "expression: date_add/date_sub does not handle overflow calculation properly"
  },
  {
    "bug_location": "resource_control/future.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Timing assertion in test case is too strict, causing intermittent test failures due to slight variations in execution duration",
    "issue_number": 17612,
    "title": "flaky test test_limited_future"
  },
  {
    "bug_location": "RocksDB Engine Component",
    "severity": 5,
    "categories": [
      "Storage",
      "CodeBug",
      "Recovery"
    ],
    "root_cause": "Invalid RocksDB options parsing during engine recovery, causing panic when loading latest options with malformed configuration",
    "issue_number": 17614,
    "title": "tikv panic repeatedly after injection one of tikv failure and recovery"
  },
  {
    "bug_location": "TiKV Query Codec/Collation Module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Storage"
    ],
    "root_cause": "Incorrect handling of GBK character encoding length, where 4-byte column definition is misinterpreted as 4-byte limit instead of 4-character limit",
    "issue_number": 17618,
    "title": "`encoding failed` when processing queries with GBK/GB18030"
  },
  {
    "bug_location": "CDC (Change Data Capture) Incremental Scan",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Inefficient handling of RocksDB tombstones during frequent table/partition drops, causing expensive iterator scanning across entire regions with continuous tombstones",
    "issue_number": 17620,
    "title": "truncate or drop table frequently may increase cdc incremental scan CPU usage obviously"
  },
  {
    "bug_location": "in_memory_engine",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Unexpected non-empty save points during write batch processing in the in-memory engine, causing an assertion failure",
    "issue_number": 17630,
    "title": "In-memory engine: save point empty assertion failed"
  },
  {
    "bug_location": "raftstore/store/fsm/peer",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Unexpected `None` value during region bucket refresh, likely caused by incorrect compilation settings in hybrid-engine crate",
    "issue_number": 17639,
    "title": "tikv panic repeatedly when tikv rolling restart after update region size"
  },
  {
    "bug_location": "Memory Management",
    "severity": 4,
    "categories": [
      "Memory",
      "Config",
      "Performance"
    ],
    "root_cause": "Potential memory allocation issue with sysinfo crate on rocky-8.9 with cgroup v2, causing unexpected OOM during TPC-C benchmark",
    "issue_number": 17641,
    "title": "tikv oom when using tiup on `rocky-8.9` for deployment"
  },
  {
    "bug_location": "GC Worker",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Direct RocksDB delete_range call bypasses WriteBatchWrapper, preventing proper region eviction tracking",
    "issue_number": 17644,
    "title": "In-memory Engine: GC worker delete range does not evict relevant regions"
  },
  {
    "bug_location": "TiKV Storage Engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Potential MVCC (Multi-Version Concurrency Control) inconsistency during concurrent delete and read operations, causing index and record count mismatch",
    "issue_number": 17645,
    "title": "In-memory Engine: data inconsistency in table: xx, index: yy, index-count:50 != record-count:49"
  },
  {
    "bug_location": "raftstore/replica_read",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Race condition in stale read state synchronization during failpoint testing, causing potential inconsistent read index message handling",
    "issue_number": 17654,
    "title": "Flaky test `tests::failpoints cases::test_replica_stale_read::test_update_apply_index_before_sync_read_state`"
  },
  {
    "bug_location": "in_memory_engine/region_manager",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Synchronization issue with in-memory engine's garbage collection state, causing an assertion failure during region GC process",
    "issue_number": 17667,
    "title": "In-memory engine: in gc assertion failed"
  },
  {
    "bug_location": "CI/Build System",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Dependency removal in build configuration causing plugin library not being built during test setup",
    "issue_number": 17671,
    "title": "build: CI tasks are failed on testing loading coprocessor plugins."
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 17673,
    "title": "tidb 5.7.25-TiDB-v5.4.3\u4e2d\u552f\u4e00\u7d22\u5f15\u9650\u5236\u5931\u6548"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 17676,
    "title": "grafana histogram parse error "
  },
  {
    "bug_location": "PD Worker / Slow Score Mechanism",
    "severity": 3,
    "categories": [
      "Performance",
      "LoadBalance",
      "Transaction"
    ],
    "root_cause": "Slow score update mechanism fails to accurately reflect IO hang conditions, preventing proper node health detection",
    "issue_number": 17679,
    "title": "Slow score fails to update during IO hang"
  },
  {
    "bug_location": "Cargo/Dependency Management",
    "severity": 2,
    "categories": [
      "Config",
      "Upgrade"
    ],
    "root_cause": "Yanked crate futures-util 0.3.15 detected in Cargo.lock, which can cause build and dependency resolution issues",
    "issue_number": 17689,
    "title": "yanked crate futures-util 0.3.15"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 17691,
    "title": "add index is slowly compared to last version after upgrade rocksdb"
  },
  {
    "bug_location": "CDC (Change Data Capture) Component",
    "severity": 5,
    "categories": [
      "Memory",
      "Performance",
      "Transaction"
    ],
    "root_cause": "CDC tasks piling up in channel, with 'old_value_cb' holding snapshots and slowing down processing, leading to memory exhaustion when Titan storage is enabled and region sizes are large",
    "issue_number": 17696,
    "title": "TiDB latency not stable and TiKV OOM if running cdc changefeed when titan is on and average region size is large"
  },
  {
    "bug_location": "in_memory_engine/write_batch.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Inconsistent region state during write preparation, where the region being prepared for write is not the current active region",
    "issue_number": 17700,
    "title": "In-memory Engine: assert fails region 26972 is prepared for write before, but it is not the current region"
  },
  {
    "bug_location": "Raft Module",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Inefficient write batch size calculation when large Raft entry max size is configured, causing potential performance degradation and latency spikes",
    "issue_number": 17701,
    "title": "When raft entry max size is large, write batch size would be too large"
  },
  {
    "bug_location": "test infrastructure",
    "severity": 2,
    "categories": [
      "CodeBug"
    ],
    "root_cause": "Potential compilation incompatibility in test modules, possibly related to failpoint configuration or import dependencies",
    "issue_number": 17707,
    "title": "*: Some tests fail to compile"
  },
  {
    "bug_location": "Raft/Peer Component",
    "severity": 4,
    "categories": [
      "Replication",
      "Storage",
      "Recovery"
    ],
    "root_cause": "resolved_ts mechanism appears to be stuck, preventing forward progress of backup/restore operations, likely due to leadership or term change issues in Raft consensus",
    "issue_number": 17708,
    "title": "resolved_ts doesn't push forward, it's stuck, which will cause ebs backup failed [restore for 15520]"
  },
  {
    "bug_location": "Backup/Restore Component",
    "severity": 4,
    "categories": [
      "Storage",
      "Config",
      "CodeBug"
    ],
    "root_cause": "Lack of proper configuration validation when Titan storage engine settings differ between backup and restore operations",
    "issue_number": 17709,
    "title": "If titan is enabled for the backup cluster, tikv will crash if titan is not enabled for restore. [restore for 15945]"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 17710,
    "title": "tikv crash during volumerestore [restore for 16673]"
  },
  {
    "bug_location": "Transaction",
    "severity": 4,
    "categories": [
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Incorrect resolved-ts calculation where commit_ts could potentially equal min_commit_ts, violating the resolved-ts definition requirement of commit_ts being strictly greater than resolved_ts",
    "issue_number": 17728,
    "title": "Resolved-ts must be strictly smaller than locks' min_commit_ts when calculating it"
  },
  {
    "bug_location": "TiFlash",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Config",
      "Storage"
    ],
    "root_cause": "Configuration incompatibility during upgrade from v6.1.7 to nightly, specifically related to option 'enable_pipelined_commit' which is no longer supported or has changed",
    "issue_number": 17731,
    "title": "tiflash is always in `Disconnected` state when upgrading from v6.1.7 to nightly"
  },
  {
    "bug_location": "raftstore/raft_log_engine",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Potential data corruption or inconsistency in Raft log sequence during TiFlash node recovery, causing unexpected file sequence number range error",
    "issue_number": 17760,
    "title": "tikv panic after down one of tiflash and recover"
  },
  {
    "bug_location": "hybrid_engine/load_eviction",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Assertion failure during peer destruction when region peers list becomes empty, likely related to in-memory engine load eviction mechanism",
    "issue_number": 17767,
    "title": "[In-memory-engine] tikv panic \"assertion failed: !region.get_peers().is_empty()\" when run workload"
  },
  {
    "bug_location": "CDC (Change Data Capture)",
    "severity": 5,
    "categories": [
      "Performance",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Slow processing of old_value_cb when handling large region sizes (280MB+) with Titan storage enabled, causing significant replication lag",
    "issue_number": 17781,
    "title": "CDC lag up to 2.5h when titan is on and region size is large (280MB+)"
  },
  {
    "bug_location": "In-Memory Engine",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Potential performance instability in in-memory engine configuration during high-load scenarios with MVCC and GC interactions",
    "issue_number": 17782,
    "title": "[In-memory-engine] qps jitter amplitude is larger when enable IME"
  },
  {
    "bug_location": "in_memory_engine/background",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Storage",
      "Memory"
    ],
    "root_cause": "Assertion failure during garbage collection process in in-memory engine background task",
    "issue_number": 17788,
    "title": "[In-memory-engine] tikv panic \"[\\\"assertion failed: success\\\"] [backtrace=\\\"   0: tikv_util::set_panic_hook::{{closure}}\\\\n             at workspace/source/tikv/components/tikv_util/src/lib.rs:479:18\" after run tpcc a long time"
  },
  {
    "bug_location": "Monitoring/Metrics",
    "severity": 3,
    "categories": [
      "Monitoring",
      "Performance"
    ],
    "root_cause": "Histogram metrics for region size only capture measurements during split planning, leading to potential underrepresentation of actual region sizes and potentially misleading monitoring data",
    "issue_number": 17791,
    "title": "Region Approximate Size metrics panel has misleading semantics"
  },
  {
    "bug_location": "in_memory_engine/region_manager.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Assertion failure in region metadata management, indicating a potential race condition or incorrect state tracking during region write operations",
    "issue_number": 17797,
    "title": "flaky test: cases::test_in_memory_engine::test_load_with_eviction"
  },
  {
    "bug_location": "Unit Testing Framework",
    "severity": 2,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "ResizableRuntime implementation causing excessive test runtime overhead",
    "issue_number": 17807,
    "title": "br: The ResizableRuntime make some unit test taking too long to finish"
  },
  {
    "bug_location": "server/storage_stats",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Performance",
      "Recovery"
    ],
    "root_cause": "Unhandled error from statx syscall during storage stats initialization, likely caused by IO failure injection",
    "issue_number": 17818,
    "title": "tikv panic [lib.rs:480] [\\\"called `Result::unwrap()` on an `Err` value: Other(Os { code: 103, kind: ConnectionAborted, message: \\\\\\\"Software caused connection abort\\\\\\\" })\\\""
  },
  {
    "bug_location": "import/sst_service.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Unhandled error condition during PD failure injection while processing SST import, causing unexpected panic in future polling mechanism",
    "issue_number": 17830,
    "title": "tikv panics when injecting pd failure during add multi index"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 17838,
    "title": "In-memory Engine: bank workload has more than 12% regression when IME is enabled"
  },
  {
    "bug_location": "raftstore/store/entry_storage.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Potential race condition or invalid state during region merge operation causing a None unwrap in term retrieval",
    "issue_number": 17840,
    "title": "tikv panic in on_raft_gc_log_tick after region is merged"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 17852,
    "title": "TiKV panic when using `radians` or `degrees`"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 17866,
    "title": "RocksDB bloom filter efficiency metrics need to be fixed"
  },
  {
    "bug_location": "raftstore/entry_storage",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Potential index tracking error in Raft log entry cache causing an assertion failure when appending entries",
    "issue_number": 17868,
    "title": "TiKV panic with \"assertion failed: cached_last < trunc_to_idx\""
  },
  {
    "bug_location": "Backup and Restore (BR) Component",
    "severity": 4,
    "categories": [
      "Upgrade",
      "Performance"
    ],
    "root_cause": "Potential performance bottleneck in restore process, resolved by increasing import thread configuration",
    "issue_number": 17874,
    "title": "br: restore failed on TiDB Cloud"
  },
  {
    "bug_location": "server/resolve.rs and server/raft_client.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Network"
    ],
    "root_cause": "Improper handling of tombstone store resolution in TiKV's address resolution and raft client, causing infinite error logging when attempting to resolve an offline store",
    "issue_number": 17875,
    "title": "resolving tikv store address return an error \"invalid store ID xxxx, not found\" leading to raft client infinite loop"
  },
  {
    "bug_location": "CDC (Change Data Capture) component",
    "severity": 5,
    "categories": [
      "CodeBug",
      "Network",
      "Transaction"
    ],
    "root_cause": "Unhandled network partition causing Option::unwrap() on a None value in CDC sink_delete method, leading to panic",
    "issue_number": 17876,
    "title": "multiple tikv panic when injection network partition between one of tikv and one of tidb"
  },
  {
    "bug_location": "tikv_server",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Network"
    ],
    "root_cause": "Potential signal handling issue on MacOSX with TOGGLE_PROF_SIG, causing panic during signal trap initialization",
    "issue_number": 1860,
    "title": "panic with TOGGLE_PROF_SIG in MacOSX"
  },
  {
    "bug_location": "server/grpc_service.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Incorrect snapshot task scheduling during leader crash, causing incomplete snapshot transfer and potential follower node panic when applying a truncated snapshot",
    "issue_number": 1907,
    "title": "Snapshot truncated if leader crashes during snapshot transferring"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 201,
    "title": "Check if we can apply snapshot when there is overlapped key ranges"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 2080,
    "title": "test: test_server_down_peers failed"
  },
  {
    "bug_location": "RocksDB Build Configuration",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incomplete SSE4.2 compilation flags preventing hardware acceleration of CRC32 computation",
    "issue_number": 2303,
    "title": "static_unportable_release doesn't enable SSE in RocksDB"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 2494,
    "title": "metric: missing PD metric on Grafana"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 2496,
    "title": "bug: StoreNotMatch"
  },
  {
    "bug_location": "RocksDB Configuration Management",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config",
      "Storage"
    ],
    "root_cause": "Inconsistent RocksDB configuration between tikv-server and tikv-ctl, causing user properties to be unintentionally cleaned during database access",
    "issue_number": 2537,
    "title": "tikv-ctl should use the same config for opening rocksdb as tikv-servers"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 2643,
    "title": "storage::test_raftkv::test_read_leader_in_lease randomly failed"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Potential race condition during leader change with uncommitted log entries causing stale command error",
    "issue_number": 2649,
    "title": "test_server_leader_change_with_uncommitted_log fails with stale command"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 2659,
    "title": "test_server_compact_lock_cf fail"
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication",
      "Network"
    ],
    "root_cause": "Potential race condition or instability in leader election during network partition test scenario",
    "issue_number": 2863,
    "title": "test_server_partition_write  failed on master"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 2909,
    "title": "test: test_storage_1gc sometimes failed in CI"
  },
  {
    "bug_location": "RaftStore",
    "severity": 3,
    "categories": [
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Unable to elect a leader during configuration change, potentially causing split brain scenario in Raft consensus",
    "issue_number": 2917,
    "title": "test: raftstore_cases::test_conf_change::test_server_split_brain failed"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Performance"
    ],
    "root_cause": "Potential race condition or timeout issue during lease read operation when node is being destroyed",
    "issue_number": 2948,
    "title": "test: raftstore_cases::test_lease_read::test_node_callback_when_destroyed panic"
  },
  {
    "bug_location": "test/snapshot",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Insufficient wait time during snapshot generation test, causing directory not to be empty as expected",
    "issue_number": 2957,
    "title": "test: test failpoints_cases::test_snap::test_generate_snapshot failed "
  },
  {
    "bug_location": "Unknown",
    "severity": 1,
    "categories": [
      "Unknown"
    ],
    "root_cause": "Insufficient information provided to determine actual bug details",
    "issue_number": 2966,
    "title": "for test sync "
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 2968,
    "title": "for test sync"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 2969,
    "title": "for test sync"
  },
  {
    "bug_location": "Configuration Management",
    "severity": 2,
    "categories": [
      "Config",
      "CodeBug"
    ],
    "root_cause": "Incorrect configuration parameter placement in configuration template file, causing potential misinterpretation of Raft write-ahead log synchronization settings",
    "issue_number": 2977,
    "title": "conf: Raft wal-bytes-per-sync is in the wrong place "
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 2983,
    "title": "for test sync"
  },
  {
    "bug_location": "raftstore/store/util.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance",
      "Monitoring"
    ],
    "root_cause": "Potential timing/clock synchronization issue causing inconsistent lease state detection, possibly related to monotonic time measurement or thread sleep precision",
    "issue_number": 3044,
    "title": "thread 'raftstore::store::util::tests::test_lease' panicked at 'assertion failed"
  },
  {
    "bug_location": "Documentation",
    "severity": 2,
    "categories": [
      "Config",
      "Human"
    ],
    "root_cause": "Broken documentation link in TiKV FAQ that led to a 404 error page",
    "issue_number": 3065,
    "title": "'How to use TiKV' is now unavailable"
  },
  {
    "bug_location": "Docker Build System",
    "severity": 2,
    "categories": [
      "Config",
      "Upgrade"
    ],
    "root_cause": "Docker image build pipeline not automatically updating dependencies and compiler versions",
    "issue_number": 3077,
    "title": "Docker images aren't updating"
  },
  {
    "bug_location": "Raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Unable to get leader of region during snapshot application, likely due to race condition or leadership election instability in Raft consensus protocol",
    "issue_number": 3079,
    "title": "raftstore_cases::test_split_region::test_server_apply_new_version_snapshot' panicked"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Unable to get leader for region during split operation, causing test failure",
    "issue_number": 3080,
    "title": "'raftstore_cases::test_split_region::test_node_base_split_region_left_derive' panicked"
  },
  {
    "bug_location": "Raft Consensus Module",
    "severity": 5,
    "categories": [
      "Replication",
      "Recovery",
      "CodeBug"
    ],
    "root_cause": "Unsafe node recovery mechanism that can potentially delete committed logs and compromise data consistency during cluster failure scenarios",
    "issue_number": 3092,
    "title": "Unsafe recover should only drop some nodes instead of all down nodes"
  },
  {
    "bug_location": "Snapshot Generation",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Recovery"
    ],
    "root_cause": "Test failure indicates potential issue with directory cleanup or snapshot generation process, where the test expects an empty directory but finds it non-empty",
    "issue_number": 3129,
    "title": "test failpoints_cases::test_snap::test_generate_snapshot failed "
  },
  {
    "bug_location": "documentation",
    "severity": 1,
    "categories": [
      "Human"
    ],
    "root_cause": "Typographical error in documentation text",
    "issue_number": 3134,
    "title": "small readme typo"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 3147,
    "title": "Test test_node_learner_conf_change failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 3162,
    "title": "Test test_server_auto_adjust_replica failed"
  },
  {
    "bug_location": "Scheduler",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Synchronous logging in scheduler thread blocking task processing, potentially causing performance bottleneck",
    "issue_number": 3202,
    "title": "Avoid slow log blocking the scheduler"
  },
  {
    "bug_location": "Snapshot Generation Test",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Test expects an empty directory, but temporary files from snapshot generation remain in the test directory",
    "issue_number": 3215,
    "title": "failpoints_cases::test_snap::test_generate_snapshot failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 3231,
    "title": "master CI failed"
  },
  {
    "bug_location": "raftstore/store/peer.rs",
    "severity": 4,
    "categories": [
      "Replication",
      "Transaction",
      "CodeBug"
    ],
    "root_cause": "Potential race condition or incorrect log matching during region merge process, causing merge operation to fail when log gaps are detected between peers",
    "issue_number": 3255,
    "title": "test failpoints_cases::test_merge::test_node_merge_restart  failed"
  },
  {
    "bug_location": "logging",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Monitoring"
    ],
    "root_cause": "Logging configuration not consistently preserving source file and line number information during log generation",
    "issue_number": 3261,
    "title": "log not outputting source"
  },
  {
    "bug_location": "Raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Potential issue with leader removal during uncommitted log state, causing test failure in leader election or log replication process",
    "issue_number": 3262,
    "title": "test test_server_remove_leader_with_uncommitted_log failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 3266,
    "title": "Adjust read pool test timeouts"
  },
  {
    "bug_location": "Raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Leadership transition or network partition causing inconsistent leader state during write operation",
    "issue_number": 3267,
    "title": "'raftstore_cases::test_transport::test_node_partition_write' panicked"
  },
  {
    "bug_location": "Raftstore",
    "severity": 4,
    "categories": [
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Region merge operation failing to complete, potentially due to synchronization or state tracking issues during region merging process",
    "issue_number": 3268,
    "title": "test raftstore_cases::test_merge::test_node_merge_brain_split failed"
  },
  {
    "bug_location": "RaftStore Test Framework",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication",
      "Network"
    ],
    "root_cause": "Test cluster setup failure, unable to establish sufficient qualified stores for quorum testing",
    "issue_number": 3272,
    "title": "Test test_multi_node_random_latency failed"
  },
  {
    "bug_location": "util::mpsc",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Timing-related assertion failure on MacOS, likely due to platform-specific timer precision or scheduling differences",
    "issue_number": 3298,
    "title": "test_unbounded fails on MacOS constantly"
  },
  {
    "bug_location": "util/mpsc.rs",
    "severity": 1,
    "categories": [
      "CodeBug"
    ],
    "root_cause": "Potential silent integer type conversion from u32 to i64 which could lead to unexpected behavior if type ranges change",
    "issue_number": 3299,
    "title": "clippy warnings in master"
  },
  {
    "bug_location": "CI/Test Infrastructure",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Clippy linting tool not properly configured to fail CI pipeline on warnings",
    "issue_number": 3300,
    "title": "CI is not failed when there is clippy issues"
  },
  {
    "bug_location": "Storage Iterator/Cursor",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Transaction"
    ],
    "root_cause": "Inconsistent state management between seek operation return value and cursor validity state, causing unexpected cursor behavior after seek operations to non-existent keys",
    "issue_number": 3378,
    "title": "Cursor remains valid when seek to nothing again"
  },
  {
    "bug_location": "Region Leader Detection",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Incorrect leader staleness detection mechanism that does not validate local peer's leadership status before raising an alert",
    "issue_number": 3384,
    "title": "False stale peer alert"
  },
  {
    "bug_location": "Raft Test Framework",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Network",
      "Replication"
    ],
    "root_cause": "Race condition in message notification mechanism during network partition recovery tests, where message notifiers are attached too late to capture expected prevote messages",
    "issue_number": 3385,
    "title": "Some Prevote tests cannot pass reliably."
  },
  {
    "bug_location": "tikv-server",
    "severity": 3,
    "categories": [
      "Config",
      "CodeBug"
    ],
    "root_cause": "Insufficient file descriptor limit detection and error reporting mechanism",
    "issue_number": 3387,
    "title": "tikv should provide error message if it can't start"
  },
  {
    "bug_location": "store.rs",
    "severity": 4,
    "categories": [
      "Replication",
      "Performance"
    ],
    "root_cause": "Lack of heartbeat message preventing leader election and peer state updates in a single-node TiKV cluster",
    "issue_number": 3393,
    "title": "Single-node TiKV cluster logs abnormal leader missing "
  },
  {
    "bug_location": "RaftKV Storage",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential snapshot retrieval failure in batch snapshot test, causing assertion error when expected snapshot is not available",
    "issue_number": 3434,
    "title": "test: test_batch_snapshot failed "
  },
  {
    "bug_location": "tikv-ctl CLI",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Human"
    ],
    "root_cause": "Improper handling of command-line argument parsing when no arguments are provided, causing an unexpected panic in the main function",
    "issue_number": 3453,
    "title": "Starting `tikv-ctl` with no arguments results in unreachable!()"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 3474,
    "title": "Go client does not correctly insert, and gives no error"
  },
  {
    "bug_location": "Logging System",
    "severity": 4,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Logging library (slog-stdlog) executing trace-level log function arguments even when trace logging is disabled, causing unnecessary performance overhead",
    "issue_number": 3537,
    "title": "trace! is executed when log level > trace"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Hard state not saved for uninitialized peers, allowing potential term regression and multiple leader election in same term",
    "issue_number": 3572,
    "title": "Potential two different leaders on a same term"
  },
  {
    "bug_location": "tikv-ctl",
    "severity": 2,
    "categories": [
      "Network",
      "CodeBug"
    ],
    "root_cause": "gRPC DNS resolution failure when URL contains 'http://' prefix, causing infinite retry loop",
    "issue_number": 3655,
    "title": "tikv-ctl compact run infinitely"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 3664,
    "title": "#[should_panic] unit tests cannot pass CI"
  },
  {
    "bug_location": "RaftStore/Region Split Test",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Race condition during region split operation with insufficient synchronization or delay, causing intermittent key retrieval failures",
    "issue_number": 3669,
    "title": "test_delay_split_region failed"
  },
  {
    "bug_location": "Coprocessor Executor",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Incorrect implementation of start_scan and stop_scan methods in selection executor, causing potential execution failures when combining selection with table scan",
    "issue_number": 3677,
    "title": "Selection executor does not handle start_scan or stop_scan correctly"
  },
  {
    "bug_location": "logging/slog-async",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance",
      "Monitoring"
    ],
    "root_cause": "Inappropriate log overflow handling strategy causing message loss during high-load logging scenarios",
    "issue_number": 3695,
    "title": "Consider choosing block strategy for slog-async"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 3709,
    "title": "TiKV Server crashed in rocksdb.bg2"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 3721,
    "title": "tikv does not handle ambiguous time zones correctly"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 3744,
    "title": "three tikv crashes with version 2.1.0-rc.4"
  },
  {
    "bug_location": "coprocessor/codec/mysql/decimal",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Memory"
    ],
    "root_cause": "Memory access and boundary issues in decimal conversion and string conversion operations during division/modulo test",
    "issue_number": 3764,
    "title": "test_div_mod cannot pass Valgrind memcheck"
  },
  {
    "bug_location": "raftstore/store/snap",
    "severity": 4,
    "categories": [
      "Memory",
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Potential memory deallocation issue during snapshot generation, causing invalid memory access during file CRC32 calculation",
    "issue_number": 3767,
    "title": "test_storage_apply_snapshot possible invalid memory write"
  },
  {
    "bug_location": "test-bench worker management",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Human"
    ],
    "root_cause": "Workers not being properly stopped/released after test execution, potentially causing resource leaks or test interference",
    "issue_number": 3768,
    "title": "Worker not correctly released in a lot of tests"
  },
  {
    "bug_location": "Raft",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Performance"
    ],
    "root_cause": "Potential race condition or timing issue during region split operation in follower replica",
    "issue_number": 3793,
    "title": "test_follower_slow_split failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 3804,
    "title": "test_handle_time failed"
  },
  {
    "bug_location": "Go Client Dependency Management",
    "severity": 2,
    "categories": [
      "Config",
      "Upgrade"
    ],
    "root_cause": "Version incompatibility between etcd dependencies and TiDB's older gRPC and etcd client versions",
    "issue_number": 3828,
    "title": "tikv go-client dependency install failed"
  },
  {
    "bug_location": "RaftStore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Potential issue with region merge synchronization in test case, where region is not merging as expected",
    "issue_number": 3829,
    "title": "test_region_change_observer failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 3830,
    "title": "test_kv_service failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 3831,
    "title": "test_node_split_brain failed"
  },
  {
    "bug_location": "RaftStore/LeaseRead",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Potential race condition or improper handling of node destruction during lease read operation",
    "issue_number": 3833,
    "title": "test_node_callback_when_destroyed failed"
  },
  {
    "bug_location": "test-bench",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Monitoring"
    ],
    "root_cause": "Log collection mechanism broke after moving test utilities to different crates, causing log prefix matching to fail",
    "issue_number": 3858,
    "title": "Tests logs disappeared"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 3860,
    "title": "test_server_auto_adjust_replica fails"
  },
  {
    "bug_location": "Metrics/Monitoring",
    "severity": 2,
    "categories": [
      "Monitoring",
      "Config"
    ],
    "root_cause": "Incorrect metric calculation for block cache size across different TiKV configurations, possibly due to shared cache implementation",
    "issue_number": 3862,
    "title": "max block cache metrics shows uncorrectly"
  },
  {
    "bug_location": "PD Client",
    "severity": 3,
    "categories": [
      "Replication",
      "Network",
      "CodeBug"
    ],
    "root_cause": "TiKV sends stale region information during reconnection to PD, causing inconsistent region metadata after region merge operations",
    "issue_number": 3868,
    "title": "PD receives stale region info from TiKV"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 3893,
    "title": "Either mod or abs has bug"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 3894,
    "title": "Bug: duplicate HTTP response headers"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Inconsistent peer configuration during region management, where an expected peer is not found in the region configuration",
    "issue_number": 3902,
    "title": " test: test_server_stale_peer_out_of_region failed"
  },
  {
    "bug_location": "coprocessor/codec/mysql/time",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Chrono library's datetime handling does not support zero month/day values, causing unexpected datetime conversion when parsing '0000-00-00' timestamp",
    "issue_number": 3953,
    "title": "copr/codec/mysql/time: invalid_zero function isn't working as expected."
  },
  {
    "bug_location": "raftstore::store::fsm::router",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Failure in sending messages through the Raft router, likely due to channel communication issues during shutdown process",
    "issue_number": 3959,
    "title": "test raftstore::store::fsm::router::tests::bench_send failed"
  },
  {
    "bug_location": "Split-check component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Storage",
      "Performance"
    ],
    "root_cause": "Potential memory corruption or unsafe memory access during LZ4 decompression in split-check operation",
    "issue_number": 3964,
    "title": "Tikv segfault at split-check"
  },
  {
    "bug_location": "coprocessor::endpoint",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Test assertion failure related to handle time measurement in coprocessor endpoint, potentially indicating a timing or performance measurement issue",
    "issue_number": 3965,
    "title": "test coprocessor::endpoint::tests::test_handle_time failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 3966,
    "title": "test_server_safe_conf_change failed"
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Inconsistent peer configuration during region split operation, causing test failure when expected peer is not found",
    "issue_number": 3969,
    "title": "raftstore::test_stale_peer::test_server_stale_peer_without_data_left_derive_when_split failed"
  },
  {
    "bug_location": "tikv::coprocessor::dag::expr::builtin_compare::ScalarFunc::in_string",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Inefficient string comparison algorithm for IN clause with multiple string values, causing linear time complexity and repeated string evaluations",
    "issue_number": 3977,
    "title": "in_string can be very slow"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 4005,
    "title": "Gabage outputs in tests"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 4026,
    "title": "test_node_merge_multiple_snapshots_together failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 4027,
    "title": "test_stale_learner failed"
  },
  {
    "bug_location": "raftstore/prevote",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Race condition in prevote message handling during minority follower reboot, potentially related to message timing and synchronization issues",
    "issue_number": 4029,
    "title": "test_prevote_reboot_minority_followers failed"
  },
  {
    "bug_location": "RocksDB",
    "severity": 4,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential memory corruption or invalid memory access during LZ4 decompression in RocksDB block reading process",
    "issue_number": 4048,
    "title": "tikv-server segfault with rocksdb:low2"
  },
  {
    "bug_location": "RocksDB Build System",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Missing or incompatible GFlags library during RocksDB compilation",
    "issue_number": 4049,
    "title": "Build TiKV failed because 'google' has not been declared when compile rust-rocksdb"
  },
  {
    "bug_location": "RaftStore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Potential issue with region splitting logic during test scenario, where expected array contents do not match actual results",
    "issue_number": 4053,
    "title": "Test test_server_half_split_region fails"
  },
  {
    "bug_location": "Raft Library (raft-rs)",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction",
      "Replication"
    ],
    "root_cause": "In handle_heartbeat_response, the code overwrites 'to_send' in each iteration, causing only the last ReadIndex request to be processed, potentially losing read requests from different slave nodes",
    "issue_number": 4071,
    "title": "raft lib bug: In handle_heartbeat_response, the leader will lose the reply request of readindex"
  },
  {
    "bug_location": "coprocessor/codec/mysql/time",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Lack of support for SQL mode ALLOW_INVALID_DATES in TiKV coprocessor's time handling logic, causing errors when processing invalid date values during SELECT operations",
    "issue_number": 4100,
    "title": "coprocessor: allow invalid dates SQL mode"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 4101,
    "title": "test_conf_change_remove_leader failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 4102,
    "title": "raftstore::store::fsm::batch::tests::test_batch fails unexpectedly"
  },
  {
    "bug_location": "server::load_statistics::linux",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Overly strict load statistic test assertion that assumes 80% CPU load, which may not be realistic in shared computing environments",
    "issue_number": 4103,
    "title": "server::load_statistics::linux::tests::test_thread_load_statistic fails unexpectedly"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 4104,
    "title": "util::mpsc::batch::tests::test_batch_receiver fails unexpectedly"
  },
  {
    "bug_location": "raftstore/store/peer_storage.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Channel closure during snapshot application in Raft configuration change, likely causing race condition or improper synchronization during leader removal",
    "issue_number": 4113,
    "title": "test: test_conf_change_remove_leader failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 4122,
    "title": "cargo tarpaulin  will report code coverage even if run tests failed"
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage",
      "Replication"
    ],
    "root_cause": "Snapshot garbage collection (GC) mechanism is not properly removing snapshot files during test, causing test failure",
    "issue_number": 4127,
    "title": "raftstore::test_snap::test_server_snap_gc fails unexpectedly"
  },
  {
    "bug_location": "gRPC",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Network"
    ],
    "root_cause": "IPv6 address parsing error where address brackets are lost during server address resolution, causing gRPC binding failure",
    "issue_number": 4136,
    "title": "TiKV fails to start with IPV6 address"
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Region split operation failure causing an empty key lookup error in the test scenario",
    "issue_number": 4139,
    "title": "test raftstore::test_split_region::test_node_half_split_region failed"
  },
  {
    "bug_location": "Raft component",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Monitoring"
    ],
    "root_cause": "Incorrect metric labeling using store ID instead of store address for tracking unreachable nodes",
    "issue_number": 4172,
    "title": "should use address for the unreachable metric "
  },
  {
    "bug_location": "Titan storage engine",
    "severity": 4,
    "categories": [
      "Storage",
      "Memory"
    ],
    "root_cause": "File handle leak in Titan blob file management, where deleted files are not properly closed, causing continuous file descriptor accumulation and potential disk space exhaustion",
    "issue_number": 4182,
    "title": "Titan may forget to close the file handle. "
  },
  {
    "bug_location": "RaftStore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Inconsistent peer management during configuration change, where a new peer cannot be added correctly to the region",
    "issue_number": 4193,
    "title": "cases::test_conf_change::test_write_after_destroy fails on CI"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 4219,
    "title": "test failed with \"Resource temporarily unavailable\""
  },
  {
    "bug_location": "Build/Test System",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Test log wrapper binary not properly linked with jemalloc memory allocator during build process",
    "issue_number": 4220,
    "title": "test log wrapper does not contain jemalloc"
  },
  {
    "bug_location": "TiKV/Importer Store Discovery",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Network"
    ],
    "root_cause": "Importer not filtering out tombstoned (invalid/deleted) stores during store discovery, potentially causing connection attempts to invalid addresses",
    "issue_number": 4221,
    "title": "TiKV/Importer should ignore tombstoned stores"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Unexpected `None` value when unwrapping an Option during store heartbeat to PD, causing a panic in the raftstore thread",
    "issue_number": 4224,
    "title": "raftstore panic when staring TiKV"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Failure in replica count adjustment mechanism during configuration change, preventing the expected replica count from being reached within the timeout period",
    "issue_number": 4225,
    "title": "test raftstore::test_conf_change::test_node_auto_adjust_replica failed"
  },
  {
    "bug_location": "Tokio Runtime / PD Worker",
    "severity": 3,
    "categories": [
      "Performance",
      "Memory",
      "Network"
    ],
    "root_cause": "Potential resource leakage or inefficient file descriptor management in Tokio runtime's thread pool and PD worker threads, causing excessive pipe and eventpoll file descriptors",
    "issue_number": 4233,
    "title": "too many pipe and eventpoll files "
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 4244,
    "title": "raftstore::store::fsm::router::tests::test_basic failed"
  },
  {
    "bug_location": "raftstore/store/peer.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Log gap during region merge is too large, causing merge proposal to be skipped",
    "issue_number": 4246,
    "title": "cases::test_stale_read::test_stale_read_during_merging failed"
  },
  {
    "bug_location": "coprocessor::endpoint",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Test assertion failure related to handle time measurement, indicating potential performance tracking or timing inconsistency in coprocessor endpoint",
    "issue_number": 4247,
    "title": "test coprocessor::endpoint::tests::test_handle_time failed"
  },
  {
    "bug_location": "pd/client.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Network",
      "Replication"
    ],
    "root_cause": "Duplicate PD endpoint validation failure causing connection issues during leader restart test",
    "issue_number": 4258,
    "title": "tests: test_restart_leader_insecure failed"
  },
  {
    "bug_location": "Titan storage engine",
    "severity": 3,
    "categories": [
      "Storage",
      "CodeBug"
    ],
    "root_cause": "Incompatibility between Titan storage engine and DeleteFilesInRanges operation causing inconsistent read behavior",
    "issue_number": 4288,
    "title": "Titan is not compatible with DeleteFilesInRanges"
  },
  {
    "bug_location": "coprocessor/endpoint.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Error handling logic fails to propagate parsing errors, instead wrapping them in a dummy handler which leads to incorrect error reporting",
    "issue_number": 4293,
    "title": "coprocessor: parsing error is never returned"
  },
  {
    "bug_location": "RocksDB/SST Import",
    "severity": 3,
    "categories": [
      "Storage",
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential file size validation issue in encrypted environment during SST file ingestion, where file size is incorrectly calculated or validated",
    "issue_number": 4295,
    "title": "test: test_sst_writer(I guess) failed with assertion failed in rocksdb"
  },
  {
    "bug_location": "util/mod.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Memory"
    ],
    "root_cause": "Destructor panics on non-empty vector, which can cause double-panic and process abort during thread cleanup",
    "issue_number": 4309,
    "title": "MustConsumeVec double-panics in dtor"
  },
  {
    "bug_location": "PendingCmd destructor",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Memory"
    ],
    "root_cause": "Unconditional panic in destructor when callback is not handled, which violates Rust's destructor guidelines and risks double-panic scenarios",
    "issue_number": 4310,
    "title": "PendingCmd panics on drop"
  },
  {
    "bug_location": "gRPC Credentials Builder",
    "severity": 3,
    "categories": [
      "Security",
      "CodeBug"
    ],
    "root_cause": "Potential secret leakage due to improper handling of sensitive credentials during object destruction",
    "issue_number": 4311,
    "title": "ChannelCredentialsBuilder leaks secrets on drop"
  },
  {
    "bug_location": "Logging/RotatingFileLogger",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Potential panic in destructor during file flush operation, which violates Rust's destructor guidelines by risking a double-panic scenario",
    "issue_number": 4312,
    "title": "RotatingFileLoggerPanics on drop"
  },
  {
    "bug_location": "RocksEngineCore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Memory",
      "Recovery"
    ],
    "root_cause": "Potential panic in destructor when joining thread handle, which can cause program abort if thread panicked",
    "issue_number": 4313,
    "title": "RocksEngineCore can probably panic on drop"
  },
  {
    "bug_location": "components/codec/src/buffer.rs",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Memory"
    ],
    "root_cause": "Unreliable memory reallocation behavior in buffer test, potentially dependent on jemalloc's memory management",
    "issue_number": 4318,
    "title": "test buffer::tests::test_vec_reallocate failed"
  },
  {
    "bug_location": "pd-client",
    "severity": 3,
    "categories": [
      "Network",
      "Config",
      "Replication"
    ],
    "root_cause": "TiKV caches PD member list and does not dynamically update when all PD members are replaced, causing potential service disruption during cluster reconfiguration",
    "issue_number": 4319,
    "title": "TiKV may fail to find PD cluster if all PD members are changed "
  },
  {
    "bug_location": "Network Socket Binding",
    "severity": 3,
    "categories": [
      "Network",
      "Config",
      "CodeBug"
    ],
    "root_cause": "Improper socket port configuration using SO_REUSEPORT without proper error detection mechanism, allowing silent port conflicts",
    "issue_number": 4323,
    "title": "TiKV may not report errors if port is in conflict"
  },
  {
    "bug_location": "TiKV Coprocessor Codec",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of float string to integer conversion, causing unexpected error instead of warning during type conversion",
    "issue_number": 4327,
    "title": "float_str_to_int_string should not return error"
  },
  {
    "bug_location": "TiKV logging system",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Monitoring"
    ],
    "root_cause": "Incomplete log flushing during panic or fatal error scenarios, causing log truncation",
    "issue_number": 4328,
    "title": "TiKV lost logs in Kubernetes/Docker environment"
  },
  {
    "bug_location": "server/node.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Recovery",
      "Storage"
    ],
    "root_cause": "Incomplete initialization state handling during TiKV restart, causing store identification and bootstrap process to fail if interrupted",
    "issue_number": 4333,
    "title": "TiKV may fail to restart if it fails after writing store ident and before bootstrapping cluster"
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Potential race condition or synchronization issue in configuration change handling during Raft peer management",
    "issue_number": 4356,
    "title": "test: raftstore::test_conf_change::test_server_split_brain failed"
  },
  {
    "bug_location": "Logging System",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Log flushing mechanism in fatal! macro is not properly implemented, causing incomplete log output during server startup or configuration errors",
    "issue_number": 4358,
    "title": "fatal! macro doesn't flush logs"
  },
  {
    "bug_location": "test_util/logging.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Monitoring"
    ],
    "root_cause": "CaseTraceLogger implementation does not output key-value pairs in log messages, causing incomplete log information during testing",
    "issue_number": 4361,
    "title": "slog kvs are missing in CaseTraceLogger "
  },
  {
    "bug_location": "test-bench",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance",
      "Monitoring"
    ],
    "root_cause": "Potential issue with thread load statistic measurement, causing incorrect CPU usage reporting",
    "issue_number": 4364,
    "title": "test: test_thread_load_statistic fails on CI"
  },
  {
    "bug_location": "Build System",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "SSE4.2 instruction set compatibility check during build process was too strict, preventing compilation on some systems even with compatible CPUs",
    "issue_number": 4367,
    "title": "error: ./target/debug/tikv-server does not enable sse4.2"
  },
  {
    "bug_location": "tikv-server startup configuration",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config",
      "Storage"
    ],
    "root_cause": "Startup configuration fails to validate default data directory path, causing potential initialization issues",
    "issue_number": 4392,
    "title": "tikv-server doesn't do data-dir check on startup if using the default data-dir value"
  },
  {
    "bug_location": "tikv_alloc",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Problematic feature configuration in cargo that makes it difficult to disable jemalloc by default, causing potential compilation and linking issues",
    "issue_number": 4409,
    "title": "Reorganize tikv_alloc's cargo features"
  },
  {
    "bug_location": "Build System",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Build configuration incompatibility preventing compilation without jemalloc allocator",
    "issue_number": 4410,
    "title": "TiKV doesn't build with the system allocator"
  },
  {
    "bug_location": "Build System",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Missing protoc compiler in Dockerfile, preventing build process from generating required protocol buffer code",
    "issue_number": 4422,
    "title": "protoc required by build but not included in Dockerfile"
  },
  {
    "bug_location": "test-bench",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Monitoring"
    ],
    "root_cause": "External dependency (Tarpaulin) issue preventing code coverage measurement",
    "issue_number": 4491,
    "title": "Coverage is not working"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Potential issue with peer management during leader transfer in Raft configuration changes, causing region peer inconsistency",
    "issue_number": 4506,
    "title": "test_server_transfer_leader_safe fails"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Potential race condition in handling unapplied log entries during leader read operations, causing unexpected read behavior",
    "issue_number": 4507,
    "title": "test_server_read_leader_with_unapplied_log fails"
  },
  {
    "bug_location": "Build System/Dependency Management",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Incompatible libunwind version linking in tcmalloc crate build process",
    "issue_number": 4520,
    "title": "Link libunwind into tcmalloc crate"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 4523,
    "title": "Make logs of protobuf structures more friendly"
  },
  {
    "bug_location": "Raft Consensus Module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Incomplete progress tracking of learner peers during region merging, causing potential log continuity issues",
    "issue_number": 4551,
    "title": "Progress of learner should also be considered before merging"
  },
  {
    "bug_location": "Raftstore",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Incorrect log synchronization during region merge causing inconsistent applied and commit indices, which prevents node restart after panic",
    "issue_number": 4560,
    "title": "Raft logs in CommitMerge should forward to raftstore first"
  },
  {
    "bug_location": "Raft Leader Election Component",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Inconsistent index tracking during log compaction and merge operations, causing potential index mismatch that triggers panic during leader election",
    "issue_number": 4581,
    "title": "Panic on leader election after `catch_up_log_for_merge`"
  },
  {
    "bug_location": "TiKV Client",
    "severity": 3,
    "categories": [
      "Network",
      "Performance"
    ],
    "root_cause": "gRPC connection closure causing repeated streaming recreation failures during high-load scenarios like sysbench",
    "issue_number": 4584,
    "title": "infinite \"batchRecvLoop re-create streaming fail\" after sysbench"
  },
  {
    "bug_location": "tikv::coprocessor::codec::mysql::Duration",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Improper handling of maximum negative i64 value during absolute value calculation, causing integer overflow when calling .abs()",
    "issue_number": 4609,
    "title": "Fuzz test failed for tikv::coprocessor::codec::mysql::Duration"
  },
  {
    "bug_location": "tikv::coprocessor::codec::mysql::Duration",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Performance"
    ],
    "root_cause": "Delayed validation of hour/minute/second values during Duration parsing, leading to potential integer overflow during multiplication",
    "issue_number": 4610,
    "title": "Fuzz test failed for tikv::coprocessor::codec::mysql::Duration::parse"
  },
  {
    "bug_location": "coprocessor",
    "severity": 3,
    "categories": [
      "Performance",
      "CodeBug",
      "Memory"
    ],
    "root_cause": "Large enum variants causing inefficient memory usage and unnecessary memory copies in coprocessor::Result type, with some error types being significantly larger than expected (176-184 bytes)",
    "issue_number": 4676,
    "title": "coprocessor::Result<T> is at least 176 bytes.."
  },
  {
    "bug_location": "raftstore/store/peer.rs",
    "severity": 4,
    "categories": [
      "Transaction",
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Lease extension mechanism fails during leader transfer, potentially causing incorrect ReadIndex responses when multiple read requests are pending",
    "issue_number": 4680,
    "title": "ReadIndex when transferring leader may not be correct"
  },
  {
    "bug_location": "Logger/Panic Hook",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Memory"
    ],
    "root_cause": "Recursive thread join during panic handling causing resource deadlock, specifically in the logging system's panic hook where the thread attempts to join itself",
    "issue_number": 4686,
    "title": "Resource deadlock avoided (os error 35) on panic hook"
  },
  {
    "bug_location": "Coprocessor",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Transaction"
    ],
    "root_cause": "Incorrect datetime validation and processing logic for edge cases like zero dates, pre-1970 dates, and invalid time representations",
    "issue_number": 4692,
    "title": "A series of date time related bugs"
  },
  {
    "bug_location": "Dependency Management",
    "severity": 2,
    "categories": [
      "Security",
      "Upgrade"
    ],
    "root_cause": "Vulnerable protobuf crate version (2.0.4) with potential out-of-memory issue in stream::read_raw_bytes_into()",
    "issue_number": 4703,
    "title": "cargo audit error"
  },
  {
    "bug_location": "raftstore/store/peer.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication",
      "Network"
    ],
    "root_cause": "Early error return in message sending logic causing potential message loss and communication disruption in Raft replication",
    "issue_number": 4717,
    "title": "Sending messages should be more tolerant"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 4736,
    "title": "raftstore::test_tombstone::test_server_readd_peer failed"
  },
  {
    "bug_location": "storage/mvcc/reader",
    "severity": 3,
    "categories": [
      "Performance",
      "Storage"
    ],
    "root_cause": "Inefficient garbage collection (GC) mechanism that does not properly account for deleted versions and compaction status, causing unnecessary region scans and GC attempts",
    "issue_number": 4740,
    "title": "MvccProperty should consider delete version"
  },
  {
    "bug_location": "storage::lock_manager::waiter_manager",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Performance",
      "Transaction"
    ],
    "root_cause": "Potential race condition or timeout issue in waiter manager test suite",
    "issue_number": 4742,
    "title": "storage::lock_manager::waiter_manager::tests::test_waiter_manager failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 4758,
    "title": "*: Many possible spelling mistakes."
  },
  {
    "bug_location": "Raft ReadIndex Service",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Duplicate context generation for pending read requests, causing potential request conflicts and loss of read operations",
    "issue_number": 4764,
    "title": "ReadIndex service may got duplicate `ctx` "
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication",
      "Network"
    ],
    "root_cause": "Potential issue with peer addition and region configuration during local read cache test, causing region not found error and disconnection during peer change requests",
    "issue_number": 4776,
    "title": "test: test_local_read_cache failed"
  },
  {
    "bug_location": "Build System",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Build script incorrectly checks SSE support even when explicitly disabled via ROCKSDB_SYS_SSE=0",
    "issue_number": 4900,
    "title": "should not check SSE if we disable it"
  },
  {
    "bug_location": "RaftStore",
    "severity": 4,
    "categories": [
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Potential leader transfer failure during network partition scenario in Raft consensus implementation",
    "issue_number": 4902,
    "title": "test_prevote_partition_leader_in_minority_detect_in_majority failed"
  },
  {
    "bug_location": "raftstore/store/fsm/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Unhandled error during commit merge when regions are shutting down, causing panic due to unwrapping a disconnected result",
    "issue_number": 5032,
    "title": "Panic on force send commit merge during shutdown "
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5034,
    "title": "libtitan_sys built failed on gcc 9.1"
  },
  {
    "bug_location": "Build Configuration",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Debug symbol configuration in Cargo profiles prevents line number display for TiKV library code during test backtraces",
    "issue_number": 5049,
    "title": "Line number isn't displayed in backtrace for tests"
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Potential race condition or error handling issue in snapshot and merge operations during Raft consensus process",
    "issue_number": 5054,
    "title": "test: test cases::test_merge::test_node_request_snapshot_reject_merge failed"
  },
  {
    "bug_location": "documentation",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Human"
    ],
    "root_cause": "Outdated documentation links in README.md after wiki restructuring, causing broken references to documentation pages",
    "issue_number": 5064,
    "title": "Broken link in Wiki (again)"
  },
  {
    "bug_location": "Build System",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Makefile rule inconsistency between build configuration and Dockerfile",
    "issue_number": 5069,
    "title": "build_release is not appear in makefile rules"
  },
  {
    "bug_location": "server::raft_client",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Network",
      "Replication"
    ],
    "root_cause": "Potential issue with Raft client reconnection mechanism causing test failure during repeated message sending",
    "issue_number": 5073,
    "title": "test: test server::raft_client::test_raft_client_reconnect failed"
  },
  {
    "bug_location": "Build System / Cargo",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Upgrade"
    ],
    "root_cause": "Cargo compilation pipeline issue with librocksdb_sys crate, likely related to build artifact resolution during concurrent compilation",
    "issue_number": 5130,
    "title": "error: crate `librocksdb_sys` required to be available in rlib format, but was not found in this form"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5159,
    "title": "How does 5 tikv nodes handle two nodes fault?"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5186,
    "title": "Unstable test: raftstore::test_prevote::test_server_isolated_follower_leader_does_not_change"
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication",
      "Recovery"
    ],
    "root_cause": "Potential race condition or data inconsistency during random server restart test scenario",
    "issue_number": 5188,
    "title": "`raftstore::test_multi::test_multi_server_random_restart` spuriously failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5202,
    "title": "test_node_merge_catch_up_logs_restart failed"
  },
  {
    "bug_location": "storage",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Assertion failure in auto garbage collection test, indicating an unexpected value state during storage test",
    "issue_number": 5206,
    "title": "test storage::test_raft_storage::test_auto_gc panic "
  },
  {
    "bug_location": "raftstore",
    "severity": 4,
    "categories": [
      "Replication",
      "CodeBug"
    ],
    "root_cause": "Inconsistent peer configuration during region merge operation, specifically with learner node configuration",
    "issue_number": 5207,
    "title": "test raftstore::test_merge::test_node_merge_with_slow_learner failed"
  },
  {
    "bug_location": "raftstore::store::fsm::router",
    "severity": 2,
    "categories": [
      "Performance",
      "CodeBug"
    ],
    "root_cause": "Potential channel or router overflow during benchmarking, causing an unwrap on a 'Full' error result",
    "issue_number": 5209,
    "title": "bench raftstore::store::fsm::router::tests::bench_send failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5239,
    "title": "util::tests::test_delete_all_files_in_range can fail"
  },
  {
    "bug_location": "Build System",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Missing 'profiler' component directory during Docker build process, causing cargo build failure",
    "issue_number": 5245,
    "title": "tag 3.0.2 build container error"
  },
  {
    "bug_location": "server/config.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Config"
    ],
    "root_cause": "Overly strict validation of advertise address prevents valid hostname patterns starting with zero",
    "issue_number": 5273,
    "title": "Cannot create TiKV with advertise addr starting with `0`"
  },
  {
    "bug_location": "Raft consensus module",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Transaction"
    ],
    "root_cause": "Incorrect handling of peer removal and re-addition in Raft configuration changes, leading to potential out-of-range commit and heartbeat issues when peers are repeatedly added and removed",
    "issue_number": 528,
    "title": "investigate failed test "
  },
  {
    "bug_location": "raftstore",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Failed to transfer leader during lease read test, indicating potential race condition or leadership transfer issue in Raft consensus protocol implementation",
    "issue_number": 5284,
    "title": "CI failed in raftstore::test_lease_read::test_read_index_when_transfer_leader_1"
  },
  {
    "bug_location": "TiKV Configuration Parser",
    "severity": 3,
    "categories": [
      "Config",
      "Upgrade"
    ],
    "root_cause": "Strict configuration parsing that does not handle deprecated or unknown configuration fields during upgrade, causing TiKV startup failure when reading last_tikv.toml with outdated settings",
    "issue_number": 5286,
    "title": "unknown field `import-dir`, expected `num-threads` or `stream-channel-window` for key `import`"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5289,
    "title": "Cargo Run workflow no longer usable"
  },
  {
    "bug_location": "raftstore/coprocessor/split_check/keys.rs",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Storage"
    ],
    "root_cause": "Function only considers transactional KV regions, ignoring raw KV regions when calculating approximate keys, causing incorrect key count reporting",
    "issue_number": 5319,
    "title": "approximate_keys for raw kv region always be 0"
  },
  {
    "bug_location": "raftstore",
    "severity": 2,
    "categories": [
      "CodeBug",
      "Replication"
    ],
    "root_cause": "Potential timestamp synchronization or heartbeat tracking issue in region heartbeat test",
    "issue_number": 5329,
    "title": "Test test_region_heartbeat_timestamp failed"
  },
  {
    "bug_location": "TiKV Metrics Server",
    "severity": 3,
    "categories": [
      "CodeBug",
      "Security"
    ],
    "root_cause": "TLS configuration for metrics endpoint not fully implemented when cluster-wide TLS is enabled",
    "issue_number": 5340,
    "title": "Metrics page not using HTTPS after TLS enabled"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 535,
    "title": "mysql-test InvalidDataType"
  },
  {
    "bug_location": "raftstore/store/fsm/peer.rs",
    "severity": 4,
    "categories": [
      "CodeBug",
      "Replication",
      "Storage"
    ],
    "root_cause": "Multiple concurrent mailbox shutdown attempts causing meta corruption during region peer destruction process",
    "issue_number": 5365,
    "title": "TiKV panic with multiple shutdown mailbox "
  },
  {
    "bug_location": "RaftStore/LocalReader",
    "severity": 3,
    "categories": [
      "Performance",
      "Transaction",
      "Replication"
    ],
    "root_cause": "Leader lease expiration increases with more read pool threads, causing unnecessary lease renewal overhead when threads increase",
    "issue_number": 5388,
    "title": "Lease expire rate increases as thread count increases"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5389,
    "title": "LibreSSL can not find a valid cert file"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5404,
    "title": "TPC-H Q1 lead to TiKV Panic"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5411,
    "title": "Test test_server_pd_transfer_leader failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5467,
    "title": "unstable test: cases::test_replica_read::test_wait_for_apply_index'"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5499,
    "title": "storage::test_raft_storage::test_auto_gc, panicked at 'called `Result::unwrap()` on an `Err` value: Txn(Engine(Request(message: \"stale command\")))', src/libcore/result.rs:1165:5"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5509,
    "title": "run `cargo test` in master's components/tidb_query failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5516,
    "title": "raftstore::test_tombstone::test_server_readd_peer response header { error { message: \"peer is not leader for region 1, leader may None\" not_leader { region_id: 1 } } } has error"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5579,
    "title": "test pd::tests::test_collect_stats failed "
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5611,
    "title": "test_server_read_leader_with_unapplied_log failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5628,
    "title": "test cases::test_split_region::test_follower_slow_split has been running for over 60 seconds"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 575,
    "title": "tikv-server failed to replicate lots of regions with OOM error."
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5786,
    "title": "test_backup::test_backup_meta failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5851,
    "title": "Tikv 2.1.18 crashes with Illegal instruction and the cluster is very unstable "
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5852,
    "title": "Split may leave stale region info"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5875,
    "title": "Replica read may cause tikv panic"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5893,
    "title": "test: test_secure_connect failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 5981,
    "title": "May meet panic when processing merge with leader changed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6033,
    "title": "tidb_query_codegen unit test fails"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6044,
    "title": "read_decimal may panic when data is corrupted"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6097,
    "title": "Failpoint tests will not run when make dev"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6107,
    "title": "tikv-ctl unsafe recover can lead to inconsistent replica list"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6145,
    "title": "Fail to execute `cargo check` in `components/tikv_util` "
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6153,
    "title": "test_detect_deadlock_when_shuffle_region failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6165,
    "title": "GC is always triggered on idle cluster"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6226,
    "title": "test server::lock_manager::test_detect_deadlock_when_shuffle_region failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6247,
    "title": "Test test_server_down_peers_with_hibernate_regions fails"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6254,
    "title": "TiKV fails to serve after network partition"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6277,
    "title": "test_thread_io_statistics fails on CI"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6344,
    "title": "Panic on late arriving confchange result"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6366,
    "title": "Two overlapped snapshot may be applied together which may cause panic or loss of data"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6420,
    "title": "Conf change is too slow"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6437,
    "title": "RawGet may be incorrect when get request batch is enabled"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6439,
    "title": "peer shouldn't hibernate if it has pending reads"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6448,
    "title": "clippy fails because of strict comparison of f64"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6460,
    "title": "A failed merge may cause a later success merge lost data"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6498,
    "title": "Configuration validation doesn't find misspelled option names in config file"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6519,
    "title": "RocksDB scanning Write CF returns keys out of order"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6540,
    "title": "When sync-log=false, fsync raftdb before kvdb persists data"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6591,
    "title": "Can not run fuzzer"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6594,
    "title": "backup: upload to S3 may have been corrupted"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6596,
    "title": "raftstore: read index may fail after node restart"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6607,
    "title": "tikv panic about replica reads"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6651,
    "title": "Coredump when running test case"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6667,
    "title": "Broadcast WakeUp message for awakening hibernated regions after node restarts"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6670,
    "title": "Catch up logs is too slow when region is idle"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6676,
    "title": "TPC-H q8 & q21 results are wrong on master"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6690,
    "title": "Coprocessor Cache is unsound"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6702,
    "title": "Welcome message are not the first message when starting anymore"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6727,
    "title": "The default size limit of grpc is too small after using batch grpc"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6985,
    "title": "Deadlock detector sometimes reconnects to the same leader"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 6994,
    "title": "tests resource leak"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7024,
    "title": "`acquire_pessimistic_lock` has very poor performance if there are many `Write::Lock`s"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7070,
    "title": "Pipelined pessimistic lock may roll back a normal transaction"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7104,
    "title": "tikv panic"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7122,
    "title": "Performance regression on Sysbench between Feb 10 and Feb 28"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7124,
    "title": "TiKV cannot send messages to PD after reconnect"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 713,
    "title": "Coprocessor Bug"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7141,
    "title": "unexpected behavior for bytes column in where clause"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7163,
    "title": "Restoring a subrange of backed up rawkv data may import keys that are out of range"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7183,
    "title": "raftstore::test_stale_peer::test_server_stale_peer_out_of_region failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7209,
    "title": "TiKV gRPC server coredump when TLS is enabled"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7229,
    "title": "Replication read should be resumed after applying snapshot"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7236,
    "title": "Backup to S3 panics for a table ~30 GiB"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7243,
    "title": "Unknown time zone 'posixrules'"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7289,
    "title": "Optimistic lock blocks reads"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7312,
    "title": "TiKV try to connect HTTP/1.x server"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7324,
    "title": "batchRecvLoop receives a unknown response when testing follower read"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7325,
    "title": "TiKV panic while running cdc"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7353,
    "title": "raft::raft_log::RaftLog<T>::commit_to paniked"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7364,
    "title": "Txn: collapsing the rollback record of pessimistic lock could cause inconsistency"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7373,
    "title": "tikv-ctl coredump"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7375,
    "title": "external_storage: if S3 upload cannot finish within 15 minutes it will fail"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7384,
    "title": "Configuration is not fully checked if the unified read pool is partially enabled"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7386,
    "title": "Wake up message should be considered even when target peer is tombstone"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7462,
    "title": "After a user starts TiKV, other users will not be able to start TiKV"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7475,
    "title": "An uninitialized peer can be recreated"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7484,
    "title": "The unified read pool should not use start_ts as task id"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7493,
    "title": "batch raft call deadlock"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7500,
    "title": "raftstore::test_prevote::test_server_isolated_follower_leader_does_not_change failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7613,
    "title": "Undefined behavior in tidb_query_datatype's RowSlice"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7653,
    "title": "Deprecated raft cmd AdminCmdType::Split is reused"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7694,
    "title": "util::tests::test_delete_all_files_in_range failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7720,
    "title": "config-check fails if raftdb dictionary doesn't exist"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7721,
    "title": "The TiKV and PD storage size is not equal"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7741,
    "title": "TiKV OOM"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7745,
    "title": "QPS continue reduce when running TPCC"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7746,
    "title": "TiKV scale out has high jitter on Latency"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7747,
    "title": "Fail to evict leader when rolling update a new hibernated cluster"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7846,
    "title": "BR ignore some GCS and S3 errors which should retry when restore"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7849,
    "title": "Progress of Backup / Restore occasionally stuck"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7850,
    "title": "AWS br backup failed when collation is open "
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7853,
    "title": "Encounter region unavailable when selecting data"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7854,
    "title": "Got oneshot canceled error when selecting data"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7855,
    "title": "Channel disconnected error is not handled correctly"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7857,
    "title": "Kill a tikv process when backup, br will exit quickly"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7891,
    "title": "tikv-ctl failed to connect to tikv when TLS is enabled"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7901,
    "title": "TiKV fails to start"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7909,
    "title": "cases::test_merge::test_node_merge_write_data_to_source_region_after_merging failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7924,
    "title": "Some metrics about GC doesn't shows properly when GC runs too quickly."
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7939,
    "title": "Point getter aborts in RocksDB iterator Next()"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7940,
    "title": "from 4.0 upgrade to nightly. the QPS improve much"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7941,
    "title": "It takes 10 mins to recover QPS"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7971,
    "title": "raftstore: incorrectly batch read index request"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 7977,
    "title": "Titan: Fix GC may delete a already deleted blob file"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8034,
    "title": "Local read may get stale data during merging"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8117,
    "title": "raftdb log is printed into tikv log file"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8155,
    "title": "Active written leaders is wrong"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8168,
    "title": "TiKV keeps OOM if there are too many change events need to be sent to CDC"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8169,
    "title": "Change max replicas may cause region unavailable"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 818,
    "title": "select statement get an error."
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8184,
    "title": "Don't allow conf remove until leader has applied to current term"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8211,
    "title": "Should notify lock observer after applying logs"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8229,
    "title": "MvccTxn can't make use of near_seek"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8243,
    "title": "TiKV panic 'error: Corruption: L6 has overlapping ranges'"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8244,
    "title": "failpoints::test_register::test_merge not work"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8248,
    "title": "cases::test_conf_change::test_destroy_local_reader not work"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8254,
    "title": "cases::test_coprocessor::test_deadline_2 not work"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8255,
    "title": "cases::test_coprocessor::test_parse_request_failed_2 not work"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8260,
    "title": "Grafana dashboard cannot display the contents of perf context normally in v3.0.16"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8272,
    "title": "raftstore::test_stale_peer::test_stale_learner failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8278,
    "title": "cases::test_merge::test_node_merge_crash_when_snapshot not work"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8285,
    "title": "cases::test_replica_read::test_duplicate_read_index_ctx not work"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8315,
    "title": "Memory  usage gradually grows up and OOM when running random merge"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8329,
    "title": "Flaky test: raftstore::test_lease_read::test_node_batch_id_in_lease"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8373,
    "title": "pending_snapshot_regions may not cleaned as expected"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8423,
    "title": "cpu_cores_quota can return 0 which makes unified thread pool hang for long running query"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8433,
    "title": "Optimization in peer/apply fsm destroy process may lead to a panic in apply fsm's register"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8438,
    "title": "Last configuration can be corrupted when disk is full"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8449,
    "title": "Status port not binded on the first start when TLS is enabled"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8456,
    "title": "Request vote messages are lost during split"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8476,
    "title": "Too many dns queries when one node is down"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8478,
    "title": "TiKV nodes output FATAL log \"[FATAL] [lib.rs:499] [\"header name validated by httparse: InvalidHeaderName { _priv: () }\"]\""
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8481,
    "title": "copr: tikv panic when executes aggr func "
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8492,
    "title": "crossbeam-channel 0.4 has undefined behavior"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8506,
    "title": "cases::test_merge::test_node_mutiple_rollback_merge failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8537,
    "title": "cases::test_gc_worker::test_notify_observer_after_apply will panic"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8544,
    "title": "raftstore::test_replication_mode::test_switching_replication_mode failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8555,
    "title": "cop: panic by assertion failure in BatchTableScanExecutor"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8557,
    "title": "copr: table scan's assertion fail while enable cluster index by default in TiDB"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8574,
    "title": "09-01 Performance regression on wide table"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8576,
    "title": "Deadlock in the lock table"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8589,
    "title": "Async commit does not ensure linearizability"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 859,
    "title": "Add a detecting module to monit system time"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8610,
    "title": "Deadlock between the PD client thread and other threads calling PD sync requests."
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8641,
    "title": "Status port refused connection when tls enabled"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 866,
    "title": "receiving two overlapped snapshot at same time causes panic"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8661,
    "title": "UnsafeDestroyRange may cause panic for a concurrent generating snap when Titan enabled"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8666,
    "title": "server::raft_client::test_raft_client_reconnect failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8678,
    "title": "TiKV always starting a new election"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8692,
    "title": "PD client panics on Deadlock's get_leader_info due to nested `block_on`"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8721,
    "title": "compression ratio of L6 is even smaller than L5/L4"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8738,
    "title": "TiKV status down when accept http requests with TLS enabled"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8761,
    "title": " Mutex conflict of encryption makes pd-worker deal with heartbeat slow."
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8768,
    "title": "backup/restore via S3 using STS transiently fail with \"Couldn't find AWS credentials in default sources or k8s environment\""
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8781,
    "title": "Assert fails on epoch_state post_propose"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8783,
    "title": "maybe_create_peer_internal fails due to hashmap entry not found"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8834,
    "title": "Wrong compaction pending bytes metrics"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8846,
    "title": "cases::test_split_region::test_follower_slow_split failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8861,
    "title": "potential endless loop in compaction_filter"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8901,
    "title": "Invalid external storage path will cause TiKV panic during backup"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8916,
    "title": "Restore slow down due to rewriting file/key.dict after enabling encryption at rest"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8941,
    "title": "Index out of range for slice in TableScan"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8964,
    "title": "Insertion after deletion meets duplicate entry error in optimistic transactions"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8967,
    "title": "Rolling upgrade from 2.1 or older version to newer version may break region replicas consistency"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8970,
    "title": "TiKV server panic after restarted"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8986,
    "title": "copr: UTF8 functions should consider collation"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8987,
    "title": "reigon is unavailable: one tikv server does not accept new connections"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 8998,
    "title": "Panic log may be lost when using async logger"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9012,
    "title": "TiKV panics when the size of response exceeds 4GB"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9022,
    "title": "Learner may not be created after a long time when enabling hibernate region"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9024,
    "title": "PD worker is blocked by `get_region_approximate_size`"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9041,
    "title": "Peer reject to transfer leader because of pending conf change"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9044,
    "title": "compaction guard: deadlock when creating compaction partitioner"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9069,
    "title": "batch_get doesn't return all results when encountering a memory lock"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9070,
    "title": "committed_cb is always invoked which affects the correctness of async apply"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9099,
    "title": "TiKV panic because of ingest failed by importer."
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9113,
    "title": "More fine-grained control on callback for raftstore"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9115,
    "title": "encryption: handle non-atomic file operations"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9119,
    "title": "Coprocessor incorrectly handles NewRowFormat"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9126,
    "title": "TiKV panic during upgrading from v4.0.8 to nightly"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9144,
    "title": "TiKV OOM because BR read a large region in memory"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9151,
    "title": "server::kv_service::test_split_region failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9162,
    "title": "Slot collision on latch with and (&) operator"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9170,
    "title": "Load split cannot work when the request covers the region"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9171,
    "title": "TiKV server panic when running with TiCDC"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9188,
    "title": "The unified pool cannot schedule kv_scan, raw_scan and analyze requests well"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9208,
    "title": "tipocket list_append fails on follower read and async commit"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 921,
    "title": "Investigate the 'multiTransfer' CI failure."
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9216,
    "title": "TiKV's distributed GC may keep scanning and consuming CPU when there's very low write flow"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9217,
    "title": "TiKV is killed during profiling via TiDB-dashboard"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9223,
    "title": "slow-query information is not accuracy"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9234,
    "title": "security: explicit error message for CN verification error"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9249,
    "title": "security: not all logs being redact when log redaction is enabled"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9253,
    "title": "FAIL: /home/jenkins/agent/workspace/tikv_ghpr_integration_ddl_test/go/src/github.com/pingcap/tidb-test/_vendor/pkg/mod/github.com/pingcap/tidb@v0.0.0-20190520045437-2d70e4da27d7/cmd/ddltest/index_test.go:127: TestDDLSuite.TestIndex failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9327,
    "title": "Can not change region-split-check-diff through config file"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9351,
    "title": "Stale read when using read_index request"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9352,
    "title": "Failed to build TiKV with PROST=1"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9388,
    "title": "ready can be advanced by a wrong peer"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9391,
    "title": "TiKV's \"read flow\" statistics are inconsistent with \"store read rate\" statistics of PD"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9392,
    "title": "TiKV  panicked when system time back"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9407,
    "title": "tikv w/ 8GB mem OOM while lightning importing"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9429,
    "title": "Files did not purged after `DeleteFilesInRanges`"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9444,
    "title": "make doc is broken"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9463,
    "title": "Blocking PD leader update leads to gRPC keepalive watchdog fire and disrupts online service"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9466,
    "title": "Performance decline in TPCC bench due to the absence of GC"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9469,
    "title": "raftstore: the invalid read delegates in local reader may not be removed forever"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9488,
    "title": "TiKV crashes continuously if access AWS KMS failed at the first time"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9496,
    "title": "TiKV panic when Lightning send twice ingest request."
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9513,
    "title": "`validate_endpoints` does not check `leader` field of `GetMembersResponse`"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9517,
    "title": "Local reader delegate keeps stale ID"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9520,
    "title": "BR will lead to the emergence of big regions with version v4.0.9"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9523,
    "title": "Resolved TS is not updated with async commit transactions"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9526,
    "title": "incompatibility between gc and async commit"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9532,
    "title": "TiKV reported memory is wrong"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9538,
    "title": "heap pprof failed"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9541,
    "title": "TiKV cannot send KMS request"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9555,
    "title": "TiKV aborts when accesses hardware info"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9579,
    "title": "The uninitialized voter which comes from splitting may lead to two same-term raft leaders"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9590,
    "title": "raft client doesn't recognize tombstone store"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9613,
    "title": "load based split can't handle raw kv well"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9615,
    "title": "commit merge and post_apply can make tikv panic"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9627,
    "title": "Potential deadlock for GC in Compaction Filter"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 964,
    "title": "raft: use monotonic clock check for leader lease read "
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9654,
    "title": "txn record found but not expected"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9681,
    "title": "Add learner operator runs very slow when hibernate region is on"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9690,
    "title": "pd connection is reconnecting every 10 minutes"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9699,
    "title": "label format is inconsistent with PD"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9701,
    "title": "Corruption when upgrade from v4.0.9 to nightly"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9704,
    "title": "Inconsisitent with Mysql When a varchar column only have empty string"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9714,
    "title": "batch messages can be too large in new raft client implementation"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9728,
    "title": "Lease check should consider tick count"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9733,
    "title": "Random crashes with \"Uniform::sample_single called with low >= high\" after update to 4.0.11"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9748,
    "title": "The propose time may be smaller than the real one"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 975,
    "title": "server panics when restarting after stopped for a few hours"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9765,
    "title": "Built-in profiling can cause a segmentation fault"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9786,
    "title": "Raftstore GC snapshot calculates crc32 which is useless and causes unnecessary IO"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9796,
    "title": "Region written stats is abnormal"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9799,
    "title": "There is no limit to reconnect with PD"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9816,
    "title": "Tiup upgrade failed because of config-check failure"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9821,
    "title": "coprocessor IN expr not handle max_u64 properly"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9859,
    "title": "raftstore: leader may not send snapshot to the follower"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9886,
    "title": "tikv can't startup after the disk is full"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9893,
    "title": "There seems to be a problem with the statistics of the Raft propose/Apply wait duration metric"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9896,
    "title": "Store used size changes a lot after restarting"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9910,
    "title": "the rule for TiKV_GC_can_not_work alert should be enhanced to check with GC speed instead of GC task"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9913,
    "title": "tikv keep panicking at components/engine_rocks/src/engine_iterator.rs:39 "
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9920,
    "title": "Incorrect available and used disk size"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9929,
    "title": "raft_client: drop connect when receive raft message with mismatch store id"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9936,
    "title": "Too many compaction pending bytes when truncating a huge table in TiDB"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9957,
    "title": "Built-in CPU profiling causes a segmentation fault on macOS 11"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9961,
    "title": "TiCDC old value and incremental scan may hold RocksDB snapshot too long"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9965,
    "title": "gc keys can't handle tombstones effectively"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9970,
    "title": "index out of bounds: the len is 4 but the index is 4"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9971,
    "title": "Avoid unnoticeable data loss due to inappropriate handling of checksum"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9976,
    "title": "The unit of some panels are wrong in Grafana"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9980,
    "title": "raftstore panics during merge"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9981,
    "title": "CDC endpoint thread CPU usage 100% with insert 800ops"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9990,
    "title": "Provided config-template.toml is not valid"
  },
  {
    "bug_location": "Unknown",
    "severity": 3,
    "categories": [
      "Unknown"
    ],
    "root_cause": "N/A - Analysis failed",
    "issue_number": 9996,
    "title": "TiKV OOM due to CDC endpoint CPU 100% under 20k insert op/s"
  }
]